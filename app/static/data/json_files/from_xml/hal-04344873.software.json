{"application": "software-mentions", "version": "0.8.0", "date": "2024-04-15T07:18+0000", "md5": "8B9B3497015B831B7ABC2A336CE0E36D", "mentions": [{"type": "software", "software-type": "software", "software-name": {"rawForm": "KGEMs", "normalizedForm": "KGEMs", "offsetStart": 0, "offsetEnd": 5}, "context": "KGEMs were implemented in PyTorch.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0002912282943725586}, "created": {"value": false, "score": 0.000546574592590332}, "shared": {"value": false, "score": 5.364418029785156e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999769330024719}, "created": {"value": true, "score": 0.9999476075172424}, "shared": {"value": false, "score": 5.364418029785156e-07}}, "references": [{"label": "[3]", "normalizedForm": "[3]", "refKey": 3}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "GEval", "normalizedForm": "GEval", "offsetStart": 0, "offsetEnd": 5}, "context": "GEval provides gold standard datasets based on DBpedia and suggest clustering models, configurations and evaluation metrics 7 .", "mentionContextAttributes": {"used": {"value": false, "score": 4.178285598754883e-05}, "created": {"value": false, "score": 1.436471939086914e-05}, "shared": {"value": false, "score": 1.7881393432617188e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999929070472717}, "created": {"value": false, "score": 0.13933587074279785}, "shared": {"value": false, "score": 2.980232238769531e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "GEval", "normalizedForm": "GEval", "offsetStart": 0, "offsetEnd": 5}, "context": "GEval proposes 4 datasets based on DBpedia. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.00010865926742553711}, "created": {"value": false, "score": 0.0018884539604187012}, "shared": {"value": false, "score": 2.980232238769531e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999929070472717}, "created": {"value": false, "score": 0.13933587074279785}, "shared": {"value": false, "score": 2.980232238769531e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "KGEMs", "normalizedForm": "KGEMs", "offsetStart": 0, "offsetEnd": 5}, "context": "KGEMs have gained significant attention in recent years due to their ability to represent structured knowledge in a continuous vector space.", "mentionContextAttributes": {"used": {"value": false, "score": 0.00022149085998535156}, "created": {"value": false, "score": 0.00524294376373291}, "shared": {"value": false, "score": 4.172325134277344e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999769330024719}, "created": {"value": true, "score": 0.9999476075172424}, "shared": {"value": false, "score": 5.364418029785156e-07}}, "references": [{"label": "[3]", "normalizedForm": "[3]", "refKey": 3}]}, {"type": "software", "software-type": "software", "wikidataId": "Q3337017", "wikipediaExternalRef": 47655250, "lang": "en", "confidence": 0.2502, "software-name": {"rawForm": "MASCHInE", "normalizedForm": "MASCHInE", "wikidataId": "Q3337017", "wikipediaExternalRef": 47655250, "lang": "en", "confidence": 0.2502, "offsetStart": 0, "offsetEnd": 8}, "context": "MASCHInE is intended to produce more versatile KGEs that can be successfully used in various tasks. ", "mentionContextAttributes": {"used": {"value": false, "score": 5.78761100769043e-05}, "created": {"value": false, "score": 0.12779122591018677}, "shared": {"value": false, "score": 6.556510925292969e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9982430338859558}, "created": {"value": true, "score": 0.9999476075172424}, "shared": {"value": false, "score": 6.556510925292969e-07}}}, {"type": "software", "software-type": "software", "wikidataId": "Q3337017", "wikipediaExternalRef": 47655250, "lang": "en", "confidence": 0.2502, "software-name": {"rawForm": "MASCHInE", "normalizedForm": "MASCHInE", "wikidataId": "Q3337017", "wikipediaExternalRef": 47655250, "lang": "en", "confidence": 0.2502, "offsetStart": 0, "offsetEnd": 8}, "context": "MASCHInE first trains embeddings on protographs and then transfers these embeddings to learn the final KGEs. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.02681785821914673}, "created": {"value": false, "score": 1.8417835235595703e-05}, "shared": {"value": false, "score": 1.7881393432617188e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9982430338859558}, "created": {"value": true, "score": 0.9999476075172424}, "shared": {"value": false, "score": 6.556510925292969e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "TaRP", "normalizedForm": "TaRP", "offsetStart": 3, "offsetEnd": 10}, "context": "In TaRP [6], type and entity-level information are simultaneously considered and encoded as prior probabilities and likelihoods of relations, respectively. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.08028978109359741}, "created": {"value": false, "score": 3.159046173095703e-06}, "shared": {"value": false, "score": 1.7881393432617188e-07}}, "documentContextAttributes": {"used": {"value": false, "score": 0.08028978109359741}, "created": {"value": false, "score": 3.159046173095703e-06}, "shared": {"value": false, "score": 1.7881393432617188e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "TransE", "normalizedForm": "TransE", "offsetStart": 4, "offsetEnd": 10}, "context": "For TransE, DistMult, and ComplEx, uniform random negative sampling [3] was used to pair each train triple with one negative counterpart.", "mentionContextAttributes": {"used": {"value": true, "score": 0.999972403049469}, "created": {"value": false, "score": 2.682209014892578e-06}, "shared": {"value": false, "score": 1.7881393432617188e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999972403049469}, "created": {"value": false, "score": 0.09587162733078003}, "shared": {"value": false, "score": 4.172325134277344e-07}}, "references": [{"label": "[3]", "normalizedForm": "[3]", "refKey": 3}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "KGEMs", "normalizedForm": "KGEMs", "offsetStart": 9, "offsetEnd": 14}, "context": "However, KGEMs are usually trained for a specific task, which makes their embeddings task-dependent.", "mentionContextAttributes": {"used": {"value": false, "score": 0.00027573108673095703}, "created": {"value": false, "score": 7.516145706176758e-05}, "shared": {"value": false, "score": 2.980232238769531e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999769330024719}, "created": {"value": true, "score": 0.9999476075172424}, "shared": {"value": false, "score": 5.364418029785156e-07}}, "references": [{"label": "[3]", "normalizedForm": "[3]", "refKey": 3}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "TransE", "normalizedForm": "TransE", "offsetStart": 9, "offsetEnd": 15}, "context": "Under V, TransE, ComplEx, and ConvE provide substantially better results on CAMAP compared to CC. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.002780437469482422}, "created": {"value": false, "score": 7.814168930053711e-05}, "shared": {"value": false, "score": 1.7881393432617188e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999972403049469}, "created": {"value": false, "score": 0.09587162733078003}, "shared": {"value": false, "score": 4.172325134277344e-07}}, "references": [{"label": "[3]", "normalizedForm": "[3]", "refKey": 3}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "KGEMs", "normalizedForm": "KGEMs", "offsetStart": 10, "offsetEnd": 15}, "context": "Then, the KGEMs are trained w.r.t. the LP task under the three different settings V, P1, and P2.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9131770730018616}, "created": {"value": false, "score": 7.68899917602539e-06}, "shared": {"value": false, "score": 1.7881393432617188e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999769330024719}, "created": {"value": true, "score": 0.9999476075172424}, "shared": {"value": false, "score": 5.364418029785156e-07}}, "references": [{"label": "[3]", "normalizedForm": "[3]", "refKey": 3}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "TuckER", "normalizedForm": "TuckER", "offsetStart": 10, "offsetEnd": 16}, "context": "ConvE and TuckER are trained using 1-N scoring [8].", "mentionContextAttributes": {"used": {"value": false, "score": 0.11797934770584106}, "created": {"value": false, "score": 4.112720489501953e-06}, "shared": {"value": false, "score": 1.7881393432617188e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9237643480300903}, "created": {"value": false, "score": 0.0003648996353149414}, "shared": {"value": false, "score": 4.172325134277344e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "GEval", "normalizedForm": "GEval", "offsetStart": 11, "offsetEnd": 16}, "context": "We rely on GEval guidelines and perform the preliminary filtering step described as follows. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9983621835708618}, "created": {"value": false, "score": 0.13933587074279785}, "shared": {"value": false, "score": 1.7881393432617188e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999929070472717}, "created": {"value": false, "score": 0.13933587074279785}, "shared": {"value": false, "score": 2.980232238769531e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "TuckER", "normalizedForm": "TuckER", "offsetStart": 11, "offsetEnd": 17}, "context": "Except for TuckER on FB15k187, both settings P1 and P2 provide substantial improvements.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0005575418472290039}, "created": {"value": false, "score": 9.357929229736328e-06}, "shared": {"value": false, "score": 4.172325134277344e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9237643480300903}, "created": {"value": false, "score": 0.0003648996353149414}, "shared": {"value": false, "score": 4.172325134277344e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "KGEMs", "normalizedForm": "KGEMs", "offsetStart": 12, "offsetEnd": 17}, "context": "Most of the KGEMs subsequently proposed aim at addressing its limitations and improve the representation expressiveness of KGEs [21], such as DistMult [24], ComplEx [20], ConvE [8], and TuckER [1].", "mentionContextAttributes": {"used": {"value": false, "score": 0.00042444467544555664}, "created": {"value": false, "score": 0.0003648996353149414}, "shared": {"value": false, "score": 1.7881393432617188e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999769330024719}, "created": {"value": true, "score": 0.9999476075172424}, "shared": {"value": false, "score": 5.364418029785156e-07}}, "references": [{"label": "[3]", "normalizedForm": "[3]", "refKey": 3}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "DistMult", "normalizedForm": "DistMult", "offsetStart": 12, "offsetEnd": 20}, "context": "For TransE, DistMult, and ComplEx, uniform random negative sampling [3] was used to pair each train triple with one negative counterpart.", "mentionContextAttributes": {"used": {"value": true, "score": 0.999972403049469}, "created": {"value": false, "score": 2.682209014892578e-06}, "shared": {"value": false, "score": 1.7881393432617188e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999972403049469}, "created": {"value": false, "score": 0.0003648996353149414}, "shared": {"value": false, "score": 1.7881393432617188e-07}}, "references": [{"label": "[24]", "normalizedForm": "[24]", "refKey": 24}, {"label": "[20]", "normalizedForm": "[20]", "refKey": 20}, {"label": "[24]", "normalizedForm": "[24]", "refKey": 24}]}, {"type": "software", "software-type": "software", "wikidataId": "Q3337017", "wikipediaExternalRef": 47655250, "lang": "en", "confidence": 0.2502, "software-name": {"rawForm": "MASCHInE", "normalizedForm": "MASCHInE", "wikidataId": "Q3337017", "wikipediaExternalRef": 47655250, "lang": "en", "confidence": 0.2502, "offsetStart": 13, "offsetEnd": 21}, "context": "The proposed MASCHInE approach allows for generating pretrained embeddings based on a protograph, to be further fine-tuned on the initial KG. ", "mentionContextAttributes": {"used": {"value": false, "score": 5.632638931274414e-05}, "created": {"value": false, "score": 0.4619837999343872}, "shared": {"value": false, "score": 1.7881393432617188e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9982430338859558}, "created": {"value": true, "score": 0.9999476075172424}, "shared": {"value": false, "score": 6.556510925292969e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "GEval", "normalizedForm": "GEval", "offsetStart": 14, "offsetEnd": 19}, "context": "Following the GEval guidelines, we additionally use Adjusted Mutual Information Score (AMI), V-Measure (VM), Fowlkes-Mallow Score (FM), Homogeneity (H), and Completeness (C).", "mentionContextAttributes": {"used": {"value": true, "score": 0.997408390045166}, "created": {"value": false, "score": 4.708766937255859e-06}, "shared": {"value": false, "score": 1.7881393432617188e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999929070472717}, "created": {"value": false, "score": 0.13933587074279785}, "shared": {"value": false, "score": 2.980232238769531e-07}}}, {"type": "software", "software-type": "software", "wikidataId": "Q3337017", "wikipediaExternalRef": 47655250, "lang": "en", "confidence": 0.2502, "software-name": {"rawForm": "MASCHInE", "normalizedForm": "MASCHInE", "wikidataId": "Q3337017", "wikipediaExternalRef": 47655250, "lang": "en", "confidence": 0.2502, "offsetStart": 14, "offsetEnd": 22}, "context": "Conceptually, MASCHInE is made of four different modules that are depicted in Figure 2 and further detailed below.", "mentionContextAttributes": {"used": {"value": false, "score": 0.030757486820220947}, "created": {"value": false, "score": 0.0010893940925598145}, "shared": {"value": false, "score": 5.364418029785156e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9982430338859558}, "created": {"value": true, "score": 0.9999476075172424}, "shared": {"value": false, "score": 6.556510925292969e-07}}}, {"type": "software", "software-type": "software", "wikidataId": "Q3337017", "wikipediaExternalRef": 47655250, "lang": "en", "confidence": 0.4656, "software-name": {"rawForm": "MASCHInE", "normalizedForm": "MASCHInE", "wikidataId": "Q3337017", "wikipediaExternalRef": 47655250, "lang": "en", "confidence": 0.4656, "offsetStart": 15, "offsetEnd": 23}, "context": "In particular, MASCHInE helps produce more versatile KGEs that yield substantially better performance for entity clustering and node classification tasks. ", "mentionContextAttributes": {"used": {"value": false, "score": 5.882978439331055e-05}, "created": {"value": false, "score": 5.608797073364258e-05}, "shared": {"value": false, "score": 2.980232238769531e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9982430338859558}, "created": {"value": true, "score": 0.9999476075172424}, "shared": {"value": false, "score": 6.556510925292969e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "KGEM", "normalizedForm": "KGEM", "offsetStart": 24, "offsetEnd": 28}, "context": "For a given dataset and KGEM, comparisons are made between the three settings, and best results are in bold fonts. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.7368515133857727}, "created": {"value": false, "score": 6.616115570068359e-06}, "shared": {"value": false, "score": 4.172325134277344e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9982430338859558}, "created": {"value": false, "score": 0.0030922889709472656}, "shared": {"value": false, "score": 4.172325134277344e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "KGEMs", "normalizedForm": "KGEMs", "offsetStart": 25, "offsetEnd": 30}, "context": "In accordance with [13], KGEMs are trained during 400 epochs. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.013751387596130371}, "created": {"value": false, "score": 2.2709369659423828e-05}, "shared": {"value": false, "score": 2.980232238769531e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999769330024719}, "created": {"value": true, "score": 0.9999476075172424}, "shared": {"value": false, "score": 5.364418029785156e-07}}, "references": [{"label": "[3]", "normalizedForm": "[3]", "refKey": 3}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "TuckER", "normalizedForm": "TuckER", "offsetStart": 25, "offsetEnd": 31}, "context": "In particular, ConvE and TuckER tend to perform better w.r.t. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.0012018680572509766}, "created": {"value": false, "score": 9.953975677490234e-06}, "shared": {"value": false, "score": 2.980232238769531e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9237643480300903}, "created": {"value": false, "score": 0.0003648996353149414}, "shared": {"value": false, "score": 4.172325134277344e-07}}}, {"type": "software", "software-type": "software", "wikidataId": "Q47509047", "wikipediaExternalRef": 54022970, "lang": "en", "confidence": 0.7826, "software-name": {"rawForm": "PyTorch", "normalizedForm": "PyTorch", "wikidataId": "Q47509047", "wikipediaExternalRef": 54022970, "lang": "en", "confidence": 0.7826, "offsetStart": 26, "offsetEnd": 33}, "context": "KGEMs were implemented in PyTorch.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0002912282943725586}, "created": {"value": false, "score": 0.000546574592590332}, "shared": {"value": false, "score": 5.364418029785156e-07}}, "documentContextAttributes": {"used": {"value": false, "score": 0.0002912282943725586}, "created": {"value": false, "score": 0.000546574592590332}, "shared": {"value": false, "score": 5.364418029785156e-07}}}, {"type": "software", "software-type": "software", "wikidataId": "Q3337017", "wikipediaExternalRef": 47655250, "lang": "en", "confidence": 0.2502, "software-name": {"rawForm": "MASCHInE", "normalizedForm": "MASCHInE", "wikidataId": "Q3337017", "wikipediaExternalRef": 47655250, "lang": "en", "confidence": 0.2502, "offsetStart": 26, "offsetEnd": 34}, "context": "In this paper, we present MASCHInE -a novel model-agnostic and protograph-assisted approach for learning KGEs. ", "mentionContextAttributes": {"used": {"value": false, "score": 4.45246696472168e-05}, "created": {"value": true, "score": 0.9999344944953918}, "shared": {"value": false, "score": 2.980232238769531e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9982430338859558}, "created": {"value": true, "score": 0.9999476075172424}, "shared": {"value": false, "score": 6.556510925292969e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "KGEMs", "normalizedForm": "KGEMs", "offsetStart": 27, "offsetEnd": 32}, "context": "Embeddings learnt by these KGEMs demonstrated potential applicability in tasks such as LP [21].", "mentionContextAttributes": {"used": {"value": true, "score": 0.9996140599250793}, "created": {"value": false, "score": 3.874301910400391e-06}, "shared": {"value": false, "score": 1.7881393432617188e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999769330024719}, "created": {"value": true, "score": 0.9999476075172424}, "shared": {"value": false, "score": 5.364418029785156e-07}}, "references": [{"label": "[3]", "normalizedForm": "[3]", "refKey": 3}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "TuckER", "normalizedForm": "TuckER", "offsetStart": 27, "offsetEnd": 33}, "context": "With the sole exception of TuckER whose performance may be due to non-optimal hyperparameter tuning, strong improvements are achieved w.r.t. the EC task on all datasets.", "mentionContextAttributes": {"used": {"value": false, "score": 0.007189333438873291}, "created": {"value": false, "score": 1.2814998626708984e-05}, "shared": {"value": false, "score": 1.7881393432617188e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9237643480300903}, "created": {"value": false, "score": 0.0003648996353149414}, "shared": {"value": false, "score": 4.172325134277344e-07}}}, {"type": "software", "software-type": "software", "wikidataId": "Q3337017", "wikipediaExternalRef": 47655250, "lang": "en", "confidence": 0.4656, "software-name": {"rawForm": "MASCHinE", "normalizedForm": "MASCHinE", "wikidataId": "Q3337017", "wikipediaExternalRef": 47655250, "lang": "en", "confidence": 0.4656, "offsetStart": 27, "offsetEnd": 35}, "context": "For link prediction, using MASCHinE substantially increases the number of semantically valid predictions with equivalent rank-based performance.", "mentionContextAttributes": {"used": {"value": false, "score": 0.006104707717895508}, "created": {"value": false, "score": 5.781650543212891e-06}, "shared": {"value": false, "score": 1.7881393432617188e-07}}, "documentContextAttributes": {"used": {"value": false, "score": 0.006104707717895508}, "created": {"value": false, "score": 5.781650543212891e-06}, "shared": {"value": false, "score": 1.7881393432617188e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "KGEMs", "normalizedForm": "KGEMs", "offsetStart": 28, "offsetEnd": 33}, "context": "Comparing our approach with KGEMs that take advantage of additional information in the KG -e.g.", "mentionContextAttributes": {"used": {"value": false, "score": 9.304285049438477e-05}, "created": {"value": false, "score": 0.00023746490478515625}, "shared": {"value": false, "score": 2.980232238769531e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999769330024719}, "created": {"value": true, "score": 0.9999476075172424}, "shared": {"value": false, "score": 5.364418029785156e-07}}, "references": [{"label": "[3]", "normalizedForm": "[3]", "refKey": 3}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "KGEM", "normalizedForm": "KGEM", "offsetStart": 29, "offsetEnd": 33}, "context": "In our experiments, the same KGEM is used for both pre-training over the protograph and fine-tuning embeddings on the KG.", "mentionContextAttributes": {"used": {"value": false, "score": 0.00727611780166626}, "created": {"value": false, "score": 0.003061652183532715}, "shared": {"value": false, "score": 1.7881393432617188e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9982430338859558}, "created": {"value": false, "score": 0.0030922889709472656}, "shared": {"value": false, "score": 4.172325134277344e-07}}}, {"type": "software", "software-type": "software", "wikidataId": "Q3337017", "wikipediaExternalRef": 47655250, "lang": "en", "confidence": 0.2502, "software-name": {"rawForm": "MASCHInE", "normalizedForm": "MASCHInE", "wikidataId": "Q3337017", "wikipediaExternalRef": 47655250, "lang": "en", "confidence": 0.2502, "offsetStart": 31, "offsetEnd": 39}, "context": "In future work, we will extend MASCHInE by adding new protograph design principles. ", "mentionContextAttributes": {"used": {"value": false, "score": 9.709596633911133e-05}, "created": {"value": true, "score": 0.9994543790817261}, "shared": {"value": false, "score": 4.172325134277344e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9982430338859558}, "created": {"value": true, "score": 0.9999476075172424}, "shared": {"value": false, "score": 6.556510925292969e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "KGEMs", "normalizedForm": "KGEMs", "offsetStart": 32, "offsetEnd": 37}, "context": "This widespread assumption that KGEMs create a semantic representation of the underlying entities and relations (i.e., project similar entities closer than dissimilar ones) has been challenged recently [14].", "mentionContextAttributes": {"used": {"value": false, "score": 9.804964065551758e-05}, "created": {"value": false, "score": 0.0003775954246520996}, "shared": {"value": false, "score": 4.172325134277344e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999769330024719}, "created": {"value": true, "score": 0.9999476075172424}, "shared": {"value": false, "score": 5.364418029785156e-07}}, "references": [{"label": "[3]", "normalizedForm": "[3]", "refKey": 3}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "TransE", "normalizedForm": "TransE", "offsetStart": 32, "offsetEnd": 38}, "context": "The seminal translational model TransE [3] represents entities and relations as lowdimensional vectors and defines the relationship between a head, a relation, and a tail using a translation operation in the embedding space.", "mentionContextAttributes": {"used": {"value": false, "score": 5.334615707397461e-05}, "created": {"value": false, "score": 0.09587162733078003}, "shared": {"value": false, "score": 4.172325134277344e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999972403049469}, "created": {"value": false, "score": 0.09587162733078003}, "shared": {"value": false, "score": 4.172325134277344e-07}}, "references": [{"label": "[3]", "normalizedForm": "[3]", "refKey": 3, "offsetStart": 6399, "offsetEnd": 6402}]}, {"type": "software", "software-type": "software", "wikidataId": "Q3337017", "wikipediaExternalRef": 47655250, "lang": "en", "confidence": 0.4656, "software-name": {"rawForm": "MASCHInE", "normalizedForm": "MASCHInE", "wikidataId": "Q3337017", "wikipediaExternalRef": 47655250, "lang": "en", "confidence": 0.4656, "offsetStart": 33, "offsetEnd": 41}, "context": "In our experiments, we apply the MASCHInE approach presented in Section 3. We experiment with five popular KGEMs. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9929482340812683}, "created": {"value": false, "score": 0.29045581817626953}, "shared": {"value": false, "score": 1.7881393432617188e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9982430338859558}, "created": {"value": true, "score": 0.9999476075172424}, "shared": {"value": false, "score": 6.556510925292969e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "KGEMs", "normalizedForm": "KGEMs", "offsetStart": 34, "offsetEnd": 39}, "context": "In addition, we observe that some KGEMs seem to benefit more from the MASCHInE approach. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.011533200740814209}, "created": {"value": false, "score": 0.0001385211944580078}, "shared": {"value": false, "score": 4.172325134277344e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999769330024719}, "created": {"value": true, "score": 0.9999476075172424}, "shared": {"value": false, "score": 5.364418029785156e-07}}, "references": [{"label": "[3]", "normalizedForm": "[3]", "refKey": 3}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "KGEMs", "normalizedForm": "KGEMs", "offsetStart": 34, "offsetEnd": 39}, "context": "Knowledge graph embedding models (KGEMs) have gained considerable traction in recent years.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0001175999641418457}, "created": {"value": false, "score": 0.005939185619354248}, "shared": {"value": false, "score": 5.364418029785156e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999769330024719}, "created": {"value": true, "score": 0.9999476075172424}, "shared": {"value": false, "score": 5.364418029785156e-07}}, "references": [{"label": "[3]", "normalizedForm": "[3]", "refKey": 3}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "KGEMs", "normalizedForm": "KGEMs", "offsetStart": 38, "offsetEnd": 43}, "context": "is also concerned with the ability of KGEMs to embed entities from the same class to the same region of the embedding space. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.0001785755157470703}, "created": {"value": false, "score": 0.0011080503463745117}, "shared": {"value": false, "score": 1.7881393432617188e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999769330024719}, "created": {"value": true, "score": 0.9999476075172424}, "shared": {"value": false, "score": 5.364418029785156e-07}}, "references": [{"label": "[3]", "normalizedForm": "[3]", "refKey": 3}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "KGEM", "normalizedForm": "KGEM", "offsetStart": 39, "offsetEnd": 43}, "context": "This can be achieved regardless of the KGEM at hand and is therefore model-agnostic 4 .", "mentionContextAttributes": {"used": {"value": false, "score": 0.0003312826156616211}, "created": {"value": false, "score": 6.908178329467773e-05}, "shared": {"value": false, "score": 1.7881393432617188e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9982430338859558}, "created": {"value": false, "score": 0.0030922889709472656}, "shared": {"value": false, "score": 4.172325134277344e-07}}}, {"type": "software", "software-type": "software", "wikidataId": "Q3337017", "wikipediaExternalRef": 47655250, "lang": "en", "confidence": 0.2502, "software-name": {"rawForm": "MASCHInE", "normalizedForm": "MASCHInE", "wikidataId": "Q3337017", "wikipediaExternalRef": 47655250, "lang": "en", "confidence": 0.2502, "offsetStart": 43, "offsetEnd": 51}, "context": "In this work, we propose an approach named MASCHInE that leverages RDF/S information in a model-agnostic way, making it applicable to arbitrary KGEMs. ", "mentionContextAttributes": {"used": {"value": false, "score": 2.5928020477294922e-05}, "created": {"value": true, "score": 0.9999476075172424}, "shared": {"value": false, "score": 2.980232238769531e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9982430338859558}, "created": {"value": true, "score": 0.9999476075172424}, "shared": {"value": false, "score": 6.556510925292969e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "KGEMs", "normalizedForm": "KGEMs", "offsetStart": 44, "offsetEnd": 49}, "context": "In parallel, the widespread assumption that KGEMs actually create a semantic representation of the underlying entities and relations (e.g., project similar entities closer than dissimilar ones) has been challenged. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.00022125244140625}, "created": {"value": false, "score": 0.0007494688034057617}, "shared": {"value": false, "score": 5.364418029785156e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999769330024719}, "created": {"value": true, "score": 0.9999476075172424}, "shared": {"value": false, "score": 5.364418029785156e-07}}, "references": [{"label": "[3]", "normalizedForm": "[3]", "refKey": 3}]}, {"type": "software", "software-type": "software", "wikidataId": "Q3337017", "wikipediaExternalRef": 47655250, "lang": "en", "confidence": 0.2502, "software-name": {"rawForm": "MASCHInE", "normalizedForm": "MASCHInE", "wikidataId": "Q3337017", "wikipediaExternalRef": 47655250, "lang": "en", "confidence": 0.2502, "offsetStart": 44, "offsetEnd": 52}, "context": "We also demonstrated the advantage of using MASCHInE for link prediction: the use of protographs in the training procedure leads KGEMs to make more semantically valid predictions, which clearly highlights that geometric distance in the embedding space can be influenced by the semantics of the underlying KG.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9370209574699402}, "created": {"value": false, "score": 0.0011321306228637695}, "shared": {"value": false, "score": 2.980232238769531e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9982430338859558}, "created": {"value": true, "score": 0.9999476075172424}, "shared": {"value": false, "score": 6.556510925292969e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "TuckER", "normalizedForm": "TuckER", "offsetStart": 46, "offsetEnd": 52}, "context": "It seems that the entity embeddings learnt by TuckER are not able to cluster entities based on their classes, which diminishes the relevancy of using this model for drawing comparisons. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.00507122278213501}, "created": {"value": false, "score": 0.00021386146545410156}, "shared": {"value": false, "score": 2.980232238769531e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9237643480300903}, "created": {"value": false, "score": 0.0003648996353149414}, "shared": {"value": false, "score": 4.172325134277344e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "GEval", "normalizedForm": "GEval", "offsetStart": 49, "offsetEnd": 54}, "context": "Embeddings of entities that do not appear in the GEval benchmark datasets are filtered out. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.8121750950813293}, "created": {"value": false, "score": 5.65648078918457e-05}, "shared": {"value": false, "score": 1.7881393432617188e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999929070472717}, "created": {"value": false, "score": 0.13933587074279785}, "shared": {"value": false, "score": 2.980232238769531e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "KGEMs", "normalizedForm": "KGEMs", "offsetStart": 49, "offsetEnd": 54}, "context": "In this experiment, the embeddings learnt by the KGEMs are most useful for performing EC with five ground-truth clusters than with two, which might seem counterintuitive. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.025674283504486084}, "created": {"value": false, "score": 1.5437602996826172e-05}, "shared": {"value": false, "score": 1.7881393432617188e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999769330024719}, "created": {"value": true, "score": 0.9999476075172424}, "shared": {"value": false, "score": 5.364418029785156e-07}}, "references": [{"label": "[3]", "normalizedForm": "[3]", "refKey": 3}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "KGEM", "normalizedForm": "KGEM", "offsetStart": 51, "offsetEnd": 55}, "context": "A few works incorporate such schemas when training KGEM, whether it is in the loss function [7,13], in the negative sampling procedure [15,16], or in the model representations [6,23].", "mentionContextAttributes": {"used": {"value": false, "score": 5.263090133666992e-05}, "created": {"value": false, "score": 2.4259090423583984e-05}, "shared": {"value": false, "score": 1.7881393432617188e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9982430338859558}, "created": {"value": false, "score": 0.0030922889709472656}, "shared": {"value": false, "score": 4.172325134277344e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "KGEM", "normalizedForm": "KGEM", "offsetStart": 51, "offsetEnd": 55}, "context": "All of those methods are usually bound to just one KGEM and therefore not universally applicable.", "mentionContextAttributes": {"used": {"value": false, "score": 0.00015401840209960938}, "created": {"value": false, "score": 3.165006637573242e-05}, "shared": {"value": false, "score": 2.980232238769531e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9982430338859558}, "created": {"value": false, "score": 0.0030922889709472656}, "shared": {"value": false, "score": 4.172325134277344e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "GEval", "normalizedForm": "GEval", "offsetStart": 52, "offsetEnd": 57}, "context": "In this follow-up entity clustering experiment, the GEval evaluation framework [17] is used. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.30064815282821655}, "created": {"value": false, "score": 0.00041216611862182617}, "shared": {"value": false, "score": 1.7881393432617188e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999929070472717}, "created": {"value": false, "score": 0.13933587074279785}, "shared": {"value": false, "score": 2.980232238769531e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "GEval", "normalizedForm": "GEval", "offsetStart": 52, "offsetEnd": 57}, "context": "Clustering with all the algorithms suggested in the GEval framework is performed on the remaining entities and evaluated w.r.t. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999929070472717}, "created": {"value": false, "score": 5.608797073364258e-05}, "shared": {"value": false, "score": 1.7881393432617188e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999929070472717}, "created": {"value": false, "score": 0.13933587074279785}, "shared": {"value": false, "score": 2.980232238769531e-07}}}, {"type": "software", "software-type": "software", "wikidataId": "Q3337017", "wikipediaExternalRef": 47655250, "lang": "en", "confidence": 0.2502, "software-name": {"rawForm": "MASCHInE", "normalizedForm": "MASCHInE", "wikidataId": "Q3337017", "wikipediaExternalRef": 47655250, "lang": "en", "confidence": 0.2502, "offsetStart": 65, "offsetEnd": 73}, "context": "In Section 4, the potential usefulness of embeddings learnt with MASCHInE is discussed, and experiments w.r.t. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.0009797215461730957}, "created": {"value": false, "score": 0.00204312801361084}, "shared": {"value": false, "score": 1.7881393432617188e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9982430338859558}, "created": {"value": true, "score": 0.9999476075172424}, "shared": {"value": false, "score": 6.556510925292969e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "KGEM", "normalizedForm": "KGEM", "offsetStart": 66, "offsetEnd": 70}, "context": "Generating a protograph using schema-based information to improve KGEM performance with respect to a given task is an under-explored avenue.", "mentionContextAttributes": {"used": {"value": false, "score": 5.78761100769043e-05}, "created": {"value": false, "score": 0.0030922889709472656}, "shared": {"value": false, "score": 1.7881393432617188e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9982430338859558}, "created": {"value": false, "score": 0.0030922889709472656}, "shared": {"value": false, "score": 4.172325134277344e-07}}}, {"type": "software", "software-type": "software", "wikidataId": "Q3337017", "wikipediaExternalRef": 47655250, "lang": "en", "confidence": 0.2502, "software-name": {"rawForm": "MASCHInE", "normalizedForm": "MASCHInE", "wikidataId": "Q3337017", "wikipediaExternalRef": 47655250, "lang": "en", "confidence": 0.2502, "offsetStart": 67, "offsetEnd": 75}, "context": "Next, we provide some perspective on the overall proposed approach MASCHInE.", "mentionContextAttributes": {"used": {"value": false, "score": 0.00010341405868530273}, "created": {"value": true, "score": 0.9997761249542236}, "shared": {"value": false, "score": 1.7881393432617188e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9982430338859558}, "created": {"value": true, "score": 0.9999476075172424}, "shared": {"value": false, "score": 6.556510925292969e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "KGEMs", "normalizedForm": "KGEMs", "offsetStart": 69, "offsetEnd": 74}, "context": "This means that training with a protograph-assisted approach led the KGEMs to generate higher-quality embeddings in terms of class separability, especially for entities in CC and CCB that are harder to distinguish.", "mentionContextAttributes": {"used": {"value": false, "score": 0.16786164045333862}, "created": {"value": false, "score": 0.0006293654441833496}, "shared": {"value": false, "score": 1.7881393432617188e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999769330024719}, "created": {"value": true, "score": 0.9999476075172424}, "shared": {"value": false, "score": 5.364418029785156e-07}}, "references": [{"label": "[3]", "normalizedForm": "[3]", "refKey": 3}]}, {"type": "software", "software-type": "software", "wikidataId": "Q3337017", "wikipediaExternalRef": 47655250, "lang": "en", "confidence": 0.4656, "software-name": {"rawForm": "MASCHInE", "normalizedForm": "MASCHInE", "wikidataId": "Q3337017", "wikipediaExternalRef": 47655250, "lang": "en", "confidence": 0.4656, "offsetStart": 69, "offsetEnd": 77}, "context": "As a result, our schema-based, protograph-assisted learning approach MASCHInE proves effective for EC, regardless of the protograph design principle.", "mentionContextAttributes": {"used": {"value": false, "score": 3.403425216674805e-05}, "created": {"value": true, "score": 0.9985624551773071}, "shared": {"value": false, "score": 4.172325134277344e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9982430338859558}, "created": {"value": true, "score": 0.9999476075172424}, "shared": {"value": false, "score": 6.556510925292969e-07}}}, {"type": "software", "software-type": "software", "wikidataId": "Q3337017", "wikipediaExternalRef": 47655250, "lang": "en", "confidence": 0.2502, "software-name": {"rawForm": "MASCHInE", "normalizedForm": "MASCHInE", "wikidataId": "Q3337017", "wikipediaExternalRef": 47655250, "lang": "en", "confidence": 0.2502, "offsetStart": 70, "offsetEnd": 78}, "context": "In addition, we observe that some KGEMs seem to benefit more from the MASCHInE approach. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.011533200740814209}, "created": {"value": false, "score": 0.0001385211944580078}, "shared": {"value": false, "score": 4.172325134277344e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9982430338859558}, "created": {"value": true, "score": 0.9999476075172424}, "shared": {"value": false, "score": 6.556510925292969e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "KGEMs", "normalizedForm": "KGEMs", "offsetStart": 73, "offsetEnd": 78}, "context": "While generating dense and numerical vectors for entities and relations, KGEMs are expected to produce KGEs that retain the underlying semantics of the KG [21].", "mentionContextAttributes": {"used": {"value": false, "score": 0.00015306472778320312}, "created": {"value": false, "score": 2.0325183868408203e-05}, "shared": {"value": false, "score": 1.7881393432617188e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999769330024719}, "created": {"value": true, "score": 0.9999476075172424}, "shared": {"value": false, "score": 5.364418029785156e-07}}, "references": [{"label": "[3]", "normalizedForm": "[3]", "refKey": 3}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "KGEMs", "normalizedForm": "KGEMs", "offsetStart": 76, "offsetEnd": 81}, "context": "Table 2: Link prediction results on YAGO14k, FB15k187, and DBpedia77k using KGEMs trained without protograph (V), with P1, and P2. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999769330024719}, "created": {"value": false, "score": 1.9669532775878906e-06}, "shared": {"value": false, "score": 1.7881393432617188e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999769330024719}, "created": {"value": true, "score": 0.9999476075172424}, "shared": {"value": false, "score": 5.364418029785156e-07}}, "references": [{"label": "[3]", "normalizedForm": "[3]", "refKey": 3}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "KGEMs", "normalizedForm": "KGEMs", "offsetStart": 84, "offsetEnd": 89}, "context": "In most cases, these problems are addressed using Knowledge graph embedding models (KGEMs), which generate vector representations for entities and relations of a KG, a.k.a.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0003712773323059082}, "created": {"value": false, "score": 0.00017595291137695312}, "shared": {"value": false, "score": 2.980232238769531e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999769330024719}, "created": {"value": true, "score": 0.9999476075172424}, "shared": {"value": false, "score": 5.364418029785156e-07}}, "references": [{"label": "[3]", "normalizedForm": "[3]", "refKey": 3}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "KGEM", "normalizedForm": "KGEM", "offsetStart": 90, "offsetEnd": 94}, "context": "For each task, we compare the vanilla setting (V), i.e., directly training the respective KGEM on the KG, with the MASCHInE approach based on P1 (resp. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9982430338859558}, "created": {"value": false, "score": 4.708766937255859e-06}, "shared": {"value": false, "score": 1.7881393432617188e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9982430338859558}, "created": {"value": false, "score": 0.0030922889709472656}, "shared": {"value": false, "score": 4.172325134277344e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "KGEMs", "normalizedForm": "KGEMs", "offsetStart": 91, "offsetEnd": 96}, "context": "In the following, protographs, KG entities and relations are encoded using five mainstream KGEMs: TransE [3], DistMult [24], ComplEx [20], ConvE [8], and TuckER [1]. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9237643480300903}, "created": {"value": false, "score": 2.205371856689453e-06}, "shared": {"value": false, "score": 1.7881393432617188e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999769330024719}, "created": {"value": true, "score": 0.9999476075172424}, "shared": {"value": false, "score": 5.364418029785156e-07}}, "references": [{"label": "[3]", "normalizedForm": "[3]", "refKey": 3, "offsetStart": 13967, "offsetEnd": 13970}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "KGEM", "normalizedForm": "KGEM", "offsetStart": 92, "offsetEnd": 96}, "context": "This may lead to relatively small-sized protographs from which there is little data for the KGEM to learn from.", "mentionContextAttributes": {"used": {"value": false, "score": 7.396936416625977e-05}, "created": {"value": false, "score": 5.251169204711914e-05}, "shared": {"value": false, "score": 2.980232238769531e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9982430338859558}, "created": {"value": false, "score": 0.0030922889709472656}, "shared": {"value": false, "score": 4.172325134277344e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "TuckER", "normalizedForm": "TuckER", "offsetStart": 98, "offsetEnd": 104}, "context": "It is further remarkable that on this problem, compared to link prediction and entity clustering, TuckER can also benefit from the protographs.", "mentionContextAttributes": {"used": {"value": false, "score": 4.380941390991211e-05}, "created": {"value": false, "score": 7.766485214233398e-05}, "shared": {"value": false, "score": 1.7881393432617188e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9237643480300903}, "created": {"value": false, "score": 0.0003648996353149414}, "shared": {"value": false, "score": 4.172325134277344e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "TransE", "normalizedForm": "TransE", "offsetStart": 98, "offsetEnd": 107}, "context": "In the following, protographs, KG entities and relations are encoded using five mainstream KGEMs: TransE [3], DistMult [24], ComplEx [20], ConvE [8], and TuckER [1]. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9237643480300903}, "created": {"value": false, "score": 2.205371856689453e-06}, "shared": {"value": false, "score": 1.7881393432617188e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999972403049469}, "created": {"value": false, "score": 0.09587162733078003}, "shared": {"value": false, "score": 4.172325134277344e-07}}, "references": [{"label": "[3]", "normalizedForm": "[3]", "refKey": 3}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "KGEMs", "normalizedForm": "KGEMs", "offsetStart": 106, "offsetEnd": 111}, "context": "Injecting schema-based information into the training phase could help enhancing the semantic awareness of KGEMs [13].", "mentionContextAttributes": {"used": {"value": false, "score": 0.014526963233947754}, "created": {"value": false, "score": 6.622076034545898e-05}, "shared": {"value": false, "score": 1.7881393432617188e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999769330024719}, "created": {"value": true, "score": 0.9999476075172424}, "shared": {"value": false, "score": 5.364418029785156e-07}}, "references": [{"label": "[13]", "normalizedForm": "[13]", "refKey": 13, "offsetStart": 2766, "offsetEnd": 2770}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "KGEMs", "normalizedForm": "KGEMs", "offsetStart": 107, "offsetEnd": 112}, "context": "In our experiments, we apply the MASCHInE approach presented in Section 3. We experiment with five popular KGEMs. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9929482340812683}, "created": {"value": false, "score": 0.2904561161994934}, "shared": {"value": false, "score": 1.7881393432617188e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999769330024719}, "created": {"value": true, "score": 0.9999476075172424}, "shared": {"value": false, "score": 5.364418029785156e-07}}, "references": [{"label": "[3]", "normalizedForm": "[3]", "refKey": 3}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "DistMult", "normalizedForm": "DistMult", "offsetStart": 110, "offsetEnd": 122}, "context": "In the following, protographs, KG entities and relations are encoded using five mainstream KGEMs: TransE [3], DistMult [24], ComplEx [20], ConvE [8], and TuckER [1]. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9237643480300903}, "created": {"value": false, "score": 2.205371856689453e-06}, "shared": {"value": false, "score": 1.7881393432617188e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999972403049469}, "created": {"value": false, "score": 0.0003648996353149414}, "shared": {"value": false, "score": 1.7881393432617188e-07}}, "references": [{"label": "[24]", "normalizedForm": "[24]", "refKey": 24}, {"label": "[20]", "normalizedForm": "[20]", "refKey": 20}, {"label": "[24]", "normalizedForm": "[24]", "refKey": 24}]}, {"type": "software", "software-type": "software", "wikidataId": "Q3337017", "wikipediaExternalRef": 47655250, "lang": "en", "confidence": 0.4656, "software-name": {"rawForm": "MASCHInE", "normalizedForm": "MASCHInE", "wikidataId": "Q3337017", "wikipediaExternalRef": 47655250, "lang": "en", "confidence": 0.4656, "offsetStart": 115, "offsetEnd": 123}, "context": "For each task, we compare the vanilla setting (V), i.e., directly training the respective KGEM on the KG, with the MASCHInE approach based on P1 (resp. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9982430338859558}, "created": {"value": false, "score": 4.708766937255859e-06}, "shared": {"value": false, "score": 1.7881393432617188e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9982430338859558}, "created": {"value": true, "score": 0.9999476075172424}, "shared": {"value": false, "score": 6.556510925292969e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "KGEMs", "normalizedForm": "KGEMs", "offsetStart": 124, "offsetEnd": 129}, "context": "Entity embeddings are retrieved at the best epoch on the validation sets of YAGO14k, FB15k187, and DB-pedia77k, for all the KGEMs and under the three settings V, P1, and P2.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9827205538749695}, "created": {"value": false, "score": 1.9669532775878906e-06}, "shared": {"value": false, "score": 1.7881393432617188e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999769330024719}, "created": {"value": true, "score": 0.9999476075172424}, "shared": {"value": false, "score": 5.364418029785156e-07}}, "references": [{"label": "[3]", "normalizedForm": "[3]", "refKey": 3}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "KGEMs", "normalizedForm": "KGEMs", "offsetStart": 129, "offsetEnd": 134}, "context": "We also demonstrated the advantage of using MASCHInE for link prediction: the use of protographs in the training procedure leads KGEMs to make more semantically valid predictions, which clearly highlights that geometric distance in the embedding space can be influenced by the semantics of the underlying KG.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9370209574699402}, "created": {"value": false, "score": 0.0011321306228637695}, "shared": {"value": false, "score": 2.980232238769531e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999769330024719}, "created": {"value": true, "score": 0.9999476075172424}, "shared": {"value": false, "score": 5.364418029785156e-07}}, "references": [{"label": "[3]", "normalizedForm": "[3]", "refKey": 3}]}, {"type": "software", "software-type": "software", "wikidataId": "Q1026367", "wikipediaExternalRef": 33490859, "lang": "en", "confidence": 0.909, "software-name": {"rawForm": "scikit-learn", "normalizedForm": "scikit-learn", "wikidataId": "Q1026367", "wikipediaExternalRef": 33490859, "lang": "en", "confidence": 0.909, "offsetStart": 129, "offsetEnd": 143}, "context": "Then, following the evaluation protocol in Jain et al. [14], the k-means clustering algorithm is run using default parameters of scikit-learn 6 .", "mentionContextAttributes": {"used": {"value": true, "score": 0.9273741245269775}, "created": {"value": false, "score": 3.2782554626464844e-06}, "shared": {"value": false, "score": 1.7881393432617188e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9273741245269775}, "created": {"value": false, "score": 3.2782554626464844e-06}, "shared": {"value": false, "score": 1.7881393432617188e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "MAS-CHInE", "normalizedForm": "MAS-CHInE", "offsetStart": 137, "offsetEnd": 146}, "context": "Related work is presented in Section 2. In Section 3, we detail our approach for building protographs and how they fit into our approach MAS-CHInE. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.0011271238327026367}, "created": {"value": true, "score": 0.9782431721687317}, "shared": {"value": false, "score": 2.980232238769531e-07}}, "documentContextAttributes": {"used": {"value": false, "score": 0.0011271238327026367}, "created": {"value": true, "score": 0.9782431721687317}, "shared": {"value": false, "score": 2.980232238769531e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "DistMult", "normalizedForm": "DistMult", "offsetStart": 142, "offsetEnd": 150}, "context": "Most of the KGEMs subsequently proposed aim at addressing its limitations and improve the representation expressiveness of KGEs [21], such as DistMult [24], ComplEx [20], ConvE [8], and TuckER [1].", "mentionContextAttributes": {"used": {"value": false, "score": 0.00042444467544555664}, "created": {"value": false, "score": 0.0003648996353149414}, "shared": {"value": false, "score": 1.7881393432617188e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999972403049469}, "created": {"value": false, "score": 0.0003648996353149414}, "shared": {"value": false, "score": 1.7881393432617188e-07}}, "references": [{"label": "[24]", "normalizedForm": "[24]", "refKey": 24, "offsetStart": 6736, "offsetEnd": 6740}, {"label": "[20]", "normalizedForm": "[20]", "refKey": 20, "offsetStart": 6750, "offsetEnd": 6754}, {"label": "[24]", "normalizedForm": "[24]", "refKey": 24, "offsetStart": 6736, "offsetEnd": 6740}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "KGEMs", "normalizedForm": "KGEMs", "offsetStart": 144, "offsetEnd": 149}, "context": "In this work, we propose an approach named MASCHInE that leverages RDF/S information in a model-agnostic way, making it applicable to arbitrary KGEMs.", "mentionContextAttributes": {"used": {"value": false, "score": 2.5928020477294922e-05}, "created": {"value": true, "score": 0.9999476075172424}, "shared": {"value": false, "score": 2.980232238769531e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999769330024719}, "created": {"value": true, "score": 0.9999476075172424}, "shared": {"value": false, "score": 5.364418029785156e-07}}, "references": [{"label": "[3]", "normalizedForm": "[3]", "refKey": 3}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "KGEM", "normalizedForm": "KGEM", "offsetStart": 151, "offsetEnd": 155}, "context": "Additionally, it would be worth exploring whether pre-training over protographs provides benefits compared to passing the relevant RDFS triples to the KGEM as part of the whole graph.", "mentionContextAttributes": {"used": {"value": false, "score": 0.14144480228424072}, "created": {"value": false, "score": 0.002139151096343994}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9982430338859558}, "created": {"value": false, "score": 0.0030922889709472656}, "shared": {"value": false, "score": 4.172325134277344e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "TuckER", "normalizedForm": "TuckER", "offsetStart": 154, "offsetEnd": 163}, "context": "In the following, protographs, KG entities and relations are encoded using five mainstream KGEMs: TransE [3], DistMult [24], ComplEx [20], ConvE [8], and TuckER [1]. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9237643480300903}, "created": {"value": false, "score": 2.1457672119140625e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9237643480300903}, "created": {"value": false, "score": 0.0003648996353149414}, "shared": {"value": false, "score": 4.172325134277344e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "KGEMs", "normalizedForm": "KGEMs", "offsetStart": 178, "offsetEnd": 183}, "context": "Particularly, the DLCC synthetic gold standard datasets are built on the basis of different DL constructors in order to analyze which KG constructs are more easily recognized by KGEMs.", "mentionContextAttributes": {"used": {"value": false, "score": 0.03576403856277466}, "created": {"value": false, "score": 0.00011277198791503906}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999769330024719}, "created": {"value": true, "score": 0.9999476075172424}, "shared": {"value": false, "score": 5.364418029785156e-07}}, "references": [{"label": "[3]", "normalizedForm": "[3]", "refKey": 3}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "KGEM", "normalizedForm": "KGEM", "offsetStart": 183, "offsetEnd": 187}, "context": "Another line of research that is less explored focuses on preprocessing graphs prior to encoding their components [5,18], rather than focusing on the representation capability of the KGEM itself.", "mentionContextAttributes": {"used": {"value": false, "score": 8.26120376586914e-05}, "created": {"value": false, "score": 0.0026418566703796387}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9982430338859558}, "created": {"value": false, "score": 0.0030922889709472656}, "shared": {"value": false, "score": 4.172325134277344e-07}}}, {"type": "software", "software-type": "software", "wikidataId": "Q3337017", "wikipediaExternalRef": 47655250, "lang": "en", "confidence": 0.4656, "software-name": {"rawForm": "MASCHInE", "normalizedForm": "MASCHInE", "wikidataId": "Q3337017", "wikipediaExternalRef": 47655250, "lang": "en", "confidence": 0.4656, "offsetStart": 185, "offsetEnd": 193}, "context": "Extensive experiments on various evaluation benchmarks demonstrate the soundness of this approach, which we call Modular and Agnostic SCHema-based Integration of protograph Embeddings (MASCHInE). ", "mentionContextAttributes": {"used": {"value": false, "score": 0.001330256462097168}, "created": {"value": false, "score": 0.091677725315094}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9982430338859558}, "created": {"value": true, "score": 0.9999476075172424}, "shared": {"value": false, "score": 6.556510925292969e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "TuckER", "normalizedForm": "TuckER", "offsetStart": 186, "offsetEnd": 195}, "context": "Most of the KGEMs subsequently proposed aim at addressing its limitations and improve the representation expressiveness of KGEs [21], such as DistMult [24], ComplEx [20], ConvE [8], and TuckER [1]. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.00042444467544555664}, "created": {"value": false, "score": 0.0003648996353149414}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9237643480300903}, "created": {"value": false, "score": 0.0003648996353149414}, "shared": {"value": false, "score": 4.172325134277344e-07}}}], "references": [{"refKey": 3, "tei": "<biblStruct xml:id=\"b3\">\n\t<analytic>\n\t\t<title level=\"a\" type=\"main\">Translating Embeddings for Modeling Multi-relational Data</title>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Antoine</forename><surname>Bordes</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Nicolas</forename><surname>Usunier</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Alberto</forename><surname>Garc\u00eda-Dur\u00e1n</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Jason</forename><surname>Weston</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Oksana</forename><surname>Yakhnenko</surname></persName>\n\t\t</author>\n\t</analytic>\n\t<monogr>\n\t\t<title level=\"m\">Conf. on Neural Information Processing Systems (NeurIPS)</title>\n\t\t<imprint>\n\t\t\t<date>2013</date>\n\t\t\t<biblScope unit=\"page\">2795</biblScope>\n\t\t</imprint>\n\t</monogr>\n</biblStruct>\n"}, {"refKey": 24, "tei": "<biblStruct xml:id=\"b24\">\n\t<analytic>\n\t\t<title level=\"a\" type=\"main\">Embedding Entities and Relations for Learning and Inference in Knowledge Bases</title>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Bishan</forename><surname>Yang</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Wen-Tau</forename><surname>Yih</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Xiaodong</forename><surname>He</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Jianfeng</forename><surname>Gao</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Li</forename><surname>Deng</surname></persName>\n\t\t</author>\n\t</analytic>\n\t<monogr>\n\t\t<title level=\"m\">3rd International Conference on Learning Representations</title>\n\t\t<imprint>\n\t\t\t<publisher>ICLR</publisher>\n\t\t\t<date>2015</date>\n\t\t</imprint>\n\t</monogr>\n</biblStruct>\n"}, {"refKey": 20, "tei": "<biblStruct xml:id=\"b20\">\n\t<analytic>\n\t\t<title level=\"a\" type=\"main\">Complex Embeddings for Simple Link Prediction</title>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Th\u00e9o</forename><surname>Trouillon</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Johannes</forename><surname>Welbl</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Sebastian</forename><surname>Riedel</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">\u00c9ric</forename><surname>Gaussier</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Guillaume</forename><surname>Bouchard</surname></persName>\n\t\t</author>\n\t</analytic>\n\t<monogr>\n\t\t<title level=\"m\">Proc. of the 33rd International Conf. on Machine Learning, ICML</title>\n\t\t<meeting>of the 33rd International Conf. on Machine Learning, ICML</meeting>\n\t\t<imprint>\n\t\t\t<date>2016</date>\n\t\t\t<biblScope unit=\"volume\">48</biblScope>\n\t\t\t<biblScope unit=\"page\">2080</biblScope>\n\t\t</imprint>\n\t</monogr>\n</biblStruct>\n"}, {"refKey": 13, "tei": "<biblStruct xml:id=\"b13\">\n\t<monogr>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Nicolas</forename><surname>Hubert</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Pierre</forename><surname>Monnin</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Armelle</forename><surname>Brun</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Davy</forename><surname>Monticolo</surname></persName>\n\t\t</author>\n\t\t<idno>CoRR abs/2303.00286</idno>\n\t\t<title level=\"m\">Treat Different Negatives Differently: Enriching Loss Functions with Domain and Range Constraints for Link Prediction</title>\n\t\t<imprint>\n\t\t\t<date>2023. 2023</date>\n\t\t</imprint>\n\t</monogr>\n</biblStruct>\n"}], "runtime": 93349, "id": "be1e902b45ad893024de6af6ff7bf38cc23dcdef", "metadata": {"id": "be1e902b45ad893024de6af6ff7bf38cc23dcdef"}, "original_file_path": "../../datalake/Samuel/SOFTware-Sync/data/xml_not_sofctied/hal-04344873.grobid.tei.xml", "file_name": "hal-04344873.grobid.tei.xml"}