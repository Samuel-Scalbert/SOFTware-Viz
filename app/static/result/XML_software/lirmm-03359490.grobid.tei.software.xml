<TEI xmlns="http://www.tei-c.org/ns/1.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xml:space="preserve" xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Efficient Incremental Computation of Aggregations over Sliding Windows</title>
				<funder ref="#_6ADxZTB">
					<orgName type="full">French government IDEX-ISITE</orgName>
				</funder>
				<funder ref="#_xbmX2Rm">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher />
				<availability status="unknown"><licence /></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Chao</forename><surname>Zhang</surname></persName>
							<email>chao.zhang@uca.fr</email>
						</author>
						<author>
							<persName><forename type="first">Reza</forename><surname>Akbarinia</surname></persName>
							<email>reza.akbarinia@inria.fr</email>
						</author>
						<author>
							<persName><forename type="first">Farouk</forename><surname>Toumani</surname></persName>
							<email>farouk.toumani@uca.fr</email>
						</author>
						<author>
							<persName><forename type="first">Zhang</forename><surname>Chao</surname></persName>
						</author>
						<author>
							<persName><surname>Limos</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">CNRS</orgName>
								<orgName type="institution" key="instit2">University of Clermont Auvergne</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="laboratory">LIRMM</orgName>
								<orgName type="institution" key="instit1">INRIA</orgName>
								<orgName type="institution" key="instit2">University of Montpellier</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="laboratory">LIMOS</orgName>
								<orgName type="institution" key="instit1">CNRS</orgName>
								<orgName type="institution" key="instit2">University of Clermont Auvergne</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Efficient Incremental Computation of Aggregations over Sliding Windows</title>
					</analytic>
					<monogr>
						<imprint>
							<date />
						</imprint>
					</monogr>
					<idno type="MD5">A2B163A6CA3CC8E36FA9EF9BDBCED1BF</idno>
					<idno type="DOI">10.1145/3447548.3467360</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-04-12T14:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid" />
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Information systems â†’ Data streaming Data Stream</term>
					<term>Streaming Algorithm</term>
					<term>Sliding Window Aggregation ACM Reference Format:</term>
				</keywords>
			</textClass>
			<abstract>
<div><p>Computing aggregation over sliding windows, i.e., finite subsets of an unbounded stream, is a core operation in streaming analytics. We propose PBA (Parallel Boundary Aggregator), a novel parallel algorithm that groups continuous slices of streaming values into chunks and exploits two buffers, cumulative slice aggregations and left cumulative slice aggregations, to compute sliding window aggregations efficiently. PBA runs in ğ‘‚ (1) time, performing at most 3 merging operations per slide while consuming ğ‘‚ (ğ‘›) space for windows with ğ‘› partial aggregations. Our empirical experiments demonstrate that PBA can improve throughput up to 4Ã— while reducing latency, compared to state-of-the-art algorithms.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div><head n="1">INTRODUCTION</head><p>Nowadays, we are witnessing the production of large volumes of continuous or real-time data in many application domains like traffic monitoring, medical monitoring, social networks, weather forecasting, network monitoring, etc. For example, every day around one trillion messages are processed through Uber data analytics infrastructure 1 while more than 500 million tweets are posted on Twitter <ref type="bibr" target="#b8">[11]</ref>. Efficient streaming algorithms are needed for analyzing data streams in such applications. In particular, aggregations <ref type="bibr" target="#b20">[23]</ref>, having the inherent property of summarizing information from data, constitute a fundamental operator to compute real-time statistics in this context. In the streaming setting, aggregations are typically computed over finite subsets of a stream, called windows. In particular, Sliding-Window Aggregation (SWAG) <ref type="bibr" target="#b17">[20,</ref><ref type="bibr" target="#b18">21,</ref><ref type="bibr" target="#b23">26,</ref><ref type="bibr" target="#b25">28]</ref> continuously computes a summary of the most recent data items in a given range ğ‘Ÿ (aka window size) and using a given slide ğ‘ . If the range and slide parameters are given in time units (e.g., seconds), then the sliding window is time-based, otherwise, i.e., if these parameters are given as the number of values, it is count-based. Fig. <ref type="figure" target="#fig_0">1</ref> presents an example of computing sum over the count-based sliding window with a range of 10 values and a slide of 2 values.</p><p>Stream processing systems (SPSs) <ref type="bibr" target="#b0">[2,</ref><ref type="bibr" target="#b3">6,</ref><ref type="bibr" target="#b16">19,</ref><ref type="bibr" target="#b27">30,</ref><ref type="bibr" target="#b29">32]</ref> are ubiquitous for analyzing continuously unbounded data. However, one of the challenges faced by SPSs is to efficiently compute aggregations over sliding windows. This requires the ability of such systems to incrementally aggregate moving data, i.e., inserting new data items and evicting old data items when a window is sliding without recomputing the aggregation from scratch. High throughput and low latency are essential requirements as SPSs are typically designed for real-time applications <ref type="bibr" target="#b10">[13]</ref>.</p><p>Two orthogonal techniques have been proposed to meet such requirements: slicing (aka partial aggregation) <ref type="bibr" target="#b4">[7,</ref><ref type="bibr" target="#b15">18,</ref><ref type="bibr" target="#b17">20,</ref><ref type="bibr" target="#b28">31]</ref>, and merging (aka incremental final aggregation) <ref type="bibr" target="#b22">[25]</ref><ref type="bibr" target="#b23">[26]</ref><ref type="bibr" target="#b24">[27]</ref><ref type="bibr" target="#b25">[28]</ref>. Slicing techniques focus on slicing the windows into smaller subsets, called slices, and then computing partial aggregation over slices, called slice aggregations. The benefit of the slicing technique is that slice aggregations (i.e., partial aggregations over slices) can be shared by different window instances. For example, in Fig. <ref type="figure" target="#fig_0">1</ref>, a slice aggregation over a slice of 2 values can be shared by 5 windows. On the basis of a slicing technique, final aggregations over sliding windows are computed by merging slice aggregations, e.g., in Fig. <ref type="figure" target="#fig_0">1</ref>, SWAGs are computed by merging 5 slice aggregations. During the merging phase, each insertion or eviction is processed over a slice aggregation rather than a value. In modern SPSs, practical sliding windows can be very large <ref type="bibr" target="#b25">[28]</ref>, thereby making the merging phase non-trivial. To efficiently merge slice aggregation, incremental computation is necessary because an approach that recalculate slice aggregations from scratch (hereafter called Recal) is very inefficient <ref type="bibr" target="#b17">[20,</ref><ref type="bibr" target="#b18">21,</ref><ref type="bibr" target="#b23">26,</ref><ref type="bibr" target="#b25">28]</ref>.</p><p>The difficulty of incremental merging depends on the considered class of aggregations: invertible or non-invertible. An aggregation is invertible if the merging function for its partial aggregations has an inverse function. For example, ğ‘ ğ‘¢ğ‘š is clearly invertible, because it has the arithmetical subtraction as the inverse function. Similarly, ğ‘ğ‘£ğ‘” and ğ‘ ğ‘¡ğ‘‘ are invertible because they use addition as merging function. Partial aggregations of invertible aggregations can be efficiently merged using the subtract-on-evict algorithm <ref type="bibr" target="#b10">[13,</ref><ref type="bibr" target="#b24">27]</ref>, i.e., maintaining a running sum over a sliding window by subtracting evicted values and adding inserted values. However, this is not the case for non-invertible aggregations, e.g., max, min, and bloom filter, where the subtract-on-evict algorithm cannot be applied at the merging phase. Incremental merging of continuous data in the context of non-invertible aggregations is challenging and requires more sophisticated algorithms (e.g., see <ref type="bibr" target="#b2">[5,</ref><ref type="bibr" target="#b22">[25]</ref><ref type="bibr" target="#b23">[26]</ref><ref type="bibr" target="#b24">[27]</ref><ref type="bibr" target="#b25">[28]</ref>).</p><p>This paper focuses on merging slice aggregations for computing non-invertible aggregations over FIFO sliding windows with an arbitrary range and an arbitrary slide. We propose PBA (Parallel Boundary Aggregator), a novel algorithm that computes incremental aggregations in parallel. PBA groups continuous slices into chunks, and maintains two buffers for each chunk containing, respectively, the cumulative slice aggregations (denoted as ğ‘ğ‘ ğ‘) and the left cumulative slice aggregations (denoted as ğ‘™ğ‘ğ‘ ) of the chunk's slices. Using such a model, SWAGs can be computed in constant time bounded by 3 for both amortized and worst-case time. Interestingly, the required ğ‘ğ‘ ğ‘ and ğ‘™ğ‘ğ‘  for each window aggregation are completely independent from each other, so the incremental computation of ğ‘ğ‘ ğ‘ and ğ‘™ğ‘ğ‘  can be achieved in parallel. These salient features put PBA ahead of state-of-the-art algorithms in terms of throughput and latency.</p><p>In this paper, we make the following main contributions:</p><p>â€¢ We propose a novel algorithm, PBA, to efficiently compute SWAGs. PBA uses chunks to divide final aggregations of window instances into sub-aggregations that are elements of ğ‘ğ‘ ğ‘ and ğ‘™ğ‘ğ‘  buffers. These two buffers can be computed incrementally and in parallel.</p><p>â€¢ We analyze the latency caused by SWAG computations with different chunk sizes in PBA, and propose an approach to optimize the chunk size leading to the minimum latency. â€¢ We conduct extensive empirical experiments, using both synthetic and real-world datasets. Our experiments show that PBA behaves very well for average and large sliding windows (e.g., with sizes higher than 1024 values), improving throughput up to 4Ã— against state-of-the-art algorithms while reducing latency. For small-size windows, the results show the superiority of the non-parallel version of PBA (denoted as SBA) that outperforms other algorithms in terms of throughput. â€¢ To show the benefit of our approach in modern streamprocessing frameworks, we implemented PBA on top of <software ContextAttributes="created">Apache Flink</software> <ref type="bibr" target="#b1">[3,</ref><ref type="bibr" target="#b3">6]</ref>, called FPBA. Our empirical evaluation shows that FPBA scales well as increasing the parallelism of <software ContextAttributes="created">Flink</software> in both local and cluster modes.</p><p>The rest of this paper is organized as follows. In Section 2, we present the background related to SWAG. We discuss state-of-theart algorithms for merging slice aggregations, and parallelization in SPSs in Section 3. PBA is presented in Section 4. Our experimental evaluation is reported in Section 5. We conclude in Section 6. All related proofs and pseudo-codes are included in our technical report, which is available online [1]. </p></div>
<div><head n="2">BACKGROUND 2.1 Sliding Window Aggregations</head></div>
<div><head>.).</head><p>We assume that the sliding window respects the FIFO semantic, where values that are first inserted into the window will be first evicted from it. In this case, SWAG is computed by aggregating streaming values according to their arrival order.</p><p>Two stream-processing models are widely used by SPSs. One is the micro-batch processing model, where a micro batch containing a finite subset of values is processed at a time, e.g., <software ContextAttributes="created">Apache Spark</software> <ref type="bibr">[4]</ref>. The other is the continuous processing model, where streaming values are processed one by one, e.g., <software ContextAttributes="created">Apache Flink</software> <ref type="bibr" target="#b1">[3]</ref>. We consider the continuous processing model in this paper. In this case, the computation of SWAGs is triggered immediately at the boundary of window instances.</p></div>
<div><head n="2.2">Stream Slicing</head><p>To share partial aggregations, a stream is sliced to generate disjoint subsets of streaming values, called slices. SWAGs can be computed by merging partial aggregations of slices. The state-of-the-art slicing technique is Cutty <ref type="bibr" target="#b4">[7]</ref>, in which a partial aggregate is incrementally computed for each slice. Given a sliding window ğ‘Š [ğ‘Ÿ, ğ‘ ], the size of each slice determined by Cutty is ğ‘  which equals to the slide size. For example, for the sliding window ğ‘Š <ref type="bibr" target="#b7">[10,</ref><ref type="bibr" target="#b1">3]</ref>, each slice has 3 streaming values, and an instance of ğ‘Š <ref type="bibr" target="#b7">[10,</ref><ref type="bibr" target="#b1">3]</ref> will cover 3 slices and 1 streaming value. We consider Cutty as the underlying slicing technique in this paper.</p></div>
<div><head n="2.3">Aggregation Functions</head><p>Modern frameworks provide a uniform interface to capture general aggregations. In the streaming context, aggregation functions can be decomposed into three functions: lift, combine, and lower <ref type="bibr" target="#b10">[13,</ref><ref type="bibr" target="#b25">28]</ref>, called hereafter the LCL framework. We use the geometric mean, i.e., ğ‘”ğ‘š(ğ‘‹ ) = ( ğ‘› ğ‘–=1 ğ‘¥ ğ‘– ) 1/ğ‘› , as an example to illustrate them: -lift maps a streaming value to a tuple of one or several values, e.g., ğ‘”ğ‘š.ğ‘™ğ‘– ğ‘“ ğ‘¡ (ğ‘£) = (ğ‘£, 1); -combine continuously merges two outputs of lift, e.g., ğ‘”ğ‘š.ğ‘ğ‘œğ‘šğ‘ğ‘–ğ‘›ğ‘’ ((ğ‘£ 1 , ğ‘› 1 ), (ğ‘£ 2 , ğ‘› 2 )) = ((ğ‘£ 1 Ã— ğ‘£ 2 ), (ğ‘› 1 + ğ‘› 2 )); -lower maps a tuple to a final aggregation result, e.g., ğ‘”ğ‘š.ğ‘™ğ‘œğ‘¤ğ‘’ğ‘Ÿ (ğ‘£, ğ‘›) = ğ‘£ 1/ğ‘› .</p><p>The combine function is also known as merging function and its outputs are called partial aggregations. We use âŠ• to denote a combine function through this paper.</p><p>An aggregation is invertible if its combine function is invertible. Let ğ‘¥, ğ‘¦ be any values from the domain of a combine function âŠ•. Then âŠ• is invertible if there exists an operation âŠ–, such that ğ‘¥ âŠ• ğ‘¦ âŠ– ğ‘¦ = ğ‘¥, e.g., the combine function of geometric mean is invertible as its inverse function is a pair of division and subtraction.</p><p>Aggregations can also be classified in distributive, algebraic and holistic <ref type="bibr" target="#b7">[10]</ref>. Distributive aggregations, e.g., sum and max, have an identical constant size for both partial and final aggregation. Algebraic aggregations have a bounded size for partial aggregation, e.g., average and geometric mean. The other aggregations are holistic, which have an unbounded size for partial aggregations, e.g., median and percentile. In this paper, we consider non-invertible aggregations that are distributive or algebraic. Note that, such consideration is consistent with state-of-the-art works <ref type="bibr" target="#b22">[25]</ref><ref type="bibr" target="#b23">[26]</ref><ref type="bibr" target="#b24">[27]</ref>.</p></div>
<div><head n="3">STATE OF THE ART</head><p>This section reviews state-of-the-art algorithms devoted to merging slice aggregations, and discusses slicing techniques and parallelization in SPSs. A more broad vision on SWAG can be found in <ref type="bibr" target="#b10">[13]</ref> while existing surveys review various facets of SPSs <ref type="bibr" target="#b5">[8,</ref><ref type="bibr" target="#b9">12,</ref><ref type="bibr" target="#b11">14]</ref>.</p></div>
<div><head n="3.1">Incremental Merging of Slice Aggregations</head><p>To merge slice aggregations for non-invertible aggregations, efficient algorithms are needed. The performance of such algorithms can impact the overall aggregation time, because latency caused by computing SWAG has the nature of propagation towards computations over the following ones. To our best knowledge, <software ContextAttributes="created">TwoStack</software> <ref type="bibr" target="#b24">[27]</ref>, DABA <ref type="bibr" target="#b24">[27]</ref>, <software ContextAttributes="created">FlatFIT</software> <ref type="bibr" target="#b22">[25]</ref>, <software ContextAttributes="created">SlickDeque</software> (Non-Inv) <ref type="bibr" target="#b23">[26]</ref> and <software ContextAttributes="created">SlideSide</software> (Non-Inv) <ref type="bibr" target="#b26">[29]</ref> are the most efficient state-of-the-art algorithms for computing non-invertible aggregations over FIFO windows. Consider each instance of a sliding window that requires merging ğ‘› slice (partial) aggregations. In Table <ref type="table" target="#tab_0">1</ref>, we present the algorithmic complexities of the state-of-the-art algorithms computed originally in <ref type="bibr" target="#b23">[26,</ref><ref type="bibr" target="#b26">29]</ref>, and also our algorithm PBA (see Section 4.4 for explanation). <software ContextAttributes="created">TwoStack</software> pushes pairs of a partial aggregation and a streaming value into a back stack and pops such pairs from a front stack. Computing an aggregation over a sliding window instance only needs to peek and merge partial aggregations from the front and back stack. However, when the front stack is empty, <software ContextAttributes="created">TwoStack</software> flips all items in the back stack onto the front stack requiring ğ‘‚ (ğ‘›) time. DABA <ref type="bibr" target="#b24">[27]</ref> leverages <software ContextAttributes="created">TwoStack</software> to gradually perform the flipping operation of <software ContextAttributes="created">TwoStack</software> which amortizes its time to ğ‘‚ (1). As a consequence, DABA is in ğ‘‚ (1) worst-case time.</p><p><software ContextAttributes="created">FlatFIT</software> <ref type="bibr" target="#b22">[25]</ref> only recalculates window aggregation one time per ğ‘› + 1 slides. Intermediate aggregation results are maintained in an appropriate index structure and reused to avoid redundant computations. Hence, on average, each of ğ‘› + 1 slides can be computed by 3 merging steps. <software ContextAttributes="created">SlickDeque</software> (Non-Inv) uses a deque to store streaming values, and not every streaming value will be added into the deque. For example, when computing max over sliding windows, <software ContextAttributes="created">SlickDeque</software> continuously replaces the end of a deque ğ‘£ â€² using a new value ğ‘£ if ğ‘£ &gt; ğ‘£ â€² . Adding a value ğ‘£ will depend on the rank of ğ‘£ w.r.t the number of values in the deque which could be ğ‘›. The worst time complexity of <software ContextAttributes="created">SlickDeque</software> is ğ‘‚ (ğ‘›) while its amortized time is less than 2 because a new value will never be compared more than twice. <software ContextAttributes="created">SlideSide</software> (Non-Inv) uses two stacks to </p></div>
<div><head>Algorithm Time (amortized) Time (worst) Space</head><formula xml:id="formula_0">TwoStack 3 ğ‘› 2ğ‘› DABA 5 8 2ğ‘› FlatFIT 3 ğ‘› 2ğ‘› SlickDeque &lt; 2 ğ‘› 2ğ‘› SlideSide 3 ğ‘› 2ğ‘› PBA &lt; 3 3 3ğ‘›+13 2</formula><p>store partial aggregations, like <software>TwoStack</software>, but shares values between them. While, an operation similar to the flipping in <software ContextAttributes="created">TwoStack</software> is still required, resulting in ğ‘‚ (ğ‘›) worst-case time.</p><p>As shown in Table <ref type="table" target="#tab_0">1</ref>, which contains complexity analysis of PBA described in Section 4.4, PBA is the best for time and space complexity, except the comparison with <software ContextAttributes="created">SlickDeque</software> in amortized time. However, unlike <software ContextAttributes="created">SlickDeque</software>, PBA runs in constant time, which is also experimentally demonstrated in Section 5.1.1. Such a feature of PBA guarantees its performance in the worst case. Note that amortized and worst-case time determine throughput and latency, respectively.</p><p>In addition to its low time and space complexities, one of the main advantages of PBA compared to the state-of-the-art algorithms, e.g., <software ContextAttributes="created">TwoStack</software> <ref type="bibr" target="#b24">[27]</ref>, DABA <ref type="bibr" target="#b24">[27]</ref>, <software ContextAttributes="created">FlatFIT</software> <ref type="bibr" target="#b22">[25]</ref>, <software ContextAttributes="created">SlickDeque</software> (Non-Inv) <ref type="bibr" target="#b23">[26]</ref>, and <software ContextAttributes="created">SlideSide</software> (Non-Inv) <ref type="bibr" target="#b26">[29]</ref>, is that the latter algorithms cannot use additional threads to get performance gains, i.e., computing intermediate results in parallel. For instance, in <software ContextAttributes="created">TwoStack</software>, the two stacks are interdependent. In <software ContextAttributes="created">SlickDeque</software>, the deque needs to be traversed in the worst case. However, the ğ‘™ğ‘ğ‘  and ğ‘ğ‘ ğ‘ buffers used for computing SWAGs in PBA are completely independent of each other, allowing their incremental computation to be done in parallel.</p></div>
<div><head n="3.2">Combining Slicing and Merging</head><p>Various slicing techniques have been proposed in the literature, among which we could mention Panes <ref type="bibr" target="#b17">[20]</ref>, Pairs <ref type="bibr" target="#b15">[18]</ref>, Cutty <ref type="bibr" target="#b4">[7]</ref>, and Scotty <ref type="bibr" target="#b28">[31]</ref>. The main goal is to determine sizes of intervals over the stream called slices which are then used to split each window into several slices in order to allow sharing the aggregations computed over slices. Slicing techniques can improve throughput by amortizing the total time of merging evenly over each stream value in each slice. However, having only a slicing technique cannot reduce latency caused by merging slice aggregations, which is especially important for a continuous stream-processing model.</p><p>A merging technique on top of a slicing one can deal with the latency issues, and additionally improves the throughput. First of all, latency can be reduced, as demonstrated in Cutty and Scotty through combining their slicing techniques with the <software ContextAttributes="created">FlatFAT</software> <ref type="bibr" target="#b25">[28]</ref> merging algorithm. Eager slicing of Scotty (using <software ContextAttributes="created">FlatFAT</software> for merging) shows up to two orders of magnitudes reduction of latency, compared to lazy slicing (with Recal as the merging algorithm). In addition, an advanced merging algorithm can also improve throughput. Specifically, <ref type="bibr" target="#b24">[27]</ref> shows that <software ContextAttributes="created">TwoStack</software> outperforms Recal, except in the case of very small window sizes, and <software ContextAttributes="created">TwoStack</software> is always faster, up to 6Ã—, than <software ContextAttributes="created">FlatFAT</software>. In this paper, we adopt the Cutty slicing technique and design a constant-time merging algorithm to reduce latency and improve throughput. It is worth noting that our merging approach remains independent from slicing techniques.</p></div>
<div><head n="3.3">Parallelization in SPSs</head><p>Parallelization in SPSs is an active research area. A large piece of work in this field focuses on inter-operator parallelism with a clear dominance of key-based splitting techniques <ref type="bibr" target="#b21">[24]</ref> in shared nothing cluster architecture. Regarding intra-operator parallelism, <ref type="bibr" target="#b19">[22]</ref> presents parallel patterns to handle generic stateful operators (e.g., sorting, join, grouping, . . . ). PBA fits into the category of exploiting intra-operator parallelism to incrementally compute SWAGs.</p><p>Novel SPSs are designed to exploit multi-core processors to process streams in parallel <ref type="bibr" target="#b14">[17,</ref><ref type="bibr" target="#b31">34]</ref>. In this line of work, SABER <ref type="bibr" target="#b14">[17]</ref> processes fixed-size batches in parallel using a hybrid architecture of CPUs and GPGPUs (i.e., modern GPUs), and merges local results of batches to produce the final result. The parallelism of SABER is suitable for the micro-batch model, giving the opportunity to process each micro batch by separate tasks (one task for each batch <ref type="bibr" target="#b14">[17]</ref>). PBA is designed for the continuous processing model. In addition, PBA merges at most 3 sub-aggregations to produce the final aggregation for arbitrary cases, i.e., requiring only constant time for merging, which is achieved by using the optimal chunk size computed based on the range and slide parameters in a continuous aggregate query.</p><p>Modern distributed SPSs <ref type="bibr">[2-4, 19, 30, 32]</ref> compute SWAGs over a stream of key-value pairs in parallel. For example, in <software ContextAttributes="created">Apache Flink</software> <ref type="bibr" target="#b1">[3,</ref><ref type="bibr" target="#b3">6]</ref>, one can transform a DataStream into a KeyedStream with disjoint partitions, where a key will be assigned to a specific partition. SWAGs over different partitions can be computed in parallel using multiple cores of nodes in a cluster. The parallel computation in PBA is orthogonal to such distributed SPSs, since PBA computes SWAGs over values with the same key in parallel, which makes PBA suitable to extend partition-oriented parallelism in distributed SPSs.</p></div>
<div><head n="4">PARALLEL BOUNDARY AGGREGATOR</head><p>In PBA, a stream is considered as a sequence of chunks having an identical number of slices. Non-invertible aggregations over window instances are computed by merging two buffers of cumulative aggregations over slice aggregations in chunks. To efficiently compute SWAG, buffers are computed in parallel using two threads. As SPSs are usually running on modern CPUs <ref type="bibr" target="#b30">[33]</ref>, in PBA we assume that the system is equipped with at least two cores allowing to run two tasks in parallel.</p></div>
<div><head n="4.1">Main Ideas</head><p>Let us consider a query with the non-invertible aggregation ğ‘šğ‘ğ‘¥ over a range of 10 values and a slide of 2 values, i.e., ğ‘Š <ref type="bibr" target="#b7">[10,</ref><ref type="bibr" target="#b0">2]</ref>. Slices of 2 values can be created, and thus each window instance needs to merge 5 slice aggregations. In general, the final aggregation needs to merge âŒŠğ‘Ÿ /ğ‘ âŒ‹ slice aggregations for each instance of ğ‘Š [ğ‘Ÿ, ğ‘ ].</p><p>PBA is an ğ‘‚ (1) time solution for ğ‘Š [ğ‘Ÿ, ğ‘ ], which is achieved by maintaining two buffers: ğ‘ğ‘ ğ‘ (cumulative slice aggregations) and ğ‘™ğ‘ğ‘  (left cumulative slice aggregations). Generally, a ğ‘ğ‘ ğ‘ buffer is computed through accumulating slice aggregations from left to right inside a chunk (an array of slices), and a ğ‘™ğ‘ğ‘  buffer from right to Example review of NBA  with the aggregation max over a stream using PBA with a chunk size of 5 slices.</p><formula xml:id="formula_1">ğ’˜ ğŸ ğ’˜ ğŸ ğ’˜ ğŸ‘ ğ’˜ ğŸ’ ğ’˜ ğŸ“ ğ‘šğ‘ğ‘¥ ğ‘¤ 2 = ğ‘šğ‘ğ‘¥(ğ‘™ğ‘ğ‘  0 2 , ğ‘ğ‘ ğ‘ 1 [1]) ğ‘šğ‘ğ‘¥ ğ‘¤ 3 = ğ‘šğ‘ğ‘¥(ğ‘™ğ‘ğ‘  0 3 , ğ‘ğ‘ ğ‘ 1 [2]) ğ’ğ’„ğ’” ğŸ ğ’„ğ’”ğ’‚ ğŸ ğ’„ ğŸ ğ‘šğ‘ğ‘¥ ğ‘¤ 0 = ğ‘ğ‘ 0 ğ‘šğ‘ğ‘¥ ğ‘¤ 1 = ğ‘šğ‘ğ‘¥(ğ‘™ğ‘ğ‘  0 1 , ğ‘ğ‘ ğ‘ 1 [0])<label>5</label></formula><p>left. Then SWAG can be computed by merging 2 elements respectively from ğ‘™ğ‘ğ‘  and ğ‘ğ‘ ğ‘ for this example. The two buffers of PBA for computing ğ‘šğ‘ğ‘¥ over ğ‘Š <ref type="bibr" target="#b7">[10,</ref><ref type="bibr" target="#b0">2]</ref> are shown in Fig. <ref type="figure" target="#fig_2">2</ref>, e.g., ğ‘ğ‘ ğ‘ 0 and ğ‘™ğ‘ğ‘  0 for chunk ğ‘ 0 . Let us explain in detail how the two buffers are computed. Streaming values are partitioned into chunks having an identical number of slices, e.g., ğ‘ 0 and ğ‘ 1 in Fig. <ref type="figure" target="#fig_2">2</ref> have 5 slices, and chunks are separated by boundaries, e.g., ğ‘ 1 between ğ‘ 0 and ğ‘ 1 . Slice aggregations inside a chunk are sequentially stored in a ğ‘ ğ‘ğ‘ (slice aggregation array), e.g., ğ‘ ğ‘ğ‘ 0 and ğ‘ ğ‘ğ‘ 1 in Fig. <ref type="figure" target="#fig_2">2</ref>. Then, ğ‘ğ‘ ğ‘ and ğ‘™ğ‘ğ‘  are computed over ğ‘ ğ‘ğ‘, which are two arrays having the same length as a ğ‘ ğ‘ğ‘ array. For example, ğ‘ğ‘ ğ‘ 1 and ğ‘™ğ‘ğ‘  0 are computed by memorization of each intermediate result during accumulating elements of ğ‘ ğ‘ğ‘ 1 and ğ‘ ğ‘ğ‘ 0 , illustrated as follows.</p><p>-ğ‘ğ‘ ğ‘ 1 is obtained by accumulating from</p><formula xml:id="formula_2">ğ‘ ğ‘ğ‘ 1 [0] to ğ‘ ğ‘ğ‘ 1 [4], -ğ‘™ğ‘ğ‘  0 is obtained by accumulating from ğ‘ ğ‘ğ‘ 0 [4] to ğ‘ ğ‘ğ‘ 0 [0].</formula><p>The last element in a ğ‘ğ‘ ğ‘ array is denoted as a ğ‘ğ‘ (chunk aggregation), e.g., ğ‘ğ‘ 1 = ğ‘ğ‘ ğ‘ 1 [4] = 9. Aggregation over a window instance is computed using elements in ğ‘™ğ‘ğ‘  and ğ‘ğ‘ ğ‘ arrays, e.g., in Fig. <ref type="figure" target="#fig_2">2</ref>,</p><formula xml:id="formula_3">ğ‘šğ‘ğ‘¥ (ğ‘¤ 3 ) = ğ‘šğ‘ğ‘¥ (ğ‘™ğ‘ğ‘  0 [3], ğ‘ğ‘ ğ‘ 1 [2]</formula><p>). Consequently, ğ‘šğ‘ğ‘¥ over each window instance, e.g., each of (ğ‘¤ 0 , ..., ğ‘¤ 4 ), only requires at most 1 ğ‘šğ‘ğ‘¥ operation.</p><p>Challenges in PBA: In order to obtain the constant-time solution described above, the two buffers, ğ‘ğ‘ ğ‘ and ğ‘™ğ‘ğ‘ , must be efficiently computed as streaming values keep arriving. ğ‘ğ‘ ğ‘ can be computed by accumulating streaming values and slice aggregations. The main difficulty is how to compute ğ‘™ğ‘ğ‘  efficiently, since computing a ğ‘™ğ‘ğ‘  array requires traversing a ğ‘ ğ‘ğ‘ array inversely, which means having to stop computing ğ‘ğ‘ ğ‘, resulting in an eventual high delay. We use a parallel strategy to deal with this issue. Our key observation is that SWAG never uses ğ‘ğ‘ ğ‘ and ğ‘™ğ‘ğ‘  from the same chunk, so the inputs required to compute ğ‘ğ‘ ğ‘ and ğ‘™ğ‘ğ‘  are completely independent. For example, in Fig. <ref type="figure" target="#fig_2">2</ref>, computing ğ‘šğ‘ğ‘¥ (ğ‘¤ 0 ) requires ğ‘™ğ‘ğ‘  0 and ğ‘ğ‘ ğ‘ 1 . ğ‘™ğ‘ğ‘  0 is computed with ğ‘ ğ‘ğ‘ 0 as input, which is complete at the boundary ğ‘ 1 . While ğ‘ğ‘ ğ‘ 1 is computed with ğ‘ ğ‘ğ‘ 1 as input, which is independent of ğ‘ ğ‘ğ‘ 0 . The independence between the inputs of ğ‘™ğ‘ğ‘  0 and ğ‘ğ‘ ğ‘ 1 allows the task of computing ğ‘™ğ‘ğ‘  0 to be carried out when streaming values reach the boundary ğ‘ 1 . This task runs in parallel and simultaneously with the task of computing ğ‘ğ‘ ğ‘ 1 . Thus, PBA can keep receiving new streaming values after ğ‘ 1 and compute ğ‘ğ‘ ğ‘ 1 . Another issue is whether the task of computing ğ‘™ğ‘ğ‘  0 can be finished before the end of ğ‘¤ 1 , i.e., whether PBA needs to wait for the result of ğ‘™ğ‘ğ‘  0 [1] to compute ğ‘šğ‘ğ‘¥ (ğ‘¤ 1 ). Such an issue is dealt with in Section 4.3, where we identify the optimal chunk size minimize the waiting time.</p></div>
<div><head n="4.2">PBA Model</head><p>In PBA, a stream is considered as a sequence of non-overlapping chunks of slices. Each chunk has an identical number of slices and starts at a specific time denoted as boundary.</p><p>-ğ‘ ğ‘– (boundary): the start of the chunks, e.g., ğ‘ 1 in Fig. <ref type="figure" target="#fig_2">2</ref>; -ğ‘ ğ‘– (chunk): a sequence of slices from ğ‘ ğ‘– to ğ‘ ğ‘–+1 , e.g., ğ‘ 0 in Fig. <ref type="figure" target="#fig_2">2</ref>; -ğ‘ (chunk size): the number of slices in a chunk, e.g., 5 in Fig. <ref type="figure" target="#fig_2">2</ref>.</p><p>We incrementally compute partial aggregations for every chunk to compute SWAGs. PBA applies two kinds of incremental aggregations: (i) computing ğ‘ğ‘ ğ‘ and (ii) computing ğ‘™ğ‘ğ‘ . To compute ğ‘ğ‘ ğ‘, PBA adopts a two-level incremental computation inside each chunk: (i.1) accumulating streaming values inside each slice to have a slice aggregation, and (i.2) accumulating slice aggregations inside each chunk to have a chunk aggregation. To compute ğ‘™ğ‘ğ‘ , PBA maintains intermediate results during accumulating slice aggregations from right to left in a slice aggregation array ğ‘ ğ‘ğ‘ ğ‘– .</p><p>Dividing SWAG into Sub-Aggregations: As the window instances, whose starts coincide with boundaries in every chunk, are trivial cases requiring only ğ‘ğ‘ ğ‘ elements e.g., ğ‘¤ 0 in Fig. <ref type="figure" target="#fig_2">2</ref>, we focus on the other cases in the sequel. To illustrate the PBA model, we first define ğ‘˜ = âŒŠ âŒŠğ‘Ÿ /ğ‘  âŒ‹ ğ‘ âŒ‹, which denotes the number of chunks a window instance can fully cover, e.g., ğ‘˜ = âŒŠ âŒŠ10/2âŒ‹ 5 âŒ‹ = 1 in Fig. <ref type="figure" target="#fig_2">2</ref>. In the PBA model, we use boundaries to divide a window instance ğ‘¤ of ğ‘Š [ğ‘Ÿ, ğ‘ ] into several sub-parts and merge partial aggregations of sub-parts to have a final aggregation of ğ‘¤. The number of sub-parts can be ğ‘˜ + 1 or ğ‘˜ + 2, depending on ğ‘Š [ğ‘Ÿ, ğ‘ ] and the used chunk size. We denote a partial aggregation of each sub-part as a sub-aggregation, and all sub-aggregations of ğ‘¤ as (ğ‘†ğ´ 0 , ğ‘†ğ´ 1 , ...), i.e., ğ‘†ğ´ 0 is the first one (see Fig. <ref type="figure">3</ref> for an example). Then the first sub-aggregation is an element in ğ‘™ğ‘ğ‘ , and the last one in ğ‘ğ‘ ğ‘. Each of the others is a chunk aggregation, i.e., the last element in a ğ‘ğ‘ ğ‘ buffer. An example of ğ‘˜ = 1 is shown in Fig. <ref type="figure" target="#fig_2">2</ref>, where each window instance is divided into 2 sub-parts and SWAGs are computed by merging 2 sub-aggregations. To illustrate the case of ğ‘˜ &gt; 1, Fig. <ref type="figure">3</ref> presents the example of computing a count-based ğ‘Š [16, 1] with a chunk size of 4, where we have ğ‘˜ = 4 and each window instance is divided into 5 sub-parts, e.g., ğ‘šğ‘ğ‘¥</p><formula xml:id="formula_4">(ğ‘¤ 1 ) = ğ‘šğ‘ğ‘¥ (ğ‘™ğ‘ğ‘  0 [1], ğ‘ğ‘ 1 , ğ‘ğ‘ 2 , ğ‘ğ‘ 3 , ğ‘ğ‘ ğ‘ 4 [0]).</formula><p>Efficient Computation of ğ‘™ğ‘ğ‘ : In the final aggregation stage, since ğ‘™ğ‘ğ‘  and ğ‘ğ‘ ğ‘ buffers used for merging are computed over completely independent ğ‘ ğ‘ğ‘ arrays, we compute ğ‘™ğ‘ğ‘  and ğ‘ğ‘ ğ‘ buffers in parallel, i.e., two tasks run simultaneously and each task focus on computing a single buffer. Specifically, we maintain two threads in PBA. The main thread is used to compute ğ‘ğ‘ ğ‘ and the final aggregation, and the other for processing the task of computing ğ‘™ğ‘ğ‘   </p></div>
<div><head n="4.3">Optimal Chunk Size in PBA</head><p>The chunk size is crucial for PBA. Specifically, a carelessly selected chunk size can cause delay to compute final aggregations. For instance, consider the example in Fig. <ref type="figure" target="#fig_2">2</ref>, and let 1 ğ‘‡ ğ‘¢ be the time required to execute âŠ•, i.e., a merging function or a combine function. Although the two tasks for computing ğ‘™ğ‘ğ‘  0 and ğ‘ğ‘ ğ‘ 1 are simultaneously started at the boundary ğ‘ 1 , ğ‘™ğ‘ğ‘  0 [1] cannot be obtained earlier than ğ‘ğ‘ ğ‘ 1 [0] because computing ğ‘™ğ‘ğ‘  0 [1] (3 ğ‘‡ ğ‘¢ ) requires two more ğ‘‡ ğ‘¢ than computing ğ‘ğ‘ ğ‘ 1 [0] (1 ğ‘‡ ğ‘¢ ). Thus, PBA needs to wait for 2 ğ‘‡ ğ‘¢ to use ğ‘™ğ‘ğ‘  0 [1] and ğ‘ğ‘ ğ‘ 1 [0] to compute ğ‘šğ‘ğ‘¥ (ğ‘¤ 1 ).</p><p>The following two theorems identify the optimal chunk size, denoted as ğ‘ * , for count-based windows and time-based window, respectively. The corresponding proofs are provided in our technical report, which is available online [1].</p><p>Theorem 1. Given a count-based sliding window ğ‘Š [ğ‘Ÿ, ğ‘ ], the optimal chunk size ğ‘ * in PBA can be obtained as follows: -if âŒŠğ‘Ÿ /ğ‘ âŒ‹ â©½ ğ‘  + (ğ‘Ÿ mod ğ‘ ) + 1, then ğ‘ * = âŒŠğ‘Ÿ /ğ‘ âŒ‹;</p><formula xml:id="formula_5">-otherwise ğ‘ * = âŒŠ ğ‘  (âŒŠğ‘Ÿ /ğ‘ âŒ‹ + 1) + ğ‘Ÿ mod ğ‘  + 1 ğ‘  + 1 âŒ‹.</formula><p>With ğ‘ * , we have ğ‘˜ = âŒŠ âŒŠğ‘Ÿ /ğ‘  âŒ‹ ğ‘ * âŒ‹ = 1. Theorem 2. Given a time-based sliding window ğ‘Š [ğ‘Ÿ, ğ‘ ], the optimal chunk size ğ‘ * in PBA can be obtained as follows:</p><formula xml:id="formula_6">-if âŒŠğ‘Ÿ /ğ‘ âŒ‹ â©½ ğ‘ +2ğ‘‡ ğ‘¢ +ğ¶ ğ‘‡ ğ‘¢ , then ğ‘ * = âŒŠğ‘Ÿ /ğ‘ âŒ‹; -otherwise ğ‘ * = âŒŠ âŒŠğ‘Ÿ /ğ‘  âŒ‹ (ğ‘ +ğ‘‡ ğ‘¢ )+ğ‘ +2ğ‘‡ ğ‘¢ +ğ¶ ğ‘ +2ğ‘‡ ğ‘¢ âŒ‹,</formula><p>where ğ¶ = 0 if ğ‘Ÿ mod ğ‘  = 0, otherwise ğ¶ = ğ‘  (ğ‘Ÿ mod ğ‘ ) + 1 ğ‘‡ ğ‘¢ . With ğ‘ * , we have ğ‘˜ = âŒŠ âŒŠğ‘Ÿ /ğ‘  âŒ‹ ğ‘ * âŒ‹ = 1. The optimal chunk size ensures that (i) each ğ‘™ğ‘ğ‘  buffer can be obtained without waiting, and (ii) ğ‘˜ = 1 which leads to the minimum number of merging operations to compute final aggregations.</p></div>
<div><head n="4.4">Complexity Analysis</head><p>As previous works <ref type="bibr" target="#b22">[25]</ref><ref type="bibr" target="#b23">[26]</ref><ref type="bibr" target="#b24">[27]</ref><ref type="bibr" target="#b26">29]</ref> present complexities by computing a count-based ğ‘Š [ğ‘›, 1] with a range of ğ‘› and a slide of 1, i.e., merging ğ‘› slice aggregations for each window instance, we also rest on such a scenario to discuss complexities of PBA.</p><p>Given a sliding window ğ‘Š [ğ‘›, 1], as PBA is used with the optimal chunk size ğ‘ * , we have ğ‘ * = âŒŠ ğ‘›+2 2 âŒ‹ and ğ‘˜ = âŒŠ âŒŠğ‘Ÿ /ğ‘  âŒ‹ ğ‘ * âŒ‹ = 1 according to Theorem 1. Thus, a window instance will be divided into either ğ‘˜ + 1 = 2 or ğ‘˜ + 2 = 3 sub-parts, i.e., spanning 2 or 3 chunks. Based on this, we discuss the time and space complexity of PBA below.</p><p>Time Complexity: In the final aggregation stage, merging 2 or 3 sub-aggregations of sub-parts needs 1 or 2 merging operations. Considering that the last streaming value of a window will be merged with ğ‘ğ‘ ğ‘, the total number of merging operations by PBA is 2 or 3. Therefore, the worst-case time is 3, and the amortized time is less than 3.</p><p>Space Complexity: As a window instance can span at most 3 chunks, w.l.o.g, we denote the three chunks as ğ‘ ğ‘–-2 , ğ‘ ğ‘–-1 , ğ‘ ğ‘– , and consider the current streaming value is arriving at a slice of ğ‘ ğ‘– . Thus, 3 ğ‘ ğ‘ğ‘ arrays of size ğ‘ * are needed. Note that the ğ‘™ğ‘ğ‘  arrays of the three chunks will be computed over ğ‘ ğ‘ğ‘ arrays and do not require extra space. In addition, PBA only needs the ğ‘ğ‘ ğ‘ array of the current chunk ğ‘ ğ‘– , i.e., ğ‘ğ‘ ğ‘ ğ‘– , and maintains one element in ğ‘ğ‘ ğ‘ ğ‘– , the current one, rather than the entire ğ‘ğ‘ ğ‘ ğ‘– array. For windows spanning 3 chunks, the chunk aggregation ğ‘ğ‘ ğ‘–-1 of chunk ğ‘ ğ‘–-1 is also needed. Thus, the total space required by PBA is:</p><formula xml:id="formula_7">3ğ‘ * + 2 = 3âŒŠ ğ‘›+2 2 âŒ‹ + 2 â‰¤ 3ğ‘›+13 2 .</formula></div>
<div><head n="5">EXPERIMENTAL EVALUATION</head><p>In this section, we evaluate the performance of PBA against the state-of-the-art algorithms for computing non-invertible aggregations over sliding windows with FIFO semantics. Moreover, we integrate PBA into <software>Apache Flink</software> and present the corresponding improvement in local and cluster modes, respectively.</p></div>
<div><head n="5.1">PBA Compared to Alternatives</head><p>State-of-the-Art Algorithms: In our experimental evaluation we compare PBA to the two best state-of-the-art algorithms: <software ContextAttributes="created">SlickDeque</software> <ref type="bibr" target="#b23">[26]</ref> and <software ContextAttributes="created">TwoStack</software> <ref type="bibr" target="#b24">[27]</ref>. <software ContextAttributes="created">SlickDeque</software> shows the best performance in terms of throughput and latency <ref type="bibr" target="#b23">[26]</ref> while <software ContextAttributes="created">TwoStack</software> is slightly worse than <software ContextAttributes="created">SlickDeque</software> but behaves better than the others <ref type="bibr" target="#b23">[26,</ref><ref type="bibr" target="#b24">27,</ref><ref type="bibr" target="#b26">29]</ref>. Note that, although DABA <ref type="bibr" target="#b24">[27]</ref> has a constant time complexity in the worst case, it does not show a better performance result compared to <software ContextAttributes="created">SlickDeque</software> in terms of throughput and latency <ref type="bibr" target="#b23">[26]</ref>, or <software ContextAttributes="created">TwoStack</software> in terms of throughput <ref type="bibr" target="#b24">[27]</ref>. We also consider the naive algorithm algorithm and call it Recal (recalculate). As the sliding window moves, it calculates an aggregation from scratch. To study the performance of our algorithm in the case of single-core machines, we implemented a sequential version of PBA, denoted as SBA (sequential boundary aggregator), which uses only one thread to compute SWAG. SBA follows the same procedure as PBA, except that the task of computing ğ‘™ğ‘ğ‘  in SBA is done in the same thread as the one that computes ğ‘ğ‘ ğ‘ buffer (SBA uses only one thread). In order to slice a data stream, we set each slice size to be equal to the slide size (as described in Section 2.2 for details). Notice that this is the underlying slice solution for all algorithms tested in our evaluation. We implemented PBA and SBA using Java 8, and the state-of-the-art algorithms based on the pseudo-codes in <ref type="bibr" target="#b24">[27]</ref> and <ref type="bibr" target="#b23">[26]</ref>. The source codes of PBA, SBA, and the implementation of state-of-the-art algorithms are available online <ref type="bibr">[1]</ref>. Datasets: We use two datasets, a real-world and a synthetic one, for throughput experiment. The real-world dataset is the DEBS12 Grand Challenge Manufacturing Equipment Dataset <ref type="bibr" target="#b12">[15]</ref>, which is also used by <software ContextAttributes="created">SlickDeque</software> <ref type="bibr" target="#b23">[26]</ref>. This dataset contains energy consumption recorded by sensors of manufacturing equipment with the frequency of 100 records per second, and each tuple is associated with a timestamp, which we will use in our time-based experiments. The original dataset contains 32 million tuples, which we made into 130 million tuples, i.e., 4 copies. The synthetic dataset contains 200 million values generated using the uniform distribution. Note that in our experiments we observed the same tendency for PBA with different datasets, this is why we have not used more than two test datasets. For the latency experiments, we used the synthetic dataset and tested latency results for 1 million window instances.</p><p>In our experiments, as aggregation function we use max, commonly used in related works <ref type="bibr" target="#b22">[25,</ref><ref type="bibr" target="#b23">26,</ref><ref type="bibr" target="#b25">28,</ref><ref type="bibr" target="#b26">29]</ref> as the representative of non-invertible aggregations. We first evaluate throughput for count-based windows with a fixed slide while varying the ranges. Then we evaluate the case with a fixed range while varying the slides. Similarly, we evaluate the throughput for time-based windows in both settings (i.e., with a fixed slide and varying ranges, and then with a fixed range and varying slides). We test latency for both count-based and time-based windows. To further study the latency of different algorithms, we also test a more complex aggregation, <software ContextAttributes="created">MomentSketch</software> = (ğ‘šğ‘–ğ‘›, ğ‘šğ‘ğ‘¥, ğ‘ ğ‘¢ğ‘š, ğ‘ğ‘œğ‘¢ğ‘›ğ‘¡, ğ‘¥ ğ‘– , ... ğ‘¥ ğ‘™ ğ‘– , ğ‘™ğ‘œğ‘”ğ‘¥ ğ‘– , ... (ğ‘™ğ‘œğ‘”ğ‘¥ ğ‘– ) ğ‘™ ) <ref type="bibr" target="#b6">[9]</ref> with ğ‘™ = 10, which can approximate percentiles.</p><p>Experiment Setup: We run the throughput and latency experiments at a server with Ubuntu 16.01.3 LTS, 6 virtual CPUs of Intel(R) Xeon(R) 2.40GHz, and 16GB main memory, where the heap size of JVM is configured to 14GB.</p></div>
<div><head n="5.1.1">Throughput Experiments.</head><p>The workload for evaluating max over count-based windows is as follows: 1) range from 2 3 to 2 20  with the slide size set to one; 2) slide from 1 to 10 with a range of 2 17 . For the time-based windows the workload is as follows: 1) range from 6 to 60 minutes with a slide of 10 milliseconds; 2) slide from 10 milliseconds to 100 milliseconds with a range of 1 hour. We do not test for larger slide sizes as the overall throughput will be amortized by the time of computing slice aggregations, such that the differences between merging algorithms cannot be observed (see Section 3.2 for detailed discussion). We run the workload 20 times and report the median of all evaluation results. We stop reporting the throughput of the Recal algorithm when the corresponding throughput is too small. Our experimental results are presented in Fig. <ref type="figure">4</ref>, Fig. <ref type="figure">5</ref>, and Fig 6 . We discuss the results in the sequel.</p><p>Count-based windows with a fixed slide. Our throughput results for this case are shown in Fig. <ref type="figure">4</ref>. PBA shows the best throughput with large window sizes, and SBA (the sequential version of PBA) shows a higher throughput than <software ContextAttributes="created">TwoStack</software> and <software ContextAttributes="created">SlickDeque</software>, even for small window sizes. More precisely, the throughput of PBA increases w.r.t to the increase of window size ğ‘Ÿ when ğ‘Ÿ &lt; 2 15 , and keeps consistent when ğ‘Ÿ â©¾ 2 15 . When ğ‘Ÿ â©¾ 2 15 , the throughput of PBA is 2.5Ã— higher than that of TwoStack and 4Ã— higher than that SlickDeque. Note that, the throughput of PBA increases w.  window size ğ‘Ÿ when ğ‘Ÿ &lt; 2 15 , because a smaller window size will lead to a smaller optimal chunk size in PBA. Thus, the parallel task in PBA, i.e., computing ğ‘™ğ‘ğ‘ , will consequently require less time but will be launched more frequently, compared to a case with a larger window size. In such a case, the constant overhead for launching a parallel task will be relatively more obvious. For example, when ğ‘Ÿ = 2 5 , the optimal chunk size of PBA is 17. Then, the corresponding parallel computation for each chunk in PBA requires applying only 15 times of the ğ‘šğ‘ğ‘¥ operation over 2 elements, and this cost is much less than the cost of launching a parallel task. But, if ğ‘Ÿ = 2 20 , the optimal chunk size is 5242884, which requires 5242882 times of ğ‘šğ‘ğ‘¥ over 2 elements. In the latter case, launching a parallel task can bring significant benefits.</p><p>Count-based windows with various slide sizes. Our throughput results for this case are shown in Fig. <ref type="figure">5</ref>. We observe that, with a larger slide size, combining slicing with merging can improve throughput. PBA shows better throughput with a small slide shown in Fig. <ref type="figure">5</ref>, because ğ‘Ÿğ‘ğ‘›ğ‘”ğ‘’ ğ‘ ğ‘™ğ‘–ğ‘‘ğ‘’ is still large in such cases, which means there are still a large number of slice aggregation that need to be merged. Then, as the slide size is getting larger, the throughput of different algorithms are getting closer. There are mainly two reasons that explain such a behavior. The first reason comes from the fact that the ratio ğ‘Ÿğ‘ğ‘›ğ‘”ğ‘’ ğ‘ ğ‘™ğ‘–ğ‘‘ğ‘’ is smaller in these cases, which leads to less slice aggregations to be merged for each window instance. Thus, the required computations are not enough to exploit the full capacity of PBA. The second reason is that the overall difference between various algorithms will be amortized by the cost of computing partial aggregations over each slice (this is due to the large number of âŠ• operations performed in each slice).  Time-based windows with various range and slide sizes. Our throughput results of this case are shown in Fig. <ref type="figure" target="#fig_6">6</ref>. PBA shows the overall best throughput with large window sizes, and SBA (the sequential version of PBA) shows higher throughput than <software ContextAttributes="created">TwoStack</software> and <software ContextAttributes="created">SlickDeque</software>. The corresponding reasons are identical to the case of count-based windows. Note that, in Fig. <ref type="figure" target="#fig_6">6</ref> (a), PBA outperforms the others because ğ‘Ÿğ‘ğ‘›ğ‘”ğ‘’ ğ‘ ğ‘™ğ‘–ğ‘‘ğ‘’ is large even for the case of ğ‘Ÿğ‘ğ‘›ğ‘”ğ‘’ = 6 minutes.</p></div>
<div><head n="5.1.2">Latency</head><p>Experiments. The workload consists in: (i) computing the aggregation Max over count-based windows with a slide of 1 and ranges of 2 13 and 2 14 respectively, and (ii) computing the aggregation MS (<software>MomentSketch</software>) over time-based windows with a slide of 1 second (containing around 100 streaming values) and ranges of 1 and 2 hours respectively. In this experiment, we compare <software ContextAttributes="created">TwoStack</software>, <software ContextAttributes="created">SlickDeque</software>, SBA and PBA. We capture the computation time in nanoseconds, corresponding to the time required by an algorithm to compute the aggregation over each window when the ends of windows are reached. We run the experiments for 1 million windows, and report the latency for the last 970 thousand ones.</p><p>Our latency results are presented in Fig. <ref type="figure" target="#fig_7">7</ref>. In Table <ref type="table" target="#tab_4">2</ref> and Table <ref type="table" target="#tab_5">3</ref>, we present 7 statistical summaries for the latency experiments, i.e., Min (minimum), Max (maximum), Avg (average), Std (standard deviation), LQ (lower quartile, i.e., 25% percentile), Med (median), HQ (higher quartile, i.e., 75% percentile). On the whole, we observe that both <software ContextAttributes="created">SlickDeque</software> and PBA do not have latency spikes, and PBA performs better than <software ContextAttributes="created">SlickDeque</software>. We discuss detailed results of all tested algorithms in the sequel.</p><p>As it can be observed, PBA has the overall best latency results in terms of all statistical summaries, especially for the Max and Std metrics shown in Table <ref type="table" target="#tab_4">2</ref> and<ref type="table" target="#tab_5">3</ref>  <software ContextAttributes="created">SlickDeque</software> has almost the same latency as <software ContextAttributes="created">TwoStack</software> for Avg, but much better than <software ContextAttributes="created">TwoStack</software> for Max and Std. As shown in Fig. <ref type="figure" target="#fig_7">7</ref>, <software ContextAttributes="created">TwoStack</software> has latency spikes periodically, and thus <software ContextAttributes="created">TwoStack</software> does not perform well in terms of Max and Std. Such latency spikes become larger when the window size increases. <software ContextAttributes="created">TwoStack</software> requires moving all elements of one stack to another one periodically, and the corresponding cost is linearly increasing w.r.t to the window size. This is why the periodical red spike of <software ContextAttributes="created">TwoStack</software> for the case of 16384 is higher than the corresponding one for the case of 8192, and similar for time-based window with a range of 2 hours compared to a range of 1 hour.</p><p>We also observe that, as aggregation getting more complex, the latency is also getting higher, which can be observed in Table <ref type="table" target="#tab_4">2</ref> and Table <ref type="table" target="#tab_5">3</ref>. As an aggregation defined in the LCL framework can have an arbitrarily complex âŠ• operation, the difference in the latency caused by using a different merging algorithm can be more obvious.</p></div>
<div><head n="5.2">Apache Flink Integration</head><p>We integrate PBA into <software ContextAttributes="created">Apache</software> <software ContextAttributes="created">Flink</software> <ref type="bibr" target="#b1">[3,</ref><ref type="bibr" target="#b3">6]</ref>, denoted as FPBA. PBA is integrated into <software ContextAttributes="created">Flink</software> through KeyedProcessFunction, i.e., a lowlevel operator in <software ContextAttributes="created">Flink</software>. To support parallel tasks of PBA, a thread pool for each task slot in <software ContextAttributes="created">Flink</software> is launched, which will be used to compute the ğ‘™ğ‘ğ‘  buffers for all key-value pairs associated with the corresponding task slot. Our goal in this subsection is to show that PBA can be easily integrated into modern distributed and parallel SPSs, and to evaluate FPBA in both local and cluster modes. In this experiment, we consider 2 10 as the representative of large windows. We compute ğ‘šğ‘ğ‘¥ over count-based sliding windows with a slide of 1 and a window size of 2 10 , and capture the throughput of <software ContextAttributes="created">Flink</software> and FPBA by using a different degree of parallelism in local mode, and a different number of nodes in cluster mode. Datasets: We use the DEBS14 Grand Challenge Smart Home Dataset <ref type="bibr" target="#b13">[16]</ref>, which contains recordings from smart plugs in private households. Each tuple of this dataset includes a power consumption and a plugin ID. The first 10 million tuples contain 14 plugin IDs with a slight data skew, which we made into different sizes with the identical distribution in our experiments, i.e., 60 million tuples with 84 plugin IDs for local execution, and 240 million tuples with 336 plugin IDs for cluster execution.</p><p>Experiment Setup: We run the cluster experiment at a <software>Flink</software> cluster having 1 master node and 6 worker nodes running Ubuntu server 16.01.3 LTS. The master node has 6 virtual CPU cores of Intel XEON 3GHz, and 16GB main memory, and the job manager of <software ContextAttributes="created">Flink</software> is configured with 6GB of memory. Every worker node has 4 virtual CPU cores of Intel XEON 3GHz, and 8GB main memory, and the task manager of <software ContextAttributes="created">Flink</software> is configured with 6GB of memory. We run the local experiment only at the master node of the <software ContextAttributes="created">Flink</software> cluster. We run both local and cluster experiments 10 times, where we consider the first 3 runs as warm-ups, and we repeat the whole execution 3 times. We report the median throughput for <software ContextAttributes="created">Flink</software> and FPBA. We use <software ContextAttributes="created">Flink</software> 1.10.0 and Java 8 in this experiment.</p><p>We present the results in Fig. <ref type="figure" target="#fig_8">8</ref>. In Fig. <ref type="figure" target="#fig_8">8</ref> (a), we observe the throughput of FPBA or <software ContextAttributes="created">Flink</software> linearly increases up to 3 parallel instances (called parallelism in <software ContextAttributes="created">Flink</software>). Each parallel instance of the execution pipeline of FPBA, or <software ContextAttributes="created">Flink</software>, contains 2 tasks, i.e., reading data and computing SWAG. Therefore, up to 3 parallel instances, the task for computing SWAG can run on a dedicated CPU core at a machine with 6 cores. For more parallel instances, the throughput growth becomes less linear compared to the first phase. In Fig. <ref type="figure" target="#fig_8">8</ref> (b), we observe the throughput of FPBA and <software ContextAttributes="created">Flink</software> linearly scale up w.r.t more nodes used in a cluster.</p></div>
<div><head n="6">CONCLUSION</head><p>In this paper, we presented PBA, a novel algorithm that calculates incremental aggregations in parallel for efficiently computing SWAGs, and also its sequential version SBA. We also proposed an approach to optimize the chunk size, which guarantees the minimum latency for PBA. Our experimental results demonstrate that SBA has higher throughput than state-of-the-art algorithms irrespective of window sizes, and PBA shows best performance for large windows (i.e., improving throughput up to 4Ã— while reducing latency). Thus, a simple strategy for obtaining high performance can be using SBA for small windows and PBA for larger ones. In addition, FPBA, an integration of PBA into <software>Apache</software> <software ContextAttributes="created">Flink</software>, shows significant improvement in both local mode and cluster mode.</p><p>As future works, we plan to extend PBA to support both multiquery processing and out-of-order streams. The idea is to replace the array structure of ğ‘™ğ‘ğ‘  buffers with a tree structure, where each leaf node stores a slice aggregation and any intermediate node stores partial aggregation of all its children (e.g., in the spirit of the array-based binary heap implementation of <software>FlatFAT</software>). In such a scenario, the major benefit of using PBA model is that any updates caused by late values can be processed only over ğ‘™ğ‘ğ‘  buffers in parallel, such that the computation for accumulating streaming values will not be delayed. One technical challenge underlying the development of such an approach lies in optimizing the chunk size that ensures the overall minimum latency, while taking into account watermarks and multiple combinations of ranges and slides.</p></div><figure xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Example of computing sum over a count-based sliding window with a range of 10 values and a slide of 2 values.</figDesc><graphic coords="3,39.08,81.21,254.45,62.89" type="bitmap" /></figure>
<figure xml:id="fig_1"><head>A</head><label /><figDesc>sliding window ğ‘Š [ğ‘Ÿ, ğ‘ ], ğ‘Ÿ &gt; ğ‘ , with a range ğ‘Ÿ and a slide ğ‘  is an infinite sequence of window instances (ğ‘¤ 0 , ğ‘¤ 1 , ğ‘¤ 2 , ...). Sliding window aggregation means computing an aggregate function ğ›¼ over each window instance of a sliding windowğ‘Š [ğ‘Ÿ, ğ‘ ], i.e., ğ›¼ (ğ‘Š [ğ‘Ÿ, ğ‘ ]) = (ğ›¼ (ğ‘¤ 0 ), ğ›¼ (ğ‘¤ 1 ), ğ›¼ (ğ‘¤ 2 ), ..</figDesc></figure>
<figure xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Example of computing the count-based ğ‘Š [10, 2]with the aggregation max over a stream using PBA with a chunk size of 5 slices.</figDesc></figure>
<figure xml:id="fig_4"><head>Figure 4 :Figure 5 :</head><label>45</label><figDesc>Figure 4: Throughput for count-based windows by varying the window size and with a fixed slide size.</figDesc></figure>
<figure xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Throughput for time-based windows.</figDesc></figure>
<figure xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Latency experiments.</figDesc></figure>
<figure xml:id="fig_8"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Throughput on top of Apache Flink.</figDesc></figure>
<figure type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Algorithmic Complexities (Non-Invertible SWAG).</figDesc><table /></figure>
<figure type="table" xml:id="tab_3"><head /><label /><figDesc>r.t to the</figDesc><table><row><cell /><cell>Recal</cell><cell>TwoStack</cell><cell>SlickDeque</cell><cell>SBA</cell><cell>PBA</cell></row><row><cell>Million items/s</cell><cell cols="5">Window size 2 3 2 4 2 5 2 6 2 7 2 8 2 9 2 10 2 11 2 12 2 13 2 14 2 15 2 16 2 17 2 18 2 19 2 20 0 50 100 150 200 250 300 50 350 10 20 30 40 Million events/s Window size 0 2 3 2 4 2 5 2 6 2 7 2 8 2 9 2 10 2 11 2 12 2 13 2 14 2 15 2 16 2 17 2 18 2 19 2 20</cell></row><row><cell /><cell>(a) DEBS'12 dataset [15]</cell><cell /><cell /><cell /></row></table></figure>
<figure type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>latency summaries of Max over count-based windows.</figDesc><table><row><cell cols="3">Algo. Range Min</cell><cell>Max</cell><cell>Avg</cell><cell>Std</cell><cell>LQ, Med, HQ</cell></row><row><cell>TS</cell><cell>2 13</cell><cell>57</cell><cell cols="2">74543 69.21</cell><cell>754.3</cell><cell>60, 60, 61</cell></row><row><cell>SD</cell><cell>2 13</cell><cell>57</cell><cell>667</cell><cell>81.77</cell><cell>22.12</cell><cell>64, 79, 88</cell></row><row><cell>SBA</cell><cell>2 13</cell><cell>43</cell><cell cols="3">15160 50.39 233.64</cell><cell>45, 46, 47</cell></row><row><cell>PBA</cell><cell>2 13</cell><cell>40</cell><cell>559</cell><cell>53.93</cell><cell>10.41</cell><cell>45, 48, 65</cell></row><row><cell>TS</cell><cell>2 14</cell><cell>63</cell><cell cols="3">150024 77.55 1077.86</cell><cell>67, 68, 70</cell></row><row><cell>SD</cell><cell>2 14</cell><cell>61</cell><cell>647</cell><cell>83.07</cell><cell>21.86</cell><cell>65, 80, 90</cell></row><row><cell>SBA</cell><cell>2 14</cell><cell>43</cell><cell cols="3">31031 50.06 331.25</cell><cell>45, 46, 47</cell></row><row><cell>PBA</cell><cell>2 14</cell><cell>42</cell><cell>537</cell><cell>46.06</cell><cell>5.24</cell><cell>44, 45, 47</cell></row></table></figure>
<figure type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>latency summaries of MS over time-based windows. PBA shown in Table1. Moreover, PBA shows better latency results with a larger window size, shown in Fig.7, and Table2and 3 (the reason has been discussed in Section 5.1.1). SBA shows similar results as PBA except for Max and Std in Table2 and 3. SBA computes a ğ‘™ğ‘ğ‘  buffer at each boundary, which postpones the computation of a ğ‘ğ‘ ğ‘ buffer until the computation of ğ‘™ğ‘ğ‘  has been completed. Therefore, SBA has a high latency periodically, e.g., in Fig.2, at boundary ğ‘ 1 , SBA needs to firstly compute ğ‘™ğ‘ğ‘  0 , and then ğ‘ğ‘ ğ‘ 1 , and this incurs high latency for ğ‘¤ 1 . Such a high latency of SBA can be observed in Fig.7. Note that, PBA does not have high latency due to computing ğ‘™ğ‘ğ‘  and ğ‘ğ‘ ğ‘ in parallel.</figDesc><table><row><cell cols="3">Algo. Range Min</cell><cell>Max</cell><cell>Avg</cell><cell>Std</cell><cell>LQ, Med, HQ</cell></row><row><cell>TS</cell><cell>1 h</cell><cell cols="5">203 365582 337.65 5348.91 223, 230, 258</cell></row><row><cell>SD</cell><cell>1 h</cell><cell>267</cell><cell cols="2">13638 385.28</cell><cell>101.5</cell><cell>336, 370, 415</cell></row><row><cell>SBA</cell><cell>1 h</cell><cell cols="5">131 103716 194.85 1566.49 160, 163, 166</cell></row><row><cell>PBA</cell><cell>1 h</cell><cell cols="4">129 12837 164.24 73.35</cell><cell>141, 145, 168</cell></row><row><cell>TS</cell><cell>2 h</cell><cell cols="5">207 934212 381.1 9999.85 226, 235, 290</cell></row><row><cell>SD</cell><cell>2 h</cell><cell>266</cell><cell cols="2">13175 392.72</cell><cell>84.86</cell><cell>342, 380, 427</cell></row><row><cell>SBA</cell><cell>2 h</cell><cell cols="5">132 226099 201.77 2471.56 162, 164, 168</cell></row><row><cell>PBA</cell><cell>2 h</cell><cell cols="4">135 12971 171.75 64.59</cell><cell>145, 150, 191</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="7">ACKNOWLEDGMENTS</head><p>This research was financed by the <rs type="funder">French government IDEX-ISITE</rs> initiative <rs type="grantNumber">16-IDEX-0001</rs> (<rs type="grantNumber">CAP 20-25</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_6ADxZTB">
					<idno type="grant-number">16-IDEX-0001</idno>
				</org>
				<org type="funding" xml:id="_xbmX2Rm">
					<idno type="grant-number">CAP 20-25</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">MillWheel: Fault-Tolerant Stream Processing at Internet Scale</title>
		<author>
			<persName><forename type="first">Tyler</forename><surname>Akidau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Balikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaya</forename><surname>BekiroÄŸlu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Slava</forename><surname>Chernyak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josh</forename><surname>Haberman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reuven</forename><surname>Lax</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Mcveety</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Mills</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Nordstrom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Whittle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. VLDB Endow</title>
		<meeting>VLDB Endow</meeting>
		<imprint>
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title />
		<author>
			<persName><forename type="first">Apache</forename><surname>Flink</surname></persName>
		</author>
		<ptr target="https://flink.apache.org" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Resource Sharing in Continuous Sliding-Window Aggregates</title>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Arasu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennifer</forename><surname>Widom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VLDB</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Apache Flink: Stream and Batch Processing in a Single Engine</title>
		<author>
			<persName><forename type="first">Paris</forename><surname>Carbone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Asterios</forename><surname>Katsifodimos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Ewen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seif</forename><surname>Volker Markl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kostas</forename><surname>Haridi</surname></persName>
		</author>
		<author>
			<persName><surname>Tzoumas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Data Eng. Bull</title>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Cutty: Aggregate Sharing for User-Defined Windows</title>
		<author>
			<persName><forename type="first">Paris</forename><surname>Carbone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Traub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Asterios</forename><surname>Katsifodimos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seif</forename><surname>Haridi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Volker</forename><surname>Markl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM '16</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Processing Flows of Information: From Data Stream to Complex Event Processing</title>
		<author>
			<persName><forename type="first">Gianpaolo</forename><surname>Cugola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Margara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Comput. Surv</title>
		<imprint>
			<date type="published" when="2012">2012. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Moment-Based Quantile Sketches for Efficient High Cardinality Aggregation Queries</title>
		<author>
			<persName><forename type="first">Edward</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jialin</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><forename type="middle">Sheng</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vatsal</forename><surname>Sharan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Bailis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. VLDB Endow</title>
		<meeting>VLDB Endow</meeting>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Data Cube: A Relational Aggregation Operator Generalizing Group-By, Cross-Tab, and Sub-Totals</title>
		<author>
			<persName><forename type="first">Jim</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Surajit</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Bosworth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Layman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Don</forename><surname>Reichart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Murali</forename><surname>Venkatrao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Pellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hamid</forename><surname>Pirahesh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Min. Knowl. Discov</title>
		<imprint>
			<date type="published" when="1997">1997. 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Real-Time Tweet Analytics Using Hybrid Hashtags on Twitter Big Data Streams</title>
		<author>
			<persName><forename type="first">Vibhuti</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rattikorn</forename><surname>Hewett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Cloud-Based Data Stream Processing</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Heinze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leonardo</forename><surname>Aniello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leonardo</forename><surname>Querzoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zbigniew</forename><surname>Jerzak</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>DEBS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Sliding-Window Aggregation Algorithms: Tutorial</title>
		<author>
			<persName><forename type="first">Martin</forename><surname>Hirzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kanat</forename><surname>Tangwongsan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>DEBS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A Catalog of Stream Processing Optimizations</title>
		<author>
			<persName><forename type="first">Martin</forename><surname>Hirzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>SoulÃ©</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">BuÄŸra</forename><surname>Gedik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Grimm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Comput. Surv</title>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">The DEBS 2012 Grand Challenge</title>
		<author>
			<persName><forename type="first">Zbigniew</forename><surname>Jerzak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Heinze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Fehr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>GrÃ¶ber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raik</forename><surname>Hartung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nenad</forename><surname>Stojanovic</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>DEBS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">The DEBS 2014 Grand Challenge</title>
		<author>
			<persName><forename type="first">Zbigniew</forename><surname>Jerzak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Holger</forename><surname>Ziekow</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>DEBS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">SABER: Window-Based Hybrid Stream Processing for Heterogeneous Architectures</title>
		<author>
			<persName><forename type="first">Alexandros</forename><surname>Koliousis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Weidlich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raul</forename><forename type="middle">Castro</forename><surname>Fernandez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">L</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paolo</forename><surname>Costa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Pietzuch</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>SIGMOD</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">On-the-Fly Sharing for Streamed Aggregation</title>
		<author>
			<persName><forename type="first">Sailesh</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chung</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Franklin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Twitter Heron: Stream Processing at Scale</title>
		<author>
			<persName><forename type="first">Sanjeev</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikunj</forename><surname>Bhagat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vikas</forename><surname>Kedigehalli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Kellogg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sailesh</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jignesh</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Ramasamy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siddarth</forename><surname>Taneja</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>SIGMOD</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Pane, No Gain: Efficient Evaluation of Sliding-Window Aggregates over Data Streams</title>
		<author>
			<persName><forename type="first">Jin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Maier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristin</forename><surname>Tufte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vassilis</forename><surname>Papadimos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">A</forename><surname>Tucker</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005. 2005</date>
			<publisher>SIGMOD Rec</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Semantics and Evaluation Techniques for Window Aggregates in Data Streams</title>
		<author>
			<persName><forename type="first">Jin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Maier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristin</forename><surname>Tufte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vassilis</forename><surname>Papadimos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">A</forename><surname>Tucker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Parallel Patterns for Window-Based Stateful Operators on Data Streams: An Algorithmic Skeleton Approach</title>
		<author>
			<persName><forename type="first">Gabriele</forename><surname>Mencagli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tiziano</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matteis</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Parallel Programming</title>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">Radko</forename><surname>Mesiar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michel</forename><surname>Grabisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean-Luc</forename><surname>Marichal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Endre</forename><surname>Pap</surname></persName>
		</author>
		<title level="m">Aggregation Functions</title>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A Comprehensive Survey on Parallelization and Elasticity in Stream Processing</title>
		<author>
			<persName><forename type="first">Henriette</forename><surname>RÃ¶ger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruben</forename><surname>Mayer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Comput. Surv</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">FlatFIT: Accelerated Incremental Sliding-Window Aggregation For Real-Time Analytics</title>
		<author>
			<persName><forename type="first">U</forename><surname>Anatoli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Panos</forename><forename type="middle">K</forename><surname>Shein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandros</forename><surname>Chrysanthis</surname></persName>
		</author>
		<author>
			<persName><surname>Labrinidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SSDBM</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">SlickDeque: High Throughput and Low Latency Incremental Sliding-Window Aggregation</title>
		<author>
			<persName><forename type="first">U</forename><surname>Anatoli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Panos</forename><forename type="middle">K</forename><surname>Shein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandros</forename><surname>Chrysanthis</surname></persName>
		</author>
		<author>
			<persName><surname>Labrinidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EDBT</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Low-Latency Sliding-Window Aggregation in Worst-Case Constant Time</title>
		<author>
			<persName><forename type="first">Kanat</forename><surname>Tangwongsan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Hirzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Schneider</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">DEBS</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">General Incremental Sliding-Window Aggregation</title>
		<author>
			<persName><forename type="first">Kanat</forename><surname>Tangwongsan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Hirzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun-Lung</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PVLDB</title>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">SlideSide: A Fast Incremental Stream Processing Algorithm for Multiple Queries</title>
		<author>
			<persName><forename type="first">Georgios</forename><surname>Theodorakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Pietzuch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Holger</forename><surname>Pirk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EDBT</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Storm@twitter</title>
		<author>
			<persName><forename type="first">Ankit</forename><surname>Toshniwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siddarth</forename><surname>Taneja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amit</forename><surname>Shukla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Ramasamy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jignesh</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjeev</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Jackson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krishna</forename><surname>Gade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jake</forename><surname>Donham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikunj</forename><surname>Bhagat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sailesh</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitriy</forename><surname>Ryaboy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Efficient Window Aggregation with General Stream Slicing</title>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Traub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Philipp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alejandro</forename><forename type="middle">Rodriguez</forename><surname>Grulich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Cuellar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Asterios</forename><surname>BreÃŸ</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tilmann</forename><surname>Katsifodimos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Volker</forename><surname>Rabl</surname></persName>
		</author>
		<author>
			<persName><surname>Markl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EDBT</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Discretized Streams: Fault-Tolerant Streaming Computation at Scale</title>
		<author>
			<persName><forename type="first">Matei</forename><surname>Zaharia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tathagata</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Hunter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Shenker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ion</forename><surname>Stoica</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SOSP</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Analyzing Efficient Stream Processing on Modern Hardware</title>
		<author>
			<persName><forename type="first">Steffen</forename><surname>Zeuch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bonaventura</forename><surname>Del Monte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeyhun</forename><surname>Karimov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clemens</forename><surname>Lutz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manuel</forename><surname>Renz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Traub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>BreÃŸ</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tilmann</forename><surname>Rabl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Volker</forename><surname>Markl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. VLDB Endow</title>
		<meeting>VLDB Endow</meeting>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Revisiting the Design of Data Stream Processing Systems on Multi-Core Processors</title>
		<author>
			<persName><forename type="first">Shuhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bingsheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Dahlmeier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amelie</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Chi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Heinze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDE</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>