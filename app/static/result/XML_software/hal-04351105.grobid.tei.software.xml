<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="fr">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">À quoi sert la spécialisation en évolution culturelle de la connaissance?</title>
				<funder ref="#_Q5MYvTj">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Andreas</forename><surname>Kalaitzakis</surname></persName>
							<email>andreas.kalaitzakis@inria.fr</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Univ. Grenoble Alpes</orgName>
								<orgName type="institution" key="instit2">Inria</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<orgName type="institution" key="instit4">Grenoble INP</orgName>
								<orgName type="institution" key="instit5">LIG</orgName>
								<address>
									<postCode>F-38000</postCode>
									<settlement>Grenoble</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jérôme</forename><surname>Euzenat</surname></persName>
							<email>jerome.euzenat@inria.fr</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Univ. Grenoble Alpes</orgName>
								<orgName type="institution" key="instit2">Inria</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<orgName type="institution" key="instit4">Grenoble INP</orgName>
								<orgName type="institution" key="instit5">LIG</orgName>
								<address>
									<postCode>F-38000</postCode>
									<settlement>Grenoble</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">À quoi sert la spécialisation en évolution culturelle de la connaissance?</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">04093F961FA382BCB40BB6CDBED89A2C</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-04-12T14:51+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Évolution culturelle</term>
					<term>Spécialisation Cultural evolution</term>
					<term>specialization</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Des agents peuvent faire évoluer leurs ontologies en accomplissant conjointement une tâche. Nous considérons un ensemble de tâches dont chaque agent ne considère qu'une partie. Nous supposons que moins un agent considère de tâches, plus la précision de sa meilleure tâche sera élevée. Pour le vérifier, nous simulons différentes populations considérant un nombre de tâches croissant. De manière contre-intuitive, l'hypothèse n'est pas vérifiée. D'une part, lorsque les agents ont une mémoire illimitée, plus un agent considère de tâches, plus il est précis. D'autre part, lorsque les agents ont une mémoire limitée, les objectifs de maximiser la précision de leur meilleures tâches et de s'accorder entre eux sont mutuellement exclusifs. Lorsque les sociétés favorisent la spécialisation, les agents n'améliorent pas leur précision. Cependant, ces agents décideront plus souvent en fonction de leurs meilleures tâches, améliorant ainsi la performance de leur société.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="fr">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>En communiquant entre eux, des agents font évoluer leurs représentations des connaissances, un phénomène qui se manifeste par un changement de comportement. Ce phénomène est étudié dans le cadre de l'évolution culturelle des connaissances, où les agents acquièrent et font évoluer leurs connaissances, en fonction de leur perception et du retour qu'ils reçoivent des autres agents. Ici, nous modélisons des agents qui représentent leurs connaissances en utilisant des ontologies. Jusqu'à présent, la plupart des travaux sur l'évolution culturelle des connaissances se concentrent sur des agents qui (1) font évoluer leurs connaissances dans des environnements mono-tâche et <ref type="bibr" target="#b1">(2)</ref>, disposent de ressources illimitées. <ref type="bibr" target="#b14">[15]</ref> propose un jeu d'alignement itératif, permettant aux agents de classer progressivement les nombres dans des ensembles identiques. Ainsi, à la fin de l'expérience, les agents partagent la même représentation des connaissances. Dans <ref type="bibr" target="#b1">[2]</ref>, les agents interagissent sur une seule tâche de décision. Après un nombre fini d'interactions, les agents se mettent d'accord sur toutes les décisions, mais pas nécessairement sur la même base.</p><p>Cependant, notre monde n'est pas monolithique et les ressources des agents ne sont pas illimitées. Les agents doivent être capables d'utiliser des ressources limitées afin d'effectuer plusieurs tâches de manière efficace. Pourtant, on sait peu de choses sur la façon dont la réalisation de plusieurs tâches affecte l'évolution culturelle des connaissances. On en sait encore moins sur les agents encouragés à se spécialiser dans des environnements multitâches. Peuvent-ils se spécialiser tout en restant en accord les uns avec les autres ? Les sociétés d'agents bénéficient-elles de la spécialisation des agents ? Nous abordons ces questions en étendant le cadre introduit dans <ref type="bibr" target="#b1">[2]</ref>. Dans ce cadre, les agents effectuent la même tâche consistant à prendre une décision abstraite dans un domaine abstrait. Dans notre cadre, les agents sont affectés à une ou plusieurs tâches et sont donc capables de prendre des décisions dans plusieurs domaines abstraits. Notre hypothèse principale est que les agents qui entreprennent moins de tâches atteignent une précision plus élevée sur leurs meilleures tâches, que les agents qui entreprennent toutes les tâches. Nous supposons qu'en interagissant sur un ensemble limité de tâches, les agents seront plus précis dans certaines tâches que dans d'autres.</p><p>Nous organisons notre travail en deux expériences. Dans la première expérience <ref type="bibr" target="#b7">[8]</ref>, des agents disposant d'une mémoire illimitée entreprennent un ensemble limité de tâches, que nous appellerons par la suite le scope. Dans notre travail, la prise de décisions pour chaque tâche repose sur différentes propriétés. Par exemple, décider de prendre un parapluie peut reposer sur la propriété humidité alors que décider de porter un t-shirt peut reposer sur la propriété température. Dans la deuxième expérience <ref type="bibr" target="#b8">[9]</ref>, les agents entreprennent les mêmes tâches, tout en faisant face à des limitations de mémoire. À cette fin, nous fixons un nombre maximum de classes à maintenir dans l'ontologie d'un agent. Lorsque les agents atteignent ces limites, ils essaient d'oublier des connaissances qui ne sont pas pertinentes pour leur scope et rendent ainsi l'espace à nouveau disponible. Nous évaluons les ontologies des agents par leur contribution à (1) la promotion de l'accord entre les agents et (2) la prise de décisions correctes pour différentes tâches.</p><p>Les entités qui constituent l'environnement, ainsi que la notation qui le décrit, sont présentés dans le §3. La section 4 présente les grandes lignes de l'expérience. La section 5 présente un ensemble d'hypothèses ainsi que le protocole utilisé pour les tester. Les résultats des tests sont présentés dans la section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">État de l'art</head><p>Il a été montré que les jeux référentiels <ref type="bibr" target="#b10">[11]</ref>, facilitent l'établissement de protocoles de communication entre des agents communicants qui tentent d'obtenir une rétribution partagée. <ref type="bibr" target="#b12">[13]</ref> soutient qu'un protocole de communication émerge lorsque les agents tentent de minimiser la complexité computationnelle de l'interprétation sémantique. <ref type="bibr" target="#b6">[7]</ref> et <ref type="bibr" target="#b2">[3]</ref> présentent différents cadres permettant l'émergence d'un langage partagé. <ref type="bibr" target="#b6">[7]</ref> étudie un cadre où deux agents développent un langage dans le but de réussir dans un jeu référentiel. <ref type="bibr" target="#b2">[3]</ref> propose un cadre permettant la transmission intra-générationnelle, en se concentrant sur la compositionnalité de la langue émergente.</p><p>[6] montre que la transmission culturelle implicite conduit à une plus grande compositionnalité de la langue. Notre travail repose également sur les agents qui essaient de se mettre d'accord entre eux. Cependant, les travaux présentés ici se concentrent sur les caractéristiques du protocole de communication émergé. Bien que des caractéristiques similaires soient également présentes dans notre travail, nous ne nous concentrons pas sur le protocole de communication mais plutôt sur l'impact de l'accord des agents sur la réalisation de plusieurs tâches.</p><p>Il a été montré que l'apprentissage multitâche améliore considérablement la classification dans différents domaines, par exemple, la robustesse des adversaires <ref type="bibr" target="#b11">[12]</ref>, la similarité visuelle interconcept <ref type="bibr" target="#b3">[4]</ref>, l'apprentissage du phénotype <ref type="bibr" target="#b4">[5]</ref>. Les agents ont également été utilisés pour étudier l'impact de l'apprentissage multitâche sur les protocoles de communication émergents. Dans <ref type="bibr" target="#b13">[14]</ref>, l'apprentissage par renforcement multiagent coopératif est considéré. La littérature présentée dans ce paragraphe est liée à notre travail car elle considère des agents qui effectuent plusieurs tâches. Cependant, tout changement dans le comportement et les connaissances de ces agents n'est pas le résultat d'une évolution culturelle.</p><p>L'évolution culturelle des connaissances a été étudiée dans <ref type="bibr" target="#b1">[2]</ref> et <ref type="bibr" target="#b14">[15]</ref>. Dans <ref type="bibr" target="#b14">[15]</ref>, les auteurs cherchent à comprendre comment les concepts sont organisés et comment un comportement collectif peut être établi de manière autonome. Ils montrent qu'il existe un large éventail de conditions sous lesquelles un consensus collectif se produit. Dans <ref type="bibr" target="#b1">[2]</ref>, une expérience en deux étapes est utilisée où les agents apprennent d'abord un classificateur et interagissent ensuite par paires. Grâce à un mécanisme d'adaptation, il est montré que les agents parviennent à une meilleure connaissance, sans nécessairement aboutir à des ontologies identiques. Nous nous différencions de ceux-ci en introduisant des agents qui font évoluer leurs connaissances par rapport à des tâches multiples.</p><p>Enfin, il est à souligner qu'aucun des travaux présentés dans cette section n'impose de limites aux agents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Cadre expérimental</head><p>Nous étendons <ref type="bibr" target="#b1">[2]</ref> en introduisant des agents qui entreprennent plusieurs tâches. Ici nous fournissons des définitions préliminaires concernant les agents, les différentes tâches, les objets, ainsi que leur environnement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Environnement</head><p>Les agents évoluent dans un environnement peuplé d'objets décrits par un ensemble P de propriétés booléennes. Les objets sont donc décrits par la présence ou l'absence d'une propriété p ∈ P, dénotée par p et ¬p respectivement. Il n'existe donc que 2 |P| types d'objets, qui sont rassemblés dans un ensemble I.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Tâches</head><p>Le terme tâche fait référence à un travail effectué par un agent. Ici, nous nous concentrerons sur un ensemble de tâches de décision : prendre une décision abstraite dans un domaine abstrait, c'est-à-dire choisir parmi d 1 , d 2 . . . d n . Il peut y avoir différentes tâches t ∈ T associées à un ensemble différent de décisions possibles D t . Dans ce contexte, chaque objet o peut être considéré par rapport à toute tâche t ∈ T . Une fonction h * (o, t) → D t fournit la décision correcte, inconnue des agents, pour un objet o par rapport à une tâche t.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Agents</head><p>Les agents sont des entités autonomes, coexistantes, capables de percevoir et de distinguer des objets en fonction de leurs propriétés. Dans ce contexte, une société d'agents multitâches A, réalise de multiples tâches. À cette fin, les agents construisent et font évoluer des connaissances sous la forme d'ontologies, privées à chaque agent, exprimées en ALC <ref type="bibr" target="#b0">[1]</ref>. Chaque agent α utilise ses connaissances pour calculer une fonction h α (o, t) → D t qui, étant donné un objet o et une tâche t, fournit une décision h α (o, t). La figure 1 montre un exemple de connaissance multi-tâche construite par un agent α. La partie inférieure représente l'ontologie privée O α de l'agent α, lui permettant de classifier les objets de l'environnement. La partie supérieure présente deux ontologies de décision, chacune contenant les décisions valides pour les tâches t 1 et t 2 . Étant donné qu'un agent α apprend au plus une décision pour un objet o et une tâche t, chaque feuille de O α ne peut être alignée plus d'une fois avec la même ontologie de décision. </p><formula xml:id="formula_0">D t 1 d 1 d 2 d 3 ⊑ ⊑ ⊑ ⊕ ⊕ ⊕ D t 2 d 1 d 2 d 3 ⊑ ⊑ ⊑ ⊕ ⊕ ⊕ O α ¬p 1 p 2 p 1 ⊓ ¬p 2 p 1 ⊓ p 2 ⊑ ⊑ ⊑ ⊑ ⊑ ⊑ ⊑ ⊑ Figure (1)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Définition du scope</head><p>Nous implémentons ici des agents qui accomplissent différents sous-ensembles de tâches, en donnant la priorité à certaines tâches par rap- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Adaptation</head><p>Nous étendons le mécanisme d'adaptation présenté dans <ref type="bibr" target="#b1">[2]</ref>, en introduisant l'adaptation en présence de plusieurs tâches (figure <ref type="figure">5</ref>). Un agent peut soit remplacer une décision existante, soit diviser une classe en 2 sous-classes. L'agent le fait sur la base d'une propriété qui permet de distinguer l'objet actuel des objets classés par la classe à diviser. Seules les décisions concernant la tâche en cours sont affectées.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Adaptation d'ontologie</head><p>1: α demande à β la définition de la classe classant l'objet o.</p><formula xml:id="formula_1">2: β répond avec C β . 3: if C α ̸ ⊑ C β then 4:</formula><p>α demande à β sa décision h β (o, t).</p><p>5:</p><formula xml:id="formula_2">β répond avec h β (o, t) 6:</formula><p>une propriété p : p ∈ C β , p / ∈ C α est sélectionnée.</p><p>7:</p><formula xml:id="formula_3">α divise C α en C α 1 ≡ C α ⊓ ¬p et C α 2 ≡ C α ⊓ p 8:</formula><p>α associe C α 1 à toutes les décisions précédemment associées à C α . 9:</p><p>if Aucune décision concernant t n'est associée à C α . then 10:</p><p>α associe C α à la décision h β (o, t).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>11:</head><p>α associe C α 2 à la décision h β (o, t). 12: else 13:</p><formula xml:id="formula_4">C α = C α ⊓ p 14:</formula><p>Associer C α à toutes les décisions précédemment associées à</p><formula xml:id="formula_5">C β . Object p 1 p 2 p 3 p 4 Tâche Décision o 1 1 1 0 0 t 1 d 1 o 2 1 0 0 1 t 2 d 1 o 3 0 1 0 1 t 1 d 2 o 4 0 0 1 0 t 2 d 2 o 5 1 1 0 1 t 1 d 1 o 6 1 1 0 0 t 2 d 1 o 7 0 1 1 0 t 1 d 2 o 8 1 0 1 1 t 2 d 2 =⇒ p 1 d t1 2 d t2 2 o 7 o 4 o 3 F a l s e p 3 d t1 1 d t2 1 o 1 o 2 o 5 o 6 F a l s e d t2 2 o 8 T r u e T r u e =⇒ D t1 d 1 d 2 d 3 ⊑ ⊑ ⊑ ⊕ ⊕ ⊕ D t2 d 1 d 2 d 3 ⊑ ⊑ ⊑ ⊕ ⊕ ⊕ O α ¬p 1 p 2 p 1 ⊓ ¬p 2 p 1 ⊓ p 2 ⊑ ⊑ ⊑ ⊑ ⊑ ⊑ ⊑ ⊑ Figure (</formula><p>3) Vue d'ensemble de la méthode d'apprentissage proposée. Les agents commencent par induire un arbre de décision basé sur un ensemble d'exemples étiquetés. Cet arbre de décision est ensuite converti en une ontologie.</p><formula xml:id="formula_6">D t 1 d 1 d 2 d 3 ⊑ ⊑ ⊑ ⊕ ⊕ ⊕ T 2 d 1 d 2 d 3 ⊑ ⊑ ⊑ ⊕ ⊕ ⊕ O α ¬p 4 p 1 p 4 ⊓ ¬p 1 p 4 ⊓ p 1 ⊑ ⊑ ⊑ ⊑ ⊑ ⊑ ⊑ ⊑ ⊑ =⇒ D t 1 d 1 d 2 d 3 ⊑ ⊑ ⊑ ⊕ ⊕ ⊕ D t 2 d 1 d 2 d 3 ⊑ ⊑ ⊑ ⊕ ⊕ ⊕ O α ¬p 4 p 4 ⊑ ⊑ ⊑ ⊑ ⊑ Figure<label>(4</label></formula><p>) Exemple de libération de mémoire par généralisation lors d'une interaction par rapport à la tâche t 2 . Soit la tâche t 2 reposant sur l'ensemble de propriétés P t2 . La propriété p 1 / ∈ P t2 , donc p 1 ne permet pas de distinguer différentes décisions pour la tâche t 2 . Dans cet exemple, l'agent a associé la même décision (en rouge), à la fois à p 4 ⊓ ¬p 1 et à p 4 ⊓ p 1 . Ces deux classes peuvent être fusionnées sans aucune perte de connaissance par rapport à t 2 , comme on le voit à droite.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Protocole</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Hypothèses</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Nous testons les hypothèses suivantes :</head><p>• Hypothèse 1 Les agents convergent sur leurs décisions après un nombre fini d'interactions.</p><p>• Hypothèse 2 Les agents qui entreprennent moins de tâches atteindront une précision plus élevée sur leur meilleurs tâches, que les agents qui entreprennent toutes les tâches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Paramètres</head><p>Nous    montre qu'une population d'agents en interaction apprendra à se mettre d'accord sur toutes leurs décisions, indépendamment de la taille de leur scope. Ceci est justifié par le fait que les agents représentés ici ne sont confrontés à aucune limitation. Par conséquent, ils adopteront progressivement les mêmes propriétés dans leurs ontologies, même si leurs classes feuilles ne coïncident pas. Nos observations confirment les résultats qui ont été présentés précédemment dans <ref type="bibr" target="#b1">[2]</ref>. De plus, elles indiquent que la taille du scope a un impact sur le taux de réussite obtenu. Plus la taille du scope est grande, plus il faut d'interactions pour se mettre d'accord sur tout, donc plus le taux de réussite moyen à la convergence est faible. La figure 6b montre que le taux de réussite se stabilise, sans toutefois converger à 1. Cela indique que soit les agents continuent à adapter leurs ontologies, soit leurs ontologies finales ne leur permettent pas de s'accorder sur toutes les décisions. Dans la section 4.4, nous avons supposé que si elle est suffisamment restreinte, une ontologie sera capable de contenir uniquement les propriétés requises pour être précis sur une seule tâche. Par conséquent, il faut s'attendre à ce que les agents interagissant sur un nombre de tâches nécessitant plus de mémoire, ne pourront pas se mettre d'accord sur toutes les décisions. Il est montré que cela est vrai même pour les agents qui interagissent sur une seule tâche. Nous ne pouvons donc soutenir l'hypothèse 1 que pour les agents qui ne sont confrontés à aucune limitation. Cependant, nous observons que plus le scope des agents est réduit, plus leur taux de réussite est élevé. En raison de l'algorithme d'induction de l'ontologie initiale, l'ontologie d'un agent qui entreprend la tâche t peut contenir des propriétés qui n'appartiennent pas à P t . Rien ne garantit que les agents parviendront à remplacer ces propriétés par des propriétés appartenant à P t . Deux cas distincts peuvent être envisagés. Dans le premier cas, un agent éliminera progressivement toutes les propriétés qui ne sont pas liées à la tâche qu'il entreprend. Cet agent va potentiellement apprendre une ontologie qui lui permettra de prendre des décisions correctes par rapport à tous les types d'objets rencontrés. Dans le second cas, un agent ne parviendra pas à éliminer toutes les propriétés qui ne sont pas liées à la tâche qu'il entreprend. Les agents qui relèvent du deuxième cas remplaceront de manière répétée les propriétés qui appartiennent à P t par des propriétés différentes qui appartiennent également à P t . Par conséquent, ces agents sont capables de prendre des décisions correctes pour différents sous-ensembles de types d'objets existants à un moment donné.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Résultats et discussion</head><p>Précision des ontologies. La figure <ref type="figure">7</ref> représente l'évolution de la précision moyenne pour différentes tailles du scope des agents. Dans la section 1, nous supposons que les agents interagissant sur un scope limité de tâches seront plus précis sur certaines tâches que sur d'autres, au détriment de leur précision moyenne. Cependant, nous ne l'observons dans aucune des deux sous-figures de la figure <ref type="figure">7</ref>. 7a montre que la réalisation de tâches supplémentaires améliore significativement la précision moyenne des agents. 7b montre que la taille du scope des agents a une incidence minimale sur leur précision moyenne. Une fois de plus, cela se justifie par les limitations appliquées. Deux cas peuvent être distingués. Dans le premier cas, un agent devient très   précis dans une tâche et nettement moins précis dans les autres. Dans le second cas, un agent apprend une ontologie lui permettant de devenir moyennement précis sur plusieurs tâches. Cela se traduit par des agents qui démontrent des précisions moyennes qui sont statistiquement indiscernables les unes des autres, indépendamment de la taille de leur scope.</p><p>La figure 8 montre l'évolution de la précision moyenne sur les tâches que les agents entreprennent pour différentes tailles de leur scope. Elle montre que plus le scope des agents est petit, plus leur précision moyenne sur les tâches effectuées est élevée. Ce résultat est attendu pour la raison suivante. Un agent dans la configuration examinée peut devenir très précis sur une tâche au maximum. Cependant, plus le scope des agents est grand, plus le nombre de tâches sur la base desquelles la précision moyenne des tâches effectuées est calculée est élevé. Par conséquent, les agents qui s'attaquent à une seule tâche font preuve d'une précision moyenne sur les tâches effectuées supérieure. La figure <ref type="figure">9</ref> dépeint l'évolution de la précision sur la meilleure tâche pour différentes tailles du scope des agents. En examinant 9a, deux observations peuvent être tirées. La première observation est que la précision moyenne des agents à mémoire illimitée (figure <ref type="figure">7a</ref>) est toujours inférieure à la précision moyenne sur la meilleure tâche des mêmes agents montrés ici. Ces agents sont donc capables de se spécialiser en restreignant le scope de leurs tâches. La deuxième observation est que les agents qui s'attaquent à moins de tâches, ont une précision moyenne sur leur meilleure tâche plus faible que les agents qui s'attaquent à toutes les tâches. Plus précisément, plus le scope des agents est restreint, plus cette précision est faible. Ainsi, nous pouvons conclure que la spécialisation des agents disposant d'une mémoire illimitée n'apporte aucun avantage en termes de précision. Dans la configuration examinée, chaque configuration dépend de propriétés différentes. Par conséquent, notre observation n'est pas liée à la transférabilité des connaissances d'une tâche à l'autre, puisque l'apprentissage de la décision par rapport à une tâche n'est pas lié à l'apprentissage de la décision pour une autre tâche. Au contraire, elle est justifiée par le fait que les agents s'attaquant à toutes les tâches construisent des ontologies plus complètes et par conséquent associent les décisions apprises à une classification plus détaillée. La figure <ref type="figure">9b</ref>  liée à la taille de leur scope, mais plutôt aux limites de la mémoire. Les agents deviendront très précis au maximum sur une tâche, qu'ils s'attaquent à une ou plusieurs tâches. En d'autres termes, le fait que les agents puissent s'abstenir de toute interaction, sauf celles qui concernent la seule tâche qu'ils entreprennent, ne leur permet pas d'améliorer leur précision sur leur meilleure tâche. Nous avons vérifié que cela est également vrai pour des capacités de mémoire plus élevées (8 et 12 classes respectivement). L'hypothèse 2 est donc soutenue uniquement pour des agents qui ne sont confrontés à aucune limitation. La figure 10 représente l'évolution du taux moyen de décisions correctes pour différentes tailles du scope des agents. Jusqu'à présent, les résultats montrent que les agents confrontés à des limitations de mémoire (1) spécialisent leurs connaissances indépendamment de la taille de leur scope et (2) leur précision moyenne ne dépend pas du nombre de tâches qu'ils entreprennent. La figure 10 nous permet d'examiner si les sociétés d'agents bénéficient d'un scope réduit. Les résultats montrent que plus ce scope est réduit, plus le taux de décisions correctes pour une société est élevé. En d'autres termes, si la modification de la taille du scope des agents ne les rend ni moins, ni plus spécialisés, elle permet aux agents de prendre des décisions pour les tâches qu'ils maîtrisent mieux.</p><p>Analyse statistique. Notre analyse de variance (one-way ANOVA) montre que la taille du scope a un impact statistiquement significatif (p ≤ 0.01) sur (1) le taux de réussite, (2) la précision moyenne sur les tâches effectuées et (3) le taux moyen de décisions correctes. La taille du scope n'a pas d'impact statistiquement significatif (p &gt; 0.01) sur (1) la précision sur leur meilleure tâche et (2), la précision moyenne sur toutes les tâches existantes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions</head><p>Ici nous examinons si les sociétés d'agents bénéficient de la spécialisation. Nous proposons une expérience où les agents font évoluer des ontologies en interagissant sur un ensemble limité de tâches. En exploitant ce cadre, nous montrons que les agents à mémoire limitée spécialiseront leurs connaissances quel que soit le nombre de tâches qu'ils effectuent. Cependant, nos résultats montrent qu'en assignant différentes tâches à différents agents, les sociétés améliorent leur taux de décision correcte. Jusqu'à présent, nous n'avons examiné que les sociétés homogènes, c'est-à-dire les sociétés où tous les agents effectuent le même nombre de tâches. Ce travail peut être un tremplin vers l'exploration de sociétés d'agents plus complexes. Par exemple, il serait intéressant d'examiner des sociétés d'agents où des agents multitâches coexistent avec des agents qui se concentrent sur des tâches spécifiques.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Nous présentons et discutons ici les résultats obtenus dans le cadre des deux expériences décrites précédemment. Chacune des figures 6, 7 et 9 est divisée en deux sous-figures. La sous-figure (a) correspond aux résultats acquis lors de l'expérience 1, tandis que la sous-figure (b) correspond aux résultats acquis lors de l'expérience 2. Puisque les agents qui ne font face à aucune limitation ne bénéficient pas de la spécialisation, les autres figures concernent uniquement les résultats de l'expérience 2.Interopérabilité inter-agent. La figure 6 montre l'évolution du taux de réussite moyen pour différentes tailles du scope. La figure6a</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Success rate depending on the maximum adapting rank |maxAdaptingRank| = {1,2,3}. Agents adapt with respect to one task.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Success rate depending on the maximum adapting rank |maxAdaptingRank| = {1,2,3}. Agents adapt with respect to one task.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Average accuracy depending on the maximum adapting rank |maxAdaptingRank| = {0,1,2}. Agents adapt with respect to one task.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 : 5 :Figure ( 8 )</head><label>358</label><figDesc>Figure 3: Average accuracy depending on the maximum adapting rank |maxAdaptingRank| = {0,1,2}. Agents adapt with respect to one tas</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :Figure 4 :Figure ( 9 ) 6 :Figure ( 10 )</head><label>449610</label><figDesc>Figure 4: Maximum accuracy depending on the maximum adapting rank |maxAdaptingRank| = {0,1,2}. Agents adapt with respect to one task.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Exemple d'une ontologie multitâche pour un agent α. Chaque couleur représente une décision différente. Le symbole ⊕ indique que deux décisions sont disjointes.</figDesc><table /><note><p><p><p><p><p><p><p><p>4 Aperçu de l'expérience</p>L'expérience est initialisée par une phase d'apprentissage, à la fin de laquelle chaque agent a appris une ontologie privée. Une fois leurs ontologies apprises, les agents passent par un nombre fixe d'interactions. En fonction du résultat d'une interaction, un agent peut adapter son ontologie. La figure 2 illustre le déroulement de l'expérience du point de vue de l'agent α. De plus amples détails sur la façon dont les agents apprennent, définissent leur scope, interagissent, oublient et adaptent leurs ontologies sont présentés dans les sous-sections 4.1, 4.2, 4.3, 4.4 et 4.5 respectivement.</p>4.1 Apprentissage initial</p>Nous abordons l'apprentissage multitâche comme un problème d'induction d'une ontologie capable de fournir une décision pour toute tâche t ∈ T . L'induction d'une telle ontologie peut être vue dans la figure</p>3</p>. Nous observons qu'il est possible pour un agent d'être capable de classifier un objet mais d'être incapable de prendre une décision concernant une ou plusieurs tâches. Par exemple, étant donné l'ontologie de la figure</p>1</p>et un objet décrit par p 1 ⊓ p 2 , il est impossible de prendre une décision par rapport à la tâche t 1 .</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Lorsque les agents sont en désaccord, la décision prise en compte est celle de l'agent ayant le score le plus élevé parmi les deux agents en interaction.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">D Hunting</cell><cell></cell><cell></cell><cell>D Cooking</cell></row><row><cell></cell><cell></cell><cell>⊑</cell><cell></cell><cell>⊑</cell><cell>⊑</cell><cell>⊑</cell></row><row><cell></cell><cell cols="2">Hunt</cell><cell>⊕</cell><cell>Avoid</cell><cell>Discard</cell><cell>⊕</cell><cell>Cook</cell></row><row><cell></cell><cell>O α</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>O β</cell></row><row><cell></cell><cell>⊑</cell><cell>⊑</cell><cell></cell><cell></cell><cell></cell><cell>⊑</cell><cell>⊑</cell></row><row><cell cols="2">¬nails</cell><cell cols="2">nails</cell><cell></cell><cell></cell><cell>¬poison</cell><cell>poison</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Rabbit</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">nails, ¬poison</cell></row><row><cell></cell><cell></cell><cell cols="2">D Hunting</cell><cell></cell><cell></cell><cell>D Cooking</cell></row><row><cell></cell><cell></cell><cell>⊑</cell><cell></cell><cell>⊑</cell><cell>⊑</cell><cell>⊑</cell></row><row><cell></cell><cell cols="2">Hunt</cell><cell>⊕</cell><cell>Avoid</cell><cell>Discard</cell><cell>⊕</cell><cell>Cook</cell></row><row><cell>O α</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>⊑</cell><cell>⊑</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>O β</cell></row><row><cell>¬nails</cell><cell>nails</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>⊑</cell><cell>⊑</cell></row><row><cell>⊑</cell><cell>⊑</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>¬poison</cell><cell>poison</cell></row><row><cell cols="4">nails ⊓ ¬poison nails ⊓ poison</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Rabbit</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Une exécution consiste en 80000 interactions, nails, ¬poison</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">chaque interaction ayant lieu entre deux agents</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">choisis au hasard parmi 18 agents. Leur envi-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">ronnement contient 64 types d'objets différents,</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">chacun étant perceptible à travers 6 propriétés.</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Les agents sont initialement formés à toutes les que leur précision moyenne sur toutes les tâches tâches |T | = {3}, chaque tâche faisant appel à 2 des 6 propriétés. L'induction de l'ontologie existantes ou entreprises.</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">initiale est basée sur un échantillon aléatoire de</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">10 % de tous les exemples étiquetés existants.</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Pour chaque tâche, il existe 4 décisions diffé-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">rentes. L'évaluation du score entre deux interac-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">tions consécutives se base sur les 60 % de tous</cell></row><row><cell cols="4">La précision de la tâche (tacc) évalue la qua-</cell><cell cols="3">les exemples étiquetés existants.</cell></row><row><cell cols="4">lité de la représentation des connaissances d'un</cell><cell></cell><cell></cell></row><row><cell cols="4">agent. Elle adapte la mesure de précision in-troduite dans [2] à différentes tâches. Elle est</cell><cell cols="3">5.3 Mesures</cell></row><row><cell cols="4">réalisons deux expériences. Dans l'expé-rience 1, les 18 agents ne sont soumis à aucune limitation de mémoire. Dans l'expérience 2, les ontologies des 18 agents sont limitées à un maxi-mum de 4 classes feuilles. Chaque expérience est définie comme la proportion de types d'objets pour lesquels une décision correcte serait prise par rapport à une tâche t, par un agent α à la n e itération de l'expérience.</cell><cell cols="3">Le taux de réussite (srate), tel qu'introduit dans [2] évalue l'interopérabilité entre les agents. Il est défini comme la proportion d'interactions réussies, sur toutes les interactions réalisées jus-</cell></row><row><cell cols="4">exécutée sous 3 configurations, correspondant à des agents qui entreprennent 1 à 3 tâches respec-|I|</cell><cell></cell><cell></cell></row><row><cell cols="4">tivement. Chaque configuration est exécutée 20 tacc est utilisé pour mesurer la précision</cell><cell></cell><cell></cell></row><row><cell cols="4">fois et la moyenne de ses résultats est calculée. moyenne sur la meilleure tâche des agents, ainsi</cell><cell></cell><cell></cell></row></table><note><p><p>qu'à la n ème interaction.</p>Le taux de décisions correctes (cdrate) évalue la performance d'une société d'agents. Il est défini comme la proportion de décisions correctes Figure (5) Suite à un échec de communication, l'agent α divise la classe nails en 2 sous-classes en effectuant une division supplémentaire sur la propriété poison. La 1ère sous-classe, nails ⊓ poison sera associée à la même décision que celle de l'agent β (Hunt). La 2ème décision sera associée à la décision précédemment associée à la classe nails (Avoid). prises par une société d'agents, sur l'ensemble des interactions réalisées jusqu'à la n ème interaction. Lorsque les agents sont d'accord, cette décision est celle prise par les deux agents. tacc(α, n, t) = |{o ∈ I : h α n (o, t) = h * (o, t)}|</p></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Remerciements</head><p>Ce travail a <rs type="projectName">été partiellement supporté par la chaire MIAI "Knowledge communication and evolution</rs>" (<rs type="grantNumber">ANR-19-P3IA-0003</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_Q5MYvTj">
					<idno type="grant-number">ANR-19-P3IA-0003</idno>
					<orgName type="project" subtype="full">été partiellement supporté par la chaire MIAI &quot;Knowledge communication and evolution</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m">The Description Logic Handbook : Theory, Implementation, and Applications</title>
		<editor>
			<persName><forename type="first">Franz</forename><surname>Baader</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Diego</forename><surname>Calvanese</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Deborah</forename><surname>Mcguinness</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Daniele</forename><surname>Nardi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Peter</forename><forename type="middle">F</forename><surname>Patel-Schneider</surname></persName>
		</editor>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Knowledge improvement and diversity under interaction-driven adaptation of learned ontologies</title>
		<author>
			<persName><forename type="first">Yasser</forename><surname>Bourahla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manuel</forename><surname>Atencia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jérôme</forename><surname>Euzenat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 20 th ACM international conference on Autonomous Agents and Multi-Agent Systems (AAMAS)</title>
		<meeting>20 th ACM international conference on Autonomous Agents and Multi-Agent Systems (AAMAS)<address><addrLine>London, United Kingdom</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="242" to="250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Emergence of compositional language with deep generational transmission</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.09067</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Integrating concept ontology and multitask learning to achieve more effective classifier training for multilevel image annotation</title>
		<author>
			<persName><forename type="first">Jianping</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuli</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hangzai</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="407" to="426" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Phenotypical ontology driven framework for multi-task learning</title>
		<author>
			<persName><forename type="first">Mohamed</forename><surname>Ghalwash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zĳun</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prithwish</forename><surname>Chakraporty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Codella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daby</forename><surname>Sow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Health, Inference, and Learning, CHIL &apos;21</title>
		<meeting>the Conference on Health, Inference, and Learning, CHIL &apos;21<address><addrLine>New York, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="183" to="192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Emergent linguistic phenomena in multi-agent communication games</title>
		<author>
			<persName><forename type="first">Laura</forename><surname>Harding Graesser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-ĲCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-ĲCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3700" to="3710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Emergence of language with multi-agent games : Learning to communicate with sequence of symbols</title>
		<author>
			<persName><forename type="first">Serhii</forename><surname>Havrylov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations (ICLR 17, workshop track)</title>
		<meeting><address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Kalaitzakis</surname></persName>
		</author>
		<ptr target="https://sake.re/20230110-MTOA" />
		<title level="m">MTOA experiment description</title>
		<imprint>
			<date type="published" when="2023">20230110. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Kalaitzakis</surname></persName>
		</author>
		<ptr target="https://sake.re/20230120-MTOA" />
		<title level="m">MTOA experiment description</title>
		<imprint>
			<date type="published" when="2023">20230120. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Lazy</forename><surname>Lavender</surname></persName>
		</author>
		<ptr target="https://gitlab.inria.fr/moex/lazylav" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Convention : A philosophical study</title>
		<author>
			<persName><forename type="first">David</forename><surname>Lewis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Synthese</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="153" to="157" />
			<date type="published" when="1969">1969</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Multitask learning strengthens adversarial robustness</title>
		<author>
			<persName><forename type="first">Chengzhi</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amogh</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vikram</forename><surname>Nitin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baishakhi</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junfeng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carl</forename><surname>Vondrick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2020</title>
		<editor>
			<persName><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Horst</forename><surname>Bischof</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Jan-Michael</forename><surname>Frahm</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="158" to="174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">What triggers the emergence of grammar ?</title>
		<author>
			<persName><forename type="first">Luc</forename><surname>Steels</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISB&apos;05 : Proceedings of the Second International Symposium on the Emergence and Evolution of Linguistic Communication (EELC&apos;05)</title>
		<meeting><address><addrLine>Hatfield, United Kingdom</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="143" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Multilingual agents through multi-headed neural networks</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raul</forename><surname>Santos-Rodriguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mihai</forename><surname>Anca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Piechocki</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">4</biblScope>
			<pubPlace>Tromsø, Norway</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Kmutual online ontology alignment</title>
		<author>
			<persName><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Les</forename><surname>Gasser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 1 st ACM international conference on Autonomous Agents and Multi-Agent Systems (AAMAS)</title>
		<meeting>1 st ACM international conference on Autonomous Agents and Multi-Agent Systems (AAMAS)<address><addrLine>Bologna, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
