<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Toward a framework for seasonal time series forecasting using clustering</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Colin</forename><surname>Leverger</surname></persName>
							<email>colin.leverger@orange.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Orange Labs</orgName>
								<address>
									<postCode>35000</postCode>
									<settlement>Rennes</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Univ Rennes</orgName>
								<orgName type="institution" key="instit2">Inria</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<orgName type="institution" key="instit4">IRISA</orgName>
								<address>
									<postCode>35000</postCode>
									<settlement>Rennes</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Simon</forename><surname>Malinowski</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Univ Rennes</orgName>
								<orgName type="institution" key="instit2">Inria</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<orgName type="institution" key="instit4">IRISA</orgName>
								<address>
									<postCode>35000</postCode>
									<settlement>Rennes</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Thomas</forename><surname>Guyet</surname></persName>
							<affiliation key="aff2">
								<orgName type="laboratory">UMR 6074</orgName>
								<orgName type="institution">AGROCAMPUS-OUEST/IRISA</orgName>
								<address>
									<postCode>35000</postCode>
									<settlement>Rennes</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Vincent</forename><surname>Lemaire</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Orange Labs</orgName>
								<address>
									<postCode>35000</postCode>
									<settlement>Rennes</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Alexis</forename><surname>Bondu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Orange Labs</orgName>
								<address>
									<postCode>35000</postCode>
									<settlement>Rennes</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Alexandre</forename><surname>Termier</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Univ Rennes</orgName>
								<orgName type="institution" key="instit2">Inria</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<orgName type="institution" key="instit4">IRISA</orgName>
								<address>
									<postCode>35000</postCode>
									<settlement>Rennes</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Toward a framework for seasonal time series forecasting using clustering</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">B636E159CEF9108B1CE236B885D5535B</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-04-12T14:50+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Time series</term>
					<term>forecasting</term>
					<term>time series clustering</term>
					<term>Naive Bayesian prediction</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Seasonal behaviours are widely encountered in various applications. For instance, requests on web servers are highly influenced by our daily activities. Seasonal forecasting consists in forecasting the whole next season for a given seasonal time series. It may help a service provider to provision correctly the potentially required resources, avoiding critical situations of over-or under provision. In this article, we propose a generic framework to make seasonal time series forecasting. The framework combines machine learning techniques 1) to identify the typical seasons and 2) to forecast the likelihood of having a season type in one season ahead. We study this framework by comparing the mean squared errors of forecasts for various settings and various datasets. The best setting is then compared to state-of-the-art time series forecasting methods. We show that it is competitive with them.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Forecasting the evolution of a temporal process is a critical research topic, with many challenging applications. One important application is the forecast of future consumer behaviour in marketing, or cloud servers load for popular applications. In this work, we focus on the forecast of time series: a time series is a timestamped sequence of numerical values, and the goal of the forecast is, at a given point of time, to predict the next h values of the time series, with h ∈ N * . A practical example is the time series of outdoor temperature values for New York: sensors capture temperature values every hour, and the goal of forecasting can be to predict temperatures for the next 24 hours. Many forecasting tasks can be reformulated as time series forecasting, making it an especially valuable research topic.</p><p>Forecasting in time series is a difficult task, which has attracted a lot of attention. The most popular methods are Auto-Regressive, ARIMA or Holt-Winters, and come from statistics. These methods build a mathematical model of the time series and focus on predicting the next value (h = 1). Fig. <ref type="figure">1</ref>. Seasonal time series example borrowed from <ref type="bibr" target="#b8">[9]</ref>. Two weeks of the Internet traffic data (in bits) from a private ISP with centres in 11 European cities. The whole data corresponds to a transatlantic link and was collected from 06:57 on 7 June to 11:17 on 31 July 2005. The time series is obviously seasonal, but the assumption of having one unique periodic pattern seems not suitable in this case.</p><p>Many time series, especially those related to human activities or natural phenomena, exhibit seasonality. This means that there is some periodic regularity in the values of the time series. In our New York temperature example, there is an obvious 24h seasonality: temperatures gradually increase in the morning, and decrease for the night. Human activity behaviours often exhibit daily and weekly seasonality. Knowing or discovering that a time series is seasonal is a precious information for forecasts, as it can restrict the search space for the mathematical model of the time series. The classical approach to deal with seasonality is called STL <ref type="bibr" target="#b4">[5]</ref>, it builds a model while taking into account three components: seasonality, trend (long-term evolution of the time series: increase or decrease) and residual (deviation from trend and seasonality). It assumes that the time series exhibits a single periodic behaviour (ex: daily periodicity of temperatures). Holt-Winters and ARIMA models have been extended to deal with seasonality. But they also assume a single and clear periodic behaviour.</p><p>However, this assumption is often violated in practical cases. Consider the time series in Figure <ref type="figure">1</ref>, which shows an Internet traffic measurement for two weeks. While there is indeed a daily periodicity, there are two types of daily patterns: weekday patterns and weekend patterns. This cannot be well captured by the STL framework and the associated statistical methods.</p><p>In this article, we address the task of forecasting the next season of a seasonal time series, with regularities in the seasonal behaviours. We propose a framework for that task, in the light of the work in <ref type="bibr" target="#b11">[12]</ref>. This framework is based on two steps: (1) the identification of typical seasonal behaviours and (2) the forecasting of the season. It only requires as input the time frame of the seasonality (ex: daily, weekly, etc.), and it determines the main seasonality patterns from the analysis of past data (without any assumption on the number of these patterns). In our previous example, it would automatically identify the weekday pattern and the weekend pattern.</p><p>Our contributions are the following:</p><p>1. we propose a general framework for seasonal time series forecasting, 2. we provide extensive experiments of our framework with various settings and various datasets, 3. we compare our framework with alternative strategies for time series forecasting.</p><p>In the remainder, we present in detail our approach, which is based on a clustering step to determine the seasonal patterns, followed by a classification step to build a "next pattern predictor". Our thorough experiments on real data determines which are the best combinations of clustering and classification method, and show that our approach compares favourably to state-of-the-art forecasting methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Time series analysis <ref type="bibr" target="#b2">[3]</ref> has become a recent challenge since more and more sensors collect data with high rates. In statistical approaches, statistical models are fitted on the data to predict future values. The nature of the statistical model is chosen depending on the data characteristics (noise, trends, stationarity, etc.). For a wide literature on statistical methods for time series forecasting, we refer the reader to Brockwell's book <ref type="bibr" target="#b2">[3]</ref>. Autoregressive methods (AR, ARIMA, etc.) demonstrate a great success in lots of applications. The advantages of those models are the hypothesis simplicity, the computational efficiency during the training phase and the handling of the noise. For instance, in <ref type="bibr" target="#b9">[10]</ref>, authors use several ARIMA models to predict day-ahead electricity prices. Kavasseri et al. <ref type="bibr" target="#b10">[11]</ref> use fractional-ARIMA to generate a day-ahead and two days ahead wind forecasts. They show that the method is much better than the persistence model. But, in these works, methods do not directly handle the seasonal dimension of the data. Indeed, ARIMA expects data that is either not seasonal or has the seasonal component removed, e.g. seasonally adjusted via methods such as seasonal differencing <ref type="bibr" target="#b1">[2]</ref>. The SARIMA model <ref type="bibr" target="#b7">[8]</ref> extends the autoregressive model to deal with seasonality. Such model exhibits autocorrelation at past lags of multiple of season period for both autoregression and moving average components.</p><p>In the machine learning communities, one of the objectives is to create data analytic tools that would require fewer modelling efforts for data scientists. Concerning time series forecasting task, machine learning has been used in mainly two ways: use of neural network techniques and of ensemble methods. Neural networks models can be efficient for forecasting tasks, especially when the data is more complex and when the process is non-linear. One drawback of those models is their tendency to over-fit <ref type="bibr" target="#b18">[19]</ref>, which may cause lower performances. LSTM <ref type="bibr" target="#b6">[7]</ref> is a classical neural network architecture used for time series forecasting, but not dedicated to seasonal time series. In <ref type="bibr" target="#b17">[18]</ref>, the authors use ANN and fuzzy clustering for creating daily clusters that are afterward being used for forecasts. Ensemble forecasting methods and hybrid models are created from several state of the art, independent models, that are mixed to create more complex chains. This strategy is the one proposed in the Arbitrated Dynamic Ensemble <ref type="bibr" target="#b3">[4]</ref>. This metalearning method combines different models, regarding to their specifies against target datasets. In <ref type="bibr" target="#b14">[15]</ref>, authors use both ARIMA and SVMs models to forecast stock prices problem, to tackle the non-linearity of some datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Seasonal time series forecasting</head><p>A time series Y is a temporal sequence of values Y = {y 1 , . . . , y n }, where y i ∈ R d (d ∈ N * ) is the value of Y at time i and n denotes the length of the time series. If d = 1, Y is said univariate. Otherwise, Y is said multivariate. We assume here that time series are univariate, regularly sampled and that there is no missing data. We also assume that there is no trend component in the time series <ref type="foot" target="#foot_0">4</ref> .</p><p>The problem of time series forecasting is a classical problem: given the knowledge of Y up to sample n, we want to predict the next samples of Y , i.e. y n+1 , . . . , y n+h , where h is called the prediction horizon. Predictions are denoted ŷn+1 , . . . , ŷn+h . Quality of the predictions is related to the difference between real and predicted values. Different measures can be used to evaluate the quality of predictions. We will introduce some measures in Section 5.</p><p>Let s be the seasonal periodicity of the considered time series. A typical season is a time series ỹ = {ỹ 1 , ỹ2 , . . . , ỹs } of length s. Let Y = {ỹ 1..k } be a collection of k typical seasons, then a seasonal time series Y = {y 1 , y 2 , . . . , y n } is a univariate time series of length n = k × s such that:</p><formula xml:id="formula_0">y i = ỹk σ(i) i-s×σ(i) + , ∀i ∈ [n] where σ(i) = i</formula><p>s is the season index of the i-th timestamp of the series, k i is type of i-th typical seasonal time series and ∼ N (0, 1) is a gaussian noise.</p><p>The seasonal time series forecasting is a forecasting of a seasonal time series at an horizon s, i.e., the prediction of the time series values for the whole next season ahead.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Framework for seasonal time series forecasting</head><p>The general sketch of our approach is composed of two different stages: one for learning the predictive models (see Figure <ref type="figure" target="#fig_0">2</ref>) and the other to use this model to predict the next season (see Figure <ref type="figure" target="#fig_2">3</ref>). The learning stage is composed of three steps: (i) data splitting, (ii) clustering of the seasons, (iii) training a classification algorithm. The predictive stage is composed of two steps: (i) predict a cluster index, (ii) forecast the next season using the predicted cluster index. We describe in this section all these steps in detail.</p><p>Let Y = {y 1 , . . . , y n } be an observed time series with m seasons of length s (n = m × s). We assume that the time series in our possession are equally sampled and that there is always the same number of points s per season. We assume that s is known beforehand: it could be daily, weekly, monthly, or even yearly; and that it does not change over time. Finding the seasonality s is not in the scope of our study. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Learning stage</head><p>The learning stage is composed of three steps (see Figure <ref type="figure" target="#fig_0">2</ref>): The data splitting step is straightforward (since input datasets are considered to be regularly sampled), for the other step we give just below details on them.</p><p>Clustering step: details and algorithms -In this step, a clustering algorithm is used to group the m seasons of D into p groups. The choice of p will be discussed later at the end of this section. A representative series is computed inside each group. These series represent typical seasons that occur in the dataset. Hence, at the end of this step, every season of D can be assigned a label (that represents in which group it has been clustered) and p typical seasons are computed. We consider in this paper four different clustering algorithms: K-Means for time series <ref type="bibr" target="#b12">[13]</ref> K-means algorithm aims at creating a partitioning of the data in k clusters by minimising the intra-cluster variance. The use of K-means implies the use of a distance measure between two time series. Two of the major distance measures available for time series are Euclidean, and Dynamic Time Warping (DTW) <ref type="bibr" target="#b16">[17]</ref>. K-Shape for time series <ref type="bibr" target="#b15">[16]</ref> K-Shape algorithm creates homogeneous and well separated clusters and uses a distance measure based on a normalised version of the cross correlation (invariant to time series scale or shifting). It can be seen as a K-means algorithm but that uses a shape-based similarity measure. Global Alignment Kernel K-means (GAK) <ref type="bibr" target="#b5">[6]</ref> Kernel k-means is a version of k-means that computes the similarity between time series by using kernels. It identifies clusters that are not linearly separable in the input space. The Global Alignment Kernel is a modified version of DTW. Co-clustering with MODL <ref type="bibr" target="#b0">[1]</ref> MODL is a nonparametric method that uses a piecewise constant density estimation to summarise similar curves. Curves are partitioned into clusters and the curve values are discretised into intervals. The cross-product of these discretisation is an estimation of the joint density of the curves and points.</p><p>The choice of the number of clusters is of particular importance in our prediction framework. We use a tuning approach: for each candidate number of clusters, the training set is used to build the overall model. This model is then used to predict the seasons of the validation set. The number of cluster that leads to the lowest error on the validation set is selected. For K-means, K-shape and GAK algorithms, candidates number of cluster are systematically chosen in a pre-defined range <ref type="bibr" target="#b1">[2,</ref><ref type="bibr">300]</ref>. Partitions with empty clusters are discarded. On another hand, MODL co-clustering estimates in a nonparametric way the best number of clusters for each input time series. This estimated number usually leads to the best description of input time series, regarding to the coclustering task. However, this model can be simplified to reduce the number of clusters. From now on, the procedure is similar: different number of clusters are evaluated and the overall model that leads to the lowest error on the validation set is kept.</p><p>Predicting the cluster index of the next season -A probabilistic classifier is then trained to estimate, using the knowledge of the current season D x , the expected group of next season. To train this classifier, we first apply a clustering model as described above to create groups of similar seasons. A learning set is then created to feed the classifier: each line of the learning set corresponds to time series values of a season D x (explanatory variables) and a target variable which corresponds to the group of the next season (the group of season D x+1 ). We think that including more data in the classifier would probably be beneficial: data about few past days, and not only the last one, data about national holidays, all sorts of exogene data we can find related to the problem treated; this last point will be studied in future work.</p><p>Any classifier could be chosen at this step, the only constraint is that the output of the classifier has to be probabilistic (not only a decision but a vector of conditional probabilities of all possible groups given the input time series). In the Figure <ref type="figure" target="#fig_2">3</ref>, K Y denotes a vector of probabilities (i.e { K 1 , ..., K p }). In this paper, three different types of probabilistic classifiers are investigated: a Naive-Bayes classifier, a Decision Tree and a Logistic Regression. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Forecasting the next season</head><p>To forecast the next season, we use the classifier learned above to predict the group index of the next season (using the knowledge of the current season), and representative series of the different groups to generate the prediction of the next season. The forecasting of the next season is done in two steps (see Figure <ref type="figure" target="#fig_2">3</ref>). First, the current season (D x ) is given to the classifier. This classifier computes the probabilities of the next season to belong to each group ({ K 1 , ..., K p }). Then, next season is predicted as a weighted combination of the groups centroids, the weights corresponding to the probabilities output by the classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We have performed different experiments to assess the performance of the proposed method. <ref type="foot" target="#foot_1">5</ref> We were curious about the impacts of the choice of both the clustering and classification algorithms on the performance of the overall method. We also aimed to find out how our approach compares with classical time series prediction approaches.</p><p>We have used nine datasets to assess the performance of the proposed approach. Each dataset is a seasonal time series monitored over many seasons.</p><p>Eight datasets are open source datasets: five are from the time series data library <ref type="bibr" target="#b8">[9]</ref>, one is from the City of Melbourne <ref type="bibr" target="#b13">[14]</ref> (Pedestrian Counting System) and one is provided by Kaggle<ref type="foot" target="#foot_2">6</ref> . The two other datasets have been provided by Orange company via a project named Orange Money. The first time series provided by Orange represents the number of people browsing a website at a time of the day. The second one is a technical metric collected on one server (CPU usage). These nine time series have different seasonalities (daily, weekly, monthly, quarterly). All the time series have been z-normalised beforehand.</p><p>Each dataset is separated into a training set, a validation set and a test set. The training set is composed of 70% of the seasons, while both validation set and test set are composed of 15% of the seasons. However, no shuffle is done during the separation, as it is important to learn the relations between contiguous seasons. The shuffle would remove those relations, which is not acceptable for a "time split". The validation set is used to select the appropriate number of clusters of the different clustering algorithms. The test set is used to assess the performance of the different approaches. Two metrics are used in this paper to evaluate the quality of the predictions: the Mean Square Error (MSE) and the Mean Absolute Error (MAE):</p><formula xml:id="formula_1">M SE(y, ŷ) = 1 s × s j=1 (y j -ŷj ) 2 , M AE(y, ŷ) = 1 s × s j=1 |y j -ŷj |</formula><p>where y = y 1 , . . . , y s is the ground truth and ŷ = ŷ1 , . . . , ŷs is the prediction. As these two measures represent prediction errors, the lower they are, the more accurate the predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Overall performance of the approach</head><p>As explained in Section 4, our proposed approach is based on a clustering algorithm and a classifier. For comparison purposes, we made use of 4 different clustering algorithms and 3 different classifiers. We want to analyse the performance of these different algorithms. Table <ref type="table" target="#tab_0">1</ref> gives the average MSE and MAE over the 9 datasets for every possible combination of clustering and classifier.</p><p>According to this table, MODL outperforms the other clustering algorithms. Combined with a Naive Bayes classifier, it reaches the best performance for 3 of the 4 settings. On the other hand, K-shape is the worse clustering algorithm for this task. A detailed comparison of the clustering algorithms and the classifiers will be made later in this section.  outperforms the other clustering algorithms. For each comparison (MODL versus another clustering algorithm), a Wilcoxon test has been carried out. The p-value of these tests is depicted in each figure . For each comparison, we can see that this probability is less than 0.05, meaning that the performance of MODL is significantly better than the one of the other clustering algorithm. We have also carried out a Nemenyi test to compare the average ranks of the different approaches. The critical diagrams associated with these tests are given in Figure <ref type="figure" target="#fig_5">5</ref>. The left-hand side represents the comparison of the average rank according to the MSE measure, while the right-hand side is associated with the MAE measure. It shows that MODL outperforms the three other algorithms, significantly for the MSE measure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Performance of the different clustering algorithms</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Performance of the different classifiers</head><p>As explained above, an important part of our method is to predict the group (or cluster) of the next season. This prediction is made using a classifier. Once the group is predicted, a weighted combination of different centroids is used to make the forecast. A centroid of a cluster is the Euclidean mean of all its curves. In this section, we first make a performance comparison of the three considered classifiers. Let us compare the performance of the three considered classifiers: a Naive-Bayes classifier, a decision tree, and a logistic regression. The critical diagrams that compare the performance of the three classifiers are given in Figure <ref type="figure" target="#fig_5">5</ref>. The left diagram is for the MSE measure and the right one for the MAE measure. We can see that for the MSE measure, the performance of the classifiers are very close (no significant difference between the ranks). For the MAE, the Naive Bayes classifier is better than the two others.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Comparison with other prediction methods</head><p>In this section, we aim at comparing the performance of the proposed approach (PA) with other competitive prediction methods. Following the analysis above, we use for our approach the following setting: MODL algorithm is used as the clustering algorithm and the classifier is chosen according to the validation set. We compare the performance of our approach with the following four other prediction methods: mean season, Markov model (MM), autoregressive models (AR) and an algorithm called Arbitrated Ensemble for Time Series Forecasting (AETSF). The mean season is a naive approach that consists in predicting the next season as the average season of the training set. The MM approach has been proposed in <ref type="bibr" target="#b11">[12]</ref>. It follows the same framework as the one proposed here. However, the classifier is replaced by a Markov model, whose transition probability matrix is used to predict the cluster of the next season. Auto-regressive models have widely been used for time series forecasting. The order of the regressive process is set to the length of the seasonality of the time series. Finally, the AETSF method has been proposed in <ref type="bibr" target="#b3">[4]</ref>. It is an ensemble method that combines prediction of several algorithms based on meta-learners. This method has been shown to reach state-of-the-art performance for time series prediction. We have used four base learners: generalised linear models, support vector regression, random forest and feed forward neural networks.</p><p>Table <ref type="table" target="#tab_1">2</ref> compares the performance of all these approaches for the nine considered datasets and the MSE measure. In terms of average MSE and mean rank, the AETSF is the best method, just in front of the proposed approach. The critical diagram associated with the Nemenyi test is given in Figure <ref type="figure" target="#fig_6">6</ref>. The difference between AETSF and our approach is not significant on these nine datasets.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We have proposed in this paper a study of a framework for seasonal time series prediction. This framework involves three steps: the clustering of the seasons into groups, the prediction of the group of the next season using a classifier, and finally the prediction of the next season. One advantage of such a framework is that it is able to produce predictions at the horizon of one season in one shot (i.e., there is no need to build different models for different horizons). We have provided a comparison of different clustering algorithms that can be used in the first step, and a comparison of different classifiers for the second step. Experiment results show that our proposed approach is competitive with other time series prediction methods. For future work, an important point will be to focus on the performance of the classifier. We think that including more exogenous data (example: holidays, social events, weather, etc.) might improve the performance of the classifier, thus the global performance of our framework.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Illustration of the learning steps.</figDesc><graphic coords="5,186.12,115.83,243.11,113.39" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>(i) The data splitting step consists in constructing a set of m seasons, D = {D 1 , ..., D i , ..., D m }, where D i represents the subseries corresponding to the i th season of measurements. We assume that observations of the D are independent. (ii) The m elements, seasons, of D are given to a clustering (or co-clustering) algorithm that gathers similar seasons into p groups of typical seasonal time series. (iii) A probabilistic classifier is then trained to estimate, using the knowledge of a current season D x , the expected group of next season (season X + 1 denoted the season just after the season X).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Illustration of the forecasting steps.</figDesc><graphic coords="7,180.67,115.84,254.03,99.21" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 -</head><label>4</label><figDesc>Figure 4-(a), (b) and (c) compare the performance of MODL against the three other considered clustering algorithms. Each point in these figures represents a combination of experimental settings: clustering algorithms, classifiers, datasets, and metrics. For each of these combinations, the performance is computed according to the considered metrics and inserted in the figure. It shows that MODL</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Comparison between MODL and the other clustering algorithms (K-means, GAK and K-shape). The axis represents an error measure (either MSE or MAE) associated with the clustering algorithm of the axis labels. A point above the diagonal is in favour of the MODL approach.</figDesc><graphic coords="9,152.06,115.84,311.24,111.66" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Critical diagram, comparison of the average ranks of the difference clustering algorithms (on the top) different classification algorithms (on the bottom) in terms of MSE (left) and MAE (right).</figDesc><graphic coords="10,186.64,115.83,242.09,180.06" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Critical diagram of the comparison between different prediction approaches.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Performances of all the possible combination of clustering and classifiers algorithms for our chain, in terms of MSE and MAE. Bold Figures are best MSE/MAE.</figDesc><table><row><cell></cell><cell></cell><cell>MSE</cell><cell></cell><cell></cell><cell>MAE</cell><cell></cell></row><row><cell></cell><cell cols="6">Naive Bayes Tree Logistic Reg. Naive Bayes Tree Logistic Reg.</cell></row><row><cell>MODL</cell><cell>0.480</cell><cell>0.581</cell><cell>0.479</cell><cell>0.462</cell><cell>0.494</cell><cell>0.487</cell></row><row><cell>K-means</cell><cell>0.799</cell><cell>0.583</cell><cell>0.492</cell><cell>0.564</cell><cell>0.526</cell><cell>0.492</cell></row><row><cell>GAK</cell><cell>0.571</cell><cell>0.638</cell><cell>0.608</cell><cell>0.538</cell><cell>0.576</cell><cell>0.590</cell></row><row><cell>K-shape</cell><cell>0.745</cell><cell>0.824</cell><cell>0.852</cell><cell>0.575</cell><cell>0.677</cell><cell>0.693</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Comparison of the proposed approach with other prediction methods.</figDesc><table><row><cell>Dataset</cell><cell cols="3">Mean season MM</cell><cell cols="3">AR AETSF PA</cell></row><row><cell>1</cell><cell></cell><cell>1.2349</cell><cell cols="4">0.1800 0.7981 0.1771 0.1750</cell></row><row><cell>2</cell><cell></cell><cell>3.2345</cell><cell cols="4">0.7698 1.7460 0.3988 0.5172</cell></row><row><cell>3</cell><cell></cell><cell>1.7163</cell><cell cols="4">1.1057 0.9060 0.1189 0.1924</cell></row><row><cell>4</cell><cell></cell><cell>0.9428</cell><cell cols="4">0.5701 1.1262 0.5392 0.6359</cell></row><row><cell>5</cell><cell></cell><cell>1.4766</cell><cell cols="4">1.1515 0.9817 0.1062 0.2599</cell></row><row><cell>6</cell><cell></cell><cell>0.7424</cell><cell cols="4">0.4260 0.4407 0.6106 0.5633</cell></row><row><cell>7</cell><cell></cell><cell>1.2194</cell><cell cols="4">1.3411 2.1578 1.3028 1.0181</cell></row><row><cell>8</cell><cell></cell><cell>1.070</cell><cell cols="4">1.5949 0.7294 0.6226 0.7187</cell></row><row><cell>9</cell><cell></cell><cell>2.0271</cell><cell cols="4">0.9542 1.3424 0.2953 0.2811</cell></row><row><cell cols="2">Average MSE</cell><cell>1.5183</cell><cell cols="4">1.1365 0.8993 0.4635 0.4846</cell></row><row><cell cols="2">Average Rank</cell><cell>4.44</cell><cell>3.67</cell><cell>3.22</cell><cell>1.78</cell><cell>1.89</cell></row><row><cell></cell><cell>CD</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>1</cell><cell>2</cell><cell>3</cell><cell></cell><cell>4</cell><cell></cell><cell>5</cell></row><row><cell>AETSF</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>AR</cell></row><row><cell>PA</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>MM</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Mean season</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_0"><p>In practice, it is not a problem, as most time series could be easily decomposed and detrended.Trend components can then be re-applied on the forecasted values.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_1"><p>For reproducible research, code and data are available online https://github.com/ColinLeverger/ IDEAL2019-coclustering-forecasts.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_2"><p>See: https://www.kaggle.com/robikscube/hourly-energy-consumption</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Functional data clustering via piecewise constant nonparametric density estimation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Boullé</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="4389" to="4401" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Time series analysis: forecasting and control</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Box</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Jenkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">C</forename><surname>Reinsel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Ljung</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>John Wiley &amp; Sons</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Brockwell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">V</forename><surname>Calder</surname></persName>
		</author>
		<title level="m">Introduction to time series and forecasting</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Arbitrated ensemble for time series forecasting</title>
		<author>
			<persName><forename type="first">V</forename><surname>Cerqueira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Torgo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Soares</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint European conference on machine learning and knowledge discovery in databases</title>
		<meeting>the Joint European conference on machine learning and knowledge discovery in databases</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="478" to="494" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">STL: A seasonaltrend decomposition</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Cleveland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">S</forename><surname>Cleveland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Mcrae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Terpenning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of official statistics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="73" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Fast global alignment kernels</title>
		<author>
			<persName><forename type="first">M</forename><surname>Cuturi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning (ICML)</title>
		<meeting>the International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="929" to="936" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">A</forename><surname>Gers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Cummins</surname></persName>
		</author>
		<title level="m">Learning to forget: Continual prediction with LSTM</title>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Chapter 13 forecasting seasonal time series</title>
		<author>
			<persName><forename type="first">E</forename><surname>Ghysels</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Osborn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Rodrigues</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Handbook of Economic Forecasting</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="659" to="711" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Hyndman</surname></persName>
		</author>
		<ptr target="http://robjhyndman.com/TSDL" />
		<title level="m">Time series data library (TSDL)</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Electricity price forecasting -ARIMA model approach</title>
		<author>
			<persName><forename type="first">T</forename><surname>Jakaša</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Andročec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sprčić</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on the European Energy Market (EEM)</title>
		<meeting>the International Conference on the European Energy Market (EEM)</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="222" to="225" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Day-ahead wind speed forecasting using f-ARIMA models</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">G</forename><surname>Kavasseri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Seetharaman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Renewable Energy</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1388" to="1393" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Day-ahead time series forecasting: application to capacity planning</title>
		<author>
			<persName><forename type="first">C</forename><surname>Leverger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Lemaire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Guyet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Rozé</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd workshop on Advanced Analytics and Learning of Temporal Data (AALTD)</title>
		<meeting>the 3rd workshop on Advanced Analytics and Learning of Temporal Data (AALTD)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Least squares quantization in PCM</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lloyd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions on information theory</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="129" to="137" />
			<date type="published" when="1982">1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Pedestrian counting system</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">O</forename><surname>Melbourne</surname></persName>
		</author>
		<ptr target="http://www.pedestrian.melbourne.vic.gov.au/" />
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A hybrid ARIMA and support vector machines model in stock price forecasting</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Pai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Omega</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="497" to="505" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">k-shape: Efficient and accurate clustering of time series</title>
		<author>
			<persName><forename type="first">J</forename><surname>Paparrizos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Gravano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Management of Data (SIG-MOD)</title>
		<meeting>the International Conference on Management of Data (SIG-MOD)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1855" to="1870" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A global averaging method for dynamic time warping, with applications to clustering</title>
		<author>
			<persName><forename type="first">F</forename><surname>Petitjean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ketterlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Gançarski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="678" to="693" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Day-ahead price forecasting in restructured power systems using artificial neural networks</title>
		<author>
			<persName><forename type="first">V</forename><surname>Vahidinasab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jadid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kazemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Electric Power Systems Research</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1332" to="1342" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Time series forecasting using a hybrid ARIMA and neural network model</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">P</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="159" to="175" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
