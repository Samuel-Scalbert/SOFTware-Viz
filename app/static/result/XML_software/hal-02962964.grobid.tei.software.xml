<TEI xmlns="http://www.tei-c.org/ns/1.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xml:space="preserve" xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Asteroid: the PyTorch-based audio source separation toolkit for researchers</title>
			</titleStmt>
			<publicationStmt>
				<publisher />
				<availability status="unknown"><licence /></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Manuel</forename><surname>Pariente</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Université de Lorraine</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<orgName type="institution" key="instit3">Inria</orgName>
								<orgName type="institution" key="instit4">LORIA</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Samuele</forename><surname>Cornell</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Università Politecnica delle Marche</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Joris</forename><surname>Cosentino</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Université de Lorraine</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<orgName type="institution" key="instit3">Inria</orgName>
								<orgName type="institution" key="instit4">LORIA</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sunit</forename><surname>Sivasankaran</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Université de Lorraine</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<orgName type="institution" key="instit3">Inria</orgName>
								<orgName type="institution" key="instit4">LORIA</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Efthymios</forename><surname>Tzinis</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jens</forename><surname>Heitkaemper</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Universität Paderborn</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Michel</forename><surname>Olvera</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Université de Lorraine</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<orgName type="institution" key="instit3">Inria</orgName>
								<orgName type="institution" key="instit4">LORIA</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Fabian-Robert</forename><surname>Stöter</surname></persName>
							<affiliation key="aff4">
								<orgName type="laboratory">Inria and LIRMM</orgName>
								<orgName type="institution">University of Montpellier</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mathieu</forename><surname>Hu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Université de Lorraine</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<orgName type="institution" key="instit3">Inria</orgName>
								<orgName type="institution" key="instit4">LORIA</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Juan</forename><forename type="middle">M</forename><surname>Martín-Doñas</surname></persName>
							<affiliation key="aff5">
								<orgName type="institution">Universidad de Granada</orgName>
								<address>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">David</forename><surname>Ditter</surname></persName>
							<affiliation key="aff6">
								<orgName type="institution">Universität Hamburg</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ariel</forename><surname>Frank</surname></persName>
							<affiliation key="aff7">
								<orgName type="institution">Technion -Israel Institute of Technology</orgName>
								<address>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Antoine</forename><surname>Deleforge</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Université de Lorraine</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<orgName type="institution" key="instit3">Inria</orgName>
								<orgName type="institution" key="instit4">LORIA</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Emmanuel</forename><surname>Vincent</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Université de Lorraine</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<orgName type="institution" key="instit3">Inria</orgName>
								<orgName type="institution" key="instit4">LORIA</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Asteroid: the PyTorch-based audio source separation toolkit for researchers</title>
					</analytic>
					<monogr>
						<imprint>
							<date />
						</imprint>
					</monogr>
					<idno type="MD5">C1BADF903ACF07EE29092D75E8874388</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-04-12T14:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid" />
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>source separation</term>
					<term>speech enhancement</term>
					<term>opensource software</term>
					<term>end-to-end</term>
				</keywords>
			</textClass>
			<abstract>
<div><p>This paper describes <software>Asteroid</software>, the <software ContextAttributes="used">PyTorch</software>-based audio source separation toolkit for researchers. Inspired by the most successful neural source separation systems, it provides all neural building blocks required to build such a system. To improve reproducibility, Kaldi-style recipes on common audio source separation datasets are also provided. This paper describes the software architecture of <software ContextAttributes="used">Asteroid</software> and its most important features. By showing experimental results obtained with <software ContextAttributes="used">Asteroid</software>'s recipes, we show that our implementations are at least on par with most results reported in reference papers. The toolkit is publicly available at github.com/mpariente/asteroid.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div><head n="1.">Introduction</head><p>Audio source separation, which aims to separate a mixture signal into individual source signals, is essential to robust speech processing in real-world acoustic environments <ref type="bibr">[1]</ref>. Classical open-source toolkits such as <software ContextAttributes="used">FASST</software> <ref type="bibr">[2]</ref>, HARK <ref type="bibr" target="#b0">[3]</ref>, <software ContextAttributes="used">ManyEars</software> <ref type="bibr" target="#b1">[4]</ref> and openBliSSART <ref type="bibr" target="#b2">[5]</ref> which are based on probabilistic modelling, non-negative matrix factorization, sound source localization and/or beamforming have been successful in the past decade. However, they are now largely outperformed by deep learning-based approaches, at least on the task of single-channel source separation <ref type="bibr" target="#b3">[6]</ref><ref type="bibr" target="#b4">[7]</ref><ref type="bibr" target="#b5">[8]</ref><ref type="bibr" target="#b6">[9]</ref><ref type="bibr" target="#b7">[10]</ref>.</p><p>Several open-source toolkits have emerged for deep learning-based source separation. These include nussl (Northwestern University Source Separation Library) <ref type="bibr" target="#b8">[11]</ref>, ONSSEN (An Open-source Speech Separation and Enhancement Library) <ref type="bibr" target="#b9">[12]</ref>, Open-Unmix <ref type="bibr" target="#b10">[13]</ref>, and countless isolated implementations replicating some important papers 1 .</p><p>We would like to thank Hervé Bredin for fruitful discussions about software design and Kaituo Xu for open-sourcing his Conv-Tasnet implementation.</p><p>Experiments presented in this paper were partially carried out using the Grid'5000 testbed, supported by a scientific interest group hosted by Inria and including CNRS, RENATER and several Universities as well as other organizations (see https://www.grid5000.fr).</p><p>High Performance Computing resources were partially provided by the EXPLOR centre hosted by the University de Lorraine 1 kaituoxu/TasNet, kaituoxu/Conv-TasNet, yluo42/TAC, JusperLee/Conv-TasNet, JusperLee/Dual-Path-RNN-Pytorch, tky1117/DNN-based source separation ShiZiqiang/dual-path-RNNs-DPRNNs-based-speech-separation Both nussl and ONSSEN are written in <software ContextAttributes="used">PyTorch</software> <ref type="bibr" target="#b11">[14]</ref> and provide training and evaluation <software ContextAttributes="used">scripts</software> for several state-of-the art methods. However, data preparation steps are not provided and experiments are not easily configurable from the command line. Open-Unmix does provide a complete pipeline from data preparation until evaluation, but only for the Open-Unmix model on the music source separation task. Regarding the isolated implementations, some of them only contain the model, while others provide training <software ContextAttributes="used">scripts</software> but assume that training data has been generated. Finally, very few provide the complete pipeline. Among the ones providing evaluation <software ContextAttributes="used">scripts</software>, differences can often be found, e.g., discarding short utterances or splitting utterances in chunks and discarding the last one. This paper describes <software ContextAttributes="used">Asteroid</software> (Audio source separation on Steroids), a new open-source toolkit for deep learning-based audio source separation and speech enhancement, designed for researchers and practitioners. Based on <software ContextAttributes="used">PyTorch</software>, one of the most widely used dynamic neural network toolkits, <software ContextAttributes="used">Asteroid</software> is meant to be user-friendly, easily extensible, to promote reproducible research, and to enable easy experimentation. As such, it supports a wide range of datasets and architectures, and comes with recipes reproducing some important papers. <software ContextAttributes="used">Asteroid</software> is built on the following principles:</p><p>1. Abstract only where necessary, i.e., use as much native</p><p><software>PyTorch</software> code as possible. 2. Allow importing third-party code with minimal changes.</p><p>3. Provide all steps from data preparation to evaluation. 4. Enable recipes to be configurable from the command line.</p><p>We present the audio source separation framework in Section 2. We describe <software>Asteroid</software>'s main features in Section 3 and their implementation in Section 4. We provide example experimental results in Section 5 and conclude in Section 6.</p></div>
<div><head n="2.">General framework</head><p>While <software>Asteroid</software> is not limited to a single task, single-channel source separation is currently its main focus. Hence, we will only consider this task in the rest of the paper. Let x be a single channel recording of J sources in noise:</p><formula xml:id="formula_0">x(t) = J j=1 sj(t) + n(t),<label>(1)</label></formula><p>where {sj}j=1..J are the source signals and n is an additive noise signal. The goal of source separation is to obtain source estimates { sj}j=1..J given x.</p><p>Most state-of-the-art neural source separation systems follow the encoder-masker-decoder approach depicted in Fig. 1 <ref type="bibr" target="#b5">[8,</ref><ref type="bibr" target="#b6">9,</ref><ref type="bibr" target="#b12">15,</ref><ref type="bibr" target="#b13">16]</ref>. The encoder computes a short-time Fourier transform (STFT)-like representation X by convolving the timedomain signal x with an analysis filterbank. The representation X is fed to the masker network that estimates a mask for each source. The masks are then multiplied entrywise with X to obtain sources estimates { Sj}j=1..J in the STFT-like domain. The time-domain source estimates { sj}j=1..J are finally obtained by applying transposed convolutions to { Sj}j=1..J with a synthesis filterbank. The three networks are jointly trained using a loss function computed on the masks or their embeddings <ref type="bibr" target="#b3">[6,</ref><ref type="bibr" target="#b14">17,</ref><ref type="bibr" target="#b15">18]</ref>, on the STFT-like domain estimates <ref type="bibr" target="#b4">[7,</ref><ref type="bibr" target="#b12">15,</ref><ref type="bibr" target="#b16">19]</ref>, or directly on the time-domain estimates <ref type="bibr" target="#b13">16,</ref><ref type="bibr" target="#b17">20]</ref>.</p></div>
<div><head>Mixture waveform Separated waveforms</head><p>Encoder Decoder STFT-like rep. </p></div>
<div><head>Masked rep. Masker</head></div>
<div><head n="3.">Functionality</head><p><software>Asteroid</software> follows the encoder-masker-decoder approach, and provides various choices of filterbanks, masker networks, and loss functions. It also provides training and evaluation tools and recipes for several datasets. We detail each of these below.</p></div>
<div><head n="3.1.">Analysis and synthesis filterbanks</head><p>As shown in <ref type="bibr" target="#b17">[20]</ref><ref type="bibr" target="#b18">[21]</ref><ref type="bibr" target="#b19">[22]</ref><ref type="bibr" target="#b20">[23]</ref>, various filterbanks can be used to train end-to-end source separation systems. A natural abstraction is to separate the filterbank object from the encoder and decoder objects. This is what we do in <software ContextAttributes="used">Asteroid</software>. All filterbanks inherit from the Filterbank class. Each Filterbank can be combined with an Encoder or a Decoder, which respectively follow the nn.Conv1d and nn.<software ContextAttributes="used">ConvTranspose1d</software> interfaces from <software ContextAttributes="used">PyTorch</software> for consistency and ease of use. Notably, the STFTFB filterbank computes the STFT using simple convolutions, and the default filterbank matrix is orthogonal.</p><p><software ContextAttributes="used">Asteroid</software> supports free filters <ref type="bibr" target="#b5">[8,</ref><ref type="bibr" target="#b6">9]</ref>, discrete Fourier transform (DFT) filters <ref type="bibr" target="#b16">[19,</ref><ref type="bibr" target="#b18">21]</ref>, analytic free filters <ref type="bibr" target="#b19">[22]</ref>, improved parameterized sinc filters <ref type="bibr" target="#b19">[22,</ref><ref type="bibr" target="#b21">24]</ref> and the multi-phase Gammatone filterbank <ref type="bibr" target="#b20">[23]</ref>. Automatic pseudo-inverse computation and dynamic filters (computed at runtime) are also supported. Because some of the filterbanks are complex-valued, we provide functions to compute magnitude and phase, and apply magnitude or complex-valued masks. We also provide interfaces to <software ContextAttributes="used">NumPy</software> <ref type="bibr" target="#b22">[25]</ref> and torchaudio 2 . Additionally, Griffin-2 github.com/pytorch/audio Lim <ref type="bibr" target="#b23">[26,</ref><ref type="bibr" target="#b24">27]</ref> and multi-input spectrogram inversion (MISI) <ref type="bibr" target="#b25">[28]</ref> algorithms are provided.</p></div>
<div><head n="3.2.">Masker network</head><p><software ContextAttributes="used">Asteroid</software> provides implementations of widely used masker networks: TasNet's stacked long short-term memory (LSTM) network <ref type="bibr" target="#b5">[8]</ref>, Conv-Tasnet's temporal convolutional network (with or without skip connections) <ref type="bibr" target="#b6">[9]</ref>, and the dual-path recurrent neural network (DPRNN) in <ref type="bibr" target="#b13">[16]</ref>. Open-Unmix <ref type="bibr" target="#b10">[13]</ref> is also supported for music source separation.</p></div>
<div><head n="3.3.">Loss functions -Permutation invariance</head><p><software ContextAttributes="used">Asteroid</software> supports several loss functions: mean squared error, scale-invariant signal-to-distortion ratio (SI-SDR) <ref type="bibr" target="#b6">[9,</ref><ref type="bibr" target="#b26">29]</ref>, scale-dependent SDR <ref type="bibr" target="#b26">[29]</ref>, signal-to-noise ratio (SNR), perceptual evaluation of speech quality (PESQ) <ref type="bibr" target="#b27">[30]</ref>, and affinity loss for deep clustering <ref type="bibr" target="#b3">[6]</ref>.</p><p>Whenever the sources are of the same nature, a permutation-invariant (PIT) loss shall be used <ref type="bibr" target="#b4">[7,</ref><ref type="bibr" target="#b28">31]</ref>. <software ContextAttributes="used">Asteroid</software> provides an optimized, versatile implementation of PIT losses. Let s = [sj(t)] t=0...T j=1...J and s = [ sj(t)] t=0...T j=1...J be the matrices of true and estimated source signals, respectively. We denote as sσ = [ s σ(j) (t)] t=0...T j=1...J a permutation of s by σ ∈ SJ , where SJ is the set of permutations of [1, ..., J]. A PIT loss LPIT is defined as</p><formula xml:id="formula_1">LPIT(θ) = min σ∈S J L( sσ, s),<label>(2)</label></formula><p>where L is a classical (permutation-dependent) loss function, which depends on the network's parameters θ through sσ.</p><p>We assume that, for a given permutation hypothesis σ, the loss L( sσ, s) can be written as L( sσ, s) = G F( s σ(1) , s1), ..., F( s σ(J) , sJ )</p><p>where sj = [sj(0), . . . , sj(T )], sj = [ sj(0), . . . , sj(T )], F computes the pairwise loss between a single true source and its hypothesized estimate, and G is the reduce function, usually a simple mean operation. Denoting by F the J × J pairwise loss matrix with entries F( si, sj), we can rewrite (2) as</p><formula xml:id="formula_3">LPIT(θ) = min σ∈S J G F σ(1)1 , ..., F σ(J)J<label>(4)</label></formula><p>and reduce the computational complexity from J! to J 2 by precomputing F's terms. Taking advantage of this, <software>Asteroid</software> provides PITLossWrapper, a simple yet powerful class that can efficiently turn any pairwise loss F or permutation-dependent loss L into a PIT loss.</p></div>
<div><head n="3.4.">Datasets</head><p><software ContextAttributes="used">Asteroid</software> provides baseline recipes for the following datasets: wsj0-2mix and wsj0-3mix <ref type="bibr" target="#b3">[6]</ref>, WHAM <ref type="bibr" target="#b29">[32]</ref>, WHAMR <ref type="bibr" target="#b30">[33]</ref>, LibriMix <ref type="bibr" target="#b31">[34]</ref> FUSS <ref type="bibr" target="#b32">[35]</ref>, Microsoft's Deep Noise Suppression challenge dataset (DNS) <ref type="bibr" target="#b33">[36]</ref>, SMS-WSJ <ref type="bibr" target="#b34">[37]</ref>, Kinect-WSJ <ref type="bibr" target="#b35">[38]</ref>, and MUSDB18 <ref type="bibr" target="#b36">[39]</ref>. Their characteristics are summarized and compared in </p></div>
<div><head n="3.5.">Training</head><p>For training source separation systems, <software ContextAttributes="used">Asteroid</software> offers a thin wrapper around <software ContextAttributes="used">PyTorch</software>-Lightning <ref type="bibr" target="#b37">[40]</ref> that seamlessly enables distributed training, experiment logging and more, without sacrificing flexibility. Regarding the optimizers, we also rely on native <software ContextAttributes="used">PyTorch</software> and torch-optimizer 3 .</p><p><software>PyTorch</software> provides basic optimizers such as SGD and Adam and torch-optimizer provides state-of-the art optimizers such as RAdam, Ranger or Yogi.</p></div>
<div><head n="3.6.">Evaluation</head><p>Evaluation is performed using <software ContextAttributes="used">pb bss eval 4</software> , a sub-toolkit of <software ContextAttributes="used">pb bss 5</software> <ref type="bibr" target="#b38">[41]</ref> written for evaluation. It natively supports most metrics used in source separation: SDR, signal-to-interference ratio (SIR), signal-to-artifacts ratio (SAR) <ref type="bibr" target="#b39">[42]</ref>, SI-SDR <ref type="bibr" target="#b26">[29]</ref>, PESQ <ref type="bibr" target="#b40">[43]</ref>, and short-time objective intelligibility (STOI) <ref type="bibr" target="#b41">[44]</ref>.</p></div>
<div><head n="4.">Implementation</head><p><software ContextAttributes="used">Asteroid</software> follows Kaldi-style recipes <ref type="bibr" target="#b42">[45]</ref>, which involve several stages as depicted in Fig. <ref type="figure" target="#fig_1">2</ref>. These recipes implement the entire pipeline from data download and preparation to model training and evaluation. We show the typical organization of a recipe's directory in Fig. <ref type="figure" target="#fig_2">3</ref>. The entry point of a recipe is the run.sh script which will execute the following stages:</p><p>• Stage 0: Download data that is needed for the recipe. In the first stage, necessary data is downloaded (if available) into a storage directory specified by the user. We use the official <software>scripts</software> provided by the dataset's authors to generate the data, and optionally perform data augmentation. All the information required by the dataset's <software ContextAttributes="used">DataLoader</software> such as filenames and paths, utterance lengths, speaker IDs, etc., is then gathered into text files under data/. The training stage is finally followed by the evaluation stage. Throughout the recipe, log files are saved under logs/ and generated data is saved under exp/.</p><p>3 github.com/jettify/pytorch-optimizer 4 pypi.org/project/pb bss eval 5 github.com/fgnt/pb bss  As can be seen in Fig. <ref type="figure" target="#fig_4">4</ref>, the model class, which is a direct subclass of PyTorch's <software ContextAttributes="used">nn.Module</software>, is defined in <software ContextAttributes="used">model.py</software>. It is imported in both training and evaluation <software ContextAttributes="used">scripts</software>. Instead of defining constants in <software ContextAttributes="used">model.py</software> and <software ContextAttributes="used">train.py</software>, most of them are gathered in a YAML configuration file <software ContextAttributes="used">conf.yml</software>. An argument parser is created from this configuration file to allow modification of these values from the command line, with run.sh passing arguments to train.py. The resulting modified configuration is saved in exp/ to enable future reuse. Other arguments such as the experiment name, the number of GPUs, etc., are directly passed to run.sh.</p></div>
<div><head n="5.">Example results</head><p>To illustrate the potential of <software ContextAttributes="used">Asteroid</software>, we compare the performance of state-of-the-art methods as reported in the corresponding papers with our implementation. We do so on two common source separation datasets: wsj0-2mix <ref type="bibr" target="#b3">[6]</ref> and WHAMR <ref type="bibr" target="#b30">[33]</ref>. wsj0-2mix consists of a 30 h training set, a 10 h validation set, and a 5 h test set of single-channel two-speaker mixtures without noise and reverberation. Utterances taken from the Wall Street Journal (WSJ) dataset are mixed together at random SNRs between -5 dB and 5 dB. Speakers in the test set are different from those in the training and validation sets. WHAMR  Table <ref type="table" target="#tab_3">2</ref> reports SI-SDR improvements (SI-SDRi) on the test set of wsj0-2mix for several well-known source separation systems. In Table <ref type="table">3</ref>, we reproduce Table <ref type="table" target="#tab_3">2</ref> from <ref type="bibr" target="#b30">[33]</ref> which reports the performance of an improved TasNet architecture (more recurrent units, overlap-add for synthesis) on the four main tasks of WHAMR: anechoic separation, noisy anechoic separation, reverberant separation, and noisy reverberant separation. On all four tasks, <software ContextAttributes="used">Asteroid</software>'s recipes achieved better results than originally reported, by up to 2.6 dB.</p></div>
<div><head>Reported Using Asteroid</head><p>Deep Clustering <ref type="bibr" target="#b43">[46]</ref> 9  <ref type="table">3</ref>: SI-SDRi (dB) on the four WHAMR tasks using the improved TasNet architecture in <ref type="bibr" target="#b30">[33]</ref>.</p><p>In both Tables <ref type="table" target="#tab_3">2</ref> and<ref type="table">3</ref>, we can see that our implementations outperform the original ones in most cases. Most often, the aforementioned architectures are trained on 4-second segments. For the architectures requiring a large amount of memory (e.g., Conv-TasNet and DPRNN), we reduce the length of the training segments in order to increase the batch size and stabilize gradients. This, as well as using a weight decay of 10 -5 for recurrent architectures increased the final performance of our systems.</p><p><software ContextAttributes="used">Asteroid</software> was designed such that writing new code is very simple and results can be quickly obtained. For instance, starting from stage 2, writing the TasNet recipe used in Table <ref type="table">3</ref> took less than a day and the results were simply generated with the command in Fig. <ref type="figure" target="#fig_6">5</ref>, where the GPU ID is specified with the --id argument.  </p></div>
<div><head n="6.">Conclusion</head><p>In this paper, we have introduced <software>Asteroid</software>, a new open-source audio source separation toolkit designed for researchers and practitioners. Comparative experiments show that results obtained with <software ContextAttributes="used">Asteroid</software> are competitive on several datasets and for several architectures. The toolkit was designed such that it can quickly be extended with new network architectures or new benchmark datasets. In the near future, pre-trained models will be made available and we intend to interface with ESPNet to enable end-to-end multi-speaker speech recognition.</p></div>
<div><head n="7.">References</head><p>[1] E. Vincent, T. Virtanen, and S. Gannot, Audio Source Separation and Speech Enhancement, 1st ed. Wiley, 2018.</p><p>[2] Y. Salaün, E. Vincent, N. Bertin, N. Souviraà-Labastie, X. Jaureguiberry, D. T. Tran, and F. Bimbot, "The Flexible Audio Source Separation Toolbox Version 2.0," ICASSP Show &amp; Tell, 2014.</p></div><figure xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Typical encoder-masker-decoder architecture.</figDesc></figure>
<figure xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Typical recipe flow in Asteroid.</figDesc></figure>
<figure xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Typical directory structure of a recipe.</figDesc></figure>
<figure xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Simplified code example.</figDesc><graphic coords="5,236.72,87.46,75.18,140.12" type="bitmap" /></figure>
<figure xml:id="fig_5"><head /><label /><figDesc>n=0 for task in clean noisy reverb reverb_noisy do ./run.sh --stage 3 --task $task --id $n n=$(($n+1)) done</figDesc></figure>
<figure xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Example command line usage.</figDesc></figure>
<figure type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc /><table><row><cell>. wsj0-2mix and MUSDB18</cell></row></table></figure>
<figure type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Datasets currently supported by Asteroid.</figDesc><table /><note><p>* White sensor noise. ** Background environmental scenes.</p></note></figure>
<figure type="table" xml:id="tab_2"><head>•</head><label /><figDesc>Stage 1: Generate mixtures with the official scripts, optionally perform data augmentation. • Stage 2: Gather data information into text files expected by the corresponding DataLoader. • Stage 3: Train the source separation system. • Stage 4: Separate test mixtures and evaluate.</figDesc><table /></figure>
<figure type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>SI-SDRi (dB) on the wsj0-2mix test set for several architectures. ks stands for for kernel size, i.e., the length of the encoder and decoder filters.</figDesc><table><row><cell /><cell>.6</cell><cell>9.8</cell></row><row><cell>TasNet [8]</cell><cell>10.8</cell><cell>15.0</cell></row><row><cell>Conv-TasNet [9]</cell><cell>15.2</cell><cell>16.2</cell></row><row><cell>TwoStep [15]</cell><cell>16.1</cell><cell>15.2</cell></row><row><cell>DPRNN (ks = 16) [16]</cell><cell>16.0</cell><cell>17.7</cell></row><row><cell>DPRNN (ks = 2) [16]</cell><cell>18.8</cell><cell>19.3</cell></row><row><cell>Wavesplit [10]</cell><cell>20.4</cell><cell>-</cell></row><row><cell cols="3">Reported Using Asteroid</cell></row><row><cell>Noise Reverb</cell><cell>[33]</cell><cell /></row><row><cell /><cell>14.2</cell><cell>16.8</cell></row><row><cell /><cell>12.0</cell><cell>13.7</cell></row><row><cell /><cell>8.9</cell><cell>10.6</cell></row><row><cell /><cell>9.2</cell><cell>11.0</cell></row><row><cell>Table</cell><cell /><cell /></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">An open source software system for robot audition HARK and its evaluation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Nakadai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">G</forename><surname>Okuno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Nakajima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hasegawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tsujino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Humanoids</title>
		<imprint>
			<biblScope unit="page" from="561" to="566" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The ManyEars open framework</title>
		<author>
			<persName><forename type="first">F</forename><surname>Grondin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Létourneau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Ferland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Rousseau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Michaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Autonomous Robots</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="217" to="232" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Blind enhancement of the rhythmic and harmonic sections by nmf: Does it help?</title>
		<author>
			<persName><forename type="first">B</forename><surname>Schuller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Weninger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Eyben</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Rigoll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICA</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="361" to="364" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep clustering: discriminative embeddings for segmentation and separation</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="31" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Permutation invariant training of deep models for speaker-independent multi-talker speech separation</title>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kolbaek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="241" to="245" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">TasNet: Time-domain audio separation network for real-time, single-channel speech separation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Mesgarani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="696" to="700" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Conv-TasNet: Surpassing ideal time-frequency magnitude masking for speech separation</title>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Trans. Audio, Speech, Lang. Process</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1256" to="1266" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Wavesplit: End-to-end speech separation by speaker clustering</title>
		<author>
			<persName><forename type="first">N</forename><surname>Zeghidour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.08933</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The Northwestern University Source Separation Library</title>
		<author>
			<persName><forename type="first">E</forename><surname>Manilow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Seetharaman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Pardo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISMIR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="297" to="305" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Onssen: an open-source speech separation and enhancement library</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Mandel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.00982</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Open-Unmix -a reference implementation for music source separation</title>
		<author>
			<persName><forename type="first">F.-R</forename><surname>Stöter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Uhlich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Liutkus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Mitsufuji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Open Source Soft</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">41</biblScope>
			<biblScope unit="page">1667</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.01703</idno>
		<title level="m">Py-Torch: An imperative style, high-performance deep learning library</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Two-step sound source separation: Training on learned latent targets</title>
		<author>
			<persName><forename type="first">E</forename><surname>Tzinis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Venkataramani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Subakan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Smaragdis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="31" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Dual-path RNN: Efficient long sequence modeling for time-domain single-channel speech separation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yoshioka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="46" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Single-channel multi-speaker separation using deep clustering</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Isik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Le Roux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Interspeech</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="545" to="549" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep attractor network for single-microphone speaker separation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Mesgarani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="246" to="250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Demystifying TasNet: A dissecting approach</title>
		<author>
			<persName><forename type="first">J</forename><surname>Heitkaemper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jakobeit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Boeddeker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Drude</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Haeb-Umbach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="6359" to="6363" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A comprehensive study of speech separation: Spectrogram vs waveform separation</title>
		<author>
			<persName><forename type="first">F</forename><surname>Bahmaninezhad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Interspeech</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4574" to="4578" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Universal sound separation</title>
		<author>
			<persName><forename type="first">I</forename><surname>Kavalerov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wisdom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Erdogan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Patton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">WASPAA</title>
		<imprint>
			<biblScope unit="page" from="175" to="179" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Filterbank design for end-to-end speech separation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Pariente</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cornell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Deleforge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="6364" to="6368" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A multi-phase gammatone filterbank for speech separation via TasNet</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ditter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Gerkmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="36" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Speaker recognition from raw waveform with SincNet</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ravanelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SLT</title>
		<imprint>
			<biblScope unit="page" from="1021" to="1028" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The NumPy array: A structure for efficient numerical computation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Van Der Walt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Colbert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computing in Science and Engineering</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="22" to="30" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Signal estimation from modified shorttime Fourier transform</title>
		<author>
			<persName><forename type="first">D</forename><surname>Griffin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Acoust., Speech, Signal Process</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="236" to="243" />
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A fast Griffin-Lim algorithm</title>
		<author>
			<persName><forename type="first">N</forename><surname>Perraudin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Balazs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Søndergaard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">WASPAA</title>
		<imprint>
			<biblScope unit="page" from="1" to="4" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Iterative phase estimation for the synthesis of separated sources from single-channel mixtures</title>
		<author>
			<persName><forename type="first">D</forename><surname>Gunawan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Letters</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="421" to="424" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">SDRhalf-baked or well done?</title>
		<author>
			<persName><forename type="first">J</forename><surname>Le Roux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wisdom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Erdogan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="626" to="630" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A deep learning loss function based on the perceptual evaluation of the speech quality</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Martín-Doñas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Peinado</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Letters</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1680" to="1684" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Multitalker speech separation with utterance-level permutation invariant training of deep recurrent neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kolbaek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z.-H</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Trans. Audio, Speech, Lang. Process</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1901" to="1913" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">WHAM!: extending speech separation to noisy environments</title>
		<author>
			<persName><forename type="first">G</forename><surname>Wichern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Antognini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">R</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Mcquinn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Crow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Manilow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Interspeech</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1368" to="1372" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">WHAMR!: Noisy and reverberant single-channel speech separation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Maciejewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wichern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Mcquinn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="696" to="700" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">LibriMix: An open-source dataset for generalizable speech separation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Cosentino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cornell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pariente</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Deleforge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Vincent</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.11262</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">What's all the fuss about free universal sound separation data?</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wisdom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Erdogan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P W</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Serizel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Turpault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Fonseca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Salamon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Seetharaman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>in preparation</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K A</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Beyrami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Gopal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.08662</idno>
		<title level="m">The Interspeech 2020 deep noise suppression challenge: Datasets, subjective speech quality and testing framework</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">SMS-WSJ: Database, performance measures, and baseline recipe for multi-channel source separation and recognition</title>
		<author>
			<persName><forename type="first">L</forename><surname>Drude</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Heitkaemper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Boeddeker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Haeb-Umbach</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.13934</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Analyzing the impact of speaker localization errors on speech separation for automatic speech recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sivasankaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Fohr</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">The MUSDB18 corpus for music separation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Rafii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Liutkus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F.-R</forename><surname>Stöter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">I</forename><surname>Mimilakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bittner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<author>
			<persName><forename type="first">W</forename><surname>Falcon</surname></persName>
		</author>
		<ptr target="https://github.com/PytorchLightning/pytorch-lightning" />
		<title level="m">Pytorch lightning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Tight integration of spatial and spectral features for BSS with deep clustering embeddings</title>
		<author>
			<persName><forename type="first">L</forename><surname>Drude</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Haeb-Umbach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Interspeech</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2650" to="2654" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Performance measurement in blind audio source separation</title>
		<author>
			<persName><forename type="first">E</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gribonval</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Févotte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Trans. Audio, Speech, Lang. Process</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1462" to="1469" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Perceptual evaluation of speech quality (PESQ) -a new method for speech quality assessment of telephone networks and codecs</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Rix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Beerends</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Hollier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Hekstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="749" to="752" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">An algorithm for intelligibility prediction of time-frequency weighted noisy speech</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Taal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Hendriks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Heusdens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Trans. Audio, Speech, Lang. Process</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2125" to="2136" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">The Kaldi speech recognition toolkit</title>
		<author>
			<persName><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ghoshal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Boulianne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Glembek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ASRU</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Alternative objective functions for deep clustering</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="686" to="690" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>