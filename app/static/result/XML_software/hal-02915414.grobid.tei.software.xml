<TEI xmlns="http://www.tei-c.org/ns/1.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xml:space="preserve" xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Knowledge Graph Enhanced Learner Model to Predict Outcomes to Questions in the Medical Field</title>
				<funder ref="#_SEVz4yt">
					<orgName type="full">Agence Nationale de la Recherche</orgName>
					<orgName type="abbreviated">ANR</orgName>
				</funder>
				<funder ref="#_r664cvr">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher />
				<availability status="unknown"><licence /></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Antonia</forename><surname>Ettorre</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University Côte d'Azur</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<orgName type="institution" key="instit3">Inria</orgName>
								<address>
									<region>I3S</region>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Oscar</forename><forename type="middle">Rodríguez</forename><surname>Rocha</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University Côte d'Azur</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<orgName type="institution" key="instit3">Inria</orgName>
								<address>
									<region>I3S</region>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Teach on Mars</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Catherine</forename><surname>Faron</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University Côte d'Azur</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<orgName type="institution" key="instit3">Inria</orgName>
								<address>
									<region>I3S</region>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Franck</forename><surname>Michel</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University Côte d'Azur</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<orgName type="institution" key="instit3">Inria</orgName>
								<address>
									<region>I3S</region>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Fabien</forename><surname>Gandon</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University Côte d'Azur</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<orgName type="institution" key="instit3">Inria</orgName>
								<address>
									<region>I3S</region>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Knowledge Graph Enhanced Learner Model to Predict Outcomes to Questions in the Medical Field</title>
					</analytic>
					<monogr>
						<imprint>
							<date />
						</imprint>
					</monogr>
					<idno type="MD5">7F0251D7E33958230CFF497AF5122255</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-04-12T14:51+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid" />
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Semantic Web</term>
					<term>Graph Embedding</term>
					<term>Prediction</term>
					<term>Learner Model</term>
					<term>Medical Field</term>
				</keywords>
			</textClass>
			<abstract>
<div><p>The training curriculum for medical doctors requires the intensive and rapid assimilation of a lot of knowledge. To help medical students optimize their learning path, the SIDES 3.0 national French project aims to extend an existing platform with intelligent learning services. This platform contains a large number of annotated learning resources, from training and evaluation questions to students' learning traces, available as an RDF knowledge graph. In order for the platform to provide personalized learning services, the knowledge and skills progressively acquired by students on each subject should be taken into account when choosing the training and evaluation questions to be presented to them, in the form of customized quizzes. To achieve such recommendation, a first step lies in the ability to predict the outcome of students when answering questions (success or failure). With this objective in mind, in this paper we propose a model of the students' learning on the SIDES platform, able to make such predictions. The model extends a state-ofthe-art approach to fit the specificity of medical data, and to take into account additional knowledge extracted from the <software>OntoSIDES</software> knowledge graph in the form of graph embeddings. Through an evaluation based on learning traces for pediatrics and cardiovascular specialties, we show that considering the vector representations of answers, questions and students nodes substantially improves the prediction results compared to baseline models.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div><head n="1">Introduction</head><p>Since 2013, teachers of French medical schools have been using a common national platform to create and give local evaluation tests on different devices. The Web-based platform, named SIDES (Intelligent Health Education System 3 ), allows to share these tests among medical schools to form a national database for training, and supports the preparation of medical students for the ECNi (National Computerized Ranking Tests).</p><p>The French national project SIDES 3.0 started at the end of 2017 and aims to develop a new version of the platform meant to offer user-centered intelligent services such as individual monitoring, enriched dashboards, personalized recommendations, augmented corrections for self-assessment, and a standardized digital environment for knowledge sharing. To achieve these goals, the approach taken leverages semantic Web models and technologies to enrich and integrate these resources in RDF with OWL ontologies. As part of the SIDES 3.0 project, existing data from the platform, such as annotated questions and students' learning traces, were converted into structured data expressed in RDF using the On-toSIDES OWL ontology <ref type="bibr" target="#b12">[13]</ref>, and stored in the <software ContextAttributes="used">OntoSIDES</software> knowledge graph.</p><p>Recommending questions to the students (i.e. the learners) in an intelligent way is a key element to achieve personalized and efficient individual learning. This requires the ability to take into account their profile, learning objectives and current level of knowledge in order to guide them in progressively improving their knowledge about a medical specialty. An important criterion for this tailored recommendation is the prediction of the outcomes of students to questions, since such predictions should allow to more effectively detect and adjust students' gaps.</p><p>Throughout the years, several research works have addressed this prediction relying on diverse machine learning techniques. Our goal is to propose a hybrid approach that combines Machine Learning and Knowledge Representation to take advantage of the most advanced learning architectures while exploiting the information provided by the knowledge graph. In this context, this paper addresses the following research questions:</p><p>-How to model students' learning on the SIDES platform to predict their outcomes to medical questions? -Which set of features should be extracted from the <software>OntoSIDES</software> knowledge graph and considered for learning the student model? -Can taking into account the knowledge graph structure of <software ContextAttributes="used">OntoSIDES</software> improve the performance of the prediction of students answers to questions?</p><p>To answer these questions, in this paper we present (1) our model to predict the outcome of students' answers to questions, and <ref type="bibr" target="#b1">(2)</ref> an evaluation of our model focused on the pediatrics and cardiovascular specialties. Our model was created on the basis of two state-of-the-art works on this domain: Knowledge Tracing Machines <ref type="bibr" target="#b19">[20]</ref> and Deep Knowledge Tracing Machines (<software ContextAttributes="used">DeepFM</software>) <ref type="bibr" target="#b6">[7]</ref>. We adapted the learning models proposed in these works to the <software ContextAttributes="used">OntoSIDES</software> knowledge graph, and extended them with calculated features and embeddings of graph nodes to exploit the knowledge captured in the <software ContextAttributes="used">OntoSIDES</software> graph. Through experimentation and evaluation, we validated a new model that makes the most accurate predictions by considering these features in the <software ContextAttributes="used">DeepFM</software> machine learning algorithm.</p><p>The remainder of this paper is organized as follows: In section 2, we review existing related works. We describe the <software>OntoSIDES</software> knowledge graph in section 3. The features extracted or computed from the OntoSIDES knowledge graph to model students' learning are detailed in section 4. In section 5, we present the experiments performed in order to define our model, and we analyse the results of these experiments in section 5.2. Finally, conclusions and future work are presented in section 6.</p></div>
<div><head n="2">Related Work</head><p>Several models have been proposed in the literature to measure and predict students' outcomes to questions. The Classical Test Theory (CTT) <ref type="bibr" target="#b11">[12]</ref> is a foundational work developed in the context of psychological tests. It builds on the assumption that the measurement of a test cannot be completely errorfree. Thus, student's observed score on a test is the sum of a true score (a score obtained if there were no measurement errors) and an error score. Several shortcomings of the CTT were underlined in <ref type="bibr" target="#b7">[8]</ref>, yet the major limitation with respect to our goal is that CTT is test-oriented and therefore is not suitable for modeling answers to individual items of a test.</p><p>The Item Response Theory (IRT) <ref type="bibr" target="#b7">[8]</ref> was proposed to overcome the shortcomings of the CTT. IRT models the relationship between persons and test items in order to predict the response of any student to any item even if similar students have never answered similar items before. The probability of correctly responding to an item is a monotonically increasing function of the measured latent trait (ability) of the student and some parameters of the question item (e.g. difficulty). For dichotomous question items, there are 4 IRT models, from one (1PL) to four parameters (4PL) models. The 1PL model (also called the Rasch model as it was originally suggested by Rasch <ref type="bibr" target="#b15">[16]</ref>) is the simplest IRT model. It describes the test items in terms of only one parameter: the item difficulty. The probability of responding correctly to an item given its difficulty and the ability level of the student is given by a logistic function. The 2PL model generalizes the 1PL model by adding the discrimination parameter. The 3PL model (which is not a logistic model unlike the previous two) generalizes the 2PL model by adding the (pseudo)guessing parameter which expresses the property that even very low ability persons have a positive probability of answering an item correctly, simply by randomly guessing the correct answer. Finally, the 4PL model adds a fourth parameter that models the "inattention" of high ability students failing to answer an easy item correctly.</p><p>Unlike IRT models, which are suitable for analyzing students' responses to items that measure a single latent trait, mIRT <ref type="bibr" target="#b16">[17]</ref> models allow to analyze richer question items that measure multiple latent traits simultaneously.</p><p>Additive Factors Model (AFM) <ref type="bibr" target="#b2">[3]</ref> is a predictive learning model based on the Logistic Regression algorithm, that takes into account student skill parameters, skill parameters and learning rates. In <ref type="bibr" target="#b13">[14]</ref>, the authors propose the Performance Factors Analysis (PFA) model. This model is also based on the Logistic Regression, however, unlike AFM, it takes into account the prior students' failures and successes on each skill to make predictions.</p><p>Bayesian Knowledge Tracing (BKT) <ref type="bibr" target="#b3">[4]</ref> is one of the most popular methods for modeling students' knowledge. It models the student's prior incorrect and correct responses to items of a particular skill in a Hidden Markov Model to estimate the probability that a student has mastered or not that skill. Its major limitation is that it cannot model the fact that question items may involve multiple skills.</p><p>Taking advantage of the advances of Deep Learning <ref type="bibr" target="#b9">[10]</ref>, in 2015, the Deep Knowledge Tracing (DKT) <ref type="bibr" target="#b14">[15]</ref> model was proposed to overcome the limitations of BKT. This model is based on the use of Recurrent Neural Networks (RNNs) to model student's learning and predict the outcomes to questions based upon students' prior activity. More specifically, two different types of RNNs are applied: a vanilla RNN model with sigmoid units and a Long Short Term Memory (LSTM) mode. However, in 2016, authors of <ref type="bibr" target="#b21">[22]</ref> have shown how IRT-based methods matched or even outperformed DKT. In particular, a hierarchical extension of IRT that captured item grouping structure performed the best. Additionally, a temporal extension of IRT improved performance over standard IRT while the RNN-based method did not.</p><p>More recently, Vie and Kashima proposed the Knowledge Tracing Machines (KTM) <ref type="bibr" target="#b20">[21]</ref> approach based on factorization machines (FM) <ref type="bibr" target="#b17">[18]</ref> to model and estimate students' learning. KTM encompasses several existing models in the educational literature as special cases, such as AFM, PFA, and mIRT. In addition, this approach provides a test bed to try new combinations of features in order to improve existing models. Finally, in <ref type="bibr" target="#b19">[20]</ref>, an approach similar to the previous one is presented, but based on Deep Factorization Machines (<software ContextAttributes="used">DeepFM</software>) <ref type="bibr" target="#b6">[7]</ref> as a classification algorithm. <software ContextAttributes="used">DeepFM</software> combines the power of FM for recommendation and Deep Learning for feature learning. The article compares the results obtained with <software ContextAttributes="used">DeepFM</software> with the ones obtained using logistic regression and Vanilla FM, showing that it outperforms the other algorithms. This is why, for the research work presented in this paper, we have taken this framework as the basis for testing new features and combinations of features to improve predictions.</p><p>When compared to the above-mentioned research works, the novelty of our model is that it exploits the knowledge captured in the <software>OntoSIDES</software> knowledge graph by means of text and graph embeddings of nodes.</p></div>
<div><head n="3">OntoSIDES</head><p><software ContextAttributes="used">OntoSIDES</software> <ref type="bibr" target="#b12">[13]</ref> is a knowledge graph that comprises a domain ontology represented in OWL and a set of factual statements about the entities on the SIDES platform, linking them to the ontology classes and properties. Being an RDF graph, it is possible to query <software ContextAttributes="used">OntoSIDES</software> with the standard query language <software ContextAttributes="used">SPARQL</software>. The <software ContextAttributes="used">OntoSIDES</software> knowledge graph was automatically generated from the relational database of the SIDES platform, and by enriching these data with the developed ontology.</p><p>The current version of the <software>OntoSIDES</software> OWL ontology contains 52 classes and 50 properties, mainly describing universities, faculties, users (students, professors, and administrative staff), tests, questions and answers. Here are the top classes of interest for our work:</p><p>Action (sides:action): the root class of actions that students can perform when they interact with the pedagogical resources of the SIDES platform. For example, it is possible to characterize the action of selecting the proposal of an answer to a question with subclass sides:action to answer. Content (sides:content): the root class of the hierarchy of resource types available in the SIDES platform. The class of questions (sides:question), the class of proposed answers to a question (sides:proposal of answer) and the class of answers (sides:answer) of a student to a question, are subclasses of sides:content.</p><p>Person (sides:person): class of persons involved in medical studies. Its subclasses correspond to the specific roles of SIDES users: for example, the class sides:student is a subclass of sides:person.</p><p>Figure <ref type="figure" target="#fig_0">1</ref> depicts the RDF graph representing an answer given by a student to a question with multiple possible answers. For each attempt of a student to a question, an instance of the class sides:answer is created. This answer is directly linked to the student through property sides:done by and to the question through sides:correspond to question. An answer is linked to multiple instances of sides:action to answer, each one representing the action of selecting a single sides:proposal of answer for the question. For example, question q666472 in Figure <ref type="figure" target="#fig_0">1</ref> is a multiple choice question (QMA) associated to two possible answers, prop3017738 and prop3017739, and student stu27880, while answering, has selected (sides:has wrongly ticked) the wrong option sides:prop3017739. The instances of sides:action to answer are used to compute the number of misticked and non-ticked answers and then the level of correctness of the given answer, value of property sides:has for result. Other useful nodes further describe questions: sides:has for textual content gives the text of a question, and sides:is linked to the medical speciality relates a question to the medical specialties it belongs to. It is also worth pointing out that questions are normally organized in evaluations that group sets of questions related to similar topics or concerning the same clinical case.</p><p>The <software>OntoSIDES</software> graph currently includes the description of 569,762,878 answers to 1,797,180 questions related to 31 medical specialties and given by 173,533 students. In total the knowledge graph contains more than 9.2 billion triples.</p></div>
<div><head n="4">Features Selected or Computed from OntoSIDES to Learn a Student Model</head><p>Based on the <software>OntoSIDES</software> knowledge graph, our aim is to predict the outcome of a student to a question, that is, the value related to an instance of sides:answer by property sides:has for result, which is equal to 1 if the student answered the question correctly, and 0 otherwise. Therefore this amounts to a binary classification.</p><p>In this section, we describe the candidate features that we selected or computed from the <software>OntoSIDES</software> knowledge graph to build a student model. We hypothesize that these features may improve the quality of the binary classification carried out by the algorithm to predict a student's outcome to a question. In section 5.2, we draw some conclusions with respect to this hypothesis based on the results of our experiments.</p></div>
<div><head n="4.1">Basic Features</head><p>A first set of basic (or raw) features concerns the entities that can be extracted by simply querying the <software>OntoSIDES</software> knowledge graph, without further processing. These features are as follows:</p><p>student: the identifier of a student who answers a question, specifically, the URI of an instance of class sides:student related to an instance of sides:answer by property sides:done by. answer: the identifier of an answer given by a student, that is, the URI of an instance of the class sides:answer. question: the identifier of a question answered by a student, that is, the URI of an instance of the class sides:question. timestamp: the date and time when a student answered a question, that is, the value related to an instance of sides:answer by property sides:has for timestamp.</p></div>
<div><head n="4.2">Calculated Features Conveying a Temporal Dimension</head><p>A set of additional features is computed from the above described raw features. They are meant to provide insight into students' level of knowledge over time, difficulty level of questions and number of prior attempts that a student carried out to answer a question. Together, they convey a temporal dimension to the model that is richer than the raw timestamp. These features are as follows:</p><p>wins: given a question and a student, it represents the number of times that this student has previously answered that question correctly. fails: given a question and a student, it represents the number of times that this student has previously answered that question incorrectly. attempts: wins + fails. question difficulty: for a given question, it is an estimation of its difficulty and assumes values between 0 and 1, 1 being the highest difficulty. It is computed by dividing the number of incorrect answers by the number of answers given to that question. static student ability: a static estimate of the student's overall ability, valued between 0 and 1, 1 being the highest ability. It is computed as the student's total number of correct answers divided by the student's total number of answers. progressive student ability: this feature follows the evolution of the student's ability over time. It draws her learning curve. For each attempt, it is computed as the ratio between the number of correct answers and the number of all the answers given by the student up to that moment. At the beginning of the training, the student's ratio of correct answers is likely to be low to medium. Then, in time, this ratio increases, reflecting the growth of her level of knowledge and expertise.</p></div>
<div><head n="4.3">Text Embeddings of Questions</head><p>We hypothesize that questions' text may provide valuable information to predict the answer of a student to a question. To test this hypothesis, we queried the <software ContextAttributes="used">OntoSIDES</software> knowledge graph to extract the text of the questions, i.e. the value of the property sides:has for textual content, and we computed their vector representation by using the state-of-the-art word embedding algorithm <software ContextAttributes="used">fastText</software> <ref type="bibr" target="#b1">[2]</ref>. We used the flair framework <ref type="bibr" target="#b0">[1]</ref> implementation which provides embeddings pre-trained with the French Wikipedia. Applying this approach to the text of each question yields vectors of 300 dimensions. Later on, we refer to this set of vectors as questions temb.</p></div>
<div><head n="4.4">Knowledge Graph Embeddings of Questions, Answers, and Users</head><p>Lastly, we hypothesize that the <software ContextAttributes="used">OntoSIDES</software> graph topology may convey valuable knowledge to predict the answer of a student to a question. To test this hypothesis, we used the state-of-the-art node2vec algorithm <ref type="bibr" target="#b5">[6]</ref> to construct vector representations of the knowledge graph nodes. node2vec learns continuous feature representations for the nodes in a graph by mapping each node to a low-dimensional space of features that maximizes the likelihood of preserving network neighborhoods of nodes. To do this, we used the SNAP project implementation <ref type="bibr" target="#b10">[11]</ref> <ref type="foot" target="#foot_1">4</ref> of node2vec to extract vector representations of dimension 100 for each of the nodes in our training dataset (described in Section 5). In the following, we refer to these vectors as answers gemb, questions gemb and students gemb respectively.</p></div>
<div><head n="5">Empirical Determination of a Learner Model</head><p>This section describes a comparative evaluation of several student models, that we carried out to determine which of them produces the best prediction of the students' answers to questions. Each model relies on a specific set of features selected among those described in section 4.</p></div>
<div><head n="5.1">Experimental Settings</head><p>Training Data. We trained the different models on a sub-graph of the SIDES knowledge graph containing the answers given by the sixth-year medical students during the 2018-2019 academic year. The sixth year corresponds to the last year of the second cycle of medical studies. Thus, predicting the outcomes of these students is a particularly relevant task because their activities are the most reliable indicator of their performance during the National Computerized Ranking Tests (ECNi).</p><p>The extracted sub-graph contains 96,511,040 answers to 831,057 different questions, given by 8,776 students. Given the large size of this sub-graph, to be able to train the models within reasonable time we decided to limit our experiments to the answers to questions related to pediatrics and cardiovascular, the two specialties with the largest number of answers. For each specialty, we randomly extracted 100,000 answers, obtaining the following sub-graphs:</p><p>pediatrics: 100,000 answers to 22,551 questions, given by 8,535 students; cardiovascular: 100,000 answers to 22,505 questions, given by 8,655 students.</p><p>Candidate Models. Relying on the benchmarking approach presented in <ref type="bibr" target="#b19">[20]</ref>, we defined 15 different models by combining the features described in section 4, in order to comparatively evaluate them and determine which one allows the classification algorithms to obtain the best prediction scores.</p><p>Each model is identified by a label whose letters each denotes a feature: s: students identifiers; q: questions identifiers; a: number of attempts; w: number of wins; f : number of fails; d: question difficulty; b: static student ability; b': progressive student ability; T: questions temb; Q: questions gemb; S: students gemb; R: answers gemb. With this notation, the candidate models are as follows.</p><p>The first three models correspond to state-of-the-art models and will serve as a baseline for richer models: sq is equivalent to the 1PL IRT model when used with the Logistic Regression algorithm. Used with the FM algorithm configured with a "number of factors used for pairwise interactions" greater than 0 (d&gt;20 ), this model is equivalent to the mIRT model <ref type="bibr" target="#b19">[20]</ref>. sqa is inspired by the AFM model as it takes into account the number of previous attempts but not the skills which is not among our features. sqawf is inspired by the AFM and PFA models as it takes into account the number of previous attempts and the distinction between correct and incorrect attempts.</p><p>Additionally, we consider the following models test other possible features combinations, notably involving text and graph embeddings: sqawfd, sqawfb, sqawfb', sqawfdbb', sqawfdbb'T, sqawfdbb'R, sqawfdbb'Q, sqawfdbb'S, sqawfdbb'RQ, sqawfdbb'TRQ, sqawfdbb'RQS and sqawfdbb'TRQS.</p><p>Classification Algorithm. As a result of our survey of related works (Section 2), we chose to rely on the <software ContextAttributes="used">DeepFM</software> classification algorithm for our experiments. We used the <software ContextAttributes="used">DeepCTR</software><ref type="foot" target="#foot_2">5</ref> Python implementation. The results reported in this paper were obtained with 256 layers of 256 units each, parameter initialize std of embedding vector was set to 0.001, and an L2 regularizer strength applied to embedding vector was set to 1e-06.</p><p>Hardware Setup. We used a Dell T640 GPU node equipped with 4 GTX 1080 Ti GPU cards, 2 Xeon Silver 4110 CPUs, 96 GB of RAM and 4 RAID-0 SSDs of 600 GB.</p><p>Temporality-Aware Cross-Validation. The performance of each model was evaluated by means of the student-based 5-fold cross-validation technique, in order to take into account the temporal dimension of the knowledge in the graph. Specifically, the list of students included in our dataset is split into five folds; four of them are directly used as training data while the remaining one is split again in two parts following the chronological order of answers: the first half is included into the training data while the last half is used as test set. The rationale behind this splitting method is as follows: for each fold, we train the model using the complete learning path of four fifths of the students, thus learning the entire trend of the students' knowledge acquisition, and learning information about all the questions. Then, using as training data the partial learning traces of the remaining students ensures that the training involves all the students. But by testing on their latest answers, we approach the real use case, in which we want to forecast future answers based on the training history of the student.</p><p>Evaluation Metrics. We evaluate the average of the results obtained on each fold in terms of Accuracy (ACC) which measures the percentage of correct predictions out of the total number of predictions, Area Under the ROC Curve (AUC) which measures the probability of correctness of each answer, F1-score and execution time.</p></div>
<div><head n="5.2">Results and Discussion</head><p>Table <ref type="table" target="#tab_0">1</ref> shows the results of the evaluation of each model on the pediatrics subgraph, using the <software ContextAttributes="used">DeepFM</software> algorithm. Columns "F1-score (pos.)" and "F1-score (neg.)" report the F1-score for the prediction of positive answers and negative answers respectively. The best results were obtained with the sqawfdbb'TRQS model (students, questions, attempts, wins, fails, question difficulty, student abilities (static and progressive), questions temb, answers gemb, questions gemb, students gemb), that is all of the features presented in section 4 (AUC=0.797, ACC=0.796). Beyond this overall result, comparing the scores obtained with each of the models can help us point out the contribution (positive, neutral or negative) of some of the features to the predictions: question difficulty, student abilities: When comparing the results obtained with models sqawf and sqawfdbb', we notice that adding the features question difficulty, static student ability and progressive student ability, increases both the ACC and AUC by approximately 3%. In particular, by comparing the models sqawfd, sqawfb and sqawfb' we see that the largest improvement (2.6%) in terms of AUC and ACC is due to introduction of the feature question difficulty, whereas static student ability has almost no effect on the quality of the prediction. questions temb: By comparing models sqawfdbb' and sqawfdbb'T, we notice that the results are significantly worse when including feature questions temb: ACC and AUC both decrease by about 5%. This result is somehow counterintuitive and may be related to the specificity and variety of medical vocabulary. To investigate further, additional experiments shall test the impact on word embeddings of techniques such as negative sampling, sub-sampling of common words or pre-processing to rewrite common medical expressions into single tokens.</p></div>
<div><head>Model</head><p>Let us also underline that including this feature substantially increases the execution time of the classification algorithm. questions gemb: When comparing the results obtained with models sqawfdbb' and sqawfdbb'Q, we observe that this feature has a negative impact on the prediction results in terms of ACC, AUC and F1-score. Again, this negative impact can seem counter-intuitive, in particular when considering that the embeddings of the answers have a significant positive impact (as described afterwards). students gemb: Similarly, feature students gemb seems to worsen the quality of the prediction. The values of ACC, AUC and F1-score are very close to those obtained when using questions gemb, presenting a decrease of 4% w.r.t. the same model without graph embeddings (sqawfdbb' ). answers gemb: Comparing the results obtained with models sqawfdbb' and sqawfdbb'R shows that this feature yields an improvement of 2% in terms of AUC and ACC. Also, a higher F1-score of 0.780 and 0.745 was obtained for the negative and positive responses respectively. Execution time remained low at 3 minutes approximately.</p><p>Although the contribution of the single features may seem negligible when they are considered separately, and, in some cases, even negative, the best performance in terms of ACC, AUC and F1-score is obtained when all the features are included in the model. Indeed, the best model sqawfdbb'TRQS presents an ACC and AUC around 80%, with a substantial increment (about 9%) when compared with the basic model (sq). It is also worth pointing out that even partial combinations of these newly added features bring significant improvements with respect to the models in which the single features are used alone. For example, even though students gemb and questions gemb do not improve the quality of the prediction when used separately, they yield a 3.5% increase of ACC and AUC when used in conjunction with answers gemb, as can be seen by comparing sqawfdbb'R and sqawfdbb'RQS. This could be explained by the fact that our model captures high-degree interactions between some features, interactions that, in some cases, turn out to be much more meaningful than the single features themselves.</p><p>Comparing the results of sqawfdbb'RQS and sqawfdbb'TRQS suggests that the improvement which questions temb is accountable for is ancillary, while it entails a significant execution time increase (14 minutes for sqawfdbb'TRQS vs. 8 minutes for sqawfdbb'RQS ). At a first sight, this may appear as a hindrance considering that we are only using a small fraction of the original data at our disposal. Nevertheless, we observe that, even with this small dataset, the quality of the prediction is fairly good. This suggests that, in the production environment of the SIDES platform, there shall be no need for training the algorithm on a much larger dataset in order to achieve good performance in the prediction task. We shall investigate further to determine a reasonable trade-off between the size of the dataset subset, the learning time and the quality of the prediction.</p><p>In order to validate our model and assess its flexibility with respect to the considered medical specialty, we trained and tested the <software ContextAttributes="used">DeepFM</software> learning algorithm on the sub-graph related to the cardiovascular answers, extracted as described in Section 5.1. As it can be seen in Table <ref type="table" target="#tab_1">2</ref>, the results for this new specialty are consistent and confirm what we observed earlier for the pediatrics sub-graph. As for the previous experiments, the best model appears to be sqawfdbb'TRQS, including all the possible features, i.e. basic and computed features and both text and graph embeddings. It produces the highest values of ACC (0.799), AUC (0.798) and F1-score (0.808 and 0.789 for positive and negative classes respectively). The new results confirm the modest impact of the questions temb feature, as can be seen by comparing the models sqawfdbb'TRQS and sqawfdbb'RQS. They also confirm the importance of the interactions between answers, questions and students graph embeddings. Indeed, in line with the previous case, we observe that, when used alone, features questions gemb and students gemb have a nega-tive impact on the accuracy and AUC of the model, while when used together with answers gemb, the quality of the prediction is improved.</p></div>
<div><head>Model</head><p>To sum up, our experiments show that the best student model combines a set of basic features obtained by directly querying the <software>OntoSIDES</software> knowledge graph -questions, attempts, wins, fails -, a set of additional features computed based on the basic ones -question difficulty, student ability (static and progressive) -, and the vector representations of the answers, questions and students nodes in the <software ContextAttributes="used">OntoSIDES</software> knowledge graph, as well as the vector representation of questions' text despite a modest impact.</p></div>
<div><head n="6">Conclusions and Future Work</head><p>In this article, we have presented, evaluated and compared several models to predict the outcome of medical students' answers to questions in pediatrics and cardiovascular specialties, on the SIDES platform, with the final goal of answering the three research questions presented in section 1. We have identified as the best model for our task the one based on Deep Knowledge Tracing Machines and relying on a rich set of features including state-of-the-art features such as wins, fails, questions' difficulties and students' abilities; textual information processed through NLP techniques (questions' text embeddings) and the structural knowledge provided by the <software>OntoSIDES</software> knowledge graph. In particular, we have shown that considering the vector representations of answers, questions and students nodes had a positive impact on the prediction results: when these three features are used in conjunction, the accuracy and AUC measures of the predictions made by the <software ContextAttributes="created">DeepFM</software> algorithm improved significantly.</p><p>As future work, we intend to consider several leads of improvement. First, we plan to evaluate our approach with other state-of-the-art graph representation algorithms, such as Complex <ref type="bibr" target="#b18">[19]</ref>, ConvE <ref type="bibr" target="#b4">[5]</ref> and LiteralE <ref type="bibr" target="#b8">[9]</ref>. Second, we wish to further exploit the knowledge contained into the graph by taking into account not only the assertional knowledge but also the ontology. Furthermore, we wish to investigate the reason why some features, such as the graph embeddings of question nodes, have a limited impact when used alone, while the impact is more important when they are used jointly with embeddings of other nodes. We also plan to extend our evaluation to questions and answers from other medical specialties present in the <software ContextAttributes="created">OntoSIDES</software> graph.</p><p>In the mid-term, we plan to identify other existing knowledge graphs containing medical training data to apply, evaluate and improve our approach. With respect to the SIDES 3.0 project, our final goal, beyond predicting answers, is to use the resulting trained model to design an algorithm that, by considering additional criteria, will be able to recommend to medical students a customized learning path that automatically adapts to their learning objectives and their current progress.</p></div><figure xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. RDF graph describing an answer of a student to a question. Blue bubbles are owl:Classes, white bubbles are instances and grey rectangles are literal values.</figDesc><graphic coords="6,134.77,115.84,345.82,127.25" type="bitmap" /></figure>
<figure type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Results for the pediatrics sub-graph. Models with the highest AUC are in bold.</figDesc><table><row><cell /><cell cols="3">ACC AUC F1-score (neg.) F1-score (pos.) Execution time</cell></row><row><cell>sq</cell><cell>0.712 0.711 0.729</cell><cell>0.693</cell><cell>0:01:38</cell></row><row><cell>sqa</cell><cell>0.715 0.710 0.739</cell><cell>0.692</cell><cell>0:01:48</cell></row><row><cell>sqawf</cell><cell>0.710 0.708 0.729</cell><cell>0.686</cell><cell>0:02:05</cell></row><row><cell>sqawfd</cell><cell>0.736 0.734 0.752</cell><cell>0.716</cell><cell>0:02:12</cell></row><row><cell>sqawfb</cell><cell>0.709 0.708 0.727</cell><cell>0.687</cell><cell>0:02:15</cell></row><row><cell>sqawfb'</cell><cell>0.722 0.723 0.734</cell><cell>0.707</cell><cell>0:02:14</cell></row><row><cell>sqawfdbb'</cell><cell>0.745 0.742 0.767</cell><cell>0.718</cell><cell>0:02:38</cell></row><row><cell>sqawfdbb'T</cell><cell>0.696 0.696 0.713</cell><cell>0.675</cell><cell>0:06:52</cell></row><row><cell>sqawfdbb'R</cell><cell>0.764 0.763 0.780</cell><cell>0.745</cell><cell>0:03:49</cell></row><row><cell>sqawfdbb'Q</cell><cell>0.708 0.706 0.725</cell><cell>0.687</cell><cell>0:03:55</cell></row><row><cell>sqawfdbb'S</cell><cell>0.706 0.704 0.730</cell><cell>0.677</cell><cell>0:03:56</cell></row><row><cell>sqawfdbb'RQ</cell><cell>0.776 0.775 0.789</cell><cell>0.759</cell><cell>0:06:26</cell></row><row><cell>sqawfdbb'TRQ</cell><cell>0.781 0.780 0.794</cell><cell>0.765</cell><cell>0:12:09</cell></row><row><cell>sqawfdbb'RQS</cell><cell>0.790 0.788 0.803</cell><cell>0.773</cell><cell>0:08:22</cell></row><row><cell cols="2">sqawfdbb'TRQS 0.797 0.796 0.811</cell><cell>0.779</cell><cell>0:14:10</cell></row></table></figure>
<figure type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Results for the cardiovascular sub-graph. Models with the highest AUC are in bold.</figDesc><table><row><cell /><cell cols="3">ACC AUC F1-score (neg.) F1-score (pos.) Execution time</cell></row><row><cell>sq</cell><cell>0.727 0.726 0.737</cell><cell>0.713</cell><cell>0:01:39</cell></row><row><cell>sqa</cell><cell>0.715 0.713 0.728</cell><cell>0.697</cell><cell>0:01:47</cell></row><row><cell>sqawf</cell><cell>0.719 0.718 0.730</cell><cell>0.705</cell><cell>0:02:05</cell></row><row><cell>sqawfd</cell><cell>0.741 0.741 0.750</cell><cell>0.730</cell><cell>0:02:54</cell></row><row><cell>sqawfb</cell><cell>0.720 0.719 0.733</cell><cell>0.705</cell><cell>0:02:55</cell></row><row><cell>sqawfb'</cell><cell>0.721 0.720 0.732</cell><cell>0.707</cell><cell>0:02:52</cell></row><row><cell>sqawfdbb'</cell><cell>0.746 0.745 0.757</cell><cell>0.733</cell><cell>0:02:12</cell></row><row><cell>sqawfdbb'T</cell><cell>0.701 0.701 0.714</cell><cell>0.687</cell><cell>0:07:04</cell></row><row><cell>sqawfdbb'R</cell><cell>0.770 0.769 0.778</cell><cell>0.759</cell><cell>0:03:49</cell></row><row><cell>sqawfdbb'Q</cell><cell>0.708 0.706 0.719</cell><cell>0.693</cell><cell>0:03:49</cell></row><row><cell>sqawfdbb'S</cell><cell>0.702 0.701 0.711</cell><cell>0.690</cell><cell>0:03:48</cell></row><row><cell>sqawfdbb'RQ</cell><cell>0.788 0.787 0.798</cell><cell>0.776</cell><cell>0:05:24</cell></row><row><cell>sqawfdbb'TRQ</cell><cell>0.791 0.789 0.798</cell><cell>0.781</cell><cell>0:10:18</cell></row><row><cell>sqawfdbb'RQS</cell><cell>0.796 0.795 0.806</cell><cell>0.784</cell><cell>0:07:00</cell></row><row><cell cols="2">sqawfdbb'TRQS 0.799 0.798 0.808</cell><cell>0.789</cell><cell>0:11:52</cell></row></table></figure>
			<note place="foot" n="3" xml:id="foot_0"><p>Système Intelligent d'Enseignement en Santé. http://side-sante.org</p></note>
			<note place="foot" n="4" xml:id="foot_1"><p>http://snap.stanford.edu/index.html</p></note>
			<note place="foot" n="5" xml:id="foot_2"><p>https://github.com/shenweichen/<software>DeepCTR</software></p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgement. This work is supported by the <rs type="funder">ANR</rs> <rs type="projectName">DUNE</rs> project <rs type="grantNumber">SIDES 3.0</rs> (<rs type="grantNumber">ANR-16-DUNE-0002-02</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_SEVz4yt">
					<idno type="grant-number">SIDES 3.0</idno>
					<orgName type="project" subtype="full">DUNE</orgName>
				</org>
				<org type="funding" xml:id="_r664cvr">
					<idno type="grant-number">ANR-16-DUNE-0002-02</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Contextual String Embeddings for Sequence Labeling</title>
		<author>
			<persName><forename type="first">A</forename><surname>Akbik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Blythe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vollgraf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Computational Linguistics (COLING 2018)</title>
		<meeting>the 27th International Conference on Computational Linguistics (COLING 2018)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1638" to="1649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Enriching Word Vectors with Subword Information</title>
		<author>
			<persName><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="135" to="146" />
			<date type="published" when="2017-12">Dec 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning Factors Analysis -A General Method for Cognitive Model Evaluation and Improvement</title>
		<author>
			<persName><forename type="first">H</forename><surname>Cen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Koedinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Junker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intelligent Tutoring Systems</title>
		<meeting><address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="164" to="175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Knowledge tracing: Modeling the acquisition of procedural knowledge</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">T</forename><surname>Corbett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Anderson</surname></persName>
		</author>
		<idno type="DOI">10.1007/BF01099821</idno>
		<ptr target="https://doi.org/10.1007/BF01099821" />
	</analytic>
	<monogr>
		<title level="j">User Modeling and User-Adapted Interaction</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="253" to="278" />
			<date type="published" when="1994-12">Dec 1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Convolutional 2D Knowledge Graph Embeddings</title>
		<author>
			<persName><forename type="first">T</forename><surname>Dettmers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Minervini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Stenetorp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Riedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd AAAI Conference on Artificial Intelligence (AAAI-18)</title>
		<meeting>the 32nd AAAI Conference on Artificial Intelligence (AAAI-18)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">node2vec: Scalable Feature Learning for Networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1607.00653" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">DeepFM: A Factorization-Machine based Neural Network for CTR Prediction</title>
		<author>
			<persName><forename type="first">H</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<idno type="DOI">10.24963/ijcai.2017/239</idno>
		<ptr target="https://doi.org/10.24963/ijcai.2017/239" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Joint Conference on Artificial Intelligence</title>
		<meeting>the 26th International Joint Conference on Artificial Intelligence<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-08">Aug 2017</date>
			<biblScope unit="page" from="1725" to="1731" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Fundamentals of Item Response Theory</title>
		<author>
			<persName><forename type="first">R</forename><surname>Hambleton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Swaminathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Rogers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Measurement Methods for the Social Science</title>
		<imprint>
			<publisher>SAGE Publications</publisher>
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Incorporating literals into knowledge graph embeddings</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kristiadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lukovnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lehmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Semantic Web -ISWC 2019</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="347" to="363" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="DOI">10.1038/nature14539</idno>
		<ptr target="https://doi.org/10.1038/nature14539" />
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="issue">7553</biblScope>
			<biblScope unit="page" from="436" to="444" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">SNAP: A General-Purpose Network Analysis and Graph-Mining Library</title>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sosič</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Intelligent Systems and Technology (TIST)</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The axioms and principal results of classical test theory</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Novick</surname></persName>
		</author>
		<idno type="DOI">10.1016/0022-2496(66)90002-2</idno>
		<ptr target="https://doi.org/10.1016/0022-2496(66)90002-2" />
	</analytic>
	<monogr>
		<title level="j">Journal of Mathematical Psychology</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="18" />
			<date type="published" when="1966">1966</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">OntoSIDES: Ontology-based student progress monitoring on the national evaluation system of French Medical Schools</title>
		<author>
			<persName><forename type="first">O</forename><surname>Palombi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Jouanot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Nziengam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Omidvar-Tehrani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Rousset</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sanchez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence in Medicine</title>
		<imprint>
			<biblScope unit="volume">96</biblScope>
			<biblScope unit="page" from="59" to="67" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Performance Factors Analysis -A New Alternative to Knowledge Tracing</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">I</forename><surname>Pavlik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Cen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Koedinger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2009 Conference on Artificial Intelligence in Education: Building Learning Systems That Care: From Knowledge Representation to Affective Modelling</title>
		<meeting>the 2009 Conference on Artificial Intelligence in Education: Building Learning Systems That Care: From Knowledge Representation to Affective Modelling<address><addrLine>Amsterdam, The Netherlands</addrLine></address></meeting>
		<imprint>
			<publisher>IOS Press</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="531" to="538" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep Knowledge Tracing</title>
		<author>
			<persName><forename type="first">C</forename><surname>Piech</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bassen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ganguli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sahami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 28</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Cortes</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="page" from="505" to="513" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Probabilistic Models for Some Intelligence and Attainment Tests</title>
		<author>
			<persName><forename type="first">G</forename><surname>Rasch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Studies in mathematical psychology</title>
		<imprint>
			<publisher>Danmarks Paedagogiske Institut</publisher>
			<date type="published" when="1960">1960</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The Past and Future of Multidimensional Item Response Theory</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Reckase</surname></persName>
		</author>
		<idno type="DOI">10.1177/0146621697211002</idno>
		<ptr target="https://doi.org/10.1177/0146621697211002" />
	</analytic>
	<monogr>
		<title level="j">Applied Psychological Measurement</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="25" to="36" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Factorization Machines</title>
		<author>
			<persName><forename type="first">S</forename><surname>Rendle</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICDM.2010.127</idno>
		<ptr target="https://doi.org/10.1109/ICDM.2010.127" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 IEEE International Conference on Data Mining</title>
		<meeting>the 2010 IEEE International Conference on Data Mining<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="995" to="1000" />
		</imprint>
	</monogr>
	<note>ICDM '10</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Knowledge Graph Completion via Complex Tensor Factorization</title>
		<author>
			<persName><forename type="first">T</forename><surname>Trouillon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Dance</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">É</forename><surname>Gaussier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Bouchard</surname></persName>
		</author>
		<idno>abs/1702.06879</idno>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep Factorization Machines for Knowledge Tracing</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Vie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th Workshop on Innovative Use of NLP for Building Educational Applications</title>
		<meeting>the 13th Workshop on Innovative Use of NLP for Building Educational Applications<address><addrLine>New Orleans, Louisiana (USA)</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Knowledge Tracing Machines: Factorization Machines for Knowledge Tracing</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Vie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kashima</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33th AAAI Conference on Artificial Intelligence (AAAI-19)</title>
		<meeting>the 33th AAAI Conference on Artificial Intelligence (AAAI-19)<address><addrLine>Honolulu, Hawai (USA)</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Back to the Basics: Bayesian extensions of IRT outperform neural networks for proficiency estimation</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">H</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Karklin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ekanadham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th International Conference on Educational Data Mining (EDM 2016)</title>
		<meeting>the 9th International Conference on Educational Data Mining (EDM 2016)<address><addrLine>Raleigh, NC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>