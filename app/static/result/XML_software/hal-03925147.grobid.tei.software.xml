<TEI xmlns="http://www.tei-c.org/ns/1.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xml:space="preserve" xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Anaphora Resolution in Dialogue: System Description (CODI-CRAC 2022 Shared Task)</title>
				<funder ref="#_bWvVpT5">
					<orgName type="full">German Ministry of Education and Research (BMBF)</orgName>
				</funder>
				<funder ref="#_dPtenwP #_ZPMtJyU">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher />
				<availability status="unknown"><licence /></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Tatiana</forename><surname>Anikina</surname></persName>
							<email>tatiana.anikina@dfki.denatalia.skachkova@dfki.de</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">DFKI</orgName>
								<orgName type="institution" key="instit2">Saarland Informatics Campus</orgName>
								<address>
									<settlement>Saarbrücken</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Natalia</forename><surname>Skachkova</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">DFKI</orgName>
								<orgName type="institution" key="instit2">Saarland Informatics Campus</orgName>
								<address>
									<settlement>Saarbrücken</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Joseph</forename><surname>Renner</surname></persName>
							<email>joseph.renner@inria.frpriyansh.trivedi@inria.fr</email>
							<affiliation key="aff1">
								<orgName type="institution">Inria</orgName>
								<address>
									<settlement>Nancy</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Priyansh</forename><surname>Trivedi</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Inria</orgName>
								<address>
									<settlement>Nancy</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Anaphora Resolution in Dialogue: System Description (CODI-CRAC 2022 Shared Task)</title>
					</analytic>
					<monogr>
						<imprint>
							<date />
						</imprint>
					</monogr>
					<idno type="MD5">2D1BE1E89C5A4E03E69CB7CC4E2EF5E0</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-04-12T14:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid" />
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div><p>We describe three models submitted for the CODI-CRAC 2022 shared task. To perform identity anaphora resolution, we test several combinations of the incremental clustering approach based on the Workspace Coreference System (WCS) with other coreference models. The best result is achieved by adding the "cluster merging" version of the coref-hoi model, which brings up to 10.33% improvement 1 over vanilla WCS clustering. Discourse deixis resolution is implemented as multi-task learning: we combine the learning objective of corefhoi with anaphor type classification. We adapt the higher-order resolution model introduced in Joshi et al. ( <ref type="formula">2019</ref>) for bridging resolution given gold mentions and anaphors.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div><head n="1">Introduction</head><p>In this paper we present our systems submitted for the CODI-CRAC 2022 Shared Task (CCST) on Anaphora, Bridging, and Discourse Deixis in Dialogue<ref type="foot" target="#foot_2">2</ref>  <ref type="bibr" target="#b26">(Yu et al., 2022)</ref>. The task is a follow-up to the one held last year and described in <ref type="bibr" target="#b6">Khosla et al. (2021)</ref>. As its name suggests, besides identity anaphora this shared task tries to cover other, less-studied, anaphoric phenomena, and offers new multi-genre data that combines several types of annotations in Universal Anaphora<ref type="foot" target="#foot_3">3</ref> format.</p><p>Main focus of the shared task is on dialogue. Dialogue data offers new challenges, like grammatically incorrect utterances, disfluencies, more deictic references, speaker grounding and long-distance conversation structure <ref type="bibr" target="#b6">(Khosla et al., 2021)</ref>. While coreference resolution in text has been very actively studied in the recent years, it is much less researched in dialogue, especially such forms as bridging, or discourse deixis. Descriptions of early systems implemented for the resolution of 'standard' and discourse deictic pronouns in dialogue can be found, e.g., in <ref type="bibr" target="#b1">Byron (2002)</ref>, <ref type="bibr" target="#b21">Strube and Müller (2003)</ref>, <ref type="bibr" target="#b13">Müller (2008)</ref>. More approaches (not implemented), together with some useful findings are presented, e.g., in <ref type="bibr" target="#b18">Rocha (1999)</ref>, <ref type="bibr" target="#b3">Eckert and Strube (2000)</ref>, and <ref type="bibr" target="#b14">Navarretta (2004)</ref>.</p><p>CCST 2021 stirred new interest in coreference resolution in dialogue. The majority of systems submitted for it represent various modifications of either the higher-order coreference resolution model (coref-hoi) by <ref type="bibr" target="#b24">Xu and Choi (2020)</ref>, or one of the earlier models by <ref type="bibr" target="#b5">Joshi et al. (2019)</ref> and <ref type="bibr" target="#b11">Lee et al. (2018)</ref>. These models were originally trained on the text data, and are span-based -each span gets associated with a score, and anaphor-antecedent pairs are established based on the pairwise scores. Designed for identity anaphora resolution, these models were also adapted for bridging and discourse deixis resolution. Examples of span-based models submitted for CCST 2021 include systems by <ref type="bibr" target="#b9">Kobayashi et al. (2021)</ref>, <ref type="bibr" target="#b17">Renner et al. (2021)</ref>, <ref type="bibr" target="#b25">Xu and Choi (2021)</ref>. Other participants presented different approaches. Thus, <ref type="bibr" target="#b7">Kim et al. (2021)</ref> perform identity anaphora and bridging resolution using pointer networks. <ref type="bibr" target="#b0">Anikina et al. (2021)</ref> cast anaphora resolution as a clustering problem, and discourse deixis resolution -as a Siamese Net based scoring function.</p><p>Inspired by the success of the span-based coreference resolution models, we submit three independent systems for CCST 2022. Our system for identity anaphora resolution uses both the Workspace Coreference System by <ref type="bibr" target="#b0">Anikina et al. (2021)</ref> and the coref-hoi model as described in Section 2. The model for discourse deixis extends coref-hoi with shallow linguistic features and aims at resolving three types of potential anaphors. It is described in Section 3. The model for bridging resolution is a modification of the system by <ref type="bibr" target="#b17">Renner et al. (2021)</ref>. The approach is explained in Section 4.</p></div>
<div><head n="2">Anaphora Resolution</head><p>For the anaphora resolution track we trained and combined the outputs of the Workspace Coreference System (WCS) and the coref-hoi system (see Table <ref type="table" target="#tab_1">1</ref>). While working on the shared task we realized that a combination of different models performs better than a single model and we explored various settings to find an optimal solution.</p></div>
<div><head n="2.1">Data</head><p>For training of the WCS system we used the datasets recommended by the shared task organizers. These include the ARRAU corpus <ref type="bibr">(Gnome,</ref><ref type="bibr">Trains_91,</ref><ref type="bibr">Trains_93,</ref><ref type="bibr">RST_DTreeBank,</ref><ref type="bibr">Pear_stories)</ref>, AMI, Switchboard, Light and Persuasion data. We used the development sets of AMI, Light and Persuasion for the internal evaluation and comparison of different configurations. We trained our system using the gold mention spans to avoid any mistakes introduced by the mention extraction module and used <software ContextAttributes="used">SpaCy</software> <ref type="bibr" target="#b4">(Honnibal et al., 2020)</ref> for mention extraction during the test phase.</p><p>For training of the coref-hoi system, we utilized the CoNLL 2012 English Shared Task dataset <ref type="bibr" target="#b16">(Pradhan et al., 2012)</ref> to supplement the datasets listed in the previous paragraph. Note that this CoNLL 2012 data does not include singleton coreference clusters, but the current dialogue shared task datasets do.</p></div>
<div><head n="2.2">Model architecture</head><p>WCS Our model is based on the implementation described in <ref type="bibr" target="#b0">Anikina et al. (2021)</ref>. It creates coreference clusters incrementally and compares each new mention to the clusters that are available in the workspace. The general flow of the model is presented in Figure <ref type="figure">1</ref>. The model uses separate layers to encode each pair of mentions where one mention represents a workspace cluster and another mention is a candidate that is being clustered. WCS passes the concatenated embeddings of the candidate mention and the cluster member through several feed-forward neural layers with the input and output dimensions shown in Table <ref type="table">2</ref>.</p><p>The network also encodes the absolute position of each mention within the document and generates a separate embedding for each speaker. The model combines this information with different word embeddings. For each mention it extracts the head and encodes it with a combination of contextual BERT embeddings <ref type="bibr" target="#b2">(Devlin et al., 2018)</ref>  (bert-base-cased) together with <software ContextAttributes="used">GloVe</software> <ref type="bibr" target="#b15">(Pennington et al., 2014)</ref> and Numberbatch <ref type="bibr" target="#b20">(Speer et al., 2017)</ref> embeddings. Unlike <ref type="bibr" target="#b0">Anikina et al. (2021)</ref> we do not generate a new random embedding for each unknown word, but take an average embedding based on all words in the <software ContextAttributes="used">GloVe</software> and Numberbatch vocabularies. This gave us slightly better results in the pilot experiments.</p><p>In order to represent the spans we take an average of all individual word embeddings based on BERT and <software>GloVe</software> correspondingly. We also experimented with <software ContextAttributes="used">SpanBERT</software> embeddings but did not observe any improvements. E.g., when we replaced our span embeddings with <software ContextAttributes="used">SpanBERT</software> and left the rest of the system unchanged we achieved 66.68% CoNLL F1 score when training and evaluating on the Light dataset. After replacing <software ContextAttributes="used">SpanBERT</software> with standard BERT and simply averaging span embeddings we achieved 67.23% CoNLL F1 score on the same data. Removing <software ContextAttributes="used">GloVe</software> embeddings and leaving only BERT, <software ContextAttributes="used">SpanBERT</software> and Numberbatch or training on more data samples also did not help. We suspect that since <software ContextAttributes="used">SpanBERT</software> embeddings have high dimensionality (representing span start, span end and span head) they dominate mention representation in WCS and allow some vague semantic matches. E.g., with <software ContextAttributes="used">SpanBERT</software> we generated clusters that included mentions like 'war' and 'peace' or 'the jamaica tourist board' and 'jamaican'. Training for more epochs or adjusting hyperparameters might help to improve clustering but the configurations that we tested have not shown an improvement.</p><p>The WCS system combines three cross-entropy losses that are added in each forward pass. The main clustering loss compares the true cluster probabilities vs. the computed ones. The true probabilities are computed with respect to the mentions that are currently in the workspace. For each mention</p></div>
<div><head>Track</head></div>
<div><head>Resolution of anaphoric identities</head></div>
<div><head>Setting</head><p>Predicted mentions Baseline WCS <ref type="bibr" target="#b0">(Anikina et al., 2021)</ref> and coref-hoi model <ref type="bibr" target="#b24">(Xu and Choi, 2020)</ref> Approach 1) Extract all nominal phrases with <software ContextAttributes="used">SpaCy</software> 2) Run WCS trained on the Shared Task dialogue data 3) Run coref-hoi with cluster merging trained on the CoNLL 2012 data 4) Combine the outputs of WCS and coref-hoi  the probability of being in that cluster is defined as the ratio of mentions that are in the same gold cluster and the current cluster over all mentions in that cluster. The coherence loss computes the difference between the gold cluster assignments and the system assignments. Basically, we create two matrices that align mentions to each other and check the overlap between these matrices in the gold annotations vs. the generated outputs (the matrix has ones if two mentions belong to the same cluster and zeros otherwise). The referring loss is used for the referring expression classification which is a binary classification task. It is needed since not all mention spans extracted by <software ContextAttributes="used">SpaCy</software> are valid referring expressions.</p><p>After computing clustering probabilities for each mention and clusters in the workspace we apply softmax and select the cluster with the highest probability. After that the workspace is updated and some clusters are moved to the history if they have not been updated for more than 100 steps. After the initial clustering we apply some post-processing as explained in <ref type="bibr" target="#b0">Anikina et al. (2021)</ref>.</p><p>We have also evaluated WCS in combination with a Crosslingual Coreference System (CCS) based on <software ContextAttributes="used">AllenNLP</software> and <software ContextAttributes="used">SpaCy</software> pipelines<ref type="foot" target="#foot_4">4</ref> . We noticed that WCS performs quite well on identifying singletons and clusters with personal pronouns but has more difficulties with other nominal phrases. Hence, in one of the experiments we combined the output of the CCS model trained on <software ContextAttributes="used">OntoNotes</software> that uses <software ContextAttributes="used">MiniLM</software> <ref type="bibr" target="#b22">(Wang et al., 2020)</ref> for mention representation with the outputs of WCS trained on the shared task data. Among the clusters generated with CCS we selected only those that do not contain any personal pronouns and from WCS we took singletons and clusters with pronouns.</p><p>We also experimented with some compatibility checks. E.g., we checked whether the first and second mentions in the cluster have the same number and we removed the first mention from the WCS cluster if the embedding similarity between the first pronoun and the first noun in that cluster was too low (compared to the cosine similarity between the first pronoun and other mentions in the cluster). E.g., mentions such as 'a presenter' and 'I' could belong to the same cluster with pronouns but mentions like 'table' and 'I' should not. We run WCS with these modifications on the shared task test set and report our results in Table <ref type="table">3</ref>. The final version that was submitted to the leaderboard combines WCS outputs with the coref-hoi system as described in the next section.</p></div>
<div><head>Coref-HOI Combination</head><p>We trained a "cluster merging" variant of the coref-hoi model. As this model was developed using the data from 2012 CoNLL dataset, which does not include singleton clusters, the model does not output singleton predictions off the shelf (one could potentially use the scores for the "dummy" antecedent as a proxy, but this could be noisy as the model is not trained to differentiate singleton clusters from simple mentions  <ref type="table" target="#tab_3">4</ref>. Looking at these scores, we found that coref-hoi struggled with singleton clusters (as expected), as the CoNLL F1 score of these predictions was much higher after removing the singletons from the annotations. WCS, on the other hand, seemed to do better on singletons than non-singletons, as evidenced by the higher scores on annotations that contain singletons vs. those without. As a result, we combined the strengths of the two systems by simply adding the singletons predictions of WCS to the cluster predictions of coref-hoi. This resulted in the highest test set scores (as shown in Table <ref type="table">3</ref>).</p></div>
<div><head n="2.3">Training WCS</head><p>The WCS system was trained for 5 epochs on Nvidia GeForce RTX 2080. We use teacher forcing for the coreference clusters with a ratio of 30%. The learning rate is set to 1e-4 and the dropout rate is 0.3. We use Adam as optimizer. It took about 26 hours to train the whole system on the complete training set.</p><p>Coref-HOI The coref-hoi system was trained for 24 epochs on a Nvidia Quadro RTX 6000. We use a pretrained <software>SpanBERT</software> Large model to initialize the base language model. We use a learning rate of 1e-5 for the base model and 3e-4 for the fine tuning layers. We follow all other hyperparameters found in the train_spanbert_large_ml0_cm_fn1000 training configuration of the coref-hoi system. Training took about 24 hours.</p></div>
<div><head n="2.4">Results and discussion</head><p>Our results on the internal development set as well as on the official test set are reported in Tables <ref type="table">3</ref> and<ref type="table" target="#tab_3">4</ref>. Based on the final cluster assignments we can recognize 4 common types of mistakes made by WCS: partial word overlaps (e.g., 'mute button' and 'volume button'), embedded mentions (e.g., 'a power supply which we get' and 'we'), wrong span boundaries (e.g., 'ok good knight') and confusing candidates that have similar surface forms but different meanings (e.g., 'the minutes of uh this meeting' and 'forty minutes'). Some of these mistakes were probably caused by the over-reliance of WCS on the head embeddings. Interestingly, when using <software ContextAttributes="used">SpanBERT</software> instead of <software ContextAttributes="used">GloVe</software> and standard BERT for span encoding we observed that many generated clusters contain mentions with spurious connections (e.g., 'the spirits of our people' and 'such dark superstitions' or 'the executive' and 'the company').</p><p>Judging from the scores on the development set reported in Table <ref type="table" target="#tab_3">4</ref>, WCS shows better performance than coref-hoi when the evaluation is done on all clusters including singletons. However, when singletons are excluded coref-hoi outperforms WCS and this was the main motivation to combine the outputs of both models. We also evaluated the span extraction performance of WCS vs. the combined system using the gold mention span annotations provided by the shared task organizers. We found that WCS had consistently higher recall but lower precision on mention span detection compared to the combined model. E.g., on the AMI dataset WCS achieved precision 82% and recall 68% whereas the combined model achieved precision 84% and recall 63%. Similar results were observed on the other two datasets that we tested (Light and Persuasion).</p><p>Looking at the mistakes of the combined model we found that some mentions have incorrect spans, e.g., 'half' and 'hour' are annotated as two separate mentions in 'see you in half and hour'. Sometimes the annotated spans are longer than the gold ones, e.g., 'close tabs on you' instead of 'close tabs' or 'Of course , good Monk' instead of 'good Monk'. This can also result in incorrect clustering such as in case of putting 'this realm' and 'this realm, stories, population' in the same cluster. The combined model also struggles with the cases like 'some' and 'they' in the following example: 'Some don't give the money out like they are suppose to. Did you heard that they now do every payment taken from people transparent?' Both mentions were assigned to the same coreference chain although 'some' should refer to the people who give the money and 'they' to those who receive it. Despite some problems with the mention span detection the combined model shows overall better clustering performance compared to vanilla WCS.</p><p>Experimenting with various combinations of the coreference systems we found that combining the strengths of different systems helps to improve the results. In the future we plan to investigate whether adding coreference signal from the pretrained models also helps boost the performance and reduce training time for systems like WCS.</p><p>For the current submission we combined the model outputs based on some simple heuristics but it would be interesting to see whether this process could be also learned by a model. Training a new model from scratch or even fine-tuning it on a new dataset might be sub-optimal or even not feasible in some cases. E.g., when we deal with dialogues instead of narrative texts or if the annotation schemes differ significantly. In such cases we believe that using a smart coreference editor that combines and checks outputs of different systems and applies some constraints or filters would be beneficial and we would like to work on such project in the future.</p></div>
<div><head n="3">Discourse Deixis Resolution</head><p>CCST 2022 offers three different tracks for discourse deixis resolution. First track (Eval-DD Pred) assumes finding antecedents for discourse deixis anaphors predicted by models given unannotated data. The second one (Eval-DD Gold M) aims at identification of discourse deixis anaphors among all types of annotated anaphors and non-referential mentions, and their subsequent resolution. The goal of the last track (Eval-DD Gold A) is to find antecedents for already annotated discourse deixis anaphors. Our team participated in all three tracks.</p><p>The core of our approach relies on the corefhoi model, because it was successfully adopted for CCST 2021 discourse deixis track by <ref type="bibr" target="#b9">Kobayashi et al. (2021)</ref>. Their model was able to achieve the CoNLL F1 score of 35.4% -52.1% depending on the dataset and shared task track, and ranked first for discourse deixis <ref type="bibr" target="#b9">(Kobayashi et al., 2021)</ref>. The summary of our system can be found in Table <ref type="table" target="#tab_4">5</ref>.</p></div>
<div><head n="3.1">Data</head><p>We use training and development data presented in Section 2.1. Coref-hoi splits input data into segments of a set length to limit the number of mention candidates. Given a segment, all possible spans/potential mentions are created. Next, this 'pool' of mentions is used to form valid anaphorantecedent pairs. In contrast to that, we only consider the occurrences of 'this', 'that', 'it' and 'which' as potential anaphors and treat all other spans in the segment as antecedent candidates. These four markables were chosen based on our observation that they often occur as discourse deixis anaphors in our training data: they make about 72.3% of all annotated discourse deictic anaphors<ref type="foot" target="#foot_5">5</ref> . Similar statistical findings (however, for other dialogue corpora) were reported, e.g., by <ref type="bibr" target="#b23">Webber (1988)</ref>, <ref type="bibr" target="#b13">Müller (2008)</ref>, <ref type="bibr" target="#b10">Kolhatkar et al. (2018)</ref>. Besides being discourse deictic, the markables in focus can also be non-referential (e.g., 'it' in expletive constructions, 'that' as a relative pronoun), or anaphoric (e.g., 'this' as a determiner in a noun phrase).</p><p>Because we focus only on certain anaphor candidates, we build segments in a slightly different way than coref-hoi does. Instead of splitting the input into non-overlapping chunks of approximately the same length, we go through the input data word by word until any of our anaphors occurs, and then create a segment. Our segment typically includes all (sub)tokens up to the current sentence end to the right of the anaphor, as well as one or more sentences to the left of it. We limit the segment's length by 256 (sub)tokens. Thus, given the same input, we build more segments than coref-hoi does, our segments are mostly overlapping, and each one contains only one anaphor candidate.</p><p>In total we build 9,827 segments/examples from training data, of which 44% contain non-referential 'this', 'that', 'it' and 'which', 41.2% -anaphoric, and only 14.8% -discourse deictic ones. To make our training data balanced, we perform undersam-</p></div>
<div><head>Track</head></div>
<div><head>Resolution of discourse deixis Setting</head><p>Predicted mentions / Gold mentions / Gold anaphors Baseline</p><p>The coref-hoi model adopted for discourse deixis by <ref type="bibr" target="#b9">Kobayashi et al. (2021)</ref> Approach 1) Consider all mentions of this, that, it and which potential anaphors 2) Consider all spans in the given segment potential antecedents 3) Represent both anaphor and antecedent candidates as embeddings with additional shallow linguistic features 4) Calculate pairwise anaphor-antecedent scores similar to coref-hoi and choose the antecedent based on the largest score 5) Use anaphor-antecedent pair representation to classify the anaphor type and discard non-discourse deictic anaphors</p></div>
<div><head>Train data</head><p>ARRAU corpus (Gnome, Trains_91, Trains_93, RST_DTreeBank, Pear_stories), AMI, Switchboard, Light and Persuasion Dev data AMI, Light, Persuasion (dev splits) pling and decrease the number of examples from the first two classes. We end up having 1,454 training samples of each anaphor class. For the sake of simplicity, undersampling is done blindly, i.e. we do not take into consideration how the instances of our three classes are distributed given each of the four markables.</p></div>
<div><head n="3.2">Model architecture</head><p>We perform discourse deixis resolution using a multi-task learning approach -besides finding the antecedents, we also need to identify the types of potential anaphors (discourse deictic, anaphoric or non-referential). Type classification is performed after the antecedent (if any) is found. It is also important to emphasize that we try to resolve any potential anaphor regardless of its type. Thus, our model also learns to resolve 'standard' coreference as a by-product. To our knowledge, our model is the first one doing that.</p><p>To perform the resolution, coref-hoi first associates each span (represented as an embedding) with a score indicating how likely this span is a valid mention (anaphor or antecedent). To speed up the training process, certain number of spans with the low scores get pruned. Next, the model learns to find the most probable antecedent for each anaphor based on their pairwise scores.</p><p>We modify their approach as follows. First, as we know exactly which span our anaphor x is, and it is the same for all antecedent candidates y, we do not score anaphors or calculate pairwise mention scores. An antecedent score s m (y) is produced by a feedforward neural network FFNN m taking as input a vector representation of span y, like in coref-hoi. Second, as shown in Table <ref type="table" target="#tab_6">6</ref>, anaphors k x = p x , ρ(x) and antecedents q y = g y , ψ(y) are composed differently. Main representations p x and g y are concatenated with shallow linguistic features ρ(x) and ψ(y) to help our model better differentiate between types of anaphors and antecedent candidates. Our approach to mention representation and motivation behind it are explained in more detail in Section 3.3. Third, we do not prune any unlikely antecedents due to the fact that each segment only contains one anaphor, which often has only one antecedent (if mention is anaphoric, there can be more). If we apply pruning, this only antecedent is very likely to be lost at the early stages of training.</p><formula xml:id="formula_0">s(x, y) = s m (y) + s f (x, y) + s s (x, y) s m (y) = FFNN m (q y ) q y = g y , ψ(y) k x = p x , ρ(x) s f (x, y) = k x • q y s s (x, y) = FFNN c (k x , q y , ϕ(x, y)) (1)</formula><p>As shown in Equation group 1, the final anaphorantecedent score is the sum of three components:</p><p>(1) anaphor score s m (y); (2) fast score s f (x, y), which is an inner product of vectors k x and q y representing anaphor and antecedent, respectively; (3) slow score s s (x, y), which is an output of a different network FFNN c taking as input an anaphorantecedent pair and pairwise features ϕ(x, y). Two of pairwise features are borrowed from the corefhoi model. They are distance feature, showing how many sentences/utterances lie between the starting tokens of two mentions, and similarity feature, which is simply a result of am element-wise multiplication of anaphor and antecedent candidate  vectors. Finally, we add a token distance feature that shows how many (sub)tokens lie between the starting tokens of the two mentions. This feature is used to help our model learn that in case both anaphor and its antecedent are parts of the same sentence, their starting tokens cannot be close to each other.</p><p>The largest s(x, y) score is used to predict the best antecedent candidate. The antecedent gets concatenated with the anaphor and is used as input for an anaphor type classifier, which is a multilayer perceptron (MLP) network consisting of two linear layers with a ReLU activation function inbetween. Similar to coref-hoi, to account for the case of non-referential 'anaphors', a dummy zero score is always prepended to the row of s(x, y) scores.</p></div>
<div><head n="3.3">Mention representation</head><p>Potential anaphors and antecedents have different representations. While the main part of an antecedent candidate embedding g y is constructed similar to coref-hoi, the main part of an anaphor embedding p x is a concatenation of the embedding of the token itself, embedding of the parent token and local context embedding, which includes eight (sub)tokens to the left and right of the anaphor.</p><p>Our decision to include the last two embeddings was motivated by the following observations. Depending on the mention type, mentions' parents have to certain extent different distributions, e.g., discourse deictic mentions more often have forms of the verb 'to be' as parents than mentions of other two types (see Table <ref type="table" target="#tab_12">11</ref> in Appendix A). Moreover, in our data about 60% of anaphor candidates have verbal parents. And certain verbs (e.g., 'assume', 'say') are only compatible with discourse deixis <ref type="bibr" target="#b3">(Eckert and Strube, 2000)</ref>. We use <software ContextAttributes="used">SpaCy</software> to identify tokens' parents, and <software ContextAttributes="used">SpanBERT</software> Large encoder to acquire tokens' embeddings. The usage of context helps capture various useful patterns that may be characteristic of discourse deixis or identity anaphora. These patterns may include, e.g., adjective-copula constructions. Subjects of such constructions with adjectives applicable to abstract entities (e.g., 'correct', 'true') usually refer to discourse entities <ref type="bibr" target="#b3">(Eckert and Strube, 2000)</ref>. Other examples are certain types of complement constructions (like 'that is why/because/what/how'), 'doobject' expressions, which also may point at verbal antecedents <ref type="bibr" target="#b13">(Müller, 2008)</ref>. The inclusion of context may also be useful for capturing any tokens that point at abstract/concrete character of reference. The size of the context window was chosen intuitively, we did not conduct any separate experiments for finding the optimal window size, but may do it in the future.</p><p>Additional linguistic features used to represent anaphors ρ(x) and antecedent candidates ψ(y) are also different. Again, we use <software ContextAttributes="used">SpaCy</software> to extract part of speech (POS) and dependency edge (DEP) tags for tokens in segments, and <software ContextAttributes="used">Berkeley</software> Neural <software ContextAttributes="used">Parser</software> <ref type="bibr" target="#b8">(Kitaev et al., 2019)</ref> to get syntactic constituents (nominal, verbal, or other). We use POS and DEP tags for anaphors. According to our statistical findings (see Table <ref type="table" target="#tab_13">12</ref> in AppendixA), there are some differences in distributions of (POS, DEP) combination depending on the mention type. E.g., the (PRON, nsubj) combination is especially frequent in case of discourse deictic anaphors, while (DET, det) is not. Our antecedent candidates encompass four additional features, of which only span width is borrowed from coref-hoi. Other features include span type <ref type="bibr">(verbal, noun, other)</ref>, POS and DEP tags of the last token. The span type feature was introduced based on the observation that discourse deictic anaphors mostly have verbal phrases or sentences as antecedents, and 'standard' anaphors -noun phrases. The other two features are meant to help identify discourse entities, which often encompass the whole sentence and thus end with a punctuation mark. Note that none of our shallow linguistic features is decisive. Moreover, both <software ContextAttributes="used">SpaCy</software> and <software ContextAttributes="used">Berkeley</software> Neural <software ContextAttributes="used">Parser</software> may not function properly on dialogue data. Still, our experiments on the toy dataset (consisting of a single light_train 2022 file) show that without all these features the model is only able to achieve 29.41% CoNLL F1 score on the light_dev 2022 data. Adding features helps increase this score up to 36.44%.</p><p>All linguistic features described in this section are represented as trainable embeddings of length 100.</p></div>
<div><head n="3.4">Training</head><p>To train our model we kept the hyperparameters reported by coref-hoi, namely BERT-and taskspecific learning rates (1e-5 and 3e-4, respectively), optimizers (AdamW and Adam), schedulers and dropout rate of 0.3. The number of training epochs was set to 24, but we had to stop training after 17 epochs. Currently the model is computationally inefficient (it is able to process only a single training example at a time), so we did not have enough time to complete the training.</p><p>The model was trained using a combination of several loss functions: (i) marginal log-likelihood of possibly correct antecedents; (ii) anaphor type loss checking how well the model distinguishes between discourse deixis, identity and non-referential anaphors; (iii) label loss that punishes the model if it tends to reject all antecedent candidates while having a referential anaphor; (iv) constituent type loss checking how well the model can differentiate between valid (verbal and nominal) and invalid (various fragments) antecedents. The addition of label loss is motivated by the fact that at early stages of training our model always tends to reject all antecedents by assigning negative scores to them. Constituent type loss is inspired by the mention loss in coref-hoi. The idea is that the model should assign larger scores to valid constituents. This loss is used with a coefficient λ = 0.02 to account for a big number of constituents and prevent it from dominating over all other losses.</p></div>
<div><head n="3.5">Results and discussion</head><p>We used the same model for all three discourse deixis tracks. Table <ref type="table" target="#tab_7">7</ref> illustrates the scores achieved by our model on the official test sets. Because the model is designed to resolve only four potential antecedents, there is no big difference in scores between the (Pred) and (Gold M) tracks. The scores for the latter are even slightly worse, as the model has to deal with numerous anaphor candidates it has not seen before. The best scores are reached for the (Gold A) track. It should be noted that here the model tries to resolve all annotated anaphors, not only the four target ones. Still, we tend to attribute the increase in performance not to a wider coverage of anaphors, but to the fact that the model does not have to classify the anaphor types. Table <ref type="table" target="#tab_8">8</ref> shows the CoNLL F1 scores achieved by our system and the winning model on the official test data 2022 for the Eval-DD (Pred) track. Our model ranks second for all the datasets with a score difference ranging from 0.27 to 7.55 points. To compare our model with the baseline model by <ref type="bibr" target="#b9">Kobayashi et al. (2021)</ref>, we also evaluate it on the test partitions of Light, AMI and Persuasion datasets without gold annotations released for the CCST 2021. We see that our approach beats the baseline on all the datasets.</p><p>To see the limitations of our model and have a better understanding of what it can/cannot learn, we additionally evaluate it on the test partitions of Light, AMI and Persuasion datasets from CCST 2021 containing gold annotations. Our analysis (see Table <ref type="table" target="#tab_14">13</ref> in Appendix A) shows that the model struggles with the anaphor type identification: out of 292 true discourse deictic 'this', 'that', 'it' and 'which' only 212 (72.6%) are classified as having the same type, 62 (21.25%) -as anaphoric, and 18 (6.16%) as non-referential ones. Interestingly, only one of all misclassified anaphors is linked to the correctly predicted antecedent. Also, all anaphors incorrectly classified as non-referential get associated with empty spans. At the same time the model successfully finds antecedents for 144 (67.92%) out of 212 correctly identified discourse deictic anaphors. It looks like anaphor type is important for the model to be able to perform resolution.</p><p>Looking at Table <ref type="table" target="#tab_14">13</ref>, we can conclude that our model also has difficulties finding split antecedents: 41 anaphors (14.04%) out of 292 refer to them, but our model only finds 7. In general, the model demonstrates a tendency to choose discourse deixis antecedents consisting of single sentences. We hypothesize that it happens for the following reasons. First, there are not enough training examples with split antecedents. Second, our model lacks mechanisms to capture relations between split antecedents making them a coherent piece relative to a discourse deictic anaphor.</p><p>The following points should also be emphasized. So far we have not evaluated the performance of our model separately for each of the four anaphor candidates. We have not analyzed the ability of our model to resolve identity anaphora. However, such analysis would be useful, so we plan on conducting it in the future. Also, using a lot of features slows down the training process. Therefore we are planning to perform experiments testing different combinations of features and various feature embeddings sizes. Additional experiments on how the usage of features influences the model trained on all available training data are also necessary. Furthermore, an investigation of the quality of the constituent types, POS and DEP tags would be beneficial, considering that we use <software>SpaCy</software> and <software ContextAttributes="used">Berkeley</software> Neural <software ContextAttributes="used">Parser</software> on dialogue data, while they were trained on text corpora.</p></div>
<div><head n="4">Bridging Resolution</head><p>In this section we introduce our submission for the resolution of bridging references. We submitted to the Eval-Br (Gold A) track, in which gold mentions and anaphors are given. This reduces the problem to the selection of antecedent (from gold mention candidates) for each given anaphor.  </p></div>
<div><head n="4.1">Data</head><p>In addition to the shared task dialogue datasets of AMI (851 bridging instances across 7 documents), Switchboard (603 instances, 11 documents), Light (381 instances, 20 documents), and Persuasion (245 instances, 21 documents), we also utilize the bridging anaphora resolution datasets of BASHI <ref type="bibr" target="#b19">(Rösiger, 2018)</ref> and ISNotes <ref type="bibr" target="#b12">(Markert et al., 2012)</ref> to train our models. BASHI is a corpus of 50 Wall Street Journal articles, containing 57,709 tokens and 410 bridging pair annotations. ISNotes is a corpus of Wall Street Journal articles as well, containing 663 bridging pair annotations. The inclusion of these supplementary datasets was important, as the shared task datasets are relatively small, and the model architecture is fairly complex and expressive, making it easy to overfit.</p></div>
<div><head n="4.2">Model architecture</head><p>Our approach is based on "independent" variant of the higher-order coreference architecture introduced in <ref type="bibr" target="#b5">Joshi et al. (2019)</ref>. We make a number of modifications to the architecture and training procedure (an overview of the original model/architecture can be found in <ref type="bibr" target="#b5">Joshi et al. (2019)</ref> and the system it is built on, introduced in <ref type="bibr" target="#b11">Lee et al. (2018)</ref>. Note that the coref-hoi system proposed alternatives to the original higher-order system presented in <ref type="bibr" target="#b5">Joshi et al. (2019)</ref>, but these alternatives (such as the cluster merging model variant) are not relevant for our system, as we are not finding clusters of coreferent mentions.</p><p>Our modifications follow that of the bridging resolution system introduced in <ref type="bibr" target="#b17">Renner et al. (2021)</ref>. The first modification is a result of the gold anaphors being given: since we do not need to detect anaphors from the text, we can pass one anaphor at a time into the model (together with the document text and gold mentions) instead of passing the whole document at once and detecting and resolving potential anaphors. While this means potentially processing each document multiple times if there are multiple bridging anaphors in the document, this is done to decrease memory requirements significantly, as the pairwise scoring function is run for just one anaphor with its candidates, instead of many anaphors with all of their candidates. This decrease in memory usage allows for changes to the architecture that make it simpler and more accurate (see next paragraph). Also, in practice, the bridging datasets are relatively small, so this extra processing of the same document results in a negligible decrease in computational efficiency.</p><p>The architecture modifications are made possible by the decrease in memory usage allowed from having the mentions given and processing one anaphor at a time. Recall that in the original architecture by <ref type="bibr" target="#b11">(Lee et al., 2018)</ref>, they use a "two stage beam search" when detecting mentions and finding coreferent pairs: first, they prune potential mentions based on a span scoring function, then they prune antecedents for each span based on a "fast" bilinear scorer (the "coarse" part of the coarse-to-fine scorer), before sending the remaining spans and their list of antecedent candidates to the more computation-and memory-heavy "fine" scorer. This beam search was proposed to allow the system to scale better to longer documents. By having the gold mentions, we can remove the "fast" span scorer from the original model, as we no longer need to enumerate all possible spans. Also, since the pairwise memory restraints are reduced by passing just one anaphor into the model at a time, we can remove the "coarse" pairwise scorer and skip directly to the "fine" scorer. We make these changes in order to use the more expressive "fine" scorer directly on all pairs, without having to filter possible mentions and antecedents based on the less expressive 'fast" span scorer and "coarse" pairwise scorer.</p><p>After these modifications, the model architecture is as follows: pass entire document through the base contextual language model, obtain span representations for the gold mentions and anaphors, compute antecedents via the higher-order mechanism introduced in <ref type="bibr" target="#b11">Lee et al. (2018)</ref>. Also, this allows the use of cross entropy loss over all possible antecedents for each anaphor, instead of the original marginal log-likelihood, leading to a more direct optimization of the pairwise scorer.</p><p>We use bert-base-uncased as our base language model.</p><p>We use this instead of bert-large-uncased because the resulting embedding is of smaller dimensionality, leading to less parameters in our token attention and span pair scoring layers. We experimented with the Span-BERT variant as well, but this led to slightly lower scores in preliminary experiments.</p></div>
<div><head n="4.3">Training</head><p>We trained the system for 5 epochs on a single Tesla P100 GPU. The learning rate was set to 3e-3 and we used Adam optimizer. We froze the base BERT model to prevent overfitting as the dataset is relatively small even with the supplementary data, set the dropout to 0.3 in the fine tuning layers, and used a higher-order depth of 2. It took about 1 hour to complete training.</p></div>
<div><head n="4.4">Results and discussion</head><p>The submission Entity-F1 scores are shown in Table <ref type="table" target="#tab_11">10</ref>. Overall, we report scores slightly higher than reported in <ref type="bibr" target="#b17">Renner et al. (2021)</ref> for bridging resolution, with scores on the Persuasion dataset being significantly higher than on the other three datasets. This setting allows for a more direct evaluation of the span embedding and pairwise scoring mechanisms from <ref type="bibr" target="#b5">Joshi et al. (2019)</ref> and <ref type="bibr" target="#b11">Lee et al. (2018)</ref>, as we can remove steps in the fine tuning architecture that are only needed to manage memory usage. These results show the effectiveness of the span embedding and pairwise score on span comparisons tasks such as gold mention/anaphor bridging resolution.</p></div>
<div><head n="5">Conclusion and Outlook</head><p>In this paper we presented our systems for identity anaphora, bridging and discourse deixis resolution.</p><p>Our system for the identity anaphora resolution combines the outputs of WCS and the coref-hoi system trained with "cluster merging". It ranked second in the shared task competition. When experimenting with WCS we tested different settings and tried replacing and adding different embeddings for mention representations (e.g., <software ContextAttributes="used">SpanBERT</software>). However, the configuration reported in <ref type="bibr" target="#b0">Anikina et al. (2021)</ref> turned out to work best on our development set. We also tested a combination of WCS trained on the shared task data and CCS trained on <software ContextAttributes="used">OntoNotes</software> as well as coref-hoi trained on a combination of dialogue and non-dialogue datasets. The analysis of the model outputs shows that WCS works reasonably well for detecting singletons and pronominal clusters but performs worse when clustering noun phrases. Hence, we combine the outputs of WCS and the coref-hoi model and achieve an average improvement of 7.95% CoNLL score over vanilla clustering with WCS.</p><p>In the future we would like to do a more fine-grained analysis of the combined model outputs and test if one could use automatic coreference annotations from other pre-trained models as a weak supervision signal for WCS. In particular, we are interested in evaluating this model on the domain adaptation task and in the low resource setting. We would also like to perform more experiments with coreference chain editing based on the outputs of several models.</p><p>The system for discourse deixis resolution ranked second for all three tracks of the shared task. It was able to reach the CoNLL F1 scores ranging from 35.91% to 62.79% depending on the track and dataset. Some of these scores are close to the scores achieved by the winning team.</p><p>The model is based on a novel idea that it is possible to combine the tasks of discourse deixis and anaphora resolution. It is our first attempt at implementing this idea, so there is much space for improvement and additional analysis. First, we plan on making our model computationally more efficient, namely, we are going to perform some experiments with adaptive span pruning and check the influence of linguistic features given a larger training set. Second, it is possible to expand the set of potential anaphors. Before doing that, we need to analyse the ability of our model to resolve identity anaphora. Depending on the results, we may use our discourse deixis resolution model to enhance the coreference resolution performed by the WCS model. Finally, the phenomenon of split antecedents requires more investigation, namely, how we can model coherence/relations between them.</p><p>The system for the resolution of gold bridging anaphors is based on a higher order coreference system adapted for the setting. While the gold mentions/anaphors setting is much simpler than full bridging (mention/anaphor detection and resolution), the results show how well the span embedding and pairwise scoring mechanisms from <ref type="bibr" target="#b5">Joshi et al. (2019)</ref> and <ref type="bibr" target="#b11">Lee et al. (2018)</ref> work for bridging pairs.</p></div>
<div><head>A Appendix: Discourse Deixis</head><p>Here we present statistical findings used to pick out features to represent anaphor candidates. Table <ref type="table" target="#tab_12">11</ref> shows the relative frequencies of parent tokens' lemmas for three types of 'anaphors': discourse deictic, anaphoric and non-referential. Table <ref type="table" target="#tab_14">13</ref> presents an error analysis of our discourse deixis resolution model on the test Light, AMI and Persuasion data from CCST 2021. We analyze the antecedent predictions made by our model as follows. If they are not empty, all predicted antecedents are divided into split and not split, depending on a simple heuristics: if a predicted sequence of (sub)tokens (the very last token is always excluded) contains a dot, a question or an exclamation mark), it is considered to be split. Next, we check if the antecedents' borders are correct. Here, four cases are possible: (i) only the left border is wrong; (ii) only the right border is wrong; (iii) both borders are wrong; (iv) both borders are correct.</p><p>The table also shows the anaphor type predicted by the model for all 292 gold discourse deictic anaphors.  </p></div><figure xml:id="fig_0"><head /><label /><figDesc>Figure 1: Workspace Coreference System Overview</figDesc></figure>
<figure type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Anaphora resolution: approach summary</figDesc><table><row><cell>Encoder</cell><cell cols="3">Input dim Hidden dim Output dim</cell></row><row><cell>BERT head</cell><cell>2*768</cell><cell>900</cell><cell>600</cell></row><row><cell>BERT span</cell><cell>2*768</cell><cell>900</cell><cell>600</cell></row><row><cell>Numberbatch</cell><cell>2*300</cell><cell>600</cell><cell>300</cell></row><row><cell>GloVe head</cell><cell>2*100</cell><cell>600</cell><cell>200</cell></row><row><cell>GloVe span</cell><cell>2*100</cell><cell>600</cell><cell>200</cell></row><row><cell>BERT masked LM</cell><cell>2*768</cell><cell>600</cell><cell>200</cell></row><row><cell cols="4">Table 2: Separate encoders are used to represent men-</cell></row><row><cell cols="4">tion pairs in WCS. Additionally, distance between the</cell></row><row><cell cols="4">mentions, their positions in the document and corre-</cell></row><row><cell cols="4">sponding speakers are encoded and added to the final</cell></row><row><cell>representation.</cell><cell /><cell /><cell /></row></table></figure>
<figure type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Evaluation of WCS and coref-hoi on dev sets. NS (No Singletons) refers to annotations with singleton clusters removed. Scores presented are CoNLL F1 scores. Note that the scores are from an internal development set.</figDesc><table><row><cell>Setting</cell><cell cols="8">Light Light NS AMI AMI NS Persuasion Persuasion NS ARRAU ARRAU NS</cell></row><row><cell>WCS</cell><cell>65.39</cell><cell>61.48</cell><cell>43.33</cell><cell>35.85</cell><cell>61.23</cell><cell>56.55</cell><cell>45.02</cell><cell>32.93</cell></row><row><cell cols="2">coref-hoi 59.84</cell><cell>76.89</cell><cell>43.30</cell><cell>54.70</cell><cell>60.60</cell><cell>81.00</cell><cell>48.32</cell><cell>66.97</cell></row></table></figure>
<figure type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Discourse deixis resolution: approach summary</figDesc><table /></figure>
<figure type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Representations of anaphor and antecedent candidates, and pairwise features</figDesc><table /></figure>
<figure type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>CoNLL F1 scores on the official test sets</figDesc><table><row><cell>Track</cell><cell cols="4">Light AMI Persuasion Swbd.</cell></row><row><cell>Eval-DD (Pred)</cell><cell cols="2">36.82 50.09</cell><cell>47.04</cell><cell>n/a</cell></row><row><cell cols="3">Eval-DD (Gold M) 35.91 47.13</cell><cell>48.24</cell><cell>n/a</cell></row><row><cell cols="3">Eval-DD (Gold A) 44.95 56.54</cell><cell>62.79</cell><cell>n/a</cell></row><row><cell>Data</cell><cell>2021</cell><cell /><cell>2022</cell><cell /></row><row><cell cols="5">Our model Winner Our model Winner</cell></row><row><cell>Light</cell><cell>48.04</cell><cell>42.7</cell><cell>36.82</cell><cell>37.09</cell></row><row><cell>AMI</cell><cell>40.34</cell><cell>35.4</cell><cell>50.09</cell><cell>53.31</cell></row><row><cell>Persuasion</cell><cell>56.68</cell><cell>39.6</cell><cell>47.04</cell><cell>54.59</cell></row><row><cell>Swbd.</cell><cell>n/a</cell><cell>35.4</cell><cell>n/a</cell><cell>49.76</cell></row></table></figure>
<figure type="table" xml:id="tab_8"><head>Table 8 :</head><label>8</label><figDesc>Model comparison: CoNLL F1 scores on official tests 2021 and 2022 for the Eval-DD (Pred) track</figDesc><table /></figure>
<figure type="table" xml:id="tab_10"><head>Table 9 :</head><label>9</label><figDesc>Bridging resolution: approach summary</figDesc><table /></figure>
<figure type="table" xml:id="tab_11"><head>Table 10 :</head><label>10</label><figDesc>Test set results for the bridging task (gold anaphors)</figDesc><table><row><cell cols="4">Switchboard Light Persuasion AMI</cell></row><row><cell>35.78</cell><cell>37.68</cell><cell>50.99</cell><cell>35.23</cell></row></table></figure>
<figure type="table" xml:id="tab_12"><head>Table 11 :</head><label>11</label><figDesc>Table 12 illustrates the joint distribution of POS and DEP labels of possible anaphor candidates, also depending on their type. All numbers were extracted from the CCST 2021 training data, namely ARRAU, Light, AMI, Persuasion, and Switchboard. Distribution of anaphors' parents depending on the anaphors' types</figDesc><table><row><cell /><cell /><cell /><cell cols="2">Anaphor's parent</cell></row><row><cell /><cell /><cell /><cell>DD</cell><cell>ID</cell><cell>non-ref</cell></row><row><cell /><cell /><cell>s</cell><cell>0.329 be</cell><cell>0.139 be</cell><cell>0.202</cell></row><row><cell /><cell /><cell>be</cell><cell>0.235 s</cell><cell>0.117 s</cell><cell>0.078</cell></row><row><cell /><cell /><cell>do</cell><cell cols="2">0.040 have 0.048 have 0.031</cell></row><row><cell /><cell /><cell cols="2">about 0.037 do</cell><cell>0.038 like</cell><cell>0.022</cell></row><row><cell /><cell>Lemma</cell><cell cols="2">sound 0.020 use like 0.020 ... ...</cell><cell>0.027 make 0.022 ...</cell></row><row><cell /><cell /><cell>have</cell><cell>0.017</cell></row><row><cell /><cell /><cell>...</cell><cell /></row><row><cell /><cell /><cell cols="2">make 0.013</cell></row><row><cell /><cell /><cell>...</cell><cell /></row><row><cell /><cell /><cell>use</cell><cell>0.003</cell></row><row><cell /><cell /><cell /><cell cols="2">Mention</cell></row><row><cell /><cell /><cell>DD</cell><cell /><cell>ID</cell><cell>non-ref</cell></row><row><cell>POS+DEP</cell><cell cols="4">(PRON, nsubj) 0.664 (PRON, nsubj) (PRON, dobj) 0.148 (PRON, dobj) 0.249 (SCONJ, mark) 0.173 0.46 (PRON, nsubj) 0.390 (PRON, pobj) 0.117 (DET, det) 0.139 (DET, det) 0.138 (DET, det) 0.03 (PRON, pobj) 0.074 (PRON, pobj) 0.110</cell></row><row><cell /><cell cols="4">(PRON, mark) 0.013 (PRON, dep)</cell><cell>0.02 (PRON, dobj)</cell><cell>0.097</cell></row></table></figure>
<figure type="table" xml:id="tab_13"><head>Table 12 :</head><label>12</label><figDesc>Distribution of anaphors' POS and dependency edges tags depending on the anaphors' types</figDesc><table /></figure>
<figure type="table" xml:id="tab_14"><head>Table 13 :</head><label>13</label><figDesc>Performance on the test partitions of AMI, Light &amp; Persuasion datasets from CODI-CRAC 2021 Shared Task</figDesc><table /></figure>
			<note place="foot" xml:id="foot_0"><p>Proceedings of the CODI-CRAC 2022 Shared Task on Anaphora, Bridging, and Discourse Deixis in Dialogue, pages 15-27 Gyeongju, Republic of Korea, October 17, 2022.</p></note>
			<note place="foot" n="1" xml:id="foot_1"><p>An average improvement over all 4 datasets is 7.95%.</p></note>
			<note place="foot" n="2" xml:id="foot_2"><p>https://codalab.lisn.upsaclay.fr/ competitions/614#learn_the_details</p></note>
			<note place="foot" n="3" xml:id="foot_3"><p>https://universalanaphora.github.io/ UniversalAnaphora/</p></note>
			<note place="foot" n="4" xml:id="foot_4"><p>https://pypi.org/project/ crosslingual-coreference/</p></note>
			<note place="foot" n="5" xml:id="foot_5"><p>We treat all discourse deictic markables with semantic type 'discourse old' as anaphors.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>The authors are supported by the <rs type="funder">German Ministry of Education and Research (BMBF)</rs>: <rs type="projectName">T. Anikina in</rs> project <rs type="projectName">CORA4NLP</rs> (grant Nr. <rs type="grantNumber">01IW20010</rs>); <rs type="person">N. Skachkova</rs>, <rs type="person">P. Trivedi</rs>, <rs type="person">J. Renner</rs> in <rs type="projectName">IMPRESS</rs> (grant Nr. <rs type="grantNumber">01IS20076</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_bWvVpT5">
					<orgName type="project" subtype="full">T. Anikina in</orgName>
				</org>
				<org type="funded-project" xml:id="_dPtenwP">
					<idno type="grant-number">01IW20010</idno>
					<orgName type="project" subtype="full">CORA4NLP</orgName>
				</org>
				<org type="funded-project" xml:id="_ZPMtJyU">
					<idno type="grant-number">01IS20076</idno>
					<orgName type="project" subtype="full">IMPRESS</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Anaphora resolution in dialogue: Description of the DFKI-TalkingRobots system for the CODI-CRAC 2021 shared-task</title>
		<author>
			<persName><forename type="first">T</forename><surname>Anikina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Oguz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Skachkova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Upadhyaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Kruijff-Korbayova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the CODI-CRAC 2021 Shared Task on Anaphora, Bridging, and Discourse Deixis in Dialogue</title>
		<meeting>the CODI-CRAC 2021 Shared Task on Anaphora, Bridging, and Discourse Deixis in Dialogue<address><addrLine>Punta Cana, Dominican Republic</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="32" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Resolving pronominal reference to abstract entities</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Byron</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
		<respStmt>
			<orgName>University of Rochester</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Dialogue acts, synchronizing units, and anaphora resolution</title>
		<author>
			<persName><forename type="first">M</forename><surname>Eckert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Strube</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Semantics</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="51" to="89" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">SpaCy: Industrial-strength Natural Language Processing in Python</title>
		<author>
			<persName><forename type="first">M</forename><surname>Honnibal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Montani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Van Landeghem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Boyd</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">BERT for coreference resolution: Baselines and analysis</title>
		<author>
			<persName><forename type="first">M</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Weld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5803" to="5808" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The CODI-CRAC 2021 shared task on anaphora, bridging, and discourse deixis in dialogue</title>
		<author>
			<persName><forename type="first">S</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Manuvinakurike</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Poesio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Strube</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rosé</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the CODI-CRAC 2021 Shared Task on Anaphora, Bridging, and Discourse Deixis in Dialogue</title>
		<meeting>the CODI-CRAC 2021 Shared Task on Anaphora, Bridging, and Discourse Deixis in Dialogue<address><addrLine>Punta Cana, Dominican Republic</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The pipeline model for resolution of anaphoric reference and resolution of entity reference</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the CODI-CRAC 2021 Shared Task on Anaphora, Bridging, and Discourse Deixis in Dialogue</title>
		<meeting>the CODI-CRAC 2021 Shared Task on Anaphora, Bridging, and Discourse Deixis in Dialogue<address><addrLine>Punta Cana, Dominican Republic</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="43" to="47" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Multilingual constituency parsing with self-attention and pretraining</title>
		<author>
			<persName><forename type="first">N</forename><surname>Kitaev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3499" to="3505" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Neural anaphora resolution in dialogue</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kobayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the CODI-CRAC 2021 Shared Task on Anaphora, Bridging, and Discourse Deixis in Dialogue</title>
		<meeting>the CODI-CRAC 2021 Shared Task on Anaphora, Bridging, and Discourse Deixis in Dialogue<address><addrLine>Punta Cana, Dominican Republic</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="16" to="31" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Anaphora With Non-nominal Antecedents in Computational Linguistics: a Survey</title>
		<author>
			<persName><forename type="first">V</forename><surname>Kolhatkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Roussel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dipper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zinsmeister</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="547" to="612" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Higherorder coreference resolution with coarse-to-fine inference</title>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="687" to="692" />
		</imprint>
	</monogr>
	<note>Short Papers</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Collective classification for fine-grained information status</title>
		<author>
			<persName><forename type="first">K</forename><surname>Markert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Strube</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 50th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Jeju Island, Korea</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="795" to="804" />
		</imprint>
	</monogr>
	<note>Long Papers</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Fully automatic resolution of'it</title>
		<author>
			<persName><forename type="first">M.-C</forename><surname>Müller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
		<respStmt>
			<orgName>Universität Tübingen</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
	<note>this', and'that'in unrestricted multi-party dialog</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Resolving individual and abstract anaphora in texts and dialogues</title>
		<author>
			<persName><forename type="first">C</forename><surname>Navarretta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING 2004: Proceedings of the 20th International Conference on Computational Linguistics</title>
		<meeting><address><addrLine>Geneva, Switzerland</addrLine></address></meeting>
		<imprint>
			<publisher>COLING</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="233" to="239" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)</title>
		<meeting>the 2014 conference on empirical methods in natural language processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">CoNLL-2012 shared task: Modeling multilingual unrestricted coreference in OntoNotes</title>
		<author>
			<persName><forename type="first">S</forename><surname>Pradhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Moschitti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Uryupina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint Conference on EMNLP and CoNLL -Shared Task</title>
		<meeting><address><addrLine>Jeju Island, Korea</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">An end-to-end approach for full bridging resolution</title>
		<author>
			<persName><forename type="first">J</forename><surname>Renner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Trivedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Maheshwari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gilleron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Denis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the CODI-CRAC 2021 Shared Task on Anaphora, Bridging, and Discourse Deixis in Dialogue</title>
		<meeting>the CODI-CRAC 2021 Shared Task on Anaphora, Bridging, and Discourse Deixis in Dialogue<address><addrLine>Punta Cana, Dominican Republic</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="48" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Coreference resolution in dialogues in English and Portuguese</title>
		<author>
			<persName><forename type="first">M</forename><surname>Rocha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Coreference and Its Applications</title>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">BASHI: A corpus of Wall Street Journal articles annotated with bridging links</title>
		<author>
			<persName><forename type="first">I</forename><surname>Rösiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018)</title>
		<meeting>the Eleventh International Conference on Language Resources and Evaluation (LREC 2018)<address><addrLine>Miyazaki, Japan</addrLine></address></meeting>
		<imprint>
			<publisher>European Language Resources Association (ELRA</publisher>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Concept-Net 5.5: An Open Multilingual Graph of General Knowledge</title>
		<author>
			<persName><forename type="first">R</forename><surname>Speer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Havasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence, AAAI'17</title>
		<meeting>the Thirty-First AAAI Conference on Artificial Intelligence, AAAI'17</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4444" to="4451" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A machine learning approach to pronoun resolution in spoken dialogue</title>
		<author>
			<persName><forename type="first">M</forename><surname>Strube</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 41st Annual Meeting of the Association for Computational Linguistics<address><addrLine>Sapporo, Japan</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="168" to="175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">MiniLM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers</title>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020</title>
		<meeting><address><addrLine>NeurIPS; virtual</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-12-06">2020. 2020. December 6-12, 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Discourse deixis and discourse processing</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">L</forename><surname>Webber</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Revealing the myth of higher-order inference in coreference resolution</title>
		<author>
			<persName><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="8527" to="8533" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Adapted end-to-end coreference resolution system for anaphoric identities in dialogues</title>
		<author>
			<persName><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the CODI-CRAC 2021 Shared Task on Anaphora, Bridging, and Discourse Deixis in Dialogue</title>
		<meeting>the CODI-CRAC 2021 Shared Task on Anaphora, Bridging, and Discourse Deixis in Dialogue<address><addrLine>Punta Cana, Dominican Republic</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="55" to="62" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The CODI-CRAC 2022 shared task on anaphora, bridging, and discourse deixis in dialogue</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Manuvinakurike</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Levin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Poesio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Strube</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rosé</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the CODI-CRAC 2022 Shared Task on Anaphora, Bridging, and Discourse Deixis in Dialogue</title>
		<meeting>the CODI-CRAC 2022 Shared Task on Anaphora, Bridging, and Discourse Deixis in Dialogue</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>