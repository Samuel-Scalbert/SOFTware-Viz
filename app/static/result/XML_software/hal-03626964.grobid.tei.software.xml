<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="fr">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Sur la v√©rification du locuteur √† partir de traces d&apos;ex√©cution de mod√®les acoustiques personnalis√©s</title>
				<funder ref="#_RDRFAA2 #_gnxyxav #_8aqefNF">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Natalia</forename><surname>Tomashenko</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">LIA</orgName>
								<orgName type="institution" key="instit2">Avignon Universit√©</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Cet article repose sur</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Salima</forename><surname>Mdhaffar</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">LIA</orgName>
								<orgName type="institution" key="instit2">Avignon Universit√©</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Cet article repose sur</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Marc</forename><surname>Tommasi</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Universit√© de Lille</orgName>
								<address>
									<settlement>Inria</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yannick</forename><surname>Est√®ve</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">LIA</orgName>
								<orgName type="institution" key="instit2">Avignon Universit√©</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Cet article repose sur</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jean-Fran√ßois</forename><surname>Bonastre</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">LIA</orgName>
								<orgName type="institution" key="instit2">Avignon Universit√©</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Cet article repose sur</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Sur la v√©rification du locuteur √† partir de traces d&apos;ex√©cution de mod√®les acoustiques personnalis√©s</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">071E8A38EF46A3C7C0000A833FB9DAC4</idno>
					<note type="submission">le travail pr√©sent√© dans (Tomashenko et al.,</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-04-12T14:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Vie priv√©e</term>
					<term>apprentissage f√©d√©r√©</term>
					<term>mod√®les acoustiques</term>
					<term>mod√®les d&apos;attaques</term>
					<term>reconnaissance vocale</term>
					<term>v√©rification du locuteur Privacy</term>
					<term>federated learning</term>
					<term>acoustic models</term>
					<term>attack models</term>
					<term>speech recognition</term>
					<term>speaker verification</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Les mod√®les acoustiques personnalis√©s sont construits par entra√Ænement √† partir de donn√©es provenant d'un locuteur unique en raffinant un mod√®le g√©n√©rique. Une question importante est de savoir si l'acc√®s √† ces mod√®les personnalis√©s permet facilement de construire une attaque permettant d'identifier le locuteur associ√©. Ce probl√®me est important dans le contexte de l'apprentissage f√©d√©r√© de mod√®les pour la reconnaissance de la parole o√π un mod√®le global est appris sur un serveur √† partir des modifications des param√®tres des mod√®les re√ßues de plusieurs clients. Nous proposons une m√©thode qui consiste √† construire des empreintes de ces mod√®les √† partir des traces de leur application sur un jeu de donn√©es fixe et ind√©pendant que nous appelons indicateur. Gr√¢ce √† ces empreintes, nous d√©veloppons deux mod√®les d'attaques tr√®s efficaces qui visent √† inf√©rer l'identit√© du locuteur.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="fr">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>L'apprentissage f√©d√©r√© (AF) pour la reconnaissance automatique de la parole (RAP) conna√Æt une grande popularit√© dans plusieurs t√¢ches et travaux <ref type="bibr" target="#b3">(Cui et al., 2021;</ref><ref type="bibr" target="#b4">Dimitriadis et al., 2020;</ref><ref type="bibr" target="#b8">Guliani, 2021;</ref><ref type="bibr" target="#b26">Yu et al., 2021;</ref><ref type="bibr">Tomashenko et al., 2022a)</ref> 1 . Le respect de la vie priv√©e est l'un des principaux d√©fis de l'AF <ref type="bibr" target="#b12">(Li et al., 2020;</ref><ref type="bibr" target="#b15">Mothukuri et al., 2021)</ref>. Contrairement aux algorithmes d'apprentissage classiques qui utilisent un serveur contenant les donn√©es d'apprentissage, l'AF apprend sur des donn√©es stock√©es localement et communique uniquement les modifications (mises √† jour). Ceci permet de prot√©ger les donn√©es personnelles puisqu'elles ne sont ni stock√©es dans un serveur, ni partag√©es avec d'autres utilisateurs. Cependant, ces mises √† jour peuvent encore contenir certaines informations sensibles <ref type="bibr" target="#b6">(Geiping et al., 2020;</ref><ref type="bibr" target="#b2">Carlini et al., 2019)</ref>. Des travaux r√©cents ont montr√© que les mod√®les appris par l'AF sont vuln√©rables √† diff√©rents types d'attaques <ref type="bibr" target="#b24">(Truex et al., 2019;</ref><ref type="bibr" target="#b25">Wang et al., 2019)</ref>. Les techniques pour am√©liorer la confidentialit√© dans un cadre de l'AF s'appuient principalement sur deux approches <ref type="bibr" target="#b15">(Mothukuri et al., 2021)</ref> : le calcul multipartite s√©curis√© <ref type="bibr" target="#b1">(Bonawitz et al., 2016)</ref> et la confidentialit√© diff√©rentielle <ref type="bibr" target="#b5">(Dwork, 2006)</ref>. Les m√©thodes de chiffrement <ref type="bibr" target="#b19">(Smaragdis &amp; Shashanka, 2007)</ref> comme le chiffrement enti√®rement homomorphe <ref type="bibr" target="#b19">(Smaragdis &amp; Shashanka, 2007)</ref> et le calcul multipartite s√©curis√© effectuent le calcul dans le domaine crypt√©. Ces m√©thodes sont trop co√ªteuses en termes de calcul. Les m√©thodes de confidentialit√© diff√©rentielle pr√©servent la confidentialit√© en ajoutant du bruit aux param√®tres des utilisateurs <ref type="bibr" target="#b5">(Dwork, 2006)</ref>. Cependant, ces solutions peuvent d√©grader les performances d'apprentissage √† cause de l'incertitude qu'elles introduisent dans les param√®tres. Les m√©thodes alternatives √† la protection de la confidentialit√© pour la parole comprennent les m√©thodes de suppression qui sont destin√©es √† l'analyse des sons ambiants, et l'anonymisation <ref type="bibr" target="#b23">(Tomashenko et al., 2022b)</ref> qui vise √† supprimer les informations personnelles identifiables dans le signal vocal en gardant tous les autres attributs. Ces m√©thodes de protection de la confidentialit√© peuvent √™tre combin√©es et int√©gr√©es de mani√®re hybride dans un cadre d'AF.</p><p>Malgr√© l'int√©r√™t r√©cent port√© √† l'AF pour la RAP et √† d'autres t√¢ches telles que le rep√©rage de motscl√©s <ref type="bibr" target="#b11">(Leroy et al., 2019)</ref>, la reconnaissance des √©motions <ref type="bibr" target="#b10">(Latif et al., 2020)</ref>, et la v√©rification du locuteur <ref type="bibr" target="#b7">(Granqvist et al., 2020)</ref>, il existe tr√®s peu d'√©tudes sur les attaques de confidentialit√©, dans un contexte d'AF, des mod√®les acoustiques (MA) pour la reconnaissance de la parole. Il a tout de m√™me √©t√© montr√© r√©cemment qu'il est possible d'extraire des informations sur le locuteur √† partir des modifications port√©es sur les poids d'un mod√®le acoustique neuronal lors de sa personnalisation <ref type="bibr" target="#b14">(Mdhaffar et al., 2022)</ref>.</p><p>Nos travaux s'inscrivent dans le cadre de ces attaques : nous √©tudions les informations propres au locuteur qui peuvent √™tre extraites √† partir de mod√®les acoustiques personnalis√©s mis √† jour localement. Nous explorons diff√©rentes mod√®les d'attaques qui op√®rent directement sur les param√®tres du mod√®le mis √† jour sans avoir acc√®s aux donn√©es r√©elles de l'utilisateur. L'id√©e principale des m√©thodes propos√©es est d'utiliser un jeu de donn√©es externe (indicateur) pour analyser l'empreinte des mod√®les acoustiques sur ces donn√©es. Une autre contribution importante de ce travail concerne l'analyse des informations sur le locuteur repr√©sent√©es dans les mod√®les acoustiques neuronaux adapt√©s.</p><p>2 Apprentissage f√©d√©r√© pour les mod√®les acoustiques de RAP Nous consid√©rons un sc√©nario classique d'apprentissage f√©d√©r√© o√π un mod√®le acoustique neuronal global est entra√Æn√© sur un serveur √† l'aide des donn√©es stock√©es localement sur plusieurs dispositifs distants <ref type="bibr" target="#b12">(Li et al., 2020)</ref>. L'apprentissage du mod√®le global est effectu√© sous la contrainte que les donn√©es vocales d'apprentissage sont stock√©es et trait√©es localement sur les dispositifs des utilisateurs (clients). Seules les mises √† jour du mod√®le sont transmises au serveur √† partir de chaque client. Le mod√®le global est appris sur le serveur en fonction des mises √† jour re√ßues de plusieurs clients. La Figure <ref type="figure" target="#fig_0">1</ref> illustre l'AF dans un r√©seau distribu√© de clients. Tout d'abord, le mod√®le acoustique initial de reconnaissance de la parole W g est distribu√© √† l'ensemble des syst√®mes des N utilisateurs (locuteurs). Ensuite, le mod√®le global initial est ex√©cut√© sur chaque dispositif utilisateur s i (i ‚àà 1..N ) et mis √† jour localement sur les donn√©es priv√©es de l'utilisateur. Les mod√®les mis √† jour W si sont ensuite transmis au serveur o√π ils sont agr√©g√©s pour obtenir un nouveau mod√®le global W * g . En g√©n√©ral, les mod√®les personnalis√©s mis √† jour sont agr√©g√©s en utilisant la moyenne f√©d√©r√©e et ses variations <ref type="bibr" target="#b13">(McMahan et al., 2017)</ref>. Ensuite, le mod√®le global mis √† jour W * g est partag√© avec les clients. Ce processus est r√©p√©t√© plusieurs fois jusqu'√† la convergence du mod√®le ou en fixant un nombre d'it√©rations. L'utilit√© et l'efficacit√© de l'apprentissage des mod√®les entra√Æn√©s dans le cadre d'AF ont √©t√© √©tudi√©es avec succ√®s dans des travaux r√©cents <ref type="bibr" target="#b3">(Cui et al., 2021;</ref><ref type="bibr" target="#b4">Dimitriadis et al., 2020;</ref><ref type="bibr" target="#b8">Guliani, 2021;</ref><ref type="bibr" target="#b26">Yu et al., 2021)</ref>. Dans cette √©tude, nous nous concentrons sur l'aspect de la protection de la vie priv√©e. </p><formula xml:id="formula_0">Server Client 1 Client 2 Client N ùëä ùëî ùëä ùëî * ùëä ùë† 1 ùëä ùë† 2 ùëä ùë† ùëÅ ùëä ùëî ùëä ùëî ùëä ùëî</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Mod√®les d'attaques</head><p>Dans cette section, nous d√©crivons le sc√©nario de protection de la confidentialit√© et nous pr√©sentons deux mod√®les d'attaques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Sc√©nario de pr√©servation de la confidentialit√©</head><p>La pr√©servation de la confidentialit√© est formul√©e comme un jeu entre des utilisateurs qui partagent certaines donn√©es et des attaquants qui acc√®dent √† ces donn√©es ou √† des donn√©es d√©riv√©es de celles-ci et visent √† d√©duire des informations sur les utilisateurs <ref type="bibr" target="#b22">(Tomashenko et al., 2020)</ref>. Les attaquants visent √† attaquer les utilisateurs en utilisant les informations d√©tenues par le serveur. Ils peuvent avoir acc√®s √† des mod√®les personnalis√©s. Dans ce travail, nous supposons qu'un attaquant a acc√®s aux donn√©es suivantes :</p><p>-Un mod√®le global initial W g .</p><p>-Un mod√®le personnalis√© W s du locuteur cible (target) s qui est inscrit dans le syst√®me d'apprentissage f√©d√©r√©. Le mod√®le personnalis√© correspondant a √©t√© obtenu √† partir du mod√®le global W g en ajustant par fine-tuning les poids de W g √† l'aide des donn√©es du locuteur. Nous consid√©rons ce mod√®le comme enrollment pour un attaquant.</p><p>- 1. ‚àÄ W si ‚àà W, ‚àÄ u j ‚àà I nous calculons les valeurs d'activation de la couche h pour les paires de mod√®les :</p><formula xml:id="formula_1">W h si (u j ) = {w h,t si,j } Tj t=1 et W h g (u j ) = {w h,t g,j } Tj t=1</formula><p>, et les diff√©rences par vecteur entre les sorties correspondantes :</p><formula xml:id="formula_2">‚àÜ h s i (uj) = {‚àÜ h,t s i ,j } T j t=1 , o√π ‚àÜ h,t s i ,j = w h,t s i ,j -w h,t g,j , t ‚àà 1..Tj.<label>(1)</label></formula><p>2. Pour chaque mod√®le personnalis√©, nous calculons les vecteurs de moyenne et d'√©cart type pour ‚àÜ h,t si,j sur tous les vecteurs de parole dans les donn√©es indicateur I :</p><formula xml:id="formula_3">¬µ h s i = I j=1 T j t=1 ‚àÜ h,t s i ,j I j=1 Tj et œÉ h s i = I j=1 T j t=1 (‚àÜ h,t s i ,j -¬µ h s i ) 2 I j=1 Tj 1 2</formula><p>.</p><p>(2)</p><p>3. Pour une paire de mod√®les personnalis√©s W si and W s k , nous calculons un score de similarit√© œÅ pour la couche cach√©e h sur l'ensemble de donn√©es indicateur sur la base de la distance euclidienne normalis√©e L 2 entre les paires de vecteurs correspondants pour les moyennes et les √©carts types :</p><formula xml:id="formula_4">œÅ(W h s i , W h s k ) = Œ±¬µ ‚à•¬µ h s i -¬µ h s k ‚à•2 ‚à•¬µ h s i ‚à•2‚à•¬µ h s k ‚à•2 + Œ±œÉ ‚à•œÉ h s i -œÉ h s k ‚à•2 ‚à•œÉ h s i ‚à•2‚à•œÉ h s k ‚à•2 ,<label>(3)</label></formula><p>o√π Œ± ¬µ , Œ± œÉ sont des param√®tres fixes dans toutes les exp√©riences.</p><p>4. En utilisant les scores de similarit√© obtenues pour toutes les paires de matrices, nous pouvons effectuer une t√¢che de v√©rification du locuteur.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pool de mod√®les personnalis√©s</head><p>Indicateur donn√©es vocales</p><formula xml:id="formula_5">ùëä ùë† 1 ‚Ñé (ùë¢) ùëä ùëî ‚Ñé (ùë¢) ùëä ùë† ùëÅ ùëä ùë† 1 ùëä ùë† 2 ùëä ùëî ùë¢ ùë¢ ùëä ùë† ùëÅ ‚Ñé (ùë¢) Œî ùë† 1 ‚Ñé (ùë¢) Œî ùë† ùëÅ ‚Ñé (ùë¢) [Œº ùë† 1 ‚Ñé , ùúé ùë† 1 ‚Ñé ] [Œº ùë† ùëÅ ‚Ñé , ùúé ùë† ùëÅ ‚Ñé ]</formula><p>‚Ä¶   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Mod√®les acoustiques pour la reconnaissance automatique de la parole</head><p>Les mod√®les acoustiques pour la RAP suivent une architecture neuronale de type TDNN <ref type="bibr" target="#b16">(Peddinti et al., 2015)</ref> et ont √©t√© entra√Æn√©s en utilisant la bo√Æte √† outils de reconnaissance vocale Kaldi <ref type="bibr" target="#b17">(Povey et al., 2011)</ref>. Les coefficients cepstraux de fr√©quence Mel (MFCC) √† 40 dimensions, concat√©n√©s √† des i-vecteurs √† 100 dimensions, ont √©t√© utilis√©s comme entr√©e dans les r√©seaux de neurones. Chaque mod√®le comporte treize couches cach√©es de 512 dimensions, suivies d'une couche softmax dans laquelle 3664 √©tats de triphonie ont √©t√© utilis√©s comme target<ref type="foot" target="#foot_0">2</ref> . Le mod√®le global initial W g a √©t√© entra√Æn√© en utilisant le crit√®re lattice-free maximum mutual information (LF-MMI) <ref type="bibr" target="#b18">(Povey et al., 2016)</ref>. Les deux types de strat√©gies d'augmentation des donn√©es vocales ont √©t√© appliqu√©es pour les donn√©es d'entra√Ænement et d'adaptation : perturbation de la vitesse (avec des facteurs de 0,9, 1,0, 1,1) et perturbation du volume, comme dans <ref type="bibr" target="#b16">(Peddinti et al., 2015)</ref>. Chaque mod√®le est compos√© d'environ 13.8 millions de param√®tres. Le mod√®le global initial W g a √©t√© entra√Æn√© sur le Train-G. Des mod√®les personnalis√©s W si ont √©t√© obtenus en ajustant finement tous les param√®tres de W g sur les donn√©es des locuteurs de Part-1 et Part-2 comme d√©crit dans <ref type="bibr">(Tomashenko et al., 2022a)</ref>. Pour tous les mod√®les personnalis√©s de locuteurs, nous utilisons approximativement la m√™me quantit√© de donn√©es vocales pour effectuer le r√©glage fin (adaptation du locuteur ou fine-tuning) -environ 4 minutes par mod√®le. Pour la plupart des locuteurs (564 dans Part-1, 463 dans Part-2), nous avons obtenu deux mod√®les personnalis√©s diff√©rents (par locuteur) sur des sous-ensembles d'adaptation disjoints, pour les autres locuteurs, nous ne disposons de donn√©es d'adaptation que pour un seul mod√®le.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Mod√®les d'attaques</head><p>Nous √©tudions deux approches pour les mod√®les d'attaques : A1 -une approche simple bas√©e sur l'analyse statistique comparative des sorties du mod√®le de RN et le score de similarit√© associ√© entre les mod√®les personnalis√©s, et A2 -une approche bas√©e sur les RN. Pour les essais sur les cibles (test target trials), nous utilisons des comparaisons entre diff√©rents mod√®les personnalis√©s des m√™mes locuteurs (564 dans le Part-2), et pour les essais sur les non-cibles (test non-target trials), nous avons s√©lectionn√© al√©atoirement 10K paires de mod√®les de diff√©rents locuteurs (choisis au hasard parmi toutes les 1079 √ó 1078/2 comparaisons possibles) dans un ensemble de donn√©es correspondant.</p><p>Mod√®le d'attaque A1. Le premier mod√®le d'attaque a √©t√© appliqu√© comme d√©crit dans la section (3.2). Les param√®tres Œ± ¬µ , Œ± œÉ dans la formule (3) sont respectivement √©gaux √† 1 et 10. Ce mod√®le a √©t√© √©valu√© sur un jeu de donn√©es de mod√®les personnalis√©s. Le jeu de donn√©es indicateur est le m√™me dans toutes les exp√©riences.</p><p>Mod√®le d'attaque A2. Pour entra√Æner le mod√®le d'attaque A2, nous utilisons 1300 mod√®les de locuteurs personnalis√©s correspondant √† 736 locuteurs uniques de Part-1. Lorsque nous avons appliqu√© la partie fixe de l'architecture pr√©sent√©e dans la Figure <ref type="figure">3</ref> au jeu de donn√©es indicateur de 32 minutes pour chaque mod√®le de locuteur dans Part-1, nous avons obtenu les donn√©es d'entra√Ænement avec la quantit√© correspondant √† environ 693h (32√ó1300). La partie entra√Æn√©e du mod√®le neuronal, illustr√©e dans la Figure <ref type="figure">3</ref>, a une topologie similaire √† celle d'un extracteur de x-vecteur conventionnel <ref type="bibr" target="#b20">(Snyder et al., 2018)</ref>. Cependant, l'extracteur de x-vecteur permet de pr√©dire l'identit√© du locuteur pour le segment de discours donn√© alors que notre mod√®le propos√© apprend √† pr√©dire l'identit√© du locuteur √† partir de la partie W h s d'un mod√®le personnalis√© du locuteur. Nous avons entra√Æn√© deux mod√®les d'attaques correspondant aux deux valeurs du param√®tre h ‚àà {1, 5} -une couche cach√©e dans les MA neuronaux RAP √† laquelle nous calculons les activations. Les valeurs h ont √©t√© choisies en fonction des r√©sultats obtenus pour le mod√®le d'attaque A1. La dimension de sortie de la partie fixe est 512. La partie fixe est suivie par la partie entra√Æn√©e qui consiste en sept couches TDNN cach√©es et une couche de regroupement statistique introduite apr√®s la cinqui√®me couche TDNN. La sortie est une couche softmax avec les cibles (sorties) correspondant aux locuteurs dans le pool de mod√®les personnalis√©s de locuteurs (nombre de locuteurs uniques dans Part-1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">R√©sultats</head><p>Les mod√®les d'attaques ont √©t√© √©valu√©s en termes de equal error rate (EER) 3 . Les r√©sultats du mod√®le d'attaque A1 sont pr√©sent√©s dans la Figure <ref type="figure">4</ref>. Les informations de locuteur peuvent √™tre captur√©es pour toutes les valeurs de h avec un succ√®s variable : EER varie de 0,86% (pour la premi√®re couche cach√©e) √† 20,51% (pour la couche cach√©e sup√©rieure). Pour analyser l'impact de chaque partie de la somme de la formule (3) sur les performances de la VAL, nous calculons s√©par√©ment le score de similarit√© œÅ en utilisant uniquement les moyennes (Œ± œÉ = 0) ou uniquement les √©carts types (Œ± ¬µ = 0). L'impact de chaque terme de la somme change pour les diff√©rentes couches cach√©es. Lorsque nous utilisons uniquement les √©carts-types, nous observons le plus faible EER sur la premi√®re couche. Dans le cas de l'utilisation des moyennes uniquement, la premi√®re couche est, au contraire, l'une des moins informatives pour la v√©rification du locuteur. Pour toutes les autres couches, la combinaison des moyennes et des √©carts-types fournit des r√©sultats sup√©rieurs √† ceux obtenus dans les cas o√π une seule de ces composantes est utilis√©e.</p><p>Nous choisissons deux valeurs h ‚àà {1, 5} qui montrent des r√©sultats prometteurs pour le mod√®le A1, et nous utilisons les sorties correspondantes pour entra√Æner deux mod√®les d'attaques avec la 3. En d√©signant par P fa (Œ∏) et P miss (Œ∏) les taux de faux positifs (false alarm) et de faux n√©gatifs (miss rates) au seuil Œ∏, l'EER correspond au seuil Œ∏ EER pour lequel les deux taux d'erreur de d√©tection sont √©gaux : EER = P fa (Œ∏ EER ) = P miss (Œ∏ EER ). </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>FIGURE 1 -</head><label>1</label><figDesc>FIGURE 1 -Apprentissage f√©d√©r√© dans un r√©seau distribu√© de clients : 1) T√©l√©chargement du mod√®le global W g par les clients. 2) Adaptation au locuteur de W g sur les appareils locaux en utilisant les donn√©es priv√©es de l'utilisateur. 3) Collecte et agr√©gation de plusieurs mod√®les personnalis√©s W s1 ,...,W s N sur le serveur. 4) Partage du mod√®le r√©sultant W * g avec les diff√©rents clients.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>FIGURE 2 -FIGURE 3 -</head><label>23</label><figDesc>FIGURE 2 -Calcul des statistiques pour le mod√®le d'attaque A1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Des mod√®les personnalis√©s des locuteurs non-cibles (non-target) et cibles (target) : W s1 ,...,W s N . Nous appellerons ces mod√®les test trial.L'objectif de l'attaquant est de r√©aliser une t√¢che de v√©rification automatique du locuteur (VAL) en utilisant le mod√®le de donn√©es d'inscription (enrollment) sous la forme de W s et les donn√©es d'essai (test trial) de test sous la forme de mod√®les W s1 ,...,W s N . Autrement dit, √©tant donn√© un mod√®le W s correspondant au locuteur cible, la t√¢che consiste √† identifier quels mod√®les parmi les W si correspondent ou non au locuteur cible<ref type="bibr" target="#b0">(Bonastre et al., 2021)</ref>.Les deux approches propos√©es reposent sur l'hypoth√®se que nous pouvons capturer des informations sur l'identit√© du locuteur s √† partir du mod√®le correspondant adapt√© au locuteur W s et du mod√®le global W g en comparant les sorties de ces deux mod√®les acoustiques neuronaux provenant des couches cach√©es h sur certaines donn√©es vocales. Nous appellerons ces donn√©es vocales indicateur. Ces donn√©es ne sont li√©es ni aux donn√©es de test ni aux donn√©es d'apprentissage des mod√®les.</figDesc><table><row><cell>3.2 Mod√®les d'attaques</cell></row></table><note><p>Mod√®le d'attaque A1. La Figure 2 illustre la VAL pour le mod√®le d'attaque A1 propos√© dans ce papier. Ce mod√®le d'attaque comporte plusieurs √©tapes. On consid√®re un ensemble d'√©nonc√©s (utterances) dans l'ensemble de donn√©es indicateur I = {u 1 , . . . , u J } ; une s√©quence de vecteurs dans l'√©nonc√© u j ={u 1 j , . . ., u Tj j } ; un ensemble de mod√®les personnalis√©s W = {W s1 , . . . , W s N } ; et un identifiant d'une couche cach√©e dans le MA global ou personnalis√© repr√©sent√© par h. Nous pr√©sentons ci-dessous les √©tapes pour le mod√®le d'attaque A1.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>heures de donn√©es vocales en anglais provenant d'environ 2K locuteurs, 16kHz. Nous avons s√©lectionn√© trois ensembles de donn√©es √† partir du corpus d'entra√Ænement TED-LIUM 3 : Train-G, Part-1, Part-2 avec des sous-ensembles de locuteurs disjoints comme indiqu√© dans le tableau 1. Le jeu de donn√©es indicateur a √©t√© utilis√© pour entra√Æner les mod√®les d'attaques. Il est compos√© de 320 √©nonc√©s (utterances) s√©lectionn√©s parmi les 32 locuteurs des ensembles de donn√©es de test et de d√©veloppement du corpus TED-LIUM 3. Les locuteurs du jeu de donn√©es indicateur sont disjoints des locuteurs de Train-G, Part-1, et Part-2. Pour chaque locuteur de l'ensemble de donn√©es indicateur, nous s√©lectionnons uniquement 10 √©nonc√©s. La taille totale du jeu de donn√©es indicateur est de 32 minutes. L'ensemble de donn√©es Train-G a √©t√© utilis√© pour entra√Æner un mod√®le acoustique global initial W g . Part-1 et Part-2 ont √©t√© utilis√©s pour obtenir deux ensembles de mod√®les personnalis√©s.</figDesc><table><row><cell></cell><cell cols="4">Train-G Part-1 Part-2 Indicateur</cell></row><row><cell>Dur√©e, heures</cell><cell>200</cell><cell>86</cell><cell>73</cell><cell>0.5</cell></row><row><cell>Nombre de locuteurs</cell><cell>880</cell><cell>736</cell><cell>634</cell><cell>32</cell></row><row><cell cols="2">Nombre de mod√®les personnalis√©s -</cell><cell cols="3">1300 1079 -</cell></row></table><note><p><p><p>Les exp√©riences ont √©t√© men√©es sur la partition d'adaptation au locuteur du corpus TED-LIUM 3</p><ref type="bibr" target="#b9">(Hernandez et al., 2018)</ref></p>. Ce corpus disponible publiquement contient les conf√©rences TED qui repr√©sentent 452</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 1 -</head><label>1</label><figDesc>Statistiques de donn√©es</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 2 -</head><label>2</label><figDesc>FIGURE 4 -EER, % pour le mod√®le d'attaque A1 en fonction de la couche cach√©e h, √©valu√© sur Part-2. ¬µ + œÉ -les moyennes et les √©carts types ont √©t√© utilis√©s pour calculer le score de similarit√©. œÅ ¬µ -uniquement les moyennes ; et œÉ -uniquement les √©carts types ont √©t√© utilis√©s. configuration A2. Les r√©sultats comparatifs des deux mod√®les d'attaques sont pr√©sent√©s dans le tableau 2. Pour h = 5, le deuxi√®me mod√®le d'attaque fournit une am√©lioration significative des performances par rapport au premier et r√©duit l'EER de 7% √† 2%. Pour h = 1, nous n'avons pu obtenir aucune am√©lioration en entra√Ænant un mod√®le d'attaque bas√© sur un r√©seau de neurones : les r√©sultats pour A2 dans ce cas sont moins bons par rapport √† l'approche simple A1. EER, % √©valu√© sur Part-2, h -indicateur d'une couche cach√©e5 ConclusionsDans cette √©tude, nous nous sommes concentr√©s sur le probl√®me de la protection de la vie priv√©e pour les mod√®les acoustique de RAP construits dans un cadre d'apprentissage f√©d√©r√©. Nous avons explor√© dans quelle mesure ces mod√®les de RAP sont vuln√©rables aux attaques contre la confidentialit√©. Nous avons d√©velopp√© deux mod√®les d'attaques qui visent √† d√©duire l'identit√© du locuteur √† partir des mod√®les personnalis√©s mis √† jour localement sans avoir acc√®s aux donn√©es vocales des locuteurs cibles. Un mod√®le d'attaque est bas√© sur le score de similarit√© propos√© entre les mod√®les acoustiques personnalis√©s, calcul√© sur un ensemble de donn√©es indicateur externe, et un autre est un mod√®le neuronal. Nous avons d√©montr√© sur le corpus TED-LIUM 3 que les deux mod√®les d'attaque sont tr√®s efficaces et peuvent fournir un EER d'environ 1% pour le mod√®le d'attaque simple A1 et 2% pour le mod√®le d'attaque neuronal A2. Une autre contribution importante de ce travail est la d√©couverte que la premi√®re couche des mod√®les acoustiques personnalis√©s contient une grande quantit√© d'informations sur le locuteur qui sont principalement contenues dans les valeurs de d√©viation standard calcul√©es sur les donn√©es indicateur. Cette propri√©t√© int√©ressante des mod√®les acoustiques neuronaux personnalis√©s ouvre de nouvelles perspectives √©galement pour la VAL. Dans des travaux futurs, nous pr√©voyons de l'utiliser pour d√©velopper un syst√®me VAL efficace.</figDesc><table><row><cell></cell><cell>30</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>25</cell><cell>+</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>EER,%</cell><cell>15 20</cell><cell></cell><cell></cell><cell>12.97</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>11.98</cell><cell>13.17</cell><cell>14.15</cell><cell>16.08</cell><cell>18.96</cell><cell>20.51</cell></row><row><cell></cell><cell>10</cell><cell></cell><cell>8.41</cell><cell></cell><cell>7.94</cell><cell>7.11</cell><cell>7.98</cell><cell>8.86</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>0.86</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>7</cell><cell>8</cell><cell>9</cell><cell>10</cell><cell>11</cell><cell>12</cell><cell>13</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Couche cach√©e</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">Mod√®le d'attaque</cell><cell cols="2">h=1 h=5</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>A1</cell><cell></cell><cell></cell><cell></cell><cell cols="2">0.86 7.11</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>A2</cell><cell></cell><cell></cell><cell cols="3">12.31 1.94</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>En suivant la notation de<ref type="bibr" target="#b16">(Peddinti et al., 2015)</ref>, la configuration du mod√®le peut √™tre d√©crite comme suit : {-1,0,1} √ó 6 couches ; {-3,0,3} √ó 7 couches.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Remerciements</head><p><rs type="programName">Ces travaux ont √©t√© financ√© par les projets VoicePersonae</rs> (<rs type="grantNumber">ANR-18-JSTS-0001</rs>), <rs type="projectName">DEEP-PRIVACY</rs> (<rs type="grantNumber">ANR18-CE23-0018</rs>) et <rs type="programName">programme de Recherche et d'Innovation Horizon 2020</rs> (<rs type="grantName">Marie Sk≈Çodowska-Curie grant</rs>, No <rs type="grantNumber">101007666</rs>). Ces travaux ont b√©n√©fici√© d'un <rs type="programName">acc√®s aux moyens de calcul de l'IDRIS au travers de l</rs>'allocation de ressources 2021-AD011013331 attribu√©e par GENCI.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_RDRFAA2">
					<idno type="grant-number">ANR-18-JSTS-0001</idno>
					<orgName type="project" subtype="full">DEEP-PRIVACY</orgName>
					<orgName type="program" subtype="full">Ces travaux ont √©t√© financ√© par les projets VoicePersonae</orgName>
				</org>
				<org type="funding" xml:id="_gnxyxav">
					<idno type="grant-number">ANR18-CE23-0018</idno>
					<orgName type="grant-name">Marie Sk≈Çodowska-Curie grant</orgName>
					<orgName type="program" subtype="full">programme de Recherche et d&apos;Innovation Horizon 2020</orgName>
				</org>
				<org type="funding" xml:id="_8aqefNF">
					<idno type="grant-number">101007666</idno>
					<orgName type="program" subtype="full">acc√®s aux moyens de calcul de l&apos;IDRIS au travers de l</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Benchmarking and challenges in security and privacy for voice biometrics</title>
		<author>
			<persName><forename type="first">Bonastre J.-F</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 2021 ISCA Symposium on Security and Privacy in Speech Communication</title>
		<meeting>2021 ISCA Symposium on Security and Privacy in Speech Communication</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="52" to="56" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>Bonawitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ivanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kreuter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.04482</idno>
		<title level="m">Practical secure aggregation for federated learning on user-held data</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">The secret sharer : Evaluating and testing unintended memorization in neural networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">√ö</forename><surname>Erlingsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="267" to="284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Federated acoustic modeling for automatic speech recognition</title>
		<author>
			<persName><forename type="first">X</forename><surname>Cui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">A federated approach in training acoustic models</title>
		<author>
			<persName><forename type="first">D</forename><surname>Dimitriadis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="981" to="985" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Differential privacy</title>
		<author>
			<persName><forename type="first">C</forename><surname>Dwork</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Inverting gradients-how easy is it to break privacy in federated learning ?</title>
		<author>
			<persName><forename type="first">J</forename><surname>Geiping</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Improving on-device speaker verification using federated learning with privacy</title>
		<author>
			<persName><forename type="first">F</forename><surname>Granqvist</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Training speech recognition models with federated learning : A quality/cost framework</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Guliani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">TED-LIUM 3 : twice as much data and corpus repartition for experiments on speaker adaptation</title>
		<author>
			<persName><forename type="first">F</forename><surname>Hernandez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ghannay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Tomashenko</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="198" to="208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Federated learning for speech emotion recognition applications</title>
		<author>
			<persName><forename type="first">S</forename><surname>Latif</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="341" to="342" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Federated learning for keyword spotting</title>
		<author>
			<persName><forename type="first">D</forename><surname>Leroy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6341" to="6345" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Federated learning : Challenges, methods, and future directions</title>
		<author>
			<persName><forename type="first">Li</forename><forename type="middle">T</forename><surname>Sahu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Talwalkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="50" to="60" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Communication-efficient learning of deep networks from decentralized data</title>
		<author>
			<persName><forename type="first">B</forename><surname>Mcmahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moore</forename><surname>Ramage D</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1273" to="1282" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Retrieving speaker information from personalized acoustic models for speech recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mdhaffar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A survey on security and privacy of federated learning</title>
		<author>
			<persName><forename type="first">V</forename><surname>Mothukuri</surname></persName>
		</author>
		<author>
			<persName><forename type="middle">M</forename><surname>Parizi R</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pouriyeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Future Generation Computer Systems</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="page" from="619" to="640" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">A time delay neural network architecture for efficient modeling of long temporal contexts</title>
		<author>
			<persName><forename type="first">V</forename><surname>Peddinti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">The Kaldi speech recognition toolkit</title>
		<author>
			<persName><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ghoshal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Boulianne</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Purely sequence-trained neural networks for ASR based on lattice-free MMI</title>
		<author>
			<persName><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Peddinti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Galvez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ghahremani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Manohar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Na</forename><forename type="middle">X</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2751" to="2755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A framework for secure speech recognition</title>
		<author>
			<persName><forename type="first">P</forename><surname>Smaragdis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shashanka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1404" to="1413" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">X-vectors : Robust DNN embeddings for speaker recognition</title>
		<author>
			<persName><forename type="first">D</forename><surname>Snyder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Garcia-Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5329" to="5333" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Privacy attacks for automatic speech recognition acoustic models in a federated learning framework</title>
		<author>
			<persName><forename type="first">N</forename><surname>Tomashenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mdhaffar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tommasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Est√®ve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bonastre J.-F</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
			<publisher>ICASSP</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Introducing the VoicePrivacy initiative</title>
		<author>
			<persName><forename type="first">N</forename><surname>Tomashenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M L</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nautsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yamagishi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Evans</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1693" to="1697" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The VoicePrivacy 2020 Challenge : Results and findings</title>
		<author>
			<persName><forename type="first">N</forename><surname>Tomashenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech &amp; Language</title>
		<imprint>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="page">101362</biblScope>
			<date type="published" when="2022">2022b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Demystifying membership inference attacks in machine learning as a service</title>
		<author>
			<persName><forename type="first">S</forename><surname>Truex</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Gursoy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><surname>Wei W</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Services Computing</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Beyond inferring class representatives : User-level privacy leakage from federated learning</title>
		<author>
			<persName><forename type="first">Wang</forename><forename type="middle">Z</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2512" to="2520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">Yu</forename><forename type="middle">W</forename><surname>Freiwald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tewes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Huennemeyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Kolossa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2109.15108</idno>
		<title level="m">Federated learning in ASR : Not as easy as you think</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
