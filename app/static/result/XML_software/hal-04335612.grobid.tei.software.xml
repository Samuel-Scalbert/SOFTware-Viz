<TEI xmlns="http://www.tei-c.org/ns/1.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xml:space="preserve" xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">WAM-studio, a Digital Audio Workstation (DAW) for the Web</title>
			</titleStmt>
			<publicationStmt>
				<publisher />
				<availability status="unknown"><licence /></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Michel</forename><surname>Buffa</surname></persName>
							<email>buffa@univ-cotedazur.fr</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University Côte d'Azur</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<orgName type="institution" key="instit3">INRIA France</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Antoine</forename><surname>Vidal</surname></persName>
							<email>antoine.vidal-mazuy@etu.univ-cotedazur.fr</email>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">University Côte d'Azur</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<orgName type="institution" key="instit3">INRIA France</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">WAM-studio, a Digital Audio Workstation (DAW) for the Web</title>
					</analytic>
					<monogr>
						<imprint>
							<date />
						</imprint>
					</monogr>
					<idno type="MD5">EE60ECFFAC6A6684CB017946E838CB33</idno>
					<idno type="DOI">10.1145/3543873.3587987</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-04-12T14:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid" />
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Software and its engineering → Abstraction</term>
					<term>modeling and modularity Web Audio</term>
					<term>DAWs</term>
					<term>plugin architecture</term>
					<term>Web standards</term>
				</keywords>
			</textClass>
			<abstract>
<div><p>This paper presents <software>WAM Studio</software>, an open source, online Digital Audio Workstation (DAW) that takes advantages of several W3C Web APIs, such as Web Audio, <software ContextAttributes="used">Web Assembly</software>, Web Components, <software ContextAttributes="used">Web Midi</software>, <software ContextAttributes="used">Media Devices</software> etc. It also uses the <software ContextAttributes="used">Web Audio Modules</software> proposal that has been designed to facilitate the development of inter-operable audio plugins (effects, virtual instruments, virtual piano keyboards as controllers etc.) and host applications. DAWs are feature-rich software and therefore particularly complex to develop in terms of design, implementation, performances and ergonomics. Very few commercial online DAWs exist today and the only open-source examples lack features (no support for inter-operable plugins, for example) and do not take advantage of the recent possibilities offered by modern W3C APIs (e.g. <software ContextAttributes="used">Au-dioWorklets</software>/<software ContextAttributes="used">Web Assembly</software>). <software ContextAttributes="used">WAM Studio</software> was developed as an open-source technology demonstrator with the aim of showcasing the potential of the web platform, made possible by these APIs. The paper highlights some of the difficulties we encountered (i.e limitations due to the sandboxed and constrained environments that are Web browsers, latency compensation etc.). An online demo, as well as a GitHub repository for the source code are available.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div><head n="1">INTRODUCTION</head><p>Computer Assisted Music (CAM) is an ever-evolving field that uses computers to record, edit, and produce music. Digital Audio Workstations (DAWs) are specially designed software for CAM , allowing users to create and manipulate digital audio content,as well as MIDI WWW'23Companion,April30-May4,2023,Austin,TX,USA https://doi.org/10.1145/3543873.3587987 content. Audio plugins are software modules that add additional features to DAWs, giving users greater flexibility and control over their music production. The CAM market became popular with the Atari ST (1985) and the <software ContextAttributes="used">Cubase</software> DAW proposed by Steinberg (1989) <ref type="bibr" target="#b0">1</ref> . Soon, the VST standard for audio plugins has been proposed (1997)  and since then thousands of plugins have been developed, that can be used in the main native DAWs available on the market. On the Web platform, the first online DAWs appeared in 2008 and used the Flash technology. The first DAWs that used HTML5 and the <software ContextAttributes="used">Web Audio API</software> for audio processing did not emerge until the period between 2015 and 2016. A DAW is a feature-rich software and therefore particularly complex to develop in terms of design, implementation, and ergonomics. It allows the creation of multi-track pieces by directly using audio samples (e.g. by incorporating into a track an audio file or by recording from a microphone or sound card input), mixing them, applying sound effects to each track (e.g. reverb, frequency equalization, or autotune on vocals), but also using tracks with virtual instruments (software recreation of a piano, violin, synthesizer, drum kit, etc.). The track is then recorded in MIDI format : as events corresponding to notes and additional parameters (such as the strength with which one pressed the keys of a piano keyboard, for example). These events allow the track to be played by requesting a virtual instruments to synthesize the signal. The DAW therefore allows for the playing, recording and mixing of both audio and MIDI tracks, edition of these tracks (copy/cut paste within a track or between tracks), the management of real-time audio effects and virtual instruments, mixing, and exporting the final project in a simple format (e.g. .wav or .mp3 file). In the native world, these effects and instruments are the "audio plugins" that extend the capabilities of standard DAWs. Since 1997, an important market for third-party plugin developers has developed. Four DAWs dominate the market (Logic Audio, Ableton, Pro Tools, <software ContextAttributes="used">Cubase</software>), and the existence of several proprietary plugin formats complicates the task of developers. On the Web, the market is emerging, the technologies more recent, and few DAWs exist, mostly commercial. Nevertheless, an inter-operable plugin format called <software ContextAttributes="used">Web Audio Modules</software> (WAM), currently undergoing active development exists and is backed by at least one commercially available online DAW.</p></div>
<div><head n="2">THE WEB AUDIO API</head><p>Since 2018, the <software>W3C</software> Web <software ContextAttributes="used">Audio API</software> has been a "candidate recommendation" (a frozen standard). It offers a set of Audio Node generators that process or produce sound. These nodes can be connected to form an "audio graph". The sound travels through this graph at a sample rate of 44100 times per second (default value, this can be changed) and undergoes transformations [3]. Some nodes are wave generators or sound sources corresponding to a microphone input or a sound file loaded into memory, others transform the sound. Connecting these nodes in the browser is done through JavaScript and enables a wide range of different applications involving real-time audio processing [6]. Music applications are not the only ones requiring complex audio back-ends, and the API is also designed to meet the needs of games and other use cases, not only for computer music. The API comes with a limited set of standard nodes for common operations such as volume control, audio filtering, delay/echo, reverb, dynamic processing, spatialization, etc.</p><p>Usually, in multi-core processor systems, audio libraries/APIs split the work between a control thread and a render thread [7, 11,  12]. The <software>Web Audio API</software> is no exception and also uses this pattern: a render thread called the "audio thread" is solely responsible for rendering the audio graph and delivering the samples to the operating system so they can be played by the hardware. This thread has hard real-time constraints and has a high priority -if it fails to provide the next block of audio samples on time (128 samples by default in the case of the <software ContextAttributes="used">Web Audio API</software>), there will be audible glitches. On the other hand, the control thread (generally running JavaScript <software ContextAttributes="used">code</software> for the GUI, and making <software ContextAttributes="used">Web Audio API</software> calls) manages all changes to the audio graph. It allows the user to connect/disconnect nodes and to adjust their parameters. Audio Nodes process the sound in the audio thread, and the algorithms used cannot be changed, only parameter changes are allowed. The recent addition (2018) of the <software ContextAttributes="used">AudioWorklet</software> node provided a solution for implementing custom low-level audio processing running in the audio thread [6].</p><p>The nodes in the <software>Web Audio API</software> can be assembled into an "audio graph," which developers can use to create more complex audio effects or instruments. Here are some examples of audio effects that could be built this way: echo (DelayNode, BiquadFilterNode, GainNode), auto-wah (BiquadFilterNode, OscillatorNode), chorus (multiple DelayNodes and OscillatorNodes for modulation), distortion (GainNode, <software ContextAttributes="used">WaveShaperNode</software>), synthesizers, samplers etc. Existing DSP code in other languages such as C/C++ or coded using Domain Specific Languages such as FAUST, can be cross-compiled to <software ContextAttributes="used">Web Assembly</software> and run in a single <software ContextAttributes="used">AudioWorklet</software> node. Over the years, lots of high-level audio effects and instruments have been developed [4]. However, it is often necessary to chain such audio effects and instruments together (for example, in a guitarist's pedalboard) and during music composition/production in DAWs, multiple effects and instruments are used. These are cases where the <software ContextAttributes="used">Web Audio API</software> nodes are too low-level, hence the need for a higher-level unit to represent the equivalent of a native audio plugin [1]. For the Web Platform, such a high-level standard for "audio plugins" and "host" applications did not exist before 2015 [8]. Several initiatives have aroused and one emerged as a "community standard", which we detail in the following section.</p></div>
<div><head n="3">WEB AUDIO MODULES</head><p>In 2015, Jari Kleimola and Olivier Larkin proposed the <software>Web Audio Modules</software> (WAM) [8], a standard for Web Audio plugins and DAWs. Soon later, Jari Kleimola was involved in the creation of ampedstudio.com, one of the first online DAWs using the <software ContextAttributes="used">WebAudio API</software>. In 2018, the WAM initial project was further developed by researchers with the help of developers from the digital audio workstation industry, resulting in a more versatile standard [3]. Finally, the 2.0 version of <software ContextAttributes="used">Web Audio Modules</software> has been released in 2021. This version aimed, in addition to setting a community standard (API, SDK), to bring more versatility to developers, more performance, to simplify access to plugin parameters, and facilitate integration in DAWs [5]. We will come back to this feature later, but WAM 2.0 uses an original design for handling the communication between plugins and host applications that do not rely on the low-level parameter management provided by the <software ContextAttributes="used">Web Audio API</software>. The main reason for that is to allow high performances in the case where both a DAW and plugins are implemented as <software ContextAttributes="used">AudioWorklets</software>. At the time the <software ContextAttributes="used">WebAudio API</software> has been designed, <software ContextAttributes="used">AudioWorklets</software> did not exist and some use cases could not be taken into account. Indeed, if the DAW is built using <software ContextAttributes="used">AudioWorklet</software> nodes for processing audio, then some parts of the code run in the high priority / audio thread. Then, if a WAM plugin is associated with a given track in a DAW project, and if the plugin is itself built using an <software ContextAttributes="used">AudioWorklet</software> node, it also has custom code running in the audio thread. The WAM framework has been designed to handle this particular case and will enable DAW/plugins communication without crossing the audio thread barrier. Let us take one example: while playing, a MIDI track sends notes to a virtual instrument plugin, and changes some of the parameters of this plugin at the frequency rate. Remember that a DAW can have multiple tracks associated to dozen of plugins, and each plugin can have dozens of parameters. The WAM framework detects this case, and will seamlessly use Shared Array Buffers and a ring buffer, without crossing the audio thread barrier. No need to send events from the control/GUI thread, that would have been mandatory if the <software ContextAttributes="used">Web Audio API</software> audio node parameter management was used. To sum up, the WAM framework streamlines the creation of plugins and host applications and enables highly efficient communication between hosts and plugins.</p></div>
<div><head n="4">RELATED WORKS: COMMERCIAL AND OPEN SOURCE DAWS</head><p>The first online DAWs based on the <software ContextAttributes="used">WebAudio API</software> appeared in 2015-2018 and are still available today: <software ContextAttributes="used">Audiotools</software> [9], <software ContextAttributes="used">Bandlab</software>, <software ContextAttributes="used">Amped-Studio</software>, <software ContextAttributes="used">Soundtrap</software> [10], <software ContextAttributes="used">Soundation</software>. These DAWs have in common the fact that they are commercial, closed source and received very little academic publication. They facilitate remote collaboration on musical projects, and have gained increased attention during the COVID-19 pandemic (2021-2022). The mode of collaboration varies (synchronous like <software ContextAttributes="used">Google Docs</software> or asynchronous through sharing links), as well as the communication tools provided (chat, video conferencing), but all these DAWs have the classic features: audio and MIDI recording, track editing, mixing, support for effects and virtual instruments, etc. Some have been designed mainly for desktop computers while others are particularly adapted to mobile devices. They mainly differ in terms of ergonomics : some of these applications are aimed at a very wide audience and have focused on simplicity (<software ContextAttributes="used">BandLab</software>, <software ContextAttributes="used">SoundTrap</software>) while others have chosen a more "professional" and sober look and feel, like <software ContextAttributes="used">AmpedStudio</software>. <software ContextAttributes="used">Audiotools</software> is more a "virtual studio" than a pure DAW and finds its market mainly in the education domain. Implementation details of these applications are unknown as the source code is not publicly accessible. However, through limited academic publications and presentations/interviews, it is known that <software ContextAttributes="used">AmpedStudio</software> uses a cross-compiled C++ engine and <software ContextAttributes="used">AudioWorklets</software>, while <software ContextAttributes="used">Soundtrap</software> has primarily utilized the standard <software ContextAttributes="used">Web Audio API</software> nodes. <software ContextAttributes="used">BandLab</software> is supposed to have a core written in C++, and relies also on <software ContextAttributes="used">Au-dioWorklet</software>, with possibly a specific mobile version. <software ContextAttributes="used">Audiotools</software> runs also in <software ContextAttributes="used">AudioWorklet</software> node(s) with a flash port of the audio engine, effects, instruments... Recently, a new DAW named <software ContextAttributes="used">Arpeggi.io</software> has emerged, which highlights its use of the blockchain technology (for tracking who, when, and how sounds and music are used) and NFTs for monetizing creations. These DAWs differ also in the way they manage audio effects and instruments. It is obvious that some support external plugins: <software ContextAttributes="used">AmpedStudio</software> supports the WAM standard -developers from the company contributed to the proposal-and the web site has a shop for "enabling" premium plugins. The other commercial online DAWs seems to use a proprietary format, while integrating some third party ports of native plugins (<software ContextAttributes="used">Soundtrap</software> proposes the famous "Autotune" plugin made by Antares). Propellerheads, a company well known for its native DAW Reason, also published web versions of its Europa synthesizer plugin (available now in <software ContextAttributes="used">AmpedStudio</software> and in <software ContextAttributes="used">Soundation</software>). So far, <software ContextAttributes="used">AmpedStudio</software> is the only DAW that supports an open plugin standard: <software ContextAttributes="used">Web Audio Modules</software>. Figure <ref type="figure" target="#fig_0">1</ref> illustrates the similarities and differences between these commercial online DAWs.</p><p><software ContextAttributes="used">Gridsound</software> is an open source DAW, that has been developed since 2015 <ref type="bibr" target="#b1">2</ref> . It supports audio and MIDI and is a fully functional DAW. It does not support plugins but comes with a set of audio effects and virtual instruments whose format is specific. There are also popular JavaScript libraries for developing a multi-track player/recorder, such as wavesurfer.js or waveform-playlist <ref type="bibr" target="#b2">3</ref> , and some audacity like audio buffer editors such as <software ContextAttributes="used">AudioMass</software> <ref type="bibr" target="#b3">4</ref> . None of these open source initiatives use <software ContextAttributes="used">AudioWorklets</software> and low-level processing, nor take care about optimizing DAW/plugin communication, preserving audio thread isolation, nor use external plugins. This is the main reason why we developed <software ContextAttributes="used">Wam-Studio</software>: as a demonstrator of these techniques.</p></div>
<div><head n="5">WAM-STUDIO DESIGN AND IMPLEMENTATION</head><p><software>Wam-Studio</software> is an online tool for creating audio projects that you can imagine as multi-track music. Each track corresponds to a different "layer" of audio content that can be recorded, edited, or just integrated (using audio files for example). Some track can be used to control virtual instruments: in that case we record the sound that is generated internally by these virtual instruments (and played using a MIDI piano keyboard, for example). Tracks can be added or removed, played isolated or with other tracks. They can also be "armed" for recording, and when the recording starts, all other tracks will play along, while the armed track will record new content.</p></div>
<div><head n="5.1">The tracks</head><p>A DAW track is a container for audio-related data that comes with an interactive display of these data, editing and processing facilities and few default parameters such as volume and left/right panning. Figure <ref type="figure" target="#fig_1">2</ref> shows an isolated audio track with the wave- form display of the associated audio buffer, and the default track controls/parameters on the left side (mute/solo, volume, stereo panning). As many tracks can be rendered, scrolled during playback, zoomed in/out and edited, we used the <software ContextAttributes="used">pixi.js</software> library to handle drawing and interactions. This library uses gpu accelerated <software ContextAttributes="used">webgl</software> rendering, and provides many features for handling multiple layers over a single HTML5 canvas. As shown in Figure <ref type="figure" target="#fig_2">4</ref>, each track can also be associated with a set of plugins for adding audio effects or for generating music (in the case of instrument plugins). Figure <ref type="figure">3</ref> shows the audio graph that corresponds to the audio processing chain of an audio track. The sounds goes from left to right: first the "track player/recorder/editor" is implemented as an <software ContextAttributes="used">AudioWorklet</software> node, using custom code for rendering an audio buffer, then the output signal has its gain and stereo panning adjusted, then we have another <software ContextAttributes="used">AudioWorket</software> node for rendering the volume in a canvas (vu-meter), and at the end we find a chain of WAM plugins for adding audio effects.</p></div>
<div><head>Figure 3: Audio graph of an audio track in Wam-Studio</head><p>There are two types of tracks: Audio tracks : they contain recorded audio, such as a vocal take, a guitar recording, or any other type of audio signal, that are generally rendered graphically as waveforms that represents the actual signal stored (as a set of sound samples). These tracks can be edited, processed and mixed by copying/cutting and pasting audio samples in the audio buffer associated to the track. Audio track's output can be further processed by a chain of audio effects. MIDI tracks : they contains MIDI data (the pitch -the note an instrument would play, velocity, and duration). MIDI data does not contain audio information but are rather used to control virtual instruments such as synthesizers, samplers, drum machines, etc. MIDI tracks can be edited, in that case, it is possible to change the MIDI events stored in the track in a matrix-like display. MIDI tracks are generally associated with one or more virtual instruments. The MIDI events can target all track instruments or some specifically. The tracks are usually organized vertically within the DAW's graphical user interface and can be separately managed in terms of volume, stereo positioning, effects processing, and automation of effects or instruments (as shown in Figure <ref type="figure" target="#fig_2">4</ref>). Each track output is connected to a "master track" where it is possible to adjust globally the volume and panning of the final mix. Like for any other tracks, it is also possible to associate a chain of audio effects to the Master track (adjusting final dynamics, frequencies). All audio effects and virtual instruments are WAM plugins in the case of our DAW. This design gives an extensive degree of control and adaptability and enables users to blend and manipulate the sound of each track with high precision and sophistication, thus making it easier to create intricate audio productions. A DAW track is also capable of "rendering" the track into an audible audio signal (when the track is being played). When one presses the play button of the DAW, all the tracks are rendered simultaneously, and the final output signal is what is called "the mix". DAWs also have an "offline rendering" option, for exporting the final mix as a file, on local hard disk or on the cloud (using the <software ContextAttributes="used">OfflineAudioContext</software><ref type="foot" target="#foot_4">5</ref> provided by the <software ContextAttributes="used">WebAudio API</software>).</p></div>
<div><head n="5.2">Track core implemented as an AudioWorklet processor</head><p>Let us focus for the moment on an Audio track. The <software>Web Audio API</software> provides the <software ContextAttributes="used">AudioBufferSourceNode</software> that is a container for an audio buffer. This standard node is enough to play any audio buffer stored in memory. However, we will explain why this is not sufficient for a high performance DAW... Remember what can be done in DAWs while a multi-track piece is being played (or recorded): parameter automation! This means interpolating plugin and track parameters at the frequency rate, 44100 times per second. Moreover, these automated parameters can encompass any characteristic of any plugin associated with any track. It is a substantial amount of information that can be exchanged between the DAW and WAM plugins, making performance a crucial aspect that requires careful consideration. By designing our own low-level audio player (instead of using the standard <software ContextAttributes="used">AudioBufferSourceNode</software>), we gain total control over the playback/recording behaviour of each track. We designed each audio track core as an <software ContextAttributes="used">AudioWorklet</software> processor, more precisely, as an instance of a subclass of the <software ContextAttributes="used">WamProcessor</software> class provided by the <software ContextAttributes="used">Web Audio Modules</software> SDK, that inherits from the <software ContextAttributes="used">AudioWorkletProcessor</software> class. According to the API, each processor implements a process method that will be called at the sampling rate. Inside this method, we output the sound samples and render the buffer, but we can also send data to WAM plugins. The WAMProcessor class we inherit from provides methods for handling DAW/plugins communication efficiently. With the <software ContextAttributes="used">WebAudio API</software>, <software ContextAttributes="used">AudioWorklets</software> are comprised of two components, reflecting the multi-threaded nature of the environment: the <software ContextAttributes="used">AudioWorkletNode</software> and the <software ContextAttributes="used">AudioWorkletProcessor</software>. The WAM framework extends them into the <software ContextAttributes="used">WamNode</software> and the <software ContextAttributes="used">WamProcessor</software> classes, which serve as the main thread and audio thread of WAM powered software, respectively. <software ContextAttributes="used">WAM-Studio</software> is a WAM host application, and its tracks are implemented as sub-classes of <software ContextAttributes="used">WamProcessor</software>. The WAM plugins associated to these tracks can also have their DSP core implemented as subclasses of <software ContextAttributes="used">WamProcessor</software>, but this is not mandatory. Some WAM plugins implementations utilize a single node, while others utilize a subgraph consisting of a combination of builtin web audio nodes and custom audio nodes, as abstracted through the <software ContextAttributes="used">WamNode</software> and <software ContextAttributes="used">WamProcessor</software> interfaces. This abstraction enables seamless interoperability between a WAM hosts and plugins, regardless of the underlying implementation, and across all threads.</p><p>In other words : if a track is a <software>WamProcessor</software> and if plugins are also <software ContextAttributes="used">WamProcessors</software>, their DSP code runs in the audio thread and highly optimized communication can be achieved: Shared Array Buffers can be used for DAW/<software ContextAttributes="used">Plugin</software> communication. If a plugin is not a <software ContextAttributes="used">WamProcessor</software>, then the DAW code that "talks" with it remains unchanged and the underlying implementation will be suboptimal and will involve crossing the thread barrier. Handling multiple plugins in a chain: The WAM framework employs a singleton object, referred to as the WamEnv, which is attached to the global scope of the audio thread and serves as a mediator for interactions between hosts and processors/plugins. Processors/plugins are structured into WamGroups, managed by the WamEnv, where each group comprises plugins created by a specific host. Considerable effort was exerted to reduce the number of assumptions made by the API regarding host implementation, with the WamEnv and WamGroup being the only objects within the WAM system that are anticipated to be supplied by the host. WamEnv is allocated when the DAW is created, and WamGroup instances are associated with each track.</p></div>
<div><head n="5.3">Managing plugin chains</head><p><software ContextAttributes="used">Plugin</software> chains are handled using a special WAM plugin that acts also as a "mini host". We called it the WAM pedalboard [1]. It connects to a plugin server that sends back the list of plugins available as a JSON array of URIs (a WAM plugin can be loaded simply using a dynamic import and its URI, see [5]). From this list of URIs, WAM plugin descriptors are retrieved, that contain metadata about the plugins: name, version, vendor, thumbnail image URI, etc. When the pedalboard plugin is being displayed in the DAW, it is empty, and plugins can be added to the processing chain, removed, re-ordered, and their parameters can be set. Any configuration can be saved as ). Presets can be organized into banks ("rock", "funk", "blues"). The management of the organization and naming of banks and presets is the responsibility of the pedalboard plugin. The parameters exposed by this plugin correspond to the entire set of parameters of the active preset (that is, the sum of the parameters of the plugins associated in the preset), and can be automated by the DAW. The <software ContextAttributes="used">Web Audio Modules API</software> allows for sending events to the DAW when changes occur in the plugin configuration (addition or removal) and the menu provided in the DAW for selecting automatable parameters is automatically updated. All WAM plugins implement <software ContextAttributes="used">getState</software>/setState methods to serialize/deserialize their state. When a project is saved, the state of each track, audio buffers, etc. are saved, as well as the state of the plugin configuration. The recording process using a DAW is more constrained and structured compared to using a simple recording app like a memo recorder, as we must take into account different kind of latencies. The input latency is the time between when an audio signal is captured by the input device and when it is processed by the audio context. It depends on the OS, the configuration of the input device, the processing time of the audio system, and the buffer size used by the audio context. The output latency represents the time between when a sound is generated and when it is actually heard by the user. The sum of these two latency represents the round-trip latency and can be measured using an external recording device with two microphones: one at the source of the physical sound (for example, on the body of a guitar plugged in a sound card), and one in front of the speakers. Hit the body of the guitar, record the output and compare the input and the output signals. We did such measurements in the past [2] and showed that the round-trip latency using MacOS with common sound cards available on the market was around 19-23ms with an audio buffer of 128 samples (on Windows this was much bigger, 50-75ms). The <software ContextAttributes="used">WebAudio API</software> exposes two properties of the AudioContext that could have been useful. The baseLatency property represents the number of seconds the AudioContext uses for its internal processing once the audio reaches the AudioDestinationNode (final output of the audio graph). On Chrome it represents the minimum guaranteed latency that an audio context will add to the input audio stream before it is played through the output, and can be calculated as the audio buffer size divided by the sample rate. It is a rough approximation of the real input latency, while on Firefox it is always zero (as Firefox processes the audio directly in the audio callback <ref type="bibr" target="#b5">6</ref> ). The outputLatency property represents the number of seconds between the audio reaching the AudioDestinationNode (conceptually the audio sink of the audio processing graph) and the audio output device. In February 2023 it is not implemented in all major browsers. This situation makes these properties nearly useless, as we will see. Figure <ref type="figure" target="#fig_4">6</ref> shows the audio graph involved during the track recording process: we need first to obtain a MediaStream from the user's microphone or from a sound card input using the MediaDevices API. Then, we can use a <software ContextAttributes="used">Web Audio API</software> MediaStreamSourceNode in the audio graph, passing the MediaStream to its constructor. This is how we expose a live audio stream to the <software ContextAttributes="used">Web Audio API</software>. In a second step, we need a way to record this stream in a buffer. This can be done using the W3C MediaRecorder API, or using low-level solutions using an <software ContextAttributes="used">AudioWorklet</software>. The MediaRecorder solution has for advantage its simplicity but all tests we conducted showed that this is an unreliable solution as the time it takes for allocating audio buffers and starting recording is unpredictable, even when we conduct latency measurements/calibration, as explained later on. We ultimately decided on a low-level, precise solution that uses a <software ContextAttributes="used">WamProcessor</software> to record the sound samples that are received. This solution was crucial, as it was necessary to have full control over the buffer in order to expand it during extended recording sessions. When recording a guitar track, for example, other tracks such as drum and bass tracks can be being played simultaneously, and the guitar sound is processed by a chain of plugins (i.e guitar amplifier simulation, EQ, delay,etc.). The processed recorded guitar sound must be heard real-time in sync with the other tracks and free from any noticeable latency while being played (and recorded). This is why the overall round-trip latency should be as low as possible. We represented it with the red path in Figure <ref type="figure" target="#fig_4">6</ref>. Using a 128 sample buffer size, the round-trip latency of 23ms we measured on MacOS [2], even with pro guitar players, was comfortable and comparable with what we can achieve using native applications in similar use cases (guitar, Logic Audio, guitar amp sim plugin, audio buffer size 128). The recording process requires also proper allocation and management of the track audio buffer, as well as continuous storage of the sound samples coming from the input stream. Here, the input latency is important as the recording process may be delayed due to hardware and software configurations (audio driver, etc.), but also the buffer allocation should be done with care: pre-allocate it before even pressing the recording button, use a ring buffer and start recording as soon as the track is "armed" (DAWs generally provide an "arm" button to select tracks that will be recorded), then store a timestamp when the recording really starts. To ensure that the recorded guitar track plays back in sync with the other tracks, we do some "latency compensation" i.e shift back in time the buffer content, depending on the input latency. Do you remember that we do not know its accurate value? The problem is that it is rather complicated to find this number in a program running, for a variety of reasons (again, see Paul Adenot blog post on footnotes). The solution we use consists in calibrating the DAW along with the hardware used, prior to recording. This process involves the use of a microphone to record a pre-generated sound emitted by the DAW and determining the time elapsed between the emission of the signal and its recording. This value is then used for latency compensation. The <software ContextAttributes="used">AmpedStudio</software> DAW proposes a similar approach for calibration in its setting menu. <software ContextAttributes="used">Soundtrap</software> authors explained that they rather use lookup tables with entries for the most common OS/soundcard pairs, calibrated by hand, and hard coded in the code [10]. They then use some heuristics to guess the current configuration (checking the OS, guessing the sound card model looking at the number of inputs/outputs exposed using the MediaDevice API, etc.). In <software ContextAttributes="used">WAM-Studio</software>, latency compensation is a work in progress: for the moment we do it manually by entering a value measured by an external utility software <ref type="bibr" target="#b6">7</ref> we developed, that uses the calibration process we just described (emit a sound, record, compare input and output).</p></div>
<div><head n="5.4">Recording process, dealing with latency</head></div>
<div><head n="6">CONCLUSION</head><p>As of today, <software ContextAttributes="used">WAM-Studio</software> is an open source example of a DAW using advanced features such as external plugin support, high performance DAW/plugins communications, audio track playing, recording and editing. As a demonstrator, it shows the capabilities of the <software ContextAttributes="used">Web Audio API</software>, and how the <software ContextAttributes="used">Web Audio Modules</software> framework can be used to ease the development of complex web based audio applications with support for external plugins. Midi tracks support is still a work in progress <ref type="bibr" target="#b7">8</ref> and development is active. The source code can be found on a GitHub repository <ref type="bibr" target="#b8">9</ref> , and the application is available online <ref type="bibr" target="#b9">10</ref> .</p></div><figure xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Comparison table of the main online DAWs on the market</figDesc><graphic coords="4,77.82,83.69,192.20,305.30" type="bitmap" /></figure>
<figure xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: A track in the DAW GUI</figDesc></figure>
<figure xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: A plugin chain associated to the selected track</figDesc><graphic coords="5,53.80,495.58,240.23,126.36" type="bitmap" /></figure>
<figure xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: the WAM Pedalboard that manages plugin chains associated to DAW tracks</figDesc><graphic coords="6,53.80,430.89,240.24,131.59" type="bitmap" /></figure>
<figure xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Track recording process in WAM-Studio</figDesc><graphic coords="6,341.98,172.30,192.19,165.11" type="bitmap" /></figure>
			<note place="foot" n="1" xml:id="foot_0"><p>https://www.musicradar.com/news/tech/a-brief-history-of-computer-music-177299</p></note>
			<note place="foot" n="2" xml:id="foot_1"><p>https://gridsound.com/. See also the GridSound video presentation at ADC 2021 conference: https://www.youtube.com/watch?v=ejTtENwRxnA</p></note>
			<note place="foot" n="3" xml:id="foot_2"><p>https://github.com/naomiaro/waveform-playlist</p></note>
			<note place="foot" n="4" xml:id="foot_3"><p>https://audiomass.co/</p></note>
			<note place="foot" n="5" xml:id="foot_4"><p>In contrast with a standard AudioContext, an <software>OfflineAudioContext</software> doesn't render the audio to the device hardware; instead, it generates it, as fast as it can, and outputs the result to an <software ContextAttributes="used">AudioBuffer</software></p></note>
			<note place="foot" n="6" xml:id="foot_5"><p>See Paul Adenot blog post https://blog.paul.cx/post/audio-video-synchronizationwith-the-web-audio-api/</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Making a guitar rack plugin -WebAudio Modules 2.0</title>
		<author>
			<persName><forename type="first">Michel</forename><surname>Buffa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Kouyoumdjian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quentin</forename><surname>Beauchet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Forner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Marynowic</surname></persName>
		</author>
		<ptr target="https://hal.inria.fr/hal-03812948" />
	</analytic>
	<monogr>
		<title level="m">Web Audio Conference</title>
		<meeting><address><addrLine>Cannes, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Real time tube guitar amplifier simulation using WebAudio</title>
		<author>
			<persName><forename type="first">Michel</forename><surname>Buffa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jerome</forename><surname>Lebrun</surname></persName>
		</author>
		<ptr target="https://hal.univ-cotedazur.fr/hal-01589229" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Web Audio Conference (Queen Mary Research Online (QMRO) repository</title>
		<meeting>the Web Audio Conference (Queen Mary Research Online (QMRO) repository<address><addrLine>London, London, United Kingdom</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
		<respStmt>
			<orgName>Mary University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Towards an open Web Audio plug-in standard</title>
		<author>
			<persName><forename type="first">Michel</forename><surname>Buffa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jerome</forename><surname>Lebrun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jari</forename><surname>Kleimola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oliver</forename><surname>Larkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephane</forename><surname>Letz</surname></persName>
		</author>
		<idno type="DOI">10.1145/3184558.3188737</idno>
		<ptr target="https://doi.org/10.1145/3184558.3188737" />
	</analytic>
	<monogr>
		<title level="m">WWW2018 -TheWebConf 2018 : The Web Conference, 27th International World Wide Web Conference</title>
		<meeting><address><addrLine>Lyon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Emerging W3C APIs opened up commercial opportunities for computer music applications</title>
		<author>
			<persName><forename type="first">Michel</forename><surname>Buffa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jerome</forename><surname>Lebrun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shihong</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stéphane</forename><surname>Letz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Orlarey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Romain</forename><surname>Michon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominique</forename><surname>Fober</surname></persName>
		</author>
		<idno type="DOI">10.13140/RG.2.2.16456.19202</idno>
		<ptr target="https://doi.org/10.13140/RG.2.2.16456.19202" />
	</analytic>
	<monogr>
		<title level="m">The Web Conference 2020 -DevTrack</title>
		<meeting><address><addrLine>Taipei, Taiwan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Web Audio Modules 2.0: An Open Web Audio Plugin Standard</title>
		<author>
			<persName><forename type="first">Michel</forename><surname>Buffa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shihong</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Owen</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jari</forename><surname>Kleimola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oliver</forename><surname>Larkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Burns</surname></persName>
		</author>
		<idno type="DOI">10.1145/3487553.3524225</idno>
		<ptr target="https://doi.org/10.1145/3487553.3524225" />
	</analytic>
	<monogr>
		<title level="m">WWW '22: The ACM Web Conference 2022</title>
		<meeting><address><addrLine>Virtual Event, France</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="364" to="369" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Audioworklet: the Future of Web Audio</title>
		<author>
			<persName><forename type="first">Hongchan</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Computer Music Conference</title>
		<meeting>the International Computer Music Conference<address><addrLine>Daegu, South Korea</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="110" to="116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">jMax: an environment for real-time musical applications</title>
		<author>
			<persName><forename type="first">François</forename><surname>Déchelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Riccardo</forename><surname>Borghesi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maurizio</forename><surname>De Cecco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Enzo</forename><surname>Maggi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Butch</forename><surname>Rovan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Norbert</forename><surname>Schnell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Music Journal</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="50" to="58" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Web Audio Modules</title>
		<author>
			<persName><forename type="first">Jari</forename><surname>Kleimola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oliver</forename><surname>Larkin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sound and Music Computing Conference</title>
		<meeting>the Sound and Music Computing Conference</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Unrevealed Potential in Delivering Distance Courses: the Instructional Value of Audio</title>
		<author>
			<persName><forename type="first">Bojan</forename><surname>Lazarevic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danijela</forename><surname>Scepanovic</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010. 2010</date>
			<publisher>eLearning &amp; Software for Education</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Soundtrap: A collaborative music studio with Web Audio</title>
		<author>
			<persName><forename type="first">Fredrik</forename><surname>Lind</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Macpherson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Web Audio Conference</title>
		<meeting>the Web Audio Conference<address><addrLine>London, London, United Kingdom</addrLine></address></meeting>
		<imprint>
			<publisher>Queen Mary University</publisher>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Rethinking the computer music language: Super collider</title>
		<author>
			<persName><forename type="first">James</forename><surname>Mccartney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Music Journal</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="61" to="68" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">FTS: A real-time monitor for multiprocessor music synthesis</title>
		<author>
			<persName><surname>Miller Puckette</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer music journal</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="58" to="67" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>