<TEI xmlns="http://www.tei-c.org/ns/1.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xml:space="preserve" xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Research Proposal: Analyzing and Understanding Embodied Interactions in Virtual Reality Systems</title>
				<funder ref="#_cypwex5 #_CUDmXMS">
					<orgName type="full">unknown</orgName>
				</funder>
				<funder ref="#_5ht8Gkv">
					<orgName type="full">French National Research Agency</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher />
				<availability status="unknown"><licence /></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Florent</forename><surname>Robert</surname></persName>
							<email>florent.robert@inria.fr</email>
						</author>
						<author>
							<persName><forename type="first">Marco</forename><surname>Winckler</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Hui-Yin</forename><surname>Wu</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="laboratory">Centre Inria d' CÃ´te d'Azur</orgName>
								<orgName type="institution" key="instit1">UniversitÃ©</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<orgName type="institution" key="instit3">I3S</orgName>
								<address>
									<settlement>Sophia-Antipolis</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">UniversitÃ© CÃ´te d'Azur</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<orgName type="institution" key="instit3">Inria</orgName>
								<address>
									<settlement>I3S Sophia-Antipolis</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution" key="instit1">UniversitÃ© CÃ´te d'Azur</orgName>
								<orgName type="institution" key="instit2">Inria Sophia-Antipolis</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution" key="instit1">Lucile Sassatelli UniversitÃ© CÃ´te d'Azur</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<orgName type="institution" key="instit3">I3S</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">Institut Universitaire de France Sophia-Antipolis</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Research Proposal: Analyzing and Understanding Embodied Interactions in Virtual Reality Systems</title>
					</analytic>
					<monogr>
						<imprint>
							<date />
						</imprint>
					</monogr>
					<idno type="MD5">C951C732FCE5DB70E2AFA2974901CCAF</idno>
					<idno type="DOI">10.1145/3524273.3533928</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-04-12T14:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid" />
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Embodied experiences</term>
					<term>interactive task modeling</term>
					<term>virtual reality</term>
					<term>user experience analysis</term>
				</keywords>
			</textClass>
			<abstract>
<div><p>Virtual reality (VR) offers opportunities in human-computer interaction research, to embody users in immersive environments and observe how they interact with 3D scenarios under well-controlled environments. VR content has stronger influences on users physical and emotional states as compared to traditional 2D media, however, a fuller understanding of this kind of embodied interaction is currently limited by the extent to which attention and behavior can be observed in a VR environment, and the accuracy at which these observations can be interpreted as, and mapped to, real-world interactions and intentions. This thesis aims at the creation of a system to help designers in the analysis of the entire user experience in VR environment: how they feel, what is their intentions when interacting with a certain object, provide them guidance based on their needs and attention. A controlled environment in which the user is guided will help to establish a better intersubjectivity between designer intention who created the experience and users who lived it and will lead to a more efficient analysis of the user behavior in VR systems for the design of better experiences.</p></div>
<div><head>CCS CONCEPTS</head><p>â€¢ Human-centered computing â†’ Systems and tools for interaction design; â€¢ Computing methodologies â†’ Virtual reality.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div><p>Figure <ref type="figure">1</ref>: According to Dourish <ref type="bibr" target="#b2">[3]</ref>, in the creation of interactive embodied experiences, gaps of perception between users and designers are introduced in ontology, intersubjectivity and intentionality, corresponding respectively to the understanding of the environment, the goals, and the users experiences by the designers.</p></div>
<div><head n="1">INTRODUCTION</head><p>Virtual reality usage is growing, increasingly used in various professional fields for education and training. It allows a high level of control of the VR environment and therefore effective monitoring of the users throughout their experience. The designers in charge of the environment must be given the necessary tools for them to create the experience, observe, and analyze to follow up with guidance adapted to the users needs and what they focus their attention on during the exploration. The intentions of the user do not always match what designers expected when they created the experience, and thus communicating the intentions between the two parties is a complicated task, especially with all the possibilities of freely exploring the VR environment and non-linear course of progression offered by VR, increasing the gap in perception.</p><p>Inspired by Dourish's work on embodiment theory <ref type="bibr" target="#b2">[3]</ref>, we identify three important components to reduce this gap in perception of the immersive environment experience between designers who created it and user who experience it:</p><p>â€¢ Ontology: building semantically understandable 3D scenarios in which the user's purposeful interactions (e.g., navigating to a location, picking up an object) can be understood without being predefined, â€¢ Intersubjectivity: designing real-time visualization and control systems that can establish shared understanding for user tasks and ongoing experience between multiple actors (e.g., between the designer and user or between multiple users), and â€¢ Intentionality: designing computational methods for the analysis and identification of users' actions and their purpose of enacting an effect on the world (e.g., taking a key in order to open a door enables navigation to a previously inaccessible space).</p><p>Gaps in perception between user, designer, and system can occur at all of these levels as shown in Figure <ref type="figure">1</ref>.</p><p>This thesis targets the investigation and design of a unified workflow enabling the understanding and analysis of user experience on these different levels of meaning in XR. For this a system will be created for the design of serious games in virtual reality, GUsT-3D (Guided Users Tasks, a tool aimed at creating interactive tasks in a VR environment). This tool allows the definition of a domain specific language (DSL) used to build an ontology to describe a VR environment as an interactive environment. Once the DSL has been defined, the tool allows the designer to create playable game scenarios and provide guidance to the users depending on their needs. The guidance also leads to a better intersubjectivity between the designer who created the experience and the users playing it by controlling the experience and orienting the user in the direction intended by the designer. Finally, multiple metrics about the users are recorded during their experience, used to analyze what the users interacted with during their experiences and how they felt in order to understand their intentionality.</p></div>
<div><head n="2">RESEARCH QUESTIONS</head><p>The analysis of embodied interactions in XR raises research challenges both theoretical and technical that have been only partially explored in specific domains of usage. We identify three main research questions that are yet to be addressed to improve the XR experience and design a workflow enabling the understanding and analysis of users experience:</p><p>â€¢ RQ1 : What are the qualitative measures characterizing user perception and interactions in XR ?</p><p>â€¢ RQ2 : How can these measures be quantified such that we can observe and understand them?</p><p>â€¢ RQ3 : How can designers use this information for the creation of more efficient, guided and meaningful experiences? A good understanding of the user experience can help in the creation of efficient guided scenarios and for a better control of the experience. This control allows designers to create experiences more aligned with their original intentions, helping them to develop scenarios focused on specific context.</p></div>
<div><head n="3">RELATED WORK</head><p>Here I present the selected findings of my review of the work published in the last five years in IEEEVR and SIGCHI using keywords virtual reality, embodiment, training simulation, and user experience analysis. 75 papers were selected for in-depth review. Around 30 papers were measuring and quantifying user perception and interaction in VR to understand user experience as done by Hawes et al. <ref type="bibr" target="#b3">[4]</ref> who investigated VR as a way to reduce anxiety of students, improve their mental health, using questionnaires to measure the evolution of their anxiety. Pfeuffer et al. <ref type="bibr" target="#b7">[8]</ref> created a workflow using motion tracking to analyze behaviors in VR while doing different type of tasks (i.e., pointing, grabbing, walking, and typing). Hochreiter et al. <ref type="bibr" target="#b4">[5]</ref> designed an exploration scenario in VR environments with and without mismatch between the real and VR environments, showing that users were less confident in mismatched cases while moving around, taking smaller steps and with elevated skin conductance responses. Existing works that measure user perception and interaction in VR uses multiple methods to analyse the experience: questionnaires, motion capture, eye tracking, and physiological sensors such as skin conductance and heart rate.</p><p>Designers can use these measures to understand the user embodied experience and guide it. A system providing these analysis would be valuable for research on the creation of training systems such as that of Clifford et al. <ref type="bibr" target="#b1">[2]</ref> who designed a system to build a stressful context in VR in order to train firefighters by providing a context close to a real firefighting situation. Questionnaires and heart rate were used to quantify the level of stress felt by the user. A deeper analysis of the feelings and the level of stress felt in real time in the VR environment could help the designer know how to influence user experience which can in turn be used to improve training systems. Hurd et al. <ref type="bibr" target="#b5">[6]</ref> and Wu et al. <ref type="bibr" target="#b9">[10]</ref> respectively created a VR system designed to train users with amblyopia disability and to train users to perform pin pong spin technique. An analysis of the attention during the training could let the designer know precisely what are the weak points and difficulties of the user to provide them with more personalized training as in Lang et al. <ref type="bibr" target="#b6">[7]</ref> where the system learns the driving habits of the user in order to create a personalized training program. As amblyopia training is usually long and repetitive, an analysis of the experience could help detect exactly what motivates the user and use it to make the experience more entertaining. In the paper of Hochreiter et al. <ref type="bibr" target="#b4">[5]</ref> mentioned previously studying the impact of mismatching VR environments on arousal, tests are made in a free exploration environment. A well-controlled environment would help the designer make sure no other factors external to the experiment interfere on the user behavior and the subsequent analysis.</p><p>There are very few works on the modelization of the lived experience in VR. Bouville et al. <ref type="bibr" target="#b0">[1]</ref> built the system #FIVE on the modelization of interactive VR environments by the annotation of elements composing the environment with properties, similar to GUsT-3D ontology part presented in part 4 however without the creation of a DSL. <ref type="bibr" target="#b8">[9]</ref> worked on a prototyping workflow for AR applications, immersing the designer in the modelized experience with a miniature physical representation of the AR experience. By showing the designer a visualization of how the user will perceive and live the experience, this work helps improve the intersubjectivity between the designer creating the experience and the user living it. Xue et al. <ref type="bibr" target="#b10">[11]</ref> created a workflow to analyze user physical and emotional behavior over time using physiological sensors, eye tracking, and questionnaires. Answering relatively similar research questions as ours on the analysis of user lived experience immersed in a VR context. This work, however, analyzes user experience watching 360Â°videos, where the exploration of the environment is more linear than that in VR game, since the users can only turn on the spot, making the investigation relatively different as the perception gap problem become easier to handle.</p><p>My next readings will be focused on SIGCHI and EICS conferences, on the thematic of ontology and intersubjectivity in order to find efficient ways to help the designer transmit their vision of the experience through the VR environment to the user such as Zhao et al. <ref type="bibr" target="#b11">[12]</ref>, who show efficient ways to attract the attention of the user on certain elements in order to help the user or orient their experience in the direction desired by the designer.</p></div>
<div><head n="4">METHODOLOGY AND PRELIMINARY RESULTS</head><p>We created the <software ContextAttributes="used">GUsT-3D</software> system to seek answers to the research questions. Divided into three components inspired from the three senses of embodiment from Dourish's work <ref type="bibr" target="#b2">[3]</ref> as shown in Figure <ref type="figure" target="#fig_0">2</ref>. It is composed of an ontology component for the creation of an interactive environment understandable by both the user and the designer, an intersubjectivity component allowing the designer to control and guide the user experience in the VR environments and an intentionality component recording multiple elements about the user and the environment during the scenario and providing visualization about the users logs to the designer to help them with the analysis of the user embodied experience. The <software ContextAttributes="used">GUsT-3D</software> system is developed on Unity using the C# language.</p><p>The ontology component intervenes in the design of an interactive scenario, defining the way each object composing the environment can interact with other objects of the environment and with the user. The DSL establishes an important basis of understanding in the experience. An important gap in perception could be created here if the properties or possible interactions are not properly defined and understood. This part allows the modification of a domain specific language (DSL) vocabulary to fit designer's needs. The DSL is composed of three elements : the environment properties (e.g., time and distance units, parameters regarding interactive and affordance properties), user properties (e.g., height of the body, parameters such as field of view and keybinding) and the layers (i.e., type of objects understood by the vocabulary of the DSL). The layers are separated in four categories :</p><p>â€¢ interactive layers describe objects including interaction properties with other objects of the environment and with the user. For example, a ğ‘“ ğ‘œğ‘Ÿğ‘˜ is a ğ‘šğ‘œğ‘£ğ‘ğ‘ğ‘™ğ‘’ object the user can move while a ğ‘¡ğ‘ğ‘ğ‘™ğ‘’ is a ğ‘ ğ‘¢ğ‘ğ‘ğ‘œğ‘Ÿğ‘¡ object on which others objects can be placed. â€¢ navigation layers describe objects structuring space where users progress, defining the navigation possibilities and constraints in the VR environment. For example, a ğ‘”ğ‘Ÿğ‘œğ‘¢ğ‘›ğ‘‘ defines a localisation while a ğ‘‘ğ‘œğ‘œğ‘Ÿ is an ğ‘’ğ‘›ğ‘¡ğ‘Ÿğ‘¦ğ‘¤ğ‘ğ‘¦ used to create a link between two different localisations in space such as ğ¾ğ‘–ğ‘¡ğ‘â„ğ‘’ğ‘› &lt;=&gt; ğµğ‘’ğ‘‘ğ‘Ÿğ‘œğ‘œğ‘š. â€¢ environment layers are assigned to objects with special properties composing the environment, like a ğ‘ğ‘ğ‘šğ‘’ğ‘Ÿğ‘ with it's own field of view or a source of ğ‘™ğ‘–ğ‘”â„ğ‘¡. â€¢ object layers are objects added by the designer using layers from other categories as sub layers to extend the vocabulary depending on their needs. The designer can for example create a ğ‘ â„ğ‘’ğ‘™ ğ‘“ , composed of two sub layers from the interactive categories, ğ‘ ğ‘¢ğ‘ğ‘ğ‘œğ‘Ÿğ‘¡ and ğ‘ğ‘œğ‘›ğ‘¡ğ‘ğ‘–ğ‘›ğ‘’ğ‘Ÿ , meaning that a shelf can have ğ‘ ğ‘¢ğ‘ğ‘ğ‘œğ‘Ÿğ‘¡ğ‘–ğ‘›ğ‘” and ğ‘ğ‘œğ‘›ğ‘¡ğ‘ğ‘–ğ‘›ğ‘–ğ‘›ğ‘” interactions with objects composing the environment.</p><p>Once the vocabulary needed by the designers has been defined, it will be used to annotate objects composing the VR environment with layers to describe it as an interactive environment, similarly to the system #FIVE <ref type="bibr" target="#b0">[1]</ref>. This allows us to create an ontology to make a semantically understandable 3D interactive environment for both the user and the designer. This ontology is then used for both remaining parts of the system.</p><p>The intersubjectivity component acts in creation of tasks and the guidance of the user, in order to orient their experience, allowing the designer to have a better perception of the user experience and how to help them properly. The ontology of the annotated environment is used for the design of game scenarios. A scenario is a list of tasks, each task with one objective the user will have to achieve. A short scenario could be composed of two tasks :</p><p>(1) ğ‘Ÿğ‘’ğ‘¡ğ‘Ÿğ‘–ğ‘’ğ‘£ğ‘’ a ğ‘“ ğ‘œğ‘Ÿğ‘˜ placed on a ğ‘¡ğ‘ğ‘ğ‘™ğ‘’ (ğ‘ ğ‘¢ğ‘ğ‘ğ‘œğ‘Ÿğ‘¡ object) in the kitchen, (2) ğ‘ ğ‘¡ğ‘œğ‘Ÿğ‘’ the ğ‘“ ğ‘œğ‘Ÿğ‘˜ in a ğ‘ â„ğ‘’ğ‘™ ğ‘“ (ğ‘ğ‘œğ‘›ğ‘¡ğ‘ğ‘–ğ‘›ğ‘’ğ‘Ÿ object) in the kitchen.</p><p>The system is composed of a query language to provide help for each task to the user depending on their need and attention. Queries are structured as follows : [How many/where is]+[an object/a layer]+[a constraint (e.g., on an object, at a localisation)]+[(the object/the layer X)] It can be used to find precise elements needed by the the user in the scenario above like this : Where is + fork + on a + table + in the localisation + kitchen The designer could decide then to provide different guidance to help the users after certain conditions if they couldn't find the fork by attracting their attention on the right direction using methodologies presented in the paper <ref type="bibr" target="#b11">[12]</ref> such as guideline or highlight.</p><p>The intentionality component is used by the designer to analyse the user experience, to support designers on the understanding of user reaction and interaction during playtime. Logs are recorded over time during the experience on multiple metrics, regarding the environment (e.g., where they are, what are they interacting with, gaze, motion) and how they felt (e.g., skin conductance, heart rate) to analyze their emotions and intentions. This part provides some statistics and correlation analysis on this data in order to provide meaningful information to the designers by making the link between the observed experience (what they did) and the observed feelings (what they felt). The designer can for example observe that at a precise moment during the experience, the user saw a spider, and at the same moment they observe a spike in skin conductance, rise in heart rate, or longer fixation, potentially meaning that the user was surprised by a spider. This allows designers to fully understand how the user lived the experience. Visualisation methods are created such as graphs or heat-maps to provide efficient ways to communicate relevant information to the designer. Thus, this system allows creation of an ontology understandable by both the designer and the user with a fully annotated environment with clear properties and interactions possibilities. It create a strong intersubjectivity by allowing the designer to build a guided scenario and orient the user in the VR environment to reduce the possible gap of perception of the experience between them. Finally, the intentionality part provides the designer an in-depth analysis tool with visualisation to understand the embodied experience of the user during the scenario and use these informations as feedback to improve the experience and provide better guidance. In it's current state, <software>GUsT-3D</software> allows the creation of an ontology with layers and environment properties, annotation of objects composing the VR environment and the creation of scenarios : a list of tasks to achieve in a precise order with goals such as going somewhere, taking something, etc. Guidance of the user is based on the time spent on a task. Records of the user experience includes interactions of the users with the VR environment (where they are, what object they interact with, what they are looking at, etc.).</p></div>
<div><head n="5">ONGOING ACTIVITIES</head><p>To assess the effectiveness of the tool in it's current state, we conducted user tests with several designers with experience in creating 3D environments. The purpose of this study was to improve GUsT-3D by collecting feedback from potential designers who would be using the system. For this, we carried out individual tests in the form of interviews on the creation of a serious game in an indoor environment (a house composed of three rooms). We recruited 6 expert users (3 men 3 women). The average number of years of experience of participants in the development of immersive 3D applications is 3 years (min 1, max 7). Regarding experience with tools available on the market, the 6 participants declared having experience with Unity and some have experience with other software such as <software>Blender</software> and <software ContextAttributes="used">Unreal Engine</software>. The interview was composed of three stages: presentation of the study, demographic questions, then a simulation with and without our tool in order to evaluate several elements of it. Due to the sanitary situation, the interviews were carried out by video conference, therefore the users did not interact directly with the system. First, we provided the participant with an environment consisting of 3 rooms: a kitchen, a bedroom, and a garage. Participants were then asked to rate the perceived ease of use of the tool to complete the following goals:</p><p>â€¢ Environment Annotation: adding and annotating an object to an existing environment in order to add interactive properties to it. â€¢ Create an interactive scenario: defining the tasks that the user must carry out in the scenario (i.e., go to the bedroom, take the lamp placed on the desk, go in the kitchen, place the lamp on the table). â€¢ Analysis of the user experience: questioning and visualizing the recordings created during the scenario in order to observe and analyze the user experience.</p><p>Overall, participants found that the tools made the workflow more efficient and responses were positive. Participants with more experience in 3D development were more critical of the flexibility of the tools. They indicated that depending on the scenario and the environment they were trying to create, they could potentially be restricted by the limitations imposed by the tool. We also observed that the most difficult step, the creation and management of a game scenario presented a steeper learning curve, but with notable efficiency. These results have been presented at the conference JFIG 2021 (Les JournÃ©es FranÃ§aises de l'Informatique Graphique), and has been accepted for publication in PACM HCI and will be presented at ACM EICS 2022.</p></div>
<div><head n="6">CONCLUSION AND FUTURE WORK</head><p>This doctoral thesis aims at the improvement of VR experience creation, analysis and understanding by providing a workflow inspired by Dourish's work <ref type="bibr" target="#b2">[3]</ref>. This workflow would help research in multiple domains where the understanding of the user behavior is essential (e.g., IT, health, neurosciences). The next steps on GUsT-3D will be focused on the extension of the DSL in order to create more complex game scenarios with help provided focused on specific metrics like attention of the user with eye-tracking, for example when crossing a road, to make sure the user saw the traffic light beforehand. Records of physiological sensor data will be added to the recorded logs and analyzed by the system in relation with the other data in order to make a link between what the users did in the environment and how they felt during the scenario to provide a snapshot of the embodied experience felt by the user over the time to the designer in the form of: at the second 22 of the experience, the user was at the sight of a pink flower. A user experiment will be conducted using the more advanced version of GUsT-3D to test its efficiency, using physiological sensors (skin conductance, heart rate) and motion records (head motion, eye-tracking) to validate our first hypothesis and to see what metrics are shown to be effective in reducing the gap of perception between a user and a designer.</p></div><figure xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Implementation of the three senses of embodiment in GUsT-3D</figDesc><graphic coords="4,73.04,83.69,201.75,90.71" type="bitmap" /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGMENTS</head><p>This work has been partially supported by the <rs type="funder">French National Research Agency</rs> through the <rs type="projectName">ANR</rs> <rs type="projectName">CREATTIVE3D</rs> project <rs type="grantNumber">ANR-21-CE33-00001</rs> and the <rs type="projectName">EUR DS4H Investments in the Future</rs> projects <rs type="grantNumber">ANR-17-EURE-0004</rs>. Thesis started in October 2021.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_5ht8Gkv">
					<orgName type="project" subtype="full">ANR</orgName>
				</org>
				<org type="funded-project" xml:id="_cypwex5">
					<idno type="grant-number">ANR-21-CE33-00001</idno>
					<orgName type="project" subtype="full">CREATTIVE3D</orgName>
				</org>
				<org type="funded-project" xml:id="_CUDmXMS">
					<idno type="grant-number">ANR-17-EURE-0004</idno>
					<orgName type="project" subtype="full">EUR DS4H Investments in the Future</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">#FIVE : High-level components for developing collaborative and interactive virtual environments</title>
		<author>
			<persName><forename type="first">Rozenn</forename><surname>Bouville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Valerie</forename><surname>Gouranton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Boggini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florian</forename><surname>Nouviale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bruno</forename><surname>Arnaldi</surname></persName>
		</author>
		<idno type="DOI">10.1109/SEARIS.2015.7854099</idno>
		<ptr target="https://doi.org/10.1109/SEARIS.2015.7854099" />
	</analytic>
	<monogr>
		<title level="m">2015 IEEE 8th Workshop on Software Engineering and Architectures for Realtime Interactive Systems (SEARIS)</title>
		<meeting><address><addrLine>Arles, France</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="33" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Creating a Stressful Decision Making Environment for Aerial Firefighter Training in Virtual Reality</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Rory</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sungchul</forename><surname>Clifford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Hoermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">W</forename><surname>Billinghurst</surname></persName>
		</author>
		<author>
			<persName><surname>Lindeman</surname></persName>
		</author>
		<idno type="DOI">10.1109/VR.2019.8797889</idno>
		<ptr target="https://doi.org/10.1109/VR.2019.8797889ISSN:2642-5254" />
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Virtual Reality and 3D User Interfaces (VR)</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page" from="181" to="189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">Paul</forename><surname>Dourish</surname></persName>
		</author>
		<title level="m">Where the action is: the foundations of embodied interaction</title>
		<meeting><address><addrLine>USA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT press</publisher>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">VR-based Student Priming to Reduce Anxiety and Increase Cognitive Bandwidth</title>
		<author>
			<persName><forename type="first">Dan</forename><surname>Hawes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Arya</surname></persName>
		</author>
		<idno type="DOI">10.1109/VR50410.2021.00046</idno>
		<ptr target="https://doi.org/10.1109/VR50410.2021.00046ISSN:2642-5254" />
	</analytic>
	<monogr>
		<title level="m">IEEE Virtual Reality and 3D User Interfaces (VR)</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
			<biblScope unit="page" from="245" to="254" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Cognitive and Touch Performance Effects of Mismatched 3D Physical and Visual Perceptions</title>
		<author>
			<persName><forename type="first">Jason</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Salam</forename><surname>Daher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gerd</forename><surname>Bruder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Welch</surname></persName>
		</author>
		<idno type="DOI">10.1109/VR.2018.8446574</idno>
		<ptr target="https://doi.org/10.1109/VR.2018.8446574" />
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Virtual Reality and 3D User Interfaces (VR)</title>
		<meeting><address><addrLine>Reutlingen</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page" from="1" to="386" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Virtual Reality Video Game Paired with Physical Monocular Blurring as Accessible Therapy for Amblyopia</title>
		<author>
			<persName><forename type="first">Ocean</forename><surname>Hurd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sri</forename><surname>Kurniawan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mircea</forename><surname>Teodorescu</surname></persName>
		</author>
		<idno type="DOI">10.1109/VR.2019.8797997</idno>
		<ptr target="https://doi.org/10.1109/VR.2019.8797997ISSN:2642-5254" />
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Virtual Reality and 3D User Interfaces (VR)</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page" from="492" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Synthesizing Personalized Training Programs for Improving Driving Habits via Virtual Reality</title>
		<author>
			<persName><forename type="first">Yining</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yibiao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lap-Fai</forename><surname>Yu</surname></persName>
		</author>
		<idno type="DOI">10.1109/VR.2018.8448290</idno>
		<ptr target="https://doi.org/10.1109/VR.2018.8448290" />
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Virtual Reality and 3D User Interfaces (VR)</title>
		<meeting><address><addrLine>Reutlingen</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page" from="297" to="304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Behavioural Biometrics in VR: Identifying People from Body Motion and Relations in Virtual Reality</title>
		<author>
			<persName><forename type="first">Ken</forename><surname>Pfeuffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><forename type="middle">J</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarah</forename><surname>Prange</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukas</forename><surname>Mecke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Buschek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florian</forename><surname>Alt</surname></persName>
		</author>
		<idno type="DOI">10.1145/3290605.3300340</idno>
		<ptr target="https://doi.org/10.1145/3290605.3300340" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems (CHI '19)</title>
		<meeting>the 2019 CHI Conference on Human Factors in Computing Systems (CHI '19)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Designers, the Stage Is Yours! Medium-Fidelity Prototyping of Augmented &amp; Virtual Reality Interfaces with 360theater</title>
		<author>
			<persName><forename type="first">Maximilian</forename><surname>Speicher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katy</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Nebeling</surname></persName>
		</author>
		<idno type="DOI">10.1145/3461727</idno>
		<ptr target="https://doi.org/10.1145/3461727" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM on Human-Computer Interaction</title>
		<meeting>the ACM on Human-Computer Interaction</meeting>
		<imprint>
			<date type="published" when="2021-06">2021. June 2021</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="1" to="205" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">SPin-Pong -Virtual Reality Table Tennis Skill Acquisition using Visual, Haptic and Temporal Cues</title>
		<author>
			<persName><forename type="first">Erwin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mitski</forename><surname>Piekenbrock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Takuto</forename><surname>Nakumura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hideki</forename><surname>Koike</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2021.3067761</idno>
		<ptr target="https://doi.org/10.1109/TVCG.2021.3067761" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2566" to="2576" />
			<date type="published" when="2021-05">2021. May 2021</date>
		</imprint>
	</monogr>
	<note>IEEE Transactions on Visualization and Computer Graphics</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">CEAP-360VR: A Continuous Physiological and Behavioral Emotion Annotation Dataset for 360 VR Videos</title>
		<author>
			<persName><forename type="first">Abdallah</forename><forename type="middle">El</forename><surname>Tong Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyi</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gangyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pablo</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><surname>Cesar</surname></persName>
		</author>
		<idno type="DOI">10.1109/TMM.2021.3124080</idno>
		<ptr target="https://doi.org/10.1109/TMM.2021.3124080" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">SeeingVR: A Set of Tools to Make Virtual Reality More Accessible to People with Low Vision</title>
		<author>
			<persName><forename type="first">Yuhang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Cutrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Holz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meredith</forename><forename type="middle">Ringel</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eyal</forename><surname>Ofek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">D</forename><surname>Wilson</surname></persName>
		</author>
		<idno type="DOI">10.1145/3290605.3300341</idno>
		<ptr target="https://doi.org/10.1145/3290605.3300341" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems</title>
		<meeting>the 2019 CHI Conference on Human Factors in Computing Systems<address><addrLine>Glasgow Scotland Uk</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>