<TEI xmlns="http://www.tei-c.org/ns/1.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xml:space="preserve" xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Massively Distributed Clustering via Dirichlet Process Mixture</title>
			</titleStmt>
			<publicationStmt>
				<publisher />
				<availability status="unknown"><licence /></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Khadidja</forename><surname>Meguelati</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">LIRMM</orgName>
								<orgName type="institution" key="instit1">Inria</orgName>
								<orgName type="institution" key="instit2">University of Montpellier</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Benedicte</forename><surname>Fontez</surname></persName>
							<email>benedicte.fontez@supagro.fr</email>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">MISTEA</orgName>
								<orgName type="department" key="dep2">Montpellier SupAgro</orgName>
								<orgName type="institution" key="instit1">Univ. Montpellier</orgName>
								<orgName type="institution" key="instit2">INRAE</orgName>
								<address>
									<settlement>Montpellier</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Nadine</forename><surname>Hilgert</surname></persName>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">MISTEA</orgName>
								<orgName type="department" key="dep2">Montpellier SupAgro</orgName>
								<orgName type="institution" key="instit1">Univ. Montpellier</orgName>
								<orgName type="institution" key="instit2">INRAE</orgName>
								<address>
									<settlement>Montpellier</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Florent</forename><surname>Masseglia</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">LIRMM</orgName>
								<orgName type="institution" key="instit1">Inria</orgName>
								<orgName type="institution" key="instit2">University of Montpellier</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Isabelle</forename><surname>Sanchez</surname></persName>
							<email>isabelle.sanchez@inrae.fr</email>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">MISTEA</orgName>
								<orgName type="department" key="dep2">Montpellier SupAgro</orgName>
								<orgName type="institution" key="instit1">Univ. Montpellier</orgName>
								<orgName type="institution" key="instit2">INRAE</orgName>
								<address>
									<settlement>Montpellier</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Massively Distributed Clustering via Dirichlet Process Mixture</title>
					</analytic>
					<monogr>
						<imprint>
							<date />
						</imprint>
					</monogr>
					<idno type="MD5">2FCA3B0F12830A8CB83C5ECDD7F0692E</idno>
					<idno type="DOI">10.1007/978-3-030-67670-4_34</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-04-12T14:50+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid" />
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Gaussian random process</term>
					<term>Dirichlet Process Mixture Model</term>
					<term>Clustering</term>
					<term>Parallelism</term>
					<term>Reproducing Kernel Hilbert Space</term>
				</keywords>
			</textClass>
			<abstract>
<div><p>Dirichlet Process Mixture (DPM) is a model used for multivariate clustering with the advantage of discovering the number of clusters automatically and offering favorable characteristics, but with prohibitive response times, which makes centralized DPM approaches inefficient. We propose a demonstration of two parallel clustering solutions : i) DC-DPM that gracefully scales to millions of data points while remaining DPM compliant, which is the challenge of distributing this process, ii) HD4C that addresses the curse of dimensionality by performing a distributed DPM clustering of high dimensional data such as time series or hyperspectral data.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div><head n="1">Introduction</head><p>Clustering is the task of grouping similar data into the same cluster and separating dissimilar data in different clusters. It is a data mining technique intensively used for data analytics, with many applications. Clustering may be used for identification in the new challenge of digital agriculture or high throughput plant phenotyping, as illustrated in the demonstration. One of the main difficulties, for clustering, is the fact that we don't know, in advance, the number of clusters to be discovered. The Dirichlet Process Mixture (DPM) approach allows estimating that number and assigning observations to clusters, in the same process <ref type="bibr" target="#b0">[1]</ref>. However, DPM is highly time consuming. Consequently, several attempts have been done to make it distributed but the resulting approaches usually suffer from convergence issues (imbalanced data distribution on computing nodes) <ref type="bibr" target="#b3">[4]</ref> or do not fully benefit from DPM properties <ref type="bibr" target="#b2">[3]</ref>. Furthermore, making DPM parallel is not straightforward since it must compare each record to the set of existing clusters, a highly repeated number of times. That impairs the global performances of the approach in parallel, since comparing all the records to all the clusters would call for a high number of communications and make the process impracticable.</p><p>In this demonstration, we show how our solutions combine the advantages of DPM (clustering quality) and of distributed computing (fast response time on large datasets).</p><p>The challenge is to build a consistent view of the clusters, despite a necessary split discovery mechanism. Obviously, such a mechanism need to guarantee low amounts of communications. This is one of the keys in distributed data science systems supporting algorithms often originally designed for centralized environments. Our solutions rely on sufficient statistics, by means of synchronization between machines in terms of proposed clusters at each step <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>. We provide an interactive demonstration of these previous results. This demonstration allows the user to check the results on various datasets and to evaluate the impact of parameter choices.</p><p>2 Distributed Clustering via Dirichlet Process Mixture DC-DPM <ref type="bibr" target="#b0">[1]</ref> presents a parallel clustering solution that takes advantage of the computing power of distributed systems by using parallel frameworks such as <software ContextAttributes="created">Spark</software>.</p><p>The main novelty of DC-DPM is to propose a model and its estimation at the master level by exploiting the sufficient statistics from the workers, in a DPM compliant approach. The challenge of using sufficient statistics, in a distributed environment, is to remain in the DPM approach at all steps, including the synchronization between the worker and master nodes. Our approach approximates the DPM model even at the master level when local data is replaced by sufficient statistics between iterations.</p><p>In our work, the distributed DPM allows each node to have a view on the local results of all the other nodes, while avoiding exhaustive data exchanges. This is made by adding weights which represent proportions of observations from all clusters evaluated on the whole dataset. These weights are updated at the master level. The workflow of DC-DPM is illustrated in Figure <ref type="figure" target="#fig_0">1</ref>. It consists in identifying local clusters on the workers and synchronizing these clusters on the master. These clusters are then communicated as a basis among workers for local clustering consistency. By iterating this process we seek global consistency of DPM in a distributed environment and obtain DPM compliant results as shown in this demonstration and detailed in Section 5.</p><p>3 High Dimensional Data Distributed Dirichlet Clustering HD4C <ref type="bibr" target="#b1">[2]</ref> presents a parallel clustering approach adapted for high dimensional data. Actually, DC-DPM is a solution proposed to this issue when data is multivariate. This solution is based on a distributed DPM. In the case of high dimensional data or signals (infinite dimension), matrix computation is no more feasible (e.g., no inverse matrix, no matrix product). We need to replace a matrix product by an inner product in an adequate space of functions and to find the adequate measure. This inner product is mandatory to compute the likelihood and the posterior. To do that, HD4C <ref type="bibr" target="#b1">[2]</ref> uses the properties of the Reproducible Kernel Hilbert Spaces (RKHS). We assume that the random variable takes its values in a space of infinite dimension. Therefore, high dimensional data will be seen as trajectories of a random process. Our work focuses on Gaussian random process because of its ability to avoid simple parametric assumptions and integrate a lot of structure. For implementation, data are defined as an autocorrelated Gaussian process called Ornstein-Uhlenbeck</p></div>
<div><head n="4">DC-DPM and HD4C in Spark</head><p>We implemented both DC-DPM and HD4C in <software ContextAttributes="used">Spark</software> and made the <software ContextAttributes="used">code</software> available for download and test at https://github.com/khadidjaM. To the best of our knowledge, this is the first time that a complete parallel solution for high dimensional data clustering is demonstrated, showing significant performance improvement over the state of the art and existing centralized solutions. Figure <ref type="figure" target="#fig_1">2</ref> illustrates the basic architecture of our solution, which is written in <software ContextAttributes="used">Scala</software>. The main components of DC-DPM and HD4C are developed within <software ContextAttributes="used">Spark</software> and deployed on top of <software ContextAttributes="used">Hadoop Distributed File System (HDFS)</software> in order to efficiently read input data, as well as to store final results, and thus to overcome the bottleneck of centralized data storing.</p><p>The main feature of <software ContextAttributes="used">Spark</software> is its distributed memory abstraction, called Resilient Distributed Datasets (RDD) and parallel operations used to handle it. RDD supports in-memory processing computation. As shown in figure <ref type="figure" target="#fig_1">2</ref>, the intermediate results are stored in a distributed memory instead of disk storage and make the system faster. Two examples from digital agriculture were used in this demonstration: i) the herd monitoring where animal activity is monitored using a collar-mounted accelerometer (high dimensional dataset), ii) clustering of image pixels to define plant labels as pre-treatment for plant phenotyping (multivariate dataset). Synthetic data were generated using a two-steps principle. First, we generated cluster centers according to a multivariate normal distribution (multivariate data) or according to some polynomials (high dimensional data). Then, for each center, we generated data by using a multivariate normal distribution or a gaussian process.</p><p>The last dataset corresponds to more than 4K spectrum of 680 dimensions representing a protein rate measured on 10 different products.</p><p>Scenarios. The demonstration GUI is divided into three tabs. First, the "Method" tab page explains the clustering approaches proposed for each type of data: DC-DPM for multivariate data and HD4C for high dimensional data. Next, in the other tabs, the GUI enables the user to use drop-downs to set the parameters (see Figure <ref type="figure" target="#fig_2">3</ref> and<ref type="figure" target="#fig_3">4</ref>). On the right, a plot of the clustering assignments for both the ground-truth and the chosen method: randomly selected individuals or all cluster centers are displayed. On the left, information about the clustering performance and scalability of each approach is given. The demo GUI is available at: http://147.100.179.112:3838/team/kmeguelati/ dpmclustering/. The demonstration video is available at: https://drive.google. com/file/d/1GHLF5csHk8Oa7PZK4dwA3RTK35KNIbne/view?usp=sharing  </p></div><figure xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Workflow of the DC-DPM approach</figDesc><graphic coords="3,309.52,404.60,166.00,98.26" type="bitmap" /></figure>
<figure xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Architecture of DC-DPM and HD4C in Spark</figDesc><graphic coords="4,134.77,384.42,345.82,120.50" type="bitmap" /></figure>
<figure xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig.3. Users can vary the input datasets, display level and number of displayed data. The demo will report back information about the clustering performances and plot the clustering assignments for both the ground-truth and the chosen method.</figDesc><graphic coords="5,134.77,345.20,169.43,89.82" type="bitmap" /></figure>
<figure xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. The specific case of image pixel clustering. Users can vary the "variance" parameter Ïƒ 2 to control the number of clusters. Better view on: http://147.100.179.112: 3838/team/kmeguelati/dpmclustering/.</figDesc><graphic coords="5,311.13,325.42,169.44,109.60" type="bitmap" /></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Dirichlet process mixture models made scalable and effective by means of massive distribution</title>
		<author>
			<persName><forename type="first">K</forename><surname>Meguelati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Fontez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Hilgert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Masseglia</surname></persName>
		</author>
		<idno type="DOI">10.1145/3297280.3297327</idno>
		<ptr target="https://doi.org/10.1145/3297280.3297327" />
	</analytic>
	<monogr>
		<title level="m">SAC: Symposium on Applied Computing</title>
		<meeting><address><addrLine>Limassol, Cyprus</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-04">Apr 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">High Dimensional Data Clustering by means of Distributed Dirichlet Process Mixture Models</title>
		<author>
			<persName><forename type="first">K</forename><surname>Meguelati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Fontez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Hilgert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Masseglia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Big Data (IEEE BigData)</title>
		<meeting><address><addrLine>Los-Angeles, United States</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12">Dec 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Scalable estimation of dirichlet process mixture models on distributed data</title>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Joint Conference on Artificial Intelligence</title>
		<meeting>the 26th International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4632" to="4639" />
		</imprint>
	</monogr>
	<note>IJCAI'17</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Parallel markov chain monte carlo for nonparametric mixture models</title>
		<author>
			<persName><forename type="first">S</forename><surname>Williamson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="98" to="106" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>