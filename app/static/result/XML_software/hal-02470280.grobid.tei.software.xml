<TEI xmlns="http://www.tei-c.org/ns/1.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xml:space="preserve" xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">New perspectives on plant disease characterization based on deep learning</title>
				<funder ref="#_vHHyVHM">
					<orgName type="full">Agropolis Fondation, Numev, Cemeb</orgName>
				</funder>
				<funder ref="#_BWn7AcE">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher />
				<availability status="unknown"><licence /></availability>
				<date>06 February 2020 0168</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Sue</forename><forename type="middle">Han</forename><surname>Lee</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory" key="lab1">AMAP</orgName>
								<orgName type="laboratory" key="lab2">CIRAD</orgName>
								<orgName type="institution" key="instit1">Univ Montpellier</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<orgName type="institution" key="instit3">INRA</orgName>
								<orgName type="institution" key="instit4">IRD</orgName>
								<address>
									<settlement>Montpellier</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hervé</forename><surname>Goëau</surname></persName>
							<email>herve.goeau@inria.fr</email>
							<affiliation key="aff0">
								<orgName type="laboratory" key="lab1">AMAP</orgName>
								<orgName type="laboratory" key="lab2">CIRAD</orgName>
								<orgName type="institution" key="instit1">Univ Montpellier</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<orgName type="institution" key="instit3">INRA</orgName>
								<orgName type="institution" key="instit4">IRD</orgName>
								<address>
									<settlement>Montpellier</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">CIRAD</orgName>
								<orgName type="institution">UMR AMAP</orgName>
								<address>
									<settlement>Montpellier</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Pierre</forename><surname>Bonnet</surname></persName>
							<email>pierre.bonnet@cirad.fr</email>
							<affiliation key="aff0">
								<orgName type="laboratory" key="lab1">AMAP</orgName>
								<orgName type="laboratory" key="lab2">CIRAD</orgName>
								<orgName type="institution" key="instit1">Univ Montpellier</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<orgName type="institution" key="instit3">INRA</orgName>
								<orgName type="institution" key="instit4">IRD</orgName>
								<address>
									<settlement>Montpellier</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">CIRAD</orgName>
								<orgName type="institution">UMR AMAP</orgName>
								<address>
									<settlement>Montpellier</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Alexis</forename><surname>Joly</surname></persName>
							<email>alexis.joly@inria.fr</email>
							<affiliation key="aff2">
								<orgName type="institution" key="instit1">INRIA Sophia-Antipolis -ZENITH team</orgName>
								<orgName type="institution" key="instit2">LIRMM</orgName>
								<address>
									<settlement>Montpellier</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">New perspectives on plant disease characterization based on deep learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published">06 February 2020 0168</date>
						</imprint>
					</monogr>
					<idno type="MD5">F63D117D404EE85CBDC8491F2351ACC8</idno>
					<idno type="DOI">10.1016/j.compag.2020.105220</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-04-12T14:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid" />
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Plant diseases Automated visual crops analysis Deep learning</keywords>
			</textClass>
			<abstract>
<div><p>The control of plant diseases is a major challenge to ensure global food security and sustainable agriculture. Several recent studies have proposed to improve existing procedures for early detection of plant diseases through modern automatic image recognition systems based on deep learning. In this article, we study these methods in detail, especially those based on convolutional neural networks. We first examine whether it is more relevant to fine-tune a pre-trained model on a plant identification task rather than a general object recognition task. In particular, we show, through visualization techniques, that the characteristics learned differ according to the approach adopted and that they do not necessarily focus on the part affected by the disease. Therefore, we introduce a more intuitive method that considers diseases independently of crops, and we show that it is more effective than the classic crop-disease pair approach, especially when dealing with disease involving crops that are not illustrated in the training database. This finding therefore encourages future research to rethink the current de facto paradigm of crop disease categorization.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div><head n="1.">Introduction</head><p>A plant disease is an alteration of the original state of the plant that affects or modifies its vital functions. It is mainly caused by bacteria, fungi, microscopic animals or viruses, and has a strong impact on agricultural yields and on farm budget. According to the Food and Agriculture Organization of the United Nations, transboundary plant diseases have increased significantly in recent years due to globalization, trade, climate change and the reduction in the resilience of production systems due to decades of agricultural intensification. The risk of transboundary epidemics is increasing and can cause huge losses in crops, threatening the livelihoods of vulnerable farmers and the food and nutritional security of millions of people. 1 Early detection of disease symptoms is one of the main challenges in protecting crops and limiting epidemics. Initial disease identification is usually done by visual assessment <ref type="bibr" target="#b0">(Barbedo, 2016)</ref> and the quality of the diagnosis depends heavily on the knowledge of human experts <ref type="bibr" target="#b20">(Liu et al., 2017)</ref>. However, human expertise is not easily acquired by all actors of the agricultural world, and is less accessible, especially in the case of small farms in developing countries.</p><p>Automatic recognition of plant diseases by image analysis represents a promising solution to overcome this problem and reduce the lack of expertise in this field <ref type="bibr" target="#b25">(Ramcharan et al., 2017)</ref>. Several studies have been carried out on isolated crops, such as maize <ref type="bibr" target="#b35">(Wiesner-Hanks et al., 2018)</ref>, cassava <ref type="bibr" target="#b25">(Ramcharan et al., 2017)</ref>, tomato <ref type="bibr" target="#b6">(Fuentes et al., 2017;</ref><ref type="bibr" target="#b4">Durmus et al., 2017;</ref><ref type="bibr" target="#b7">Fuentes et al., 2018)</ref>, apple <ref type="bibr" target="#b20">(Liu et al., 2017)</ref>, wheat <ref type="bibr" target="#b15">(Johannes et al., 2017;</ref><ref type="bibr" target="#b23">Picon et al., 2018)</ref>, citrus <ref type="bibr" target="#b14">(Iqbal et al., 2018)</ref> or potato <ref type="bibr" target="#b22">(Oppenheim and Shani, 2017)</ref>. Deep Learning (DL) techniques, including Convolutional Neural Networks (CNN), have emerged as the most promising approaches given their ability to learn reliable and discriminative visual characteristics. Tranfer learning, which is used in most cases <ref type="bibr" target="#b28">(Slado Jevic et al., 2016;</ref><ref type="bibr" target="#b33">Too et al., 2019;</ref><ref type="bibr" target="#b6">Fuentes et al., 2017;</ref><ref type="bibr" target="#b20">Liu et al., 2017;</ref><ref type="bibr" target="#b21">Mohanty et al., 2016;</ref><ref type="bibr" target="#b25">Ramcharan et al., 2017)</ref>, is not a single technique, but a whole family of methods, comprising, among others those commonly known as fine-tuning. Unlike learning from scratch where the weights of a model are learned from scratch, the weights of a pre-trained model on a large general dataset (in terms of number of images and classes, such as ImageNet) are fine-tuned. Transfer learning allows to build accurate models even on a specialized dataset such as those containing plant diseases where there are usually a small number of images and classes compared to ImageNet. Indeed, when used to training a CNN model based on images of diseased and healthy plant leaves captured under controlled conditions, it has been proven that fine-tuning is better than training a deep model from scratch <ref type="bibr" target="#b21">(Mohanty et al., 2016)</ref>. However, the ability of a deep learning model to transfer knowledge from one domain to https://doi.org/10.1016/j.compag.2020.105220 Received 11 January 2019; Received in revised form 24 December 2019; Accepted 8 January 2020 another, from one task to another, is not yet well understood and performances may vary according to the used datasets for pre-training a model <ref type="bibr" target="#b34">(Torralba and Efros, 2011)</ref>. In particular, <ref type="bibr" target="#b39">(Zhou et al., 2014)</ref> showed that fine-tuning a CNN pre-trained on scene recognition datasets rather than the general Imagenet dataset can perform better in any urban scene recognition tasks. We can then assume that a model pretrained on a dataset of images dedicated to plant identification could provide better features for plant disease identification rather than a pretrained model on ImageNet which contains actually few visual botanical concepts. However, no previous study has reported such results. The first contribution of this article is to study the impact of transfer learning depending on whether the transfer learning is from a general domain or from the plant domain.</p><p>In recent years, more and more studies address the multi-crop disease problems where data modelling covers several varieties of crops and diseases at the same time <ref type="bibr" target="#b21">(Mohanty et al., 2016;</ref><ref type="bibr" target="#b33">Too et al., 2019;</ref><ref type="bibr" target="#b28">Slado Jevic et al., 2016;</ref><ref type="bibr" target="#b5">Ferentinos, 2018)</ref>. These investigations correspond to the needs of actors such as farm technicians, agricultural engineers, market gardeners or arboriculturists, who would potentially be confronted with different types of diseases on different types of crops, and where a more general approach (particularly through the use of smartphones) could provide a valuable help <ref type="bibr" target="#b21">(Mohanty et al., 2016)</ref>. From a technological point of view, this type of use is now possible thanks to deep learning techniques that prove to have sufficiently high recognition capabilities for wide datasets in terms of number of images, thus avoiding the need to design hand-crafted features dedicated to a specific domain. To date, two main datasets have been used to assess identification performance on multi-crop diseases: Plant Village (PV)<ref type="foot" target="#foot_0">2</ref>  <ref type="bibr">(Hughes and Salathé, 2015)</ref> and Digipathos<ref type="foot" target="#foot_1">3</ref>  <ref type="bibr" target="#b3">(Barbedo et al., 2018)</ref>. The majority of studies are based on the PV dataset <ref type="bibr" target="#b28">(Slado Jevic et al., 2016;</ref><ref type="bibr" target="#b5">Ferentinos, 2018;</ref><ref type="bibr" target="#b21">Mohanty et al., 2016;</ref><ref type="bibr">Hughes and Salathé, 2015)</ref>, which is currently the largest dataset in terms of number of images. The PV dataset is organized into target classes where each one represents a crop-disease pair, i.e. an association of one type of crop and one disease. In the PV dataset, a particular crop may be present in several classes and, conversely, visually similar diseases with the same disease common name may be present in several classes. Most of the previous studies based on CNNs follow this data organization, where the last layer dedicated to the classification contains output of crop-disease pairs <ref type="bibr" target="#b21">(Mohanty et al., 2016;</ref><ref type="bibr" target="#b33">Too et al., 2019;</ref><ref type="bibr" target="#b28">Slado Jevic et al., 2016;</ref><ref type="bibr" target="#b5">Ferentinos, 2018)</ref>.</p><p>However, although previous studies have reported high identification performances, it is possible that the models learned are not optimal by extracting to many irrelevant features related more to crop than to disease. For example, as reported in <ref type="bibr" target="#b21">(Mohanty et al., 2016)</ref>, it has been proven that removing the background with image segmentation is less effective than using the ordinary colored background images to train a CNN, confirming a dependency of background features for disease identification. Furthermore, <ref type="bibr" target="#b31">(Toda and Okura, 2019)</ref> showed that model which is supposed to learn plant disease visual appearance, tends to highlight irrelevant crop features like the shape of the leaf, and in fact, it is possible when a crop contains more visual discrimination characteristics than the disease itself such as a deeply grooved leaf of tomato. Therefore, the second contribution of this paper is to propose a new intuitive way of characterizing plant diseases in the context of multi-crop diseases, focusing on the common names of diseases regardless of the type of crop. We show that this approach is more generalizable and robust in identifying diseases of new data taken in domains different from those in the training set.</p><p>Next, we consider that it would be very difficult, if not impossible, in a real-world application to collect a complete set of images illustrating each crop-disease pair. Also, it may not be easy to establish a specific list of all diseases for each host species. We note that pathogens with a wide host range can infect many different host species. For example, as reported in <ref type="bibr" target="#b24">(Prospero and Cleary, 2017)</ref>, nearly 200 plant species can be infected with bacterial wilt (Ralstonia solanacearum). Therefore, the ability of a model to transfer knowledge from one disease context to another is crucial for practical application to reduce the time and cost of data collection and retraining a deep model. We show that by using our new strategy mentioned above to identify plant diseases, we could have the significant advantage of being able to identify a disease known to the classifier without the type of crop being known. Therefore, the third contribution of this paper is to study the generalization of a deep model to identify diseases of "unseen" crops. Unseen crops in this context refer to unknown crops that have never been seen by a deep model during the training.</p><p>To summarize the contributions of this paper:</p><p>1. We investigate the impact of two transfer learning strategies, one based on a pre-trained model on a plant dataset and the other using a general object dataset. We found that, it is sometimes more effective to fine-tune a model pre-trained on plant identification, or one which is pre-trained on general object recognition. 2. We qualitatively investigate the features learned from the models trained with crop-disease classes <ref type="bibr" target="#b21">(Mohanty et al., 2016;</ref><ref type="bibr" target="#b32">Too et al., 2018)</ref>. We highlight that the learned features might not necessarily be relevant to plant disease identification because they also focus on crop-specific features such as the leaf venation and lamina. 3. We demonstrate a simple and intuitive way of considering plant diseases alone, without any association with a crop. With this, it shown to enable a better generalization not only to new data taken in domains different from those in the training (different in data distribution), but also to unseen crops.</p></div>
<div><head n="2.">Datasets</head></div>
<div><head n="2.1.">Plant Village</head><p>Plant Village (PV) <ref type="bibr">(Hughes and Salathé, 2015)</ref> is a popular dataset collected for evaluation of automatic plant disease identification systems. It contains healthy and infected leaves isolated on a uniform background. We used the initial version of the PV dataset 2 that is still publicly available as a benchmark in this study. It has 38 crop-disease pairs, with 26 crop-disease categories for 14 crop plants. Note that, the data comes with predefined training and test subsets. In our study, we use the reported configuration as the one that produces the best identification performances in the original document <ref type="bibr" target="#b21">(Mohanty et al., 2016)</ref>, where 80% of the data is for the training set and the remaining 20% for the test (more precisely, 43,810 training images and 10,495 images for the test). Table <ref type="table" target="#tab_0">1</ref> gives for the 38 pairs, the common names of the host plant and the disease, the scientific name of the disease and the number of training and testing images.</p><p>We can see in this table that different crops can be infected with pathogens associated with the same common disease name. It is mainly due to their similar ways of infecting the crops, which hence has led the agricultural community to call them by the same name. Table <ref type="table" target="#tab_1">2</ref> illustrates the distribution of images based on a total of 14 crops and 20 common diseases<ref type="foot" target="#foot_2">4</ref> with 1 healthy class. Some diseases such as Apple scab or Tomato mosaic virus can only be found in apple and tomato crops respectively, but other diseases such as Black rot or Bacterial spot are common in different crops.</p></div>
<div><head n="2.2.">IPM and Bing test datasets</head><p>Two complementary datasets named IPM and <software ContextAttributes="used">Bing</software>, which were initially introduced in <ref type="bibr" target="#b21">(Mohanty et al., 2016)</ref>, are used in this work to evaluate the robustness of a model for predicting plant diseases on exterior images. Unlike in PV, these test images are not limited to a controlled environment (see Fig. <ref type="figure" target="#fig_0">1</ref>) and are extracted from a reliable online sources for IPM<ref type="foot" target="#foot_3">5</ref> and downloaded from <software ContextAttributes="used">Bing Image Search</software>. These images were verified in <ref type="bibr" target="#b21">(Mohanty et al., 2016)</ref>. We were able to retrieve all the 119 images from IPM<ref type="foot" target="#foot_4">6</ref> mentioned in <ref type="bibr" target="#b21">(Mohanty et al., 2016)</ref>, but we could only retrieve 64 of the 121 images expected for <software ContextAttributes="used">Bing</software> 6 (missing images are no longer available online). Tables <ref type="table" target="#tab_2">3</ref> and<ref type="table" target="#tab_3">4</ref> show the distribution of the IPM and <software ContextAttributes="used">Bing</software> images. Among the 38 crop-disease pairs in PV, the IPM test set allows to evaluate 19 pairs while <software ContextAttributes="used">Bing</software> test set allows to evaluate 26 pairs. A qualitative assessment of the data, as shown in Fig. <ref type="figure" target="#fig_0">1</ref>, shows that <software ContextAttributes="used">Bing</software> images contain background noise (i.e. cluttered backgrounds) more often than IPM image, suggesting a more difficult recognition on <software ContextAttributes="used">Bing</software> than on IPM.</p></div>
<div><head n="3.">Transfer learning and pre-trained models</head><p>Transfer learning is not a single technique, but a whole family of methods, comprising, among others those commonly known as finetuning. Using such approaches, a model trained on a task is re-purposed to a second related task <ref type="bibr" target="#b9">(Goodfellow et al., 2016)</ref>. A basic example of transfer learning is using a deep model just as fixed feature extractor <ref type="bibr" target="#b17">(Karpathy, 2019)</ref>, without tuning the weights of a pre-trained model and by removing the last fully connected layer related to the class outputs of the initial task. Then any learning algorithms such as support-vector machines can be used to perform new classification. One of the most popular transfer learning approaches is to fine-tune all the weights of a pre-trained model, with the last fully connected layer being replaced and randomly initialized on a new classification task. In fact, it is possible to fine-tune only some layers, which are generally the last layers corresponding to a higher level of abstraction. However, <ref type="bibr" target="#b36">(Yosinski et al., 2014)</ref> showed that by transferring the features and finetuning all the layers of a network, one can boost the generalization capability of a model. Fine-tuning helps to prevent overfitting, leading to significantly better generalization especially when the number of labelled examples is scarce, or in a transfer setting where we have lots of examples for some source tasks but very few for some target tasks <ref type="bibr" target="#b18">(LeCun et al., 2015)</ref>. Additionally, it has been proven to be more effective if the pre-training and the target task are correlated <ref type="bibr" target="#b36">(Yosinski et al., 2014;</ref><ref type="bibr">Huh et al., 1608)</ref>.</p><p>In plant disease classification tasks, it has been shown that the transfer learning approach is more accurate than a "from scratch" learning approach where all the weights of a model are learned from scratch <ref type="bibr" target="#b32">(Too et al., 2018;</ref><ref type="bibr" target="#b21">Mohanty et al., 2016;</ref><ref type="bibr" target="#b28">Slado Jevic et al., 2016;</ref><ref type="bibr" target="#b5">Ferentinos, 2018)</ref>. Most of these previous works are based on models pre-trained on ImageNet <ref type="bibr" target="#b26">(Russakovsky et al., 2015)</ref>, a general image database containing about 1,2 million images associated to 1000 very diverse classes (animals, vehicles, buildings, landscapes, plants, etc). However, in the case of plant diseases, one can suppose that it would be appropriate to pre-train a model on domain with a dataset containing exclusively plant images. Therefore, in this study, we compare two finetuning strategies, one directly using a model pre-trained on ImageNet and another one using <software ContextAttributes="used">PlantCLEF</software>2015, a dataset dedicated to plant identification (H. Go¨eau, P. Bonnet, A. Joly, Lifeclef plant identification task, 2015). This dataset is related to the plant identification challenge in <software ContextAttributes="used">LifeCLEF</software> <ref type="bibr" target="#b16">(Joly et al., 2015)</ref> and addresses the problem of species identification based on multi-organ and multi-image observations of specimens. It has the same number of classes as ImageNet, but is two orders of magnitude smaller with precisely 91,759 training images (see Fig. <ref type="figure" target="#fig_1">2</ref> for some illustrations).</p></div>
<div><head n="4.">Identification based on common names of diseases and generalization to unseen crops</head></div>
<div><head n="4.1.">Identification based on visual features of common diseases</head><p>With regard to previous works on crop-disease pairs <ref type="bibr" target="#b21">(Mohanty et al., 2016;</ref><ref type="bibr" target="#b5">Ferentinos, 2018)</ref>, there remains a doubt that a deep model can be forced to discriminate similar diseases that have the same common name and most likely visually similar ways of infecting the crops. A potentially undesirable effect that may occur is that the model learns more from the content related to the crop than the visual content related to the disease (Toda and Okura, 2019; <ref type="bibr" target="#b21">Mohanty et al., 2016)</ref>. This adverse effect can potentially be amplified if a crop has a very pronounced characteristic such as toothed margins leaves, while the disease is not very visible, for example, with a slight bleaching at an early stage of infection. In this section, we study an alternative approach to the usual crop-disease pair modeling by considering only on common names of diseases regardless of crop categories. The hypothesis is that such intuitive modelling will engage the model to learn more independent features from the host plants.</p><p>In order to train a disease classifier based on common diseases independently of crops, the images of the PV dataset are first separated into 21 classes (20 diseases and one healthy class). We randomly took a subset of images (~20%) from each healthy crop and combined them    into a single healthy class, so as to limit the amount to 2601 images in order to avoid imbalanced distributions (39,508 training images in total). During the inference, we calculate the classification result from the 21 probability outputs produced by the trained model. We call this disease modeling strategy S co which symbolizes the CNN modeling strategy based on the common name of diseases.</p><p>We compare S co with another disease modeling strategy called S cd , which instantiates the CNN modeling strategy based on the crop-disease pairs. In S cd , we first train a CNN based on 38 crop-disease pairs with a total of 43,810 training images. During the inference, from the 38 probability outputs produced by the trained model, we perform a late fusion method to integrate the probabilities corresponding to the individual common diseases. More precisely, we compute the average probability output with respect to each individual common name of disease for the conjoined crop-disease pairs. For example, the average probability of the Back rot common disease which is associated with apple and grape, is computed as:</p><formula xml:id="formula_0">= P mean P P ( , ) Blackrot Apple Black rot Grape Black rot __ _ __ _</formula><p>With this formulation, S cd has the same number of accuracy outputs as S co .</p></div>
<div><head n="4.2.">Generalization to unseen crops</head><p>There are several hundred common plant diseases in the world that can each of them potentially infect dozens or even hundreds of different types of crops. It is difficult, if not impossible, to collect a complete set of images illustrating each crop-disease combination for building an exhaustive training set. In addition, diseases can mutate and become transmissible to new cultivated species and then it is necessary to complete regularly the training set with new crop-disease combinations. On the other hand, an approach that considers diseases independently of crops has the advantage of making a classifier more easily extensible to new crops, by allowing a model to identify a known disease in the training dataset, even if the crop hosting the diseases on a test image is unknown in the training dataset. To evaluate the ability of a deep model to generalize to an unknown crop, in our methodology, all the images related to pepper crops such as those from the class Pepper_bell__Bacterial_spot and the class Pepper_bell__healthy are removed from the training set, and the model is trained only with those images which are not from pepper crop. It should be noted that, in doing so, the number of crop-disease pairs to be learned by a model is reduced from 38 to 36, giving a total of 41,762 and 37,727 images in the training set to train S cd and S co respectively.</p></div>
<div><head n="5.">Approach</head></div>
<div><head n="5.1.">Deep architectures</head><p>In our study, we employ three types of CNN architectures with different depth size: VGG16 (Simonyan and Zisserman, 1409) (16 layers), InceptionV3 <ref type="bibr" target="#b30">(Szegedy et al., 2016)</ref> (48 layers) and our proposed architecture GoogLeNetBN<ref type="foot" target="#foot_5">7</ref> (34 layers). This last architecture is inspired by the InceptionV2 and InceptionV3 architectures <ref type="bibr" target="#b13">(Ioffe and Szegedy, 2015;</ref><ref type="bibr" target="#b30">Szegedy et al., 2016)</ref> that bring several upgrades to the initial GoogLeNet architecture <ref type="bibr" target="#b29">(Szegedy et al., 2014)</ref> for increasing the accuracy and reducing the computational complexity. It is a 34 layers deep CNN adding to each convolution a batch normalization (BN) operation. BN has been proven to speed up convergence and limit overfitting. Besides applying BN to GoogLeNet, we update the network by replacing the 5 × 5 convolutional layers with two consecutive layers of 3x3 convolutions like for the InceptionV3 architecture, so as to improve computational speed. The ImageNet dataset used for the ILSVRC challenge <ref type="bibr" target="#b26">(Russakovsky et al., 2015)</ref>, containing about 1.2 M images related to 1000 classes, was then used to train the model from scratch for a few days on a mini-cluster of GPU cards to produce a "pre-trained" model.</p></div>
<div><head n="5.2.">Training from scratch versus transfer learning</head><p>All CNNs are trained by using stochastic gradient descent (SGD) optimization technique. The batch size was set to 45, and momentum set to 0.9. The L2 weight decay with penalty multiplier was set to × 2 10 4 . By employing the CNNs mentioned in Section 5.1 and training them based on the following training schemes, we will have 9 trained models based on 3 experimental configurations: For the FTIN configuration, a model is first pre-trained with ImageNet and then fine-tuned directly on the PV dataset. For the FTPC configuration, a model is first pre-trained with ImageNet, then finetuned on the <software>PlantCLEF</software>2015 dataset before being fine-tuned on the PV dataset. For both configurations, all weights of the pre-trained models are fine-tuned simultaneously, with the last fully connected layer being replaced and initialized with target task classes. All the choices of training hyper-parameters were made based on empirical observation of the convergence of the network training as well as the training performance. For the two architectures VGG16 and GoogLeNetBN, the learning rate was initially set with a relatively low value of 10 4 in order to change the weights not too quickly, except for the last full-connected layer where the learning rate was set to a higher value of 10 3 to ac- celerate the convergence of the weights initialized with random values. For each model, the training are carried out for a total of 15 epochs, where one epoch is defined as the number of iterations required for the neural network to pass through the entire training set. The learning rate followed a "step" policy, meaning that it is decreased by a factor of 10 during the training, for every 15/3 epochs in our case. The architecture InceptionV3 with the configuration FTPC used the same hyper-parameters as the aforementioned two architectures, but when using the configuration FTIN on ImageNet the initial learning rate was set to 10 3 for all the layers because we noticed that the model converged too slowly with a low initial learning rate of 10 4 . For the FS configuration, we set for all the three models an initial learning rate of 10 3 for all the layers. In this case, since all the weights are initialized with random values, we need more iterations, 30 epochs in these experiments, for the models to converge towards their best performances. Concerning the inputs, during the training, the image at the input of GoogLeNetBN and VGG16 is resized to 256 × 256 pixels and cropped to 224 × 224 pixels. As with InceptionV3, the image is resized to 300 × 300 pixels and cropped to 299 × 299 pixels. We augment our training image by random cropping and mirroring. For final model evaluations, each test image is cropped in its center and resized to the size imposed by the model architecture.</p><p>Fig. <ref type="figure" target="#fig_3">3</ref> shows the progression of the top-1 accuracy on the training set for all the different models and configurations limited to 15 epochs. We can see that the models learned with a fine-tuning approach all progress very quickly and tend to achieve a top-1 accuracy above 0.95 in less than 2 epochs, except for the InceptionV3 model pre-trained on ImageNet, which requires at least 7 epochs to reach an equivalent accuracy. We can also see that, in the FS configuration, the models converge more slowly and requires at least 11 epochs before reaching accuracies equivalent to the transfer learning configurations, except for the model with the VGG16 architecture which seems to stagnate around an accuracy of 0.91.</p></div>
<div><head n="6.">Experiments</head><p>In this section, we first studied the impact of the use of transfer learning on the identification of plant diseases. Then, we evaluated a new classification problem considering common names of diseases as single classes, regardless of crop categories. Finally, we studied and analyzed the extent to which the deep models we learned can be generalizable to diseases despite the fact that the hosting crop is not illustrated in the training set.</p></div>
<div><head n="6.1.">Comparison of models with different learning configurations</head><p>Evaluation measures on the test sets. To measure model classification performance, the top-1 and top-3 accuracies were used to measure the performance of model classification in our experiments. Further, we also used the average accuracy per class to assess the performance of an individual class:</p><formula xml:id="formula_1">= = AVE T C i C i 1 1</formula><p>, where T i is the average accuracy for all test images related to the C i class and C is the total number of classes. Next, to overcome the stochasticity of learning a single neural network, we used the same approach as described in <ref type="bibr" target="#b21">(Mohanty et al., 2016)</ref>, which combines the predictions of several trained models. Specifically, we aggregated and calculated the average of all the probabilities given by the models saved at the end of each epoch, then we calculated the average and overall accuracy of the entire test. By doing so, we are able to reduce the variance of the prediction results and obtain a fair comparison result with <ref type="bibr" target="#b21">(Mohanty et al., 2016)</ref>.</p><p>Generalization. Table <ref type="table" target="#tab_4">5</ref> shows the classification performance of the CNNs trained with the different learning strategies. It can be seen that, although models trained with FS configuration, especially the deeper architectures, achieve relatively high performance in PV test set, they have lower classification performance in IPM and <software ContextAttributes="used">Bing</software> than fine-tuned models. This result is consistent with the previous study <ref type="bibr" target="#b21">(Mohanty et al., 2016)</ref>, showing the advantage of fine-tuning in this task. The GoogLeNet model <ref type="bibr" target="#b21">(Mohanty et al., 2016)</ref>, which achieves the highest accuracy on the PV test set, shows lower performance on IPM images compared to GoogLeNetBN. This leads us to conclude that this model is particularly overfitted. Unlike what is generally observed in many other areas, the VGG16 model achieves the best performance on IPM and <software ContextAttributes="used">Bing</software> test sets, 44.54% and 28.13% respectively, surpassing deeper architectures. This finding suggests that those models with deeper architectures are more likely to be overfitting for this task, probably because the PV dataset is not large or diverse enough.</p><p>Pre-trained models. It is difficult to conclude which pre-training task works best for the external IPM and <software ContextAttributes="used">Bing</software> test data mainly because the size of these two set of images is quite small. However, by comparing the overall top-1 accuracy of the combined IPM and <software ContextAttributes="used">Bing</software> images, the  VGG16 pre-trained with the ImageNet dataset shows a better result than the one pre-trained with <software ContextAttributes="used">PlantCLEF</software>2015, which are 44.54% and 36.13% respectively. As for the GoogLeNetBN and InceptionV3, the model pre-trained on <software ContextAttributes="used">PlantCLEF</software>2015 leads to better performance than the one pre-trained on the ImageNet dataset, unlike the VGG16. From this observation, it is unfortunately not possible to conclude on the usefulness or not of pre-training the models on a task closer to the target domain rather than the general object dataset, ImageNet. Confusions and misclassifications. By analyzing the performance of each individual class of the PV test set (Supplementary Fig. <ref type="figure" target="#fig_0">1</ref>), we found that most classification errors occur in the Tomato Early blight class. A closer analysis of the confusion matrix (Supplementary Table <ref type="table" target="#tab_0">1-</ref> <ref type="bibr" target="#b39">6)</ref> shows that these errors generally result from confusion between the classes Tomato Early blight, Tomato Late blight, Tomato Septoria leaf spot and Tomato Target spot. Fig. <ref type="figure" target="#fig_4">4</ref> gives some examples of misclassified images related to these confusions and illustrates how they visually share similarities in leaf appearance and disease patterns. Although the disease Tomato Early blight may have specific visual characteristics, the intra-and inter-class variability makes their learning very difficult. This factor was discussed in <ref type="bibr">(Barbedo, 2018)</ref>, and one suggested solution was to extend the training dataset to a more exhaustive and complete set, covering all the visual variability of symptoms, which is in fact very difficult to collect. Next, we found that misclassification tends to occur within similar crops, particularly in maize and potatoes, and is also misleading due to visually similar leaf appearance and disease patterns.</p><p>Next, by analyzing the performance of the <software ContextAttributes="used">Bing</software> images (Supplementary Table <ref type="table" target="#tab_6">7</ref>-12), we observed that with the exception of the VGG16 model pre-trained on <software ContextAttributes="used">PlantCLEF</software>2015 (Supplementary Table <ref type="table" target="#tab_0">10</ref>), which has the correct prediction for all the images of Potato Late blight, the rest of the model configurations show major classification errors, confusing these images with Tomato Late blight, which is actually related to the same common name of disease but on different crops. This observation suggests that a common disease, although infecting different host types, produces very similar disease patterns.</p></div>
<div><head n="6.1.1.">Qualitative results</head><p>Attention map visualization. To better understand the visual features learned by the models, we used one of the visualization methods presented in <ref type="bibr" target="#b37">(Yosinski et al., 2015)</ref>, which consists of plotting the activation values of neurons in a CNN layer in response to an input image. Instead of individually visualizing the activation of a CNN layer as implemented in <ref type="bibr" target="#b21">(Mohanty et al., 2016)</ref>, we examined the highest global activation within all feature maps in a layer, so as to locate the regions most voted by the model and then exploit those that have the most impact in the final classification. More precisely, we started by plotting the position of the neuron with the highest activation for all the features maps extracted from the last convolutional layer, and from there we accumulated the first 30 dominant activations and matched them to the original image. It is possible to visualize any number of neural activations, but, based on our empirical observation, the accumulation of 30 dominant activations in this case is enough to visualize the difference in the focus of attention between different models. We chose to visualize the last convolutional layer because this layer corresponded to the highest level of abstraction and it is more specific to the target classes <ref type="bibr" target="#b19">(Lee et al., 2017;</ref><ref type="bibr" target="#b38">Zeiler et al., 2011)</ref>.</p><p>By comparing the visualization outputs of the GoogLeNetBN and VGG16, we found that their neuron activations were activated in different regions. For example, in Fig. <ref type="figure" target="#fig_5">5a</ref>, both the GoogLeNetBN models pre-trained with ImageNet and <software ContextAttributes="used">PlantCLEF</software>2015 are activated by areas of yellowish venation, while the VGG16 models are activated by areas infected with large brown spots with yellow tissues around. However, when comparing the visualization outputs of different pre-training tasks on the same network architecture, we found a certain degree of similarity in the activated areas. For example, in Fig. <ref type="figure" target="#fig_5">5b</ref>, the GoogLeNetBN models pre-trained with ImageNet and <software ContextAttributes="used">PlantCLEF</software>2015 concentrate mainly on the center of the leaf, while the VGG16 models mostly focus on infected secondary veins. On closer observation, we found that the activated regions do not necessarily focus on the largest infected area, but on regions with obvious leaf characteristics such as the venations.</p></div>
<div><head n="6.1.2.">Discussion</head><p>In connection to the above findings, we hereby make a few deductions. Firstly, we found that VGG achieves a better classification performance compared to other deeper network architectures which tend to overfit on the PV dataset. We believe that the lack of diversity in the PV data set, which could make the learned model sometimes misleading when used in real field data such as IPM and <software>Bing</software>. Secondly, it is not possible to conclude that a model pre-trained on a task closer to the final task is the best option to solve the PV dataset, as in our study we found that pre-training on plant domains improves performance in the case of deep networks, InceptionV3 and GoogLeNetBN, but not for the simpler and shallower networks such as VGG16. However, in our experiments, the generalization performances of the models were studied on IPM and <software ContextAttributes="used">Bing</software>, which is currently the only field test set available online with verified field truth. It would be interesting to see if our results hold up on a larger number of images. If not, it might suggest that the size of the IPM and <software ContextAttributes="used">Bing</software> is too small to generalize the results. Further investigation of the complementary field datasets will be required to reach a more robust conclusion on this point. We hope that this article will stimulate the curiosity of the scientific community in this field and facilitate further research on this topic related to the generalization of the deep model in cross domains to improve the identification of plant diseases.</p><p>Lastly, we noticed that the features learned might not necessarily look at the most obviously infected area but the region with the most distinctive leaf characteristics, such as the venation. This is believed to be due to the fact that models are trained with the terminology of cropdisease pairs, where models trained in this way could be biased towards crop-specific patterns, especially when crop features appear to be more discriminative than diseases. Therefore, in the next section, we explore an alternative approach to crop-disease modelling, attempting an intuitive way to direct a deep model to learn feature representations based on common diseases, regardless of crop.</p></div>
<div><head n="6.2.">Disease identification based on common names of diseases and generalization to unseen crops</head><p>In this experiment we used the VGG16 model, which had been proven to be the best model in previous experiments, to experiment with the principle of learning based on common names of diseases. We compared two disease modeling strategies that are: (1) the formation of a model based on training solely on the common names of diseases (S co ), and (2) based on training on previously proposed crop-disease target classes and the subsequent implementation of a late fusion method to integrate probabilities corresponding to the individual common names of diseases (S cd ). Details of these approaches can be found in section 4.</p></div>
<div><head n="6.2.1.">Evaluation on seen crops</head><p>Table <ref type="table" target="#tab_5">6</ref> shows that the S cd approach is generally better than the S co approach at predicting the PV (seen) set. The VGG16 model pre-trained with ImageNet gives the best result up to 98.98% top-1 accuracy and 98.20% average accuracy per class AVE. However, due to the nature of the PV data set, which seriously lacks diversity, both strategies were found achieving a relatively high accuracy (:98% for top-1 accuracy).</p><p>Through the analysis of the confusion matrices (Supplementary table 19-20) of the S co and S cd approaches, we found that, compared to S cd , S co has more classification errors between early blight and late blight. In fact, this misclassification occurs mainly in tomato crops where the number of disease classes is highest in PV. From this observation, we deduce that the lack of crop information may seem to affect S co 's ability to recognize diseases with a very similar visual appearance. However, we note that although crop characteristics that could be extracted by S cd may provide additional information to help differentiate these classes, these crop characteristics do not necessarily correspond to the characteristics of plant diseases.</p><p>Next, we analyzed the performance of deep models to identify new data (IPM and <software ContextAttributes="used">Bing</software>) from other domains, typically pictures in the field. Note that there are only two pepper crop images in IPM, and one pepper crop image in <software ContextAttributes="used">Bing</software>, of which the number of samples is insufficient to infer the performance of deep models in recognizing the disease of an unseen crop. We hence tested the deep model using only the seen crop images. From Table <ref type="table" target="#tab_5">6</ref>, we can see that S co approach in general works better than S cd on IPM and <software ContextAttributes="used">Bing</software>. Next, using the S co approach, in order to determine which pre-training task is the most effective, we compared the top-1 accuracy of the combined IPM and <software ContextAttributes="used">Bing</software> images, and found </p></div>
<div><head n="6.2.2.">Generalization to unseen crops</head><p>From Table <ref type="table" target="#tab_5">6</ref>, we can see that, despite the fact that S cd can achieve a better result in the PV (seen) set, S co generalizes better to PV (unseen) set, especially the <software ContextAttributes="used">PlantCLEF</software>2015 model which reaches 65.11%, the highest top-1 accuracy. Note that the overall accuracy of the PV (unseen) in Table <ref type="table" target="#tab_5">6</ref> was calculated from all PV (unseen) images, which means a combination of images from the Pepper_bell__Bacterial_spot and Pepper_bell__healthy classes. Next, we explored the classification performance of each individual PV (unseen) class in which, in this case, the overall accuracy is calculated according to each individual PV (unseen) class. The result of the classification is presented in Table <ref type="table" target="#tab_6">7</ref>.</p><p>We observed that different pre-training tasks can exert substantial influence on different categories of pepper classes. Specifically, it can be seen that, for the S co , the ImageNet pre-trained model (83.89%) can distinguish better between healthy and unhealthy pepper leaf samples compared to that pre-trained with <software>PlantCLEF</software>2015 (65.40%). However, the class of pepper leaves with bacteria spots shows a contrary result where the model pre-trained with the <software ContextAttributes="used">PlantCLEF</software>2015 (65.75%) is better than the one pre-trained with the ImageNet (43.06%).</p><p>Next, based on failure analysis of the PV (unseen), we observed that, with the ImageNet pre-trained models, a majority of Pepper bell Bacterial spot are confused with target spot, Huanglongbing (Citrus greening) and early blight; with the <software ContextAttributes="used">PlantCLEF</software>2015 pre-trained models, most of the Pepper bell Bacterial spot images are confused with Haunglongbing (Citrus greening), Tomato Yellow Leaf Curl Virus and early blight. Through visual analysis, we noticed that the symptoms of those misclassified samples are very similar to the disease from the other class. For example, in Fig. <ref type="figure" target="#fig_6">6a</ref>, the samples of Pepper bell Bacterial spot that are confused with Haunglongbing (Citrus greening) have yellowish infected regions and also yellow veins, resembling the characteristics of Haunglongbing (Citrus greening). Also, those samples that are confused with the Target spot disease as shown in Fig. <ref type="figure" target="#fig_6">6b</ref> have visually similar dark brown spots characteristics. Likewise, the healthy pepper samples are also found to be misled for the same reason. A majority of pepper healthy samples are wrongly identified as Haunglongbing (Citrus greening), most probably due to the characteristic of yellowing pepper bell leaves.</p><p>From this analysis, it is obvious that the recognition of deep models is highly affected by the symptom variations of one disease, analogous to the findings reported in <ref type="bibr">(Barbedo, 2018)</ref>. However, it is undeniable that, for the models formulated by both S cd and S co that have never seen any pepper bell leaf samples, they are still able to capture very similar characteristics of pepper bell which could somehow help to improve the predictive modeling for unseen crops.</p><p>Lastly, by aggregating multi-class outputs into two classes, we analyzed the performance of the models to differentiate between healthy and unhealthy classes of the PV set. Note that, in this experiment, we verified the classification performance on the basis of the entire PV set, which includes the seen and unseen test set. From the ROC (Receiver Operating Characteristic) curves shown in Supplementary Fig. <ref type="figure" target="#fig_1">2</ref>, it can be seen that the S co is more robust in differentiating between healthy and unhealthy leaf samples compared to the model trained with S cd . Indeed, through the AUC (Area Under the Curve) as shown in Table <ref type="table" target="#tab_7">8</ref>, S co as a whole performed better than S cd .</p></div>
<div><head n="6.2.3.">Discussion</head><p>Based on this experiment, we make several deductions. Firstly, classification based on S co was proven generalizes better and robust in identifying diseases of unseen crops as well as plant images taken in domains different from those in the training set. Next, S co can also better distinguish between healthy and unhealthy leaf samples. This is particularly important in the context of phenotyping large field crops, in which it is possible with new agricultural robots or drones to generate    massive volumes of visual data that can't be manually exhaustively analyzed. In such cases, an early detection of unhealthy leaf samples can contribute to quickly investigate their origins, and potentially implement appropriate solutions.</p></div>
<div><head n="7.">Conclusion</head><p>First, this paper has examined and compared the performance of different transfer learning mechanisms based on different (1) pretraining tasks: the plant specialized domain and general object domain, and the (2) network architectures: VGG16 (16 layers), GoogLeNetBN (34 layers) and InceptionV3 (48 layers). We have experimentally proven that pre-training with plant specialized tasks could reduce the impact of overfitting for the deeper Inception-based model but the VGG16 model with ImageNet pre-training has shown a better generalization in adapting to new data.</p><p>Second, the fact that the VGG16 perform better than the inceptionbased models would be due to the lack of diversity in the PV data set, which leads to a constraint in the deployment of deeper architectures. Nevertheless, with regard to the fact that ImageNet pre-training does not necessarily regularize or improve the accuracy of final target tasks, as indicated in <ref type="bibr">(He et al., 1811)</ref>, we suggest that the agricultural community move forward to create a broader and more diversified plant disease database, without having to use a pre-training model. In addition, for the model to better adapt to large-scale crops, a dataset of images captured under real cultivation conditions <ref type="bibr">(Barbedo, 2018;</ref><ref type="bibr" target="#b5">Ferentinos, 2018)</ref> is needed, as demonstrated by <ref type="bibr" target="#b5">(Ferentinos, 2018)</ref>.</p><p>Third, by interpreting the underlying learned characteristics through the visualization of activations, we have showed that a CNN trained in this crop-disease terminology may not always focus on disease regions but crop-specific characteristics such as leaf venation or lamination to facilitate data discrimination. Therefore, in order to capture the visual symptoms of plant diseases, we demonstrated an intuitive way of leading a deep model to learn the representation of characteristics based on the common names of diseases instead of target classes of crop-disease pairs. We have shown experimentally that the model trained with common diseases, regardless of crop is more generalizable, especially for new data taken in different domains and also for unseen crops.</p></div><figure xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Examples of the (a) IPM and (b) Bing images.</figDesc><graphic coords="6,72.68,56.99,180.86,99.94" type="bitmap" /></figure>
<figure xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Examples of the PlantCLEF2015 images. The images are composed of different plant organs, and captured in environment with cluttered backgrounds, reflecting a realworld scenario.</figDesc><graphic coords="6,319.18,565.25,225.94,144.14" type="bitmap" /></figure>
<figure xml:id="fig_2"><head /><label /><figDesc>Fine-tuning of ImageNet pre-trained model (FTIN) (b) Fine-tuning of PlantCLEF2015 pre-trained model (FTPC) 2. Training from scratch (FS)</figDesc></figure>
<figure xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Progression of top-1 accuracy on the training set for the 3 models and the 3 configurations.</figDesc><graphic coords="8,103.86,57.03,387.50,228.82" type="bitmap" /></figure>
<figure xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Examples of leaf samples which are confused between classes of: (a) Tomato Early blight and Tomato Late blight, and (b) Tomato Early blight and Tomato Septoria leaf spot. It can be seen that they share very similar leaf appearance and disease patterns.</figDesc><graphic coords="9,107.32,57.02,380.59,117.94" type="bitmap" /></figure>
<figure xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Visualization of the attention maps of the GoogLeNetBN and VGG16 for the PV sample. The upper and lower row images are the output visualization of the model pre-trained with ImageNet and PlantCLEF2015 respectively.</figDesc><graphic coords="10,115.26,57.00,364.75,405.50" type="bitmap" /></figure>
<figure xml:id="fig_6"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Examples of pepper bell leaves that are confused with (a) Haunglongbing (Citrus greening) and (b) Target spot disease.</figDesc><graphic coords="11,315.95,199.82,232.32,69.36" type="bitmap" /></figure>
<figure type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc>Description of the PV dataset. Note that (%) is the percentage per class.</figDesc><table><row><cell>Class</cell><cell>Plant common name</cell><cell>Disease common name</cell><cell>Disease scientific name</cell><cell>Train</cell><cell>(%)</cell><cell>Test</cell><cell>(%)</cell></row><row><cell>C1</cell><cell>Apple</cell><cell>Apple scab</cell><cell>Venturia inaequalis</cell><cell>498</cell><cell>1.14</cell><cell /><cell>1.26</cell></row><row><cell>C2</cell><cell>Apple</cell><cell>Black rot</cell><cell>Diplodia seriata</cell><cell>484</cell><cell>1.10</cell><cell /><cell>1.31</cell></row><row><cell>C3</cell><cell>Apple</cell><cell>Cedar apple rust</cell><cell>Gymnosporangium juniperi-virginianae</cell><cell>220</cell><cell>0.50</cell><cell /><cell>0.52</cell></row><row><cell>C4</cell><cell>Apple</cell><cell>Healthy</cell><cell>-</cell><cell>1336</cell><cell>30.5</cell><cell /><cell>2.94</cell></row><row><cell>C5</cell><cell>Blueberry</cell><cell>Healthy</cell><cell>-</cell><cell>1231</cell><cell>2.81</cell><cell /><cell>2.58</cell></row><row><cell>C6</cell><cell>Cherry (including sour)</cell><cell>Powdery mildew</cell><cell>Podosphaera clandestina</cell><cell>948</cell><cell>2.16</cell><cell /><cell>0.99</cell></row><row><cell>C7</cell><cell>Cherry (including sour)</cell><cell>Healthy</cell><cell>-</cell><cell>703</cell><cell>1.60</cell><cell /><cell>1.44</cell></row><row><cell>C8</cell><cell>Corn (maize)</cell><cell>Cercospora leaf spot Gray leaf spot</cell><cell>Cercospora zeae-maydis</cell><cell>409</cell><cell>0.93</cell><cell /><cell>0.99</cell></row><row><cell>C9</cell><cell>Corn (maize)</cell><cell>Common rust</cell><cell>Puccinia sorghi</cell><cell>954</cell><cell>2.18</cell><cell /><cell>2.27</cell></row><row><cell>C10</cell><cell>Corn (maize)</cell><cell>Northern Leaf Blight</cell><cell>Setosphaeria turcica</cell><cell>798</cell><cell>1.82</cell><cell /><cell>1.78</cell></row><row><cell>C11</cell><cell>Corn (maize)</cell><cell>Healthy</cell><cell>-</cell><cell>934</cell><cell>2.13</cell><cell /><cell>2.17</cell></row><row><cell>C12</cell><cell>Grape</cell><cell>Black rot</cell><cell>Guignardia bidwellii</cell><cell>884</cell><cell>2.02</cell><cell /><cell>2.82</cell></row><row><cell>C13</cell><cell>Grape</cell><cell>Esca (Black Measles)</cell><cell>Togninia minima</cell><cell>1099</cell><cell>2.51</cell><cell /><cell>2.71</cell></row><row><cell>C14</cell><cell>Grape</cell><cell>Leaf blight (Isariopsis Leaf Spot)</cell><cell>Pseudocercospora vitis</cell><cell>828</cell><cell>1.89</cell><cell /><cell>2.36</cell></row><row><cell>C15</cell><cell>Grape</cell><cell>Healthy</cell><cell>-</cell><cell>341</cell><cell>0.78</cell><cell /><cell>0.78</cell></row><row><cell>C16</cell><cell>Orange</cell><cell>Haunglongbing (Citrus greening)</cell><cell>Liberibacter asiaticus</cell><cell>4361</cell><cell>9.95</cell><cell /><cell>10.92</cell></row><row><cell>C17</cell><cell>Peach</cell><cell>Bacterial spot</cell><cell>Xanthomonas arboricola</cell><cell>1819</cell><cell>4.15</cell><cell /><cell>4.55</cell></row><row><cell>C18</cell><cell>Peach</cell><cell>Healthy</cell><cell>-</cell><cell>280</cell><cell>0.64</cell><cell /><cell>0.76</cell></row><row><cell>C19</cell><cell>Pepper bell</cell><cell>Bacteria spot</cell><cell>Xanthomonas campestris</cell><cell>781</cell><cell>1.78</cell><cell /><cell>2.06</cell></row><row><cell>C20</cell><cell>Pepper bell</cell><cell>Healthy</cell><cell>-</cell><cell>1267</cell><cell>2.89</cell><cell /><cell>2.01</cell></row><row><cell>C21</cell><cell>Potato</cell><cell>Early blight</cell><cell>Alternaria solani</cell><cell>824</cell><cell>1.88</cell><cell /><cell>1.68</cell></row><row><cell>C22</cell><cell>Potato</cell><cell>Late blight</cell><cell>Phytophthora infestans</cell><cell>768</cell><cell>1.75</cell><cell /><cell>2.21</cell></row><row><cell>C23</cell><cell>Potato</cell><cell>Healthy</cell><cell>-</cell><cell>116</cell><cell>0.26</cell><cell /><cell>0.34</cell></row><row><cell>C24</cell><cell>Raspberry</cell><cell>Healthy</cell><cell>-</cell><cell>208</cell><cell>0.47</cell><cell /><cell>1.55</cell></row><row><cell>C25</cell><cell>Soybean</cell><cell>Healthy</cell><cell>-</cell><cell>4202</cell><cell>9.59</cell><cell /><cell>8.46</cell></row><row><cell>C26</cell><cell>Squash</cell><cell>Powdery mildew</cell><cell>Podosphaera xanthii (including Erysiphe cichoracearum)</cell><cell>1503</cell><cell>3.43</cell><cell /><cell>3.16</cell></row><row><cell>C27</cell><cell>Strawberry</cell><cell>Leaf scorch</cell><cell>Diplocarpon earlianum</cell><cell>931</cell><cell>2.13</cell><cell /><cell>1.70</cell></row><row><cell>C28</cell><cell>Strawberry</cell><cell>Healthy</cell><cell>-</cell><cell>388</cell><cell>0.89</cell><cell /><cell>0.65</cell></row><row><cell>C29</cell><cell>Tomato</cell><cell>Bacteria spot</cell><cell>Xanthomonas campestris</cell><cell>1739</cell><cell>3.97</cell><cell /><cell>3.70</cell></row><row><cell>C30</cell><cell>Tomato</cell><cell>Early blight</cell><cell>Alternaria solani</cell><cell>839</cell><cell>1.92</cell><cell /><cell>1.53</cell></row><row><cell>C31</cell><cell>Tomato</cell><cell>Late blight</cell><cell>Phytophthora infestans</cell><cell>1560</cell><cell>3.56</cell><cell /><cell>3.33</cell></row><row><cell>C32</cell><cell>Tomato</cell><cell>Leaf mold</cell><cell>Mycovellosiella fulva</cell><cell>768</cell><cell>1.75</cell><cell /><cell>1.75</cell></row><row><cell>C33</cell><cell>Tomato</cell><cell>Septoria leaf spot</cell><cell>Septoria lycopersici</cell><cell>1456</cell><cell>3.32</cell><cell /><cell>3.00</cell></row><row><cell>C34</cell><cell>Tomato</cell><cell>Spider mites Two-spotted spider mite</cell><cell>Tetranychidae spp.</cell><cell>1312</cell><cell>2.99</cell><cell /><cell>3.47</cell></row><row><cell>C35</cell><cell>Tomato</cell><cell>Target Spot</cell><cell>Corynespora cassiicola</cell><cell>1136</cell><cell>2.59</cell><cell /><cell>2.55</cell></row><row><cell>C36</cell><cell>Tomato</cell><cell>Tomato Yellow Leaf Curl Virus</cell><cell>TYLCV</cell><cell>4312</cell><cell>9.84</cell><cell /><cell>9.96</cell></row><row><cell>C37</cell><cell>Tomato</cell><cell>Tomato mosaic virus</cell><cell>ToMV</cell><cell>307</cell><cell>0.70</cell><cell /><cell>0.63</cell></row><row><cell>C38</cell><cell>Tomato</cell><cell>Healthy</cell><cell>-</cell><cell>1266</cell><cell>2.89</cell><cell /><cell>3.10</cell></row><row><cell>Total</cell><cell /><cell /><cell /><cell>43,810</cell><cell>100</cell><cell>10,495</cell><cell>100</cell></row></table></figure>
<figure type="table" xml:id="tab_1"><head>Table 2</head><label>2</label><figDesc>Distribution of diseases for different types of crops. Note that, (%) is the percentage of images per row/column.</figDesc><table><row><cell>Bacterial</cell><cell>spot</cell><cell /><cell /><cell /><cell /><cell /><cell /><cell /><cell>2297</cell><cell>997</cell><cell /><cell /><cell /><cell /><cell /><cell>2127</cell><cell>5421</cell><cell>9.98</cell><cell>(%)</cell><cell /><cell>5.84</cell><cell>2.77</cell><cell>3.51</cell><cell>7.09</cell><cell>7.48</cell><cell>10.14</cell><cell>4.89</cell><cell>4.56</cell><cell>3.96</cell><cell>0.68</cell><cell>9.37</cell><cell>3.38</cell><cell>2.88</cell><cell>33.44</cell><cell>100</cell></row><row><cell>Haunglong-</cell><cell>bing (Citrus</cell><cell>greening)</cell><cell /><cell /><cell /><cell /><cell /><cell>5507</cell><cell /><cell /><cell /><cell /><cell /><cell /><cell /><cell /><cell>5507</cell><cell>10.14</cell><cell>Total</cell><cell /><cell>3171</cell><cell>1502</cell><cell>1906</cell><cell>3852</cell><cell>4062</cell><cell>5507</cell><cell>2657</cell><cell>2475</cell><cell>2152</cell><cell>371</cell><cell>5090</cell><cell>1835</cell><cell>1565</cell><cell>18,160</cell><cell>54,305</cell><cell>100</cell></row><row><cell>Leaf blight</cell><cell>(Isariopsis</cell><cell>Leaf Spot)</cell><cell /><cell /><cell /><cell /><cell /><cell /><cell /><cell /><cell /><cell /><cell /><cell /><cell /><cell /><cell>1076</cell><cell>1.98</cell><cell>Tomato</cell><cell>mosaic virus</cell><cell /><cell /><cell /><cell /><cell /><cell /><cell /><cell /><cell /><cell /><cell /><cell>373</cell><cell>373</cell><cell>0.69</cell></row><row><cell>Esca (Black</cell><cell>Measles)</cell><cell /><cell /><cell /><cell /><cell /><cell /><cell /><cell /><cell /><cell /><cell /><cell /><cell /><cell /><cell /><cell>1383</cell><cell>2.55</cell><cell>Tomato</cell><cell>Yellow Leaf</cell><cell>Curl Virus</cell><cell /><cell /><cell /><cell /><cell /><cell /><cell /><cell /><cell /><cell /><cell>5357</cell><cell>5357</cell><cell>9.86</cell></row><row><cell>Northern</cell><cell>leaf blight</cell><cell /><cell /><cell /><cell /><cell>985</cell><cell /><cell /><cell /><cell /><cell /><cell /><cell /><cell /><cell /><cell /><cell>985</cell><cell>1.81</cell><cell>Target Spot</cell><cell /><cell /><cell /><cell /><cell /><cell /><cell /><cell /><cell /><cell /><cell /><cell /><cell>1404</cell><cell>1404</cell><cell>2.59</cell></row><row><cell>Powdery Cercospora Common</cell><cell>mildew leaf spot rust</cell><cell>Gray leaf</cell><cell>spot</cell><cell /><cell>1052</cell><cell>513 1192</cell><cell /><cell /><cell /><cell /><cell /><cell /><cell /><cell>1835</cell><cell /><cell /><cell>2887 513 1192</cell><cell>5.32 0.94 2.20</cell><cell>Septoria leaf Spider mites</cell><cell>spot Two-spotted</cell><cell>spider mite</cell><cell /><cell /><cell /><cell /><cell /><cell /><cell /><cell /><cell /><cell /><cell>1771 1676</cell><cell>1771 1676</cell><cell>3.26 3.09</cell></row><row><cell>Cedar apple</cell><cell>rust</cell><cell /><cell>275</cell><cell /><cell /><cell /><cell /><cell /><cell /><cell /><cell /><cell /><cell /><cell /><cell /><cell /><cell>275</cell><cell>0.51</cell><cell>Leaf Mold</cell><cell /><cell /><cell /><cell /><cell /><cell /><cell /><cell /><cell /><cell /><cell /><cell /><cell>952</cell><cell>952</cell><cell>1.75</cell></row><row><cell>Black rot</cell><cell /><cell /><cell>621</cell><cell /><cell /><cell /><cell /><cell /><cell /><cell /><cell /><cell /><cell /><cell /><cell /><cell /><cell>1801</cell><cell>3.32</cell><cell>Leaf scorch</cell><cell /><cell /><cell /><cell /><cell /><cell /><cell /><cell /><cell /><cell /><cell /><cell /><cell>1109</cell><cell>1109</cell><cell>2.04</cell></row><row><cell>Apple scab</cell><cell /><cell /><cell>630</cell><cell /><cell /><cell /><cell /><cell /><cell /><cell /><cell /><cell /><cell /><cell /><cell /><cell /><cell>630</cell><cell>1.16</cell><cell>Late blight</cell><cell /><cell /><cell /><cell /><cell /><cell /><cell /><cell /><cell /><cell>1000</cell><cell /><cell /><cell>1909</cell><cell>2909</cell><cell>5.36</cell></row><row><cell>Healthy</cell><cell /><cell /><cell>1645</cell><cell>1502</cell><cell>854</cell><cell>1162</cell><cell /><cell /><cell>360</cell><cell>1478</cell><cell>152</cell><cell>371</cell><cell>5090</cell><cell /><cell>456</cell><cell>1591</cell><cell>15,084</cell><cell>27.78</cell><cell>Early blight</cell><cell /><cell /><cell /><cell /><cell /><cell /><cell /><cell /><cell /><cell>1000</cell><cell /><cell /><cell>1000</cell><cell>2000</cell><cell>3.68</cell></row><row><cell /><cell /><cell /><cell>Apple</cell><cell>Blueberry</cell><cell>Cherry</cell><cell>Corn</cell><cell>Grape</cell><cell>Orange</cell><cell>Peach</cell><cell>Pepper bell</cell><cell>Potato</cell><cell>Raspberry</cell><cell>Soybean</cell><cell>Squash</cell><cell>Strawberry</cell><cell>Tomato</cell><cell>Total</cell><cell>(%)</cell><cell /><cell /><cell>Apple</cell><cell>Blueberry</cell><cell>Cherry</cell><cell>Corn</cell><cell>Grape</cell><cell>Orange</cell><cell>Peach</cell><cell>Pepper bell</cell><cell>Potato</cell><cell>Raspberry</cell><cell>Soybean</cell><cell>Squash</cell><cell>Strawberry</cell><cell>Tomato</cell><cell>Total</cell><cell>(%)</cell></row></table></figure>
<figure type="table" xml:id="tab_2"><head>Table 3</head><label>3</label><figDesc>Description of the IPM images.</figDesc><table><row><cell cols="2">Class Plant common</cell><cell>Disease common</cell><cell>Disease scientific name</cell><cell>Test</cell></row><row><cell /><cell>name</cell><cell>name</cell><cell /><cell /></row><row><cell>C1</cell><cell>Apple</cell><cell>Apple scab</cell><cell>Venturia inaequalis</cell><cell>3</cell></row><row><cell>C2</cell><cell>Apple</cell><cell>Black rot</cell><cell>Diplodia seriata</cell><cell>2</cell></row><row><cell>C3</cell><cell>Apple</cell><cell>Cedar apple rust</cell><cell>Gymnosporangium</cell><cell>3</cell></row><row><cell /><cell /><cell /><cell>juniperi-virginianae</cell><cell /></row><row><cell>C6</cell><cell>Cherry</cell><cell>Powdery mildew</cell><cell>Podosphaera clandestina</cell><cell>2</cell></row><row><cell /><cell>(including</cell><cell /><cell /><cell /></row><row><cell /><cell>sour)</cell><cell /><cell /><cell /></row><row><cell>C8</cell><cell>Corn (maize)</cell><cell>Cercospora leaf spot</cell><cell>Cercospora zeae-maydis</cell><cell>10</cell></row><row><cell /><cell /><cell>Gray leaf spot</cell><cell /><cell /></row><row><cell>C9</cell><cell>Corn (maize)</cell><cell>Common rust</cell><cell>Puccinia sorghi</cell><cell>20</cell></row><row><cell>C10</cell><cell>Corn (maize)</cell><cell>Northern Leaf Blight</cell><cell>Setosphaeria turcica</cell><cell>8</cell></row><row><cell>C12</cell><cell>Grape</cell><cell>Black rot</cell><cell>Guignardia bidwellii</cell><cell>8</cell></row><row><cell>C14</cell><cell>Grape</cell><cell>Leaf blight (Isariopsis</cell><cell>Pseudocercospora vitis</cell><cell>2</cell></row><row><cell /><cell /><cell>Leaf Spot)</cell><cell /><cell /></row><row><cell>C16</cell><cell>Orange</cell><cell>Haunglongbing</cell><cell>Liberibacter asiaticus</cell><cell>13</cell></row><row><cell /><cell /><cell>(Citrus greening)</cell><cell /><cell /></row><row><cell>C17</cell><cell>Peach</cell><cell>Bacterial spot</cell><cell>Xanthomonas arboricola</cell><cell>12</cell></row><row><cell>C19</cell><cell>Pepper bell</cell><cell>Bacteria spot</cell><cell>Xanthomonas campestris</cell><cell>2</cell></row><row><cell>C21</cell><cell>Potato</cell><cell>Early blight</cell><cell>Alternaria solani</cell><cell>5</cell></row><row><cell>C27</cell><cell>Strawberry</cell><cell>Leaf scorch</cell><cell>Diplocarpon earlianum</cell><cell>5</cell></row><row><cell>C29</cell><cell>Tomato</cell><cell>Bacteria spot</cell><cell>Xanthomonas campestris</cell><cell>5</cell></row><row><cell>C30</cell><cell>Tomato</cell><cell>Early blight</cell><cell>Alternaria solani</cell><cell>7</cell></row><row><cell>C31</cell><cell>Tomato</cell><cell>Late blight</cell><cell>Phytophthora infestans</cell><cell>4</cell></row><row><cell>C32</cell><cell>Tomato</cell><cell>Leaf mold</cell><cell>Mycovellosiella fulva</cell><cell>4</cell></row><row><cell>C33</cell><cell>Tomato</cell><cell>Septoria leaf spot</cell><cell>Septoria lycopersici</cell><cell>4</cell></row><row><cell>Total</cell><cell /><cell /><cell /><cell>119</cell></row></table></figure>
<figure type="table" xml:id="tab_3"><head>Table 4</head><label>4</label><figDesc>Description of the Bing images.</figDesc><table><row><cell cols="2">Class Plant common</cell><cell>Disease common</cell><cell>Disease scientific name</cell><cell>Test</cell></row><row><cell /><cell>name</cell><cell>name</cell><cell /></row><row><cell>C1</cell><cell>Apple</cell><cell>Apple scab</cell><cell>Venturia inaequalis</cell></row><row><cell>C2</cell><cell>Apple</cell><cell>Black rot</cell><cell>Diplodia seriata</cell></row><row><cell>C3</cell><cell>Apple</cell><cell>Cedar apple rust</cell><cell>Gymnosporangium</cell></row><row><cell /><cell /><cell /><cell>juniperi-virginianae</cell></row><row><cell>C4</cell><cell>Apple</cell><cell>Healthy</cell><cell>-</cell></row><row><cell>C7</cell><cell>Cherry</cell><cell>Healthy</cell><cell>-</cell></row><row><cell /><cell>(including</cell><cell /><cell /></row><row><cell /><cell>sour)</cell><cell /><cell /></row><row><cell>C8</cell><cell>Corn (maize)</cell><cell>Cercospora leaf spot</cell><cell>Cercospora zeae-maydis</cell></row><row><cell /><cell /><cell>Gray leaf spot</cell><cell /></row><row><cell>C9</cell><cell>Corn (maize)</cell><cell>Common rust</cell><cell>Puccinia sorghi</cell></row><row><cell>C10</cell><cell>Corn (maize)</cell><cell>Northern Leaf Blight</cell><cell>Setosphaeria turcica</cell></row><row><cell>C11</cell><cell>Corn (maize)</cell><cell>Healthy</cell><cell>-</cell></row><row><cell>C14</cell><cell>Grape</cell><cell>Leaf blight (Isariopsis</cell><cell>Pseudocercospora vitis</cell></row><row><cell /><cell /><cell>Leaf Spot)</cell><cell /></row><row><cell>C16</cell><cell>Orange</cell><cell>Haunglongbing</cell><cell>Liberibacter asiaticus</cell></row><row><cell /><cell /><cell>(Citrus greening)</cell><cell /></row><row><cell>C18</cell><cell>Peach</cell><cell>Healthy</cell><cell>-</cell></row><row><cell>C19</cell><cell>Pepper bell</cell><cell>Bacteria spot</cell><cell>Xanthomonas campestris</cell></row><row><cell>C21</cell><cell>Potato</cell><cell>Early blight</cell><cell>Alternaria solani</cell></row><row><cell>C22</cell><cell>Potato</cell><cell>Late blight</cell><cell>Phytophthora infestans</cell></row><row><cell>C24</cell><cell>Raspberry</cell><cell>Healthy</cell><cell>-</cell></row><row><cell>C28</cell><cell>Strawberry</cell><cell>Healthy</cell><cell>-</cell></row><row><cell>C29</cell><cell>Tomato</cell><cell>Bacteria spot</cell><cell>Xanthomonas campestris</cell></row><row><cell>C30</cell><cell>Tomato</cell><cell>Early blight</cell><cell>Alternaria solani</cell></row><row><cell>C31</cell><cell>Tomato</cell><cell>Late blight</cell><cell>Phytophthora infestans</cell></row><row><cell>C32</cell><cell>Tomato</cell><cell>Leaf mold</cell><cell>Mycovellosiella fulva</cell></row><row><cell>C33</cell><cell>Tomato</cell><cell>Septoria leaf spot</cell><cell>Septoria lycopersici</cell></row><row><cell>C34</cell><cell>Tomato</cell><cell>Spider mites Two-</cell><cell>Tetranychidae spp.</cell></row><row><cell /><cell /><cell>spotted spider mite</cell><cell /></row><row><cell>C35</cell><cell>Tomato</cell><cell>Target Spot</cell><cell>Corynespora cassiicola</cell></row><row><cell>C36</cell><cell>Tomato</cell><cell>Tomato Yellow Leaf</cell><cell>TYLCV</cell></row><row><cell /><cell /><cell>Curl Virus</cell><cell /></row><row><cell>C38</cell><cell>Tomato</cell><cell>Healthy</cell><cell>-</cell></row><row><cell>Total</cell><cell /><cell /><cell /></row></table></figure>
<figure type="table" xml:id="tab_4"><head>Table 5</head><label>5</label><figDesc>Classification results of CNNs trained with 3 different training configurations: fine-tuning on ImageNet pre-trained CNN (FTIN), fine-tuning on PlantCLEF2015 pretrained CNN (FTPC) and training from scratch (FS). It shows the top-1 and top-3 accuracy (%) and the average accuracy per class AVE (%) for all the test sets.</figDesc><table><row><cell>2*Architecture</cell><cell>2*Configuration</cell><cell>PV test set</cell><cell /><cell /><cell>IPM images</cell><cell /><cell /><cell>Bing images</cell><cell /><cell /></row><row><cell /><cell /><cell>top-1</cell><cell>top-3</cell><cell>AVE</cell><cell>top-1</cell><cell>top-3</cell><cell>AVE</cell><cell>top-1</cell><cell>top-3</cell><cell>AVE</cell></row><row><cell>GoogLeNetBN</cell><cell>FTIN</cell><cell>98.22</cell><cell>99.80</cell><cell>97.51</cell><cell>39.50</cell><cell>63.87</cell><cell>32.98</cell><cell>17.19</cell><cell>39.06</cell><cell>23.08</cell></row><row><cell /><cell>FTPC</cell><cell>99.09</cell><cell>99.95</cell><cell>98.83</cell><cell>42.86</cell><cell>62.18</cell><cell>39.85</cell><cell>23.44</cell><cell>43.75</cell><cell>26.92</cell></row><row><cell /><cell>FS</cell><cell>98.21</cell><cell>99.80</cell><cell>97.40</cell><cell>20.17</cell><cell>45.38</cell><cell>19.95</cell><cell>10.94</cell><cell>21.88</cell><cell>10.26</cell></row><row><cell>VGG16 (Simonyan and Zisserman, 1409)</cell><cell>FTIN</cell><cell>99.00</cell><cell>99.93</cell><cell>98.61</cell><cell>44.54</cell><cell>67.23</cell><cell>45.95</cell><cell>26.56</cell><cell>46.88</cell><cell>26.92</cell></row><row><cell /><cell>FTPC</cell><cell>98.56</cell><cell>99.90</cell><cell>97.84</cell><cell>36.13</cell><cell>64.71</cell><cell>35.43</cell><cell>28.13</cell><cell>43.75</cell><cell>33.97</cell></row><row><cell /><cell>FS</cell><cell>91.97</cell><cell>98.47</cell><cell>88.10</cell><cell>15.97</cell><cell>24.37</cell><cell>16.85</cell><cell>6.25</cell><cell>10.94</cell><cell>6.41</cell></row><row><cell>InceptionV3 (Szegedy et al., 2016)</cell><cell>FTIN</cell><cell>98.33</cell><cell>99.82</cell><cell>97.47</cell><cell>26.89</cell><cell>53.78</cell><cell>36.70</cell><cell>15.63</cell><cell>34.38</cell><cell>21.14</cell></row><row><cell /><cell>FTPC</cell><cell>98.46</cell><cell>99.89</cell><cell>97.71</cell><cell>37.82</cell><cell>65.55</cell><cell>34.36</cell><cell>23.44</cell><cell>45.31</cell><cell>27.56</cell></row><row><cell /><cell>FS</cell><cell>99.31</cell><cell>99.81</cell><cell>99.01</cell><cell>21.01</cell><cell>37.82</cell><cell>17.83</cell><cell>10.94</cell><cell>18.75</cell><cell>10.26</cell></row><row><cell>GoogLeNet (Mohanty et al., 2016)</cell><cell>FTIN</cell><cell>99.35</cell><cell>-</cell><cell>-</cell><cell>31.69</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell /><cell /></row></table></figure>
<figure type="table" xml:id="tab_5"><head>Table 6</head><label>6</label><figDesc>Performance of the VGG16 models on the disease classification problem based on S co and S cd . Note that the performance of IPM and Bing is based on only seen crop images while the unseen crop refers to Pepper_bellBacterial_spot and Pepper_bellhealthy.</figDesc><table><row><cell>Pretrained dataset</cell><cell>PV (seen) (%)</cell><cell /><cell /><cell cols="2">PV (unseen) (%)</cell><cell cols="2">IPM images (%)</cell><cell /><cell cols="2">Bing images (%)</cell><cell /></row><row><cell>(Scd/S co )</cell><cell>top-1</cell><cell>top-3</cell><cell>AVE</cell><cell>top-1</cell><cell>top-3</cell><cell>top-1</cell><cell>top-3</cell><cell>AVE</cell><cell>top-1</cell><cell>top-3</cell><cell>AVE</cell></row><row><cell>ImageNet (S co )</cell><cell>98.94</cell><cell>99.96</cell><cell>98.19</cell><cell>63.23</cell><cell>90.87</cell><cell>46.15</cell><cell>76.92</cell><cell>52.29</cell><cell>42.86</cell><cell>63.49</cell><cell>34.31</cell></row><row><cell>ImageNet (Scd)</cell><cell>98.98</cell><cell>99.96</cell><cell>98.20</cell><cell>34.89</cell><cell>79.86</cell><cell>45.30</cell><cell>74.36</cell><cell>48.54</cell><cell>39.68</cell><cell>61.90</cell><cell>35.78</cell></row><row><cell>PlantCLEF2015 (S co )</cell><cell>98.42</cell><cell>99.91</cell><cell>97.31</cell><cell>65.11</cell><cell>90.87</cell><cell>48.72</cell><cell>79.49</cell><cell>52.29</cell><cell>30.16</cell><cell>58.73</cell><cell>29.90</cell></row><row><cell>PlantCLEF2015 (Scd)</cell><cell>98.52</cell><cell>99.86</cell><cell>97.70</cell><cell>37.47</cell><cell>81.97</cell><cell>43.59</cell><cell>73.50</cell><cell>44.92</cell><cell>26.98</cell><cell>47.62</cell><cell>26.47</cell></row></table></figure>
<figure type="table" xml:id="tab_6"><head>Table 7</head><label>7</label><figDesc>Results of the individual classification of diseases related to the pepper bell crop for the PV (unseen). It presents the top-1 accuracy of the Pepper_bellBacterial_spot and Pepper_bellhealthy for the S co and S cd with different pre-training tasks.</figDesc><table><row><cell>Pepper class</cell><cell>ImageNet</cell><cell /><cell>PlantCLEF2015</cell><cell /></row><row><cell /><cell>S</cell><cell>S</cell><cell>S</cell><cell>S</cell></row><row><cell /><cell>co</cell><cell>cd</cell><cell>co</cell><cell>cd</cell></row><row><cell>bacteria spot</cell><cell>43.06</cell><cell>18.98</cell><cell>65.74</cell><cell>42.59</cell></row><row><cell>healthy</cell><cell>83.89</cell><cell>51.18</cell><cell>65.40</cell><cell>32.23</cell></row></table></figure>
<figure type="table" xml:id="tab_7"><head>Table 8</head><label>8</label><figDesc>Performance of the VGG16 models on the healthy and unhealthy classification of the PV set.</figDesc><table><row><cell>Pretraining task</cell><cell>Classifier</cell><cell>AUC</cell></row><row><cell>ImageNet</cell><cell>S co</cell><cell>0.972</cell></row><row><cell /><cell>S cd</cell><cell>0.744</cell></row><row><cell>PlantCLEF2015</cell><cell>S co</cell><cell>0.967</cell></row><row><cell /><cell>S cd</cell><cell>0.740</cell></row></table></figure>
			<note place="foot" n="2" xml:id="foot_0"><p>https://github.com/spMohanty/PlantVillage-Dataset.</p></note>
			<note place="foot" n="3" xml:id="foot_1"><p>https://www.digipathos-rep.cnptia.embrapa.br/.</p></note>
			<note place="foot" n="4" xml:id="foot_2"><p>common names of plant diseases that have been recommended based on similar symptoms of potentially different pathogens.</p></note>
			<note place="foot" n="5" xml:id="foot_3"><p>https://images.bugwood.org/.</p></note>
			<note place="foot" n="6" xml:id="foot_4"><p>https://github.com/salathegroup/plantvillage_deeplearning_paper_analysis.</p></note>
			<note place="foot" n="7" xml:id="foot_5"><p>https://github.com/AdelineMomo/CNN-plant-disease (Caffe).</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgement</head><p>This project is supported by <rs type="funder">Agropolis Fondation, Numev, Cemeb</rs>, #<rs type="projectName">DigitAG</rs>, under the reference ID 1604-019 through the &lt; <rs type="programName">&lt; Investissements davenir &gt; &gt; programme</rs> (<rs type="projectName">Labex Agro</rs>:<rs type="grantNumber">ANR-10-LABX-0001-01</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_vHHyVHM">
					<orgName type="project" subtype="full">DigitAG</orgName>
					<orgName type="program" subtype="full">&lt; Investissements davenir &gt; &gt; programme</orgName>
				</org>
				<org type="funded-project" xml:id="_BWn7AcE">
					<idno type="grant-number">ANR-10-LABX-0001-01</idno>
					<orgName type="project" subtype="full">Labex Agro</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div><head>Declaration of Competing Interest</head><p>The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.</p></div>
<div><head>Appendix A. Supplementary material</head><p>Supplementary data to this article can be found online at https:// doi.org/10.1016/j.compag.2020.105220.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A review on the main challenges in automatic plant disease identification based on visible range images</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G A</forename><surname>Barbedo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biosyst. Eng</title>
		<imprint>
			<biblScope unit="volume">144</biblScope>
			<biblScope unit="page" from="52" to="60" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Impact of dataset size and variety on the effectiveness of deep learning and transfer learning for plant disease classification</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G A</forename><surname>Barbedo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Electron. Agric</title>
		<imprint>
			<biblScope unit="volume">153</biblScope>
			<biblScope unit="page" from="46" to="53" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Factors influencing the use of deep learning for plant disease recognition</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Barbedo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biosyst. Eng</title>
		<imprint>
			<biblScope unit="volume">172</biblScope>
			<biblScope unit="page" from="84" to="91" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Annotated plant pathology databases for image-based detection and recognition of diseases</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G A</forename><surname>Barbedo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">V</forename><surname>Koenigkan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Halfeld-Vieira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">V</forename><surname>Costa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">L</forename><surname>Nechet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">V</forename><surname>Godoy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Junior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">R A</forename><surname>Patricio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Talamini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">G</forename><surname>Chitarra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Lat. Am. Trans</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="1749" to="1757" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Disease detection on the leaves of the tomato plants by using deep learning</title>
		<author>
			<persName><forename type="first">H</forename><surname>Durmus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">O</forename><surname>Gune</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Kırcı</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">6th International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017. 2017. 2017</date>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
	<note>Agro-Geoinformatics</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep learning models for plant disease detection and diagnosis</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">P</forename><surname>Ferentinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Electron. Agric</title>
		<imprint>
			<biblScope unit="volume">145</biblScope>
			<biblScope unit="page" from="311" to="318" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A robust deep-learning based detector for real-time tomato plant diseases and pests recognition</title>
		<author>
			<persName><forename type="first">A</forename><surname>Fuentes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<date type="published" when="2017">2017. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">High-performance deep neural networkbased tomato plant diseases and pests diagnosis system with refinement filter bank</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Fuentes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Front. Plant Sci</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Lifeclef plant identification task 2015</title>
		<author>
			<persName><forename type="first">H</forename><surname>Goeau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-1391/157-CR.pdf" />
	</analytic>
	<monogr>
		<title level="m">Working Notes of CLEF 2015 -Conference and Labs of the Evaluation forum</title>
		<meeting><address><addrLine>Toulouse, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-09-08">2015. September 8-11, 2015, 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep learning (adaptive computation and machine learning series)</title>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adapt. Computat. Mach. Learn. Series</title>
		<imprint>
			<biblScope unit="volume">800</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.08883</idno>
		<title level="m">Rethinking imagenet pre-training</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Salathe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.08060</idno>
		<title level="m">An open access repository of images on plant health to enable the development of mobile disease diagnostics</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Huh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.08614</idno>
		<title level="m">What makes imagenet good for transfer learning?</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno>arXiv:</idno>
		<ptr target="1502.03167" />
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">An automated detection and classification of citrus plant diseases using image processing techniques: a review</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sharif</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Ur Rehman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Javed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Electron. Agric</title>
		<imprint>
			<biblScope unit="volume">153</biblScope>
			<biblScope unit="page" from="12" to="32" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Automatic plant disease diagnosis using mobile capture devices, applied on a wheat use case</title>
		<author>
			<persName><forename type="first">A</forename><surname>Johannes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Picon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Alvarez-Gila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Echazarra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rodriguez Vaamonde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Nava Jas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ortiz-Barredo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Electron. Agric</title>
		<imprint>
			<biblScope unit="volume">138</biblScope>
			<biblScope unit="page" from="200" to="209" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Lifeclef 2015: multimedia life species identification challenges</title>
		<author>
			<persName><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Goeau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Spampinato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-P</forename><surname>Vellinga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Planque</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rauber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Palazzo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Fisher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Experimental IR Meets Multilinguality, Multimodality, and Interaction</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">CS231n Convolutional Neural Networks for Visual Recognition transfer learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<ptr target="http://cs231n.github.io/transfer-learning/" />
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="page">436</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">How deep learning extracts and learns leaf features for plant classification</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Mayo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Remagnino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recogn</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Identification of apple leaf diseases based on deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Symmetry</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Using deep learning for image based plant disease detection</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Mohanty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Salathé</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Front. Plant Sci</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">1419</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Potato disease classification using convolution neural networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Oppenheim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Shani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Anim. Biosci</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="244" to="249" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep convolutional neural networks for mobile capture device-based crop disease classification in the wild</title>
		<author>
			<persName><forename type="first">A</forename><surname>Picon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Alvarez-Gila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Seitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ortiz-Barredo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Echazarra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Johannes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Electron. Agric</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Effects of host variability on the spread of invasive forest diseases</title>
		<author>
			<persName><forename type="first">S</forename><surname>Prospero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cleary</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Forests</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">80</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep learning for image-based cassava disease detection</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ramcharan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Baranowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mccloskey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Legg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Hughes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Front. Plant Sci</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">1852</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for largescale image recognition</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deep neural networks based recognition of plant diseases by leaf image classification</title>
		<author>
			<persName><forename type="first">S</forename><surname>Slado Jevic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Arsenovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Anderla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Culibrk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Stefanovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computat. Intell. Neurosci</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<idno>arXiv:</idno>
		<ptr target="1409.4842" />
		<title level="m">Going deeper with convolutions</title>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">Wo</forename><surname>Jna</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2016.308</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2016.308.doi:10.1109/CVPR.2016.308" />
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016</title>
		<meeting><address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">June 27-30, 2016, 2016</date>
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">How convolutional neural networks diagnose plant disease</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Toda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Okura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Plant Phenomics</title>
		<imprint>
			<biblScope unit="page">9237136</biblScope>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A comparative study of fine tuning deep learning models for plant disease identification</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">C</forename><surname>Too</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yujian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Njuki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yingchun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Electron. Agric</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A comparative study of finetuning deep learning models for plant disease identification</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">C</forename><surname>Too</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yujian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Njuki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yingchun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Electron. Agric</title>
		<imprint>
			<biblScope unit="volume">161</biblScope>
			<biblScope unit="page" from="272" to="279" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Unbiased look at dataset bias</title>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1521" to="1528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Image set for deep learning: field images of maize annotated with disease symptoms</title>
		<author>
			<persName><forename type="first">T</forename><surname>Wiesner-Hanks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">L</forename><surname>Stewart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kaczmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Dechant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lipson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Gore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Res. Notes</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">440</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">How transferable are features in deep neural networks?</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Clune</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lipson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inform. Process. Syst</title>
		<imprint>
			<biblScope unit="page" from="3320" to="3328" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Understanding neural networks through deep visualization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Clune</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Fuchs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lipson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Deep Learning Workshop, International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Adaptive deconvolutional networks for mid and high level feature learning</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011">2011. 2011</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Learning deep features for scene recognition using places database</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inform. Process. Syst</title>
		<imprint>
			<biblScope unit="page" from="487" to="495" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>