<TEI xmlns="http://www.tei-c.org/ns/1.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xml:space="preserve" xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Distributed Algorithms to Find Similar Time Series</title>
			</titleStmt>
			<publicationStmt>
				<publisher />
				<availability status="unknown"><licence /></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Oleksandra</forename><surname>Levchenko</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Inria &amp; LIRMM</orgName>
								<orgName type="institution">Univ. Montpellier</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Boyan</forename><surname>Kolev</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Inria &amp; LIRMM</orgName>
								<orgName type="institution">Univ. Montpellier</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Djamel-</forename><forename type="middle">Edine</forename><surname>Yagoubi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Inria &amp; LIRMM</orgName>
								<orgName type="institution">Univ. Montpellier</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Dennis</forename><surname>Shasha</surname></persName>
							<email>shasha@cs.nyu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Inria &amp; LIRMM</orgName>
								<orgName type="institution">Univ. Montpellier</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Dep. of Computer Sc</orgName>
								<orgName type="institution">New York University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Themis</forename><surname>Palpanas</surname></persName>
							<email>themis@mi.parisdescartes.fr</email>
							<affiliation key="aff2">
								<orgName type="institution">University of Paris</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Patrick</forename><surname>Valduriez</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Inria &amp; LIRMM</orgName>
								<orgName type="institution">Univ. Montpellier</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Reza</forename><surname>Akbarinia</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Inria &amp; LIRMM</orgName>
								<orgName type="institution">Univ. Montpellier</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Florent</forename><surname>Masseglia</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Inria &amp; LIRMM</orgName>
								<orgName type="institution">Univ. Montpellier</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Distributed Algorithms to Find Similar Time Series</title>
					</analytic>
					<monogr>
						<imprint>
							<date />
						</imprint>
					</monogr>
					<idno type="MD5">BB5E6481491A75CDC5A30D2DD8C00C9F</idno>
					<idno type="DOI">10.1007/978-3-030-46133-1_51</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-04-12T14:51+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid" />
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Time series</term>
					<term>Indexing</term>
					<term>Similarity search</term>
					<term>Distributed data processing</term>
					<term>Spark</term>
				</keywords>
			</textClass>
			<abstract>
<div><p>As sensors improve in both bandwidth and quantity over time, the need for high performance sensor fusion increases. This requires both better (quasi-linear time if possible) algorithms and parallelism. This demonstration uses financial and seismic data to show how two state-of-the-art algorithms construct indexes and answer similarity queries using <software>Spark</software>. Demo visitors will be able to choose query time series, see how each algorithm approximates nearest neighbors and compare times in a parallel environment.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div><head n="1">Introduction</head><p>As hardware technology improves for sensors, the need for efficient and scaleable algorithms increases to fuse the resulting time series. Sensors produce thousands and up to billions of time series, so the first step in fusion is often to find similar time series. Applications include statistical arbitrage strategies in finance and the detection of earthquakes in seismic data.</p><p>To handle such large numbers of time series, algorithms require high performance indexing. Creating an index over billions of time series by using traditional centralized approaches is highly time consuming.</p><p>An appealing opportunity for improving performance of the index construction and similarity search on such massive sets of time series, therefore, is to take advantage of the computing power of distributed systems and parallel frameworks. However, a naive parallel implementation of existing techniques would</p><p>The research leading to these results has received funds from the European Union's Horizon 2020 Framework Programme for Research and Innovation, under grant agreement No. 732051.</p><p>under-exploit the available computing power. We have implemented parallel algorithms for two state-of-the-art approaches to construct indexes and to provide similarity search on large sets of time series by carefully distributing the work load. Our solution takes advantage of the computing power of distributed systems by using parallel frameworks, in this case <software>Spark</software>.</p></div>
<div><head n="2">Parallel Similarity Search Methods</head><p>This section reviews similarity search methods with specific attention to parallel index construction both to increase speed and improve quality.</p></div>
<div><head n="2.1">parSketch</head><p><software ContextAttributes="used">parSketch</software> <ref type="bibr" target="#b2">[3]</ref> is a parallel implementation of the sketch / random projectionbased method, both for index construction and querying. The basic idea is to multiply each time series in a database (or in a sliding window context, each window of a time series) with a set of random vectors, yielding a dot product. The vector of those dot products is a "sketch" for each time series. Then two time series can be compared by comparing sketches with approximation guarantees <ref type="bibr" target="#b0">[1]</ref> that improve the more random vectors there are.</p><p>In our implementation of this idea, given a length m time series or a window of a time series, t ∈ R m , we compute its dot product with N -1/+1 random vectors r i ∈ {1, -1} m . This results in N inner products (dot products) called the sketch (or random projection) of t i . Specifically, sketch</p><formula xml:id="formula_0">(t i ) = (t i • r 1 , t i • r 2 , ..., t i • r N ).</formula><p>We compute sketches for t 1 , ..., t b using the same random vectors r 1 , ..., r N . By the Johnson-Lindenstrauss lemma <ref type="bibr" target="#b0">[1]</ref>, the distance sketch(t i ) -sketch(t j ) is a good appproximation of t i -t j . Specifically, if sketch(t i ) -sketch(t j ) &lt; sketch(t k ) -sketch(t m ) , then it's very likely that t i -t j &lt; t k -t m . Our index is a set of grid structures to hold the time series sketches. Each grid maintains the sketch values corresponding to a specific set of random vectors over all time series. Our implementation of the sketch-based approach <software ContextAttributes="used">parSketch</software> parallelizes every step of algorithm: the computation of sketches, the creation of multiple grid structures, and the computation of pairwise similarity, thus exploiting each available core and taking full advantage of parallel data processing</p></div>
<div><head n="2.2">DPiSAX</head><p><software ContextAttributes="used">DPiSAX</software> <ref type="bibr" target="#b3">[4]</ref> is a parallel solution to construct the state-of-the-art iSAX-based index <ref type="bibr" target="#b1">[2]</ref>. The iSAX representation is based on the PAA representation which allows for dimensionality reduction while providing the important lower bounding property. The idea of PAA is to have a fixed segment size, and minimize dimensionality by using the mean values of each segment.</p><p>The SAX representation takes as input the reduced time series obtained using PAA. It discretizes this representation into a predefined set of symbols, with a given cardinality, where a symbol is a binary number. The iSAX representation uses a variable cardinality for each symbol of SAX representation, each symbol is accompanied by a number that denotes its cardinality.</p><p>Our parallel partitioned version of iSAX algorithm is based on a sampling phase that allows anticipating the distribution of time series among the computing nodes. <software>DPiSAX</software> splits the full dataset for distribution into partitions using the partition table constructed at the sampling stage. Then each worker builds an independent iSAX index on its partition.</p></div>
<div><head n="3">Experimental evaluation</head><p>In order to provide an unbiased comparison, (i) all methods were implemented using the same tools, (ii) all the experiments were run in the same pre-deployed computing environment, and (iii) on the same datasets. Applications were implemented with <software>Scala</software> and <software ContextAttributes="used">Apache Spark</software>. A distributed relational storage, set up as a number of <software ContextAttributes="used">PostgreSQL</software> instances, is used for <software ContextAttributes="used">parSketch</software> to store indexed data (grids). The implementation makes use of indexes to achieve efficient query processing. The <software ContextAttributes="used">DPiSAX</software> implementation uses an HDFS cluster to keep index data in distributed files, so that partitions of the index are stored and retrieved in parallel.</p><p>Experiments were conducted on a cluster<ref type="foot" target="#foot_0">4</ref> of 16 compute nodes with two 8 cores Intel Xeon E5-2630 v3 CPUs, 128 GB RAM, 2x558GB capacity storage per node. The cluster is running under <software ContextAttributes="used">Hadoop</software> version 2.7, <software ContextAttributes="used">Spark</software> v. 2.4 and <software ContextAttributes="used">PostgreSQL</software> v. 9.4 as a relational database system.</p><p>Search methods were evaluated over two real datasets and two synthetic ones. The real datasets are: Seismic that contains 40 million time series, and Finance with 72 million time series. For the purpose of experimentation, we generated synthetic Random Walk input datasets, whose sizes/volumes vary between 50M to 500M time series size of 256 points. At each time point, a random walk generator cumulatively adds to the value of the previous time point a random number drawn from a Gaussian distribution N(0,1). Another synthetic dataset is Random, containing 200M series, each of which is a close approximation to "white noise."</p></div>
<div><head n="4">Demonstration</head><p>The user can observe and compare search method performances on a range of input datasets. The demonstration GUI enables the user to use drop-downs to choose the input dataset and set of queries, to vary specific parameters for methods: grid cell size (affects only the output of <software ContextAttributes="used">parSketch</software>), Search type (only for <software ContextAttributes="used">DPiSAX</software> ) and then to observe the difference in performance (Figure <ref type="figure">4</ref>).</p><p>A bar chart compares 3 methods in terms of time performance per batch of queries. We use three quality metrics: (i) Quality Ratio is defined to be correlation of the 10th time series found by a particular method divided by the 10th closest time series found by direct computation of correlation. (ii) Recall is calculated as a fraction of relevant items in the top 10 time series found by particular method over the top 10 time series found by direct correlations. (iii) Mean Average Precision which considers the order of top 10 time series found by particular search method over ranked sequence of time series returned by direct correlations.</p><p>Line charts on the right side of the screen depict the top 10 time series found for the given input. The scroll bar allows the user to examine each query in the batch using visual plots and the quality ratio, for the different search methods. </p></div><figure xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Users can select input dataset and number of queries to execute, and can examine individual queries and answers, as well as quality and execution time of searches. The demonstration GUI and video are available at: http://imitates.gforge.inria.fr/</figDesc><graphic coords="5,144.01,246.05,327.35,208.05" type="bitmap" /></figure>
			<note place="foot" n="4" xml:id="foot_0"><p>http://www.grid5000.fr</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Database-friendly random projections: Johnson-lindenstrauss with binary coins</title>
		<author>
			<persName><forename type="first">D</forename><surname>Achlioptas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Comput. Syst. Sci</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Beyond one billion time series: indexing and mining very large time series collections with iSAX2+</title>
		<author>
			<persName><forename type="first">A</forename><surname>Camerra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Palpanas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Rakthanmanon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Keogh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowl. Inf. Syst</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Parcorr: efficient parallel methods to identify similar time series pairs across sliding windows</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Yagoubi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Akbarinia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kolev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Levchenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Masseglia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Valduriez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Shasha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">DMKD</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1481" to="1507" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Dpisax: Massively distributed partitioned isax</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Yagoubi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Akbarinia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Masseglia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Palpanas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Data Mining (ICDM)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>