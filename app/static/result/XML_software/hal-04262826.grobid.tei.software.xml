<TEI xmlns="http://www.tei-c.org/ns/1.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xml:space="preserve" xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Draw Me Like My Triples: Leveraging Generative AI for Wikidata Image Completion</title>
				<funder>
					<orgName type="full">Berlin</orgName>
				</funder>
				<funder ref="#_ZAD9P8S">
					<orgName type="full">Agence Nationale de la Recherche</orgName>
					<orgName type="abbreviated">ANR</orgName>
				</funder>
				<funder ref="#_gp7jK3c">
					<orgName type="full">L√§nder</orgName>
				</funder>
				<funder>
					<orgName type="full">German Federal Ministry of Education and Research (BMBF)</orgName>
				</funder>
				<funder>
					<orgName type="full">3IA C√¥te d'Azur Investments</orgName>
				</funder>
				<funder>
					<orgName type="full">French government</orgName>
				</funder>
				<funder>
					<orgName type="full">Excellence Strategy of the Federal Government</orgName>
				</funder>
				<funder ref="#_WNyFSJ7">
					<orgName type="full">European Union</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher />
				<availability status="unknown"><licence /></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Raia</forename><forename type="middle">Abu</forename><surname>Ahmad</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Deutsches Forschungszentrum f√ºr K√ºnstliche Intelligenz (DFKI GmbH)</orgName>
								<address>
									<settlement>Berlin</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Martin</forename><surname>Critelli</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University Ca'Foscari of Venice</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">≈ûefika</forename><surname>Efeoƒülu</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Freie Universit√§t Berlin</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">Technische Universit√§t Berlin</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Eleonora</forename><surname>Mancini</surname></persName>
							<affiliation key="aff4">
								<orgName type="institution">University of Bologna</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">C√©lian</forename><surname>Ringwald</surname></persName>
							<affiliation key="aff5">
								<orgName type="institution" key="instit1">Universit√© C√¥te d'Azur</orgName>
								<orgName type="institution" key="instit2">Inria</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<address>
									<region>I3S</region>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xinyue</forename><surname>Zhang</surname></persName>
							<affiliation key="aff6">
								<orgName type="institution">University of Oxford</orgName>
								<address>
									<settlement>Oxford</settlement>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Albert</forename><surname>Mero√±o-Pe√±uela</surname></persName>
							<affiliation key="aff7">
								<orgName type="institution">King's College London</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Draw Me Like My Triples: Leveraging Generative AI for Wikidata Image Completion</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">39A39D21176BB46E1B35672A57192A1B</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-04-12T14:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid" />
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Generative AI</term>
					<term>Image Generation</term>
					<term>Automated Prompt Generation</term>
				</keywords>
			</textClass>
			<abstract>
<div><p>Humans are critical for the creation and maintenance of high-quality Knowledge Graphs (KGs). However, creating and maintaining large KGs only with humans does not scale, especially for contributions based on multimedia (e.g. images) that are hard to find and reuse on the Web and expensive to generate by humans from scratch. Therefore, we leverage generative AI for the task of creating images for Wikidata items that do not have them. Our approach uses knowledge contained in Wikidata triples of items describing fictional characters and uses the fine-tuned T5 model based on the WDV dataset to generate natural text descriptions of items about fictional characters with missing images. We use those natural text descriptions as prompts for a transformer-based text-to-image model, <software>Stable Diffusion</software> v2.1, to generate plausible candidate images for Wikidata image completion. We design and implement quantitative and qualitative approaches to evaluate the plausibility of our methods, which include conducting a survey to assess the quality of the generated images.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div><head n="1.">Introduction</head><p>Large knowledge bases (KBs) such as Wikidata are maintained by human editors in a collaborative manner in order to provide structured data of high quality <ref type="bibr" target="#b0">[1]</ref>. However, given the size of this platform, there is an evident problem of incompleteness that creates several content gaps <ref type="bibr" target="#b1">[2]</ref>. We note that this is especially true for contributions based on multimedia (such as images, Wikidata'23: Wikidata workshop at ISWC 2023 Envelope raia.abu_ahmad@dfki.de (R. Abu Ahmad); martin.critelli@unive.it (M. Critelli); sefika.efeoglu@fu-berlin.de,tu-berlin.de (≈û. Efeoƒülu); e.mancini@unibo.it (E. Mancini); celian.ringwald@inria.fr (C. Ringwald); xinyue.zhang@cs.ox.ac.uk (X. Zhang); albert.merono@kcl.ac.uk (A. Mero√±o-Pe√±uela) Orcid 0009-0004-8720-0116 (R. Abu Ahmad); 0000-0002-8177-730X (M. Critelli); 0000-0002-9232-4840 (≈û. Efeoƒülu); 0000-0001-9205-3289 (E. Mancini); 0000-0002-9232-4840 (C. Ringwald); 0000-0002-9232-4840 (X. Zhang); 0000-0003-4646-5842 (A. Mero√±o-Pe√±uela) audio, and video) since it is difficult for editors to find such high-quality contributions on the Web, and even more difficult and expensive to create them from scratch. In this work, we examine the problem of missing images for a specific class of Wikidata entities: fictional characters. We motivate this choice by the fact that querying Wikidata shows that only 7% out of the 83.7K instances of the fictional character class, including its sub-classes, have an image <ref type="foot" target="#foot_0">1</ref> . It is important to note that this class was specifically chosen due to ethical and privacy concerns, as other classes (e.g. person) can have detrimental consequences if automatic images are created to portray them. Although alternative methods for finding images for this class of entities exist (e.g. fan-created images), they are unreliable in terms of the objective representation of characters and demand thorough and manual research by editors to make sure that they correctly align with Wikidata's knowledge about each entity. We propose a novel method of leveraging knowledge from Wikidata triples about each fictional character entity in order to create a representative image for it using generative artificial intelligence (AI) models. This is done by <ref type="bibr" target="#b0">(1)</ref> extracting triples from Wikidata entities, <ref type="bibr" target="#b1">(2)</ref> creating English prompts to be fed into a generative text-to-image model such as <software ContextAttributes="used">Stable Diffusion</software> <ref type="bibr" target="#b2">[3]</ref>, and (3) generating a representative image of the character that could potentially be used on Wikidata. We investigate the effectiveness of this approach by generating four different types of prompts in English, including using triple verbalisation with large language models (LLMs), for each character and comparing the resulting images. We evaluate our approach based on a ground-truth dataset that consists of fictional characters which already have an image on Wikidata. We select different metrics of automatic image comparison to measure how similar each generated image is to the ground-truth one. Additionally, since automatic measures for image comparison are limited, we conduct a human evaluation survey in which we ask participants to evaluate image similarity. Our work addresses the following research questions (RQs):</p><p>‚Ä¢ RQ1: To what extent can different types of prompts based on triples be used in text-toimage models to produce high-quality images? ‚Ä¢ RQ2: To what extent can the output of generative AI be used for Wikidata image completion? ‚Ä¢ RQ3: How can generative text-to-image models be evaluated? To the best of our knowledge, no previous study has explored the realm of using Wikidata as a source for creating prompts for generative text-to-image models. Our work 2 offers the following contributions:</p><p>‚Ä¢ A framework that generates prompts for a text-to-image model (<software>Stable Diffusion</software> v2.1 3 ) with different levels of structure and natural language text based on Wikidata triples. ‚Ä¢ A dataset of generated images for fictional characters extracted from Wikidata that can potentially be used by editors for image completion. ‚Ä¢ An evaluation strategy showing evidence of relevancy and adequacy of using AI-generated images for our use case.</p></div>
<div><head n="2.">Related Work</head><p>Generative AI: Current groundbreaking advances in AI enable machines to generate novel and original content based on textual prompts. Such generative applications include text-totext <ref type="bibr" target="#b3">[4]</ref>, text-to-image <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref>, and even text-to-music <ref type="bibr" target="#b6">[7]</ref>. Generally, these models can capture complex patterns from the input text and produce coherent outputs. A recent survey <ref type="bibr" target="#b7">[8]</ref> shows that text-to-image applications specifically have been emerging since 2015, when <software ContextAttributes="used">AlignDRAW</software> <ref type="bibr" target="#b8">[9]</ref> pioneered the field by leveraging recurrent neural networks (RNNs) to encode textual captions and produce corresponding images. Since then, end-to-end models started leveraging architectures such as deep convolutional generative adversarial networks (GANs) <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12]</ref>, autoregressive methods <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15]</ref>, latent space models <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18]</ref>, and the current state-ofthe-art diffusion-based methods <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21]</ref>.</p><p>Prompt Engineering: Because of the aforementioned advances, a novel area of prompt engineering has emerged, in which humans interact with AI in an iterative process to produce the best prompt (i.e. textual input) for a specific desired output <ref type="bibr" target="#b21">[22]</ref>. Recent work has shed light on prompt engineering for AI art specifically <ref type="bibr" target="#b22">[23]</ref>, concluding that simple and intuitive prompts written by humans are not enough to get desired results. Rather, writing good prompts is a learned skill that is enhanced by the usage of specific prompt templates <ref type="foot" target="#foot_2">4</ref> and modifiers <ref type="bibr" target="#b23">[24]</ref>. Automatic Prompt Generation: When it comes to automatic prompt generation, previous studies tend to investigate using LLMs to construct prompts using techniques such as text mining, text paraphrasing, and data augmentation <ref type="bibr" target="#b24">[25]</ref>. However, to the best of our knowledge, no work has touched upon using large KBs such as Wikidata for prompt engineering and generation.</p></div>
<div><head n="3.">Proposed Approach</head><p>We conduct our study on instances of the class designated as fictional character with Q95074 item ID. The initial stage of our approach involves the extraction of relevant triples pertaining to a specific entity. Subsequently, these triples are used to form various types of prompts in English, functioning as inputs to <software ContextAttributes="used">Stable Diffusion</software> <ref type="bibr" target="#b20">[21]</ref>, a text-to-image AI model. We generate different types of prompts related to the triples, including a verbalised triples prompt which uses the T5 language model fine-tuned on the WDV dataset <ref type="bibr" target="#b25">[26]</ref>. This verbalisation model converts triples into fluent language <ref type="bibr" target="#b26">[27]</ref>. The ultimate goal is to generate suitable images that can serve as accurate visual identifiers for their corresponding Wikidata entities. This pipeline is shown in Fig. <ref type="figure" target="#fig_0">1</ref>.</p></div>
<div><head n="3.1.">Triple Extraction</head><p>Generating an image for a specific character requires a description that can be gathered from its related triples. In Wikidata, these can be obtained through SPARQL queries <ref type="foot" target="#foot_3">5</ref> , yielding all triples with the character as the subject. Moreover, since properties and entities might be represented by item IDs and property IDs, we also extract and translate each triple from these IDs to their corresponding labels when the triple has an entity and property ID.</p></div>
<div><head n="3.2.">Prompt Generation</head><p>In order to investigate if prompts closer to natural language work better at generating better images, we generate four distinct prompts for each character. The first three are created utilising the set of triples that have been extracted for the respective entity from Wikidata, while the last prompt utilises English DBpedia abstracts. These prompts are defined as follows (see Table <ref type="table" target="#tab_9">15</ref> for prompt samples of an entity):</p><p>1. Basic Label: This prompt merely employs the "label" that Wikidata assigns to its entities.</p></div>
<div><head n="2.">Plain Triples:</head><p>This prompt is derived by concatenating the subject, predicate, and object of a triple to form a single sentence, utilising all available triples linked to a specific entity. Notably, sentences generated from plain triples may lack proper structure and grammar. 3. Verbalised Triples: Triple verbalisation is defined as the transformation of structured data (i.e., triples) into human-readable formats (i.e., text). These serve as a summarised paragraph of all input triples. 4. DBpedia Abstracts: We use DBpedia abstracts as prompts obtained by querying the English chapter of DBpedia <ref type="bibr" target="#b27">[28]</ref>. Originally written by human editors on Wikipedia, these abstracts are automatically extracted by DBpedia, preprocessed, and shortened. Unlike previous prompt types, this is the only one originally written by a human in natural language.</p><p>When examining the triples for a single entity, we observe that triples sharing the same predicate tend to contain redundant information. As a result, prompts generated directly from these plain or verbalised triples will repetitively state the same facts. However, the "instance of" predicate seems to provide distinct information for each triple. To avoid duplicating facts in prompt types (2) and (3), we remove duplicate predicates, except for "instance of", for the input triples. Among the remaining triples that share the same predicate, we keep only the one with the longest object, since longer objects likely contain more detailed information than shorter objects with the same predicate.</p></div>
<div><head n="3.3.">Image Generation</head><p>To ensure reproducibility in image generation, we utilise <software ContextAttributes="used">Stable Diffusion</software> version 2.1 <ref type="foot" target="#foot_4">6</ref> , an open-source text-to-image model developed by Stability AI limited to the English language. We chose version 2.1 because it supports all input shapes up to 1024x1024 and has a better performance according to benchmark evaluation results <ref type="bibr" target="#b28">[29]</ref>.</p><p>It is important to note that this particular model has inherent limitations when it comes to generating images related to the human body. To address this issue and enhance its image generation capabilities, we employ the implementation of negative prompts that have been suggested and shared on a public GitHub repository <ref type="foot" target="#foot_5">7</ref> . By incorporating these negative prompts, we aim to mitigate malformations in images (e.g. crossed eyes, more than five fingers, etc.). Moreover, since <software ContextAttributes="used">Stable Diffusion</software> has a limitation on the number of tokens allowed in the prompt sentence(s), we embed the prompt by utilising the encoder and tokenizer from <software ContextAttributes="used">Stable Diffusion</software>, courtesy of the Compel library <ref type="foot" target="#foot_6">8</ref> . The model runs positive prompts of 1500 fictional characters without existing images on Wikidata and 1500 with images on Wikidata, the latter to be used for building a ground-truth dataset for evaluation.</p></div>
<div><head n="4.">Collected Dataset</head><p>We construct an extensive dataset <ref type="foot" target="#foot_7">9</ref> comprising 1500 fictional characters with images, as well as 1500 fictional characters without images, which are randomly chosen from the entire set of fictional characters on Wikidata. Our motivation for collecting data on fictional characters rather than real people lies in our commitment to upholding ethical standards and safeguarding privacy. Also, there is no available dataset about fictional characters, and our data collection source codes<ref type="foot" target="#foot_8">10</ref> can be easily applied to different domains by changing parameter settings.</p><p>In addition, we extend our data by fetching the Wikipedia abstracts of each fictional character from the English chapter of DBpedia. Although a majority of these fictional characters lack information in DBpedia because it is constructed using English Wikipedia, this is not a problem in our case since the <software ContextAttributes="used">Stable Diffusion</software> model can only use English text as input. Since most of the fictional characters on Wikidata (ca. 78% <ref type="foot" target="#foot_9">11</ref> ) do not have any English Wikipedia page, we only managed to gather DBpedia abstracts for 925 fictional characters with images on Wikidata and 341 fictional characters without images (see Table <ref type="table">1</ref>). By analysing basic statistics from Table <ref type="table">1</ref>, we directly notice a big descriptive gap in terms of triples, the number of extracted unique relations, and the length of the prompts between the two datasets we constructed. Moreover, we notice that the length of the prompt is usually the shortest for verbalised triples and the longest for plain triples. After gathering the data about the fictional characters from Wikidata and DBpedia, four different prompts are automatically constructed by using the approach described in section 3. One example is shown in Figure <ref type="figure" target="#fig_1">2</ref>, which depicts the ground-truth image for the character Harlequin with its four generated images. Based on this example, it is instantly clear that some of the prompts can produce images more similar to the ground truth.</p></div>
<div><head>Table 1</head><p>Statistical information about the datasets used in the evaluation of the approach. </p></div>
<div><head>Characters with Images</head></div>
<div><head>Characters</head></div>
<div><head n="5.">Evaluation</head><p>In order to understand whether the generated images can plausibly be used for representing fictional characters based on their Wikidata triples, we employ two evaluation strategies. The first is an automatic evaluation of image similarity using different metrics, while the second is a human evaluation survey. Since the task of identifying whether two images portray the same character is subjective and difficult, we consider both qualitative and quantitative evaluation approaches. This helps us better understand the effect of the prompt type on the quality of the different generated images. In this section, we first describe the evaluation framework used, explaining the different metrics we took into account. Then, we present the obtained results.</p></div>
<div><head n="5.1.">Evaluation Framework</head></div>
<div><head n="5.1.1.">Automatic evaluation</head><p>We utilise automated evaluation methods based on three image comparison metrics:</p><p>‚Ä¢ UQI <ref type="bibr" target="#b29">[30]</ref>: computes a pixel-based similarity score by comparing generated images with their corresponding ground-truth images. Notably, since the majority of the original images are in grayscale, the similarity computation also takes into account their grayscale versions. UQI evaluates "image quality based on factors such as loss of correlation, luminance distortion, and contrast distortion" <ref type="bibr" target="#b29">[30]</ref>.</p><p>‚Ä¢ CLIPscore <ref type="bibr" target="#b30">[31]</ref>: leverages embeddings produced by a contrastive language-image pretrained model <ref type="bibr" target="#b4">[5]</ref>. It is used for measuring image-caption compatibility by comparing image and text embeddings using cosine similarity. CLIP embeddings can be used for image-to-image comparisons as well, which we did by using the image encoder of the CLIP-Visual Transformer model <ref type="bibr" target="#b31">[32]</ref>: ViT-L/14. ‚Ä¢ FID <ref type="bibr" target="#b32">[33]</ref>: is an improved version of Inception Distance (IS) proposed to measure the quality of images produced by generative models. Since the computation of the FID metric is more time-consuming than the other two, we compute it only on a small subset of our dataset consisting of images generated for ten random characters. On the other hand, UQI and CLIPscores are computed on the entire dataset. Additionally, we employ statistical methods for evaluating if the metrics above can measure the impact of the prompt on the quality of the generated images. For this purpose, we perform ANOVA to measure the effect of the prompt on the metric. We also perform Tukey's HSD (honestly significant difference) tests on the metrics to reflect the prompts' effect on the generated images. These statistical methods were computed on two subsets of our dataset: characters that have DBpedia abstracts and characters that do not. Finally, we performed several Student tests to evaluate if a given property (e.g., the gender and occupation of a character) could lead to better results, and we separately made the test only on the values of the instance of property (P31). To carry out these tests we extract the types and properties used more than 100 times. For each property, we build two subsets. The first one includes evaluation metric results of the characters that contain the property, and the second is built by randomly choosing characters that do not have the evaluated property.</p></div>
<div><head n="5.1.2.">Human evaluation</head><p>Although the above-mentioned evaluation metrics can provide automatic measures to compare images, they are still unreliable in comparing whether the generated images successfully portray the same characters as the ground-truth images. This is because noise such as the image style or its color can affect the results of the automatic metrics. Therefore, we conduct a human evaluation study in which we ask participants to rate how likely it is that a pair of images (consisting of 1. the ground-truth image and 2. the generated image) portray the same character.</p><p>Additionally, we ask participants to list the criteria they think about when comparing two images. The latter was done to get an idea of important features to look for when generating images of fictional characters. For evaluating the agreement of the participants we compute Krippendorff's Alpha <ref type="bibr" target="#b33">[34]</ref> on three levels: globally, per evaluated image, and per prompt type.</p></div>
<div><head n="5.2.">Evaluation Results</head></div>
<div><head n="5.2.1.">Automatic Evaluation</head><p>The results we obtained from the automatic evaluation metrics show different outcomes. In terms of UQI, all four prompt types yield a similar average similarity score of ca. 0.5, concluding that this metric is not optimal for our purposes. FID results (Table <ref type="table" target="#tab_0">5</ref>) show that images created from DBpedia are more similar to the ground-truth. When it comes to CLIPscores, we see a hierarchy of prompt types in terms of the obtained average similarity scores, with images generated by basic labels being the least similar (with a score of 0.48), followed by plain triples (with a score of 0.55), verbalised triples (with a score of 0.56), and lastly DBpedia abstract prompt seem to generate the most similar images to the ground-truth with a CLIPscore of 0.6. Results for UQI ad CLIP are shown in Table <ref type="table">4</ref>. It is important to note that contrary to UQI and CLIPscores, the FID metric is performed only on a subset of images generated for ten fictional characters, which makes it hard to make any concrete conclusions about this method. The ANOVA conducted on the UQI and the CLIPscores is shown in Table <ref type="table" target="#tab_1">6</ref> and Table <ref type="table" target="#tab_2">7</ref>. The results show that the UQI is not able to underline a significant difference between the prompt type as a main fixed effect on the quality of the generated image. In contrast, CLIPscores are able to reflect this effect with high confidence. The results of Tukey's HSD test are shown in Tables <ref type="table" target="#tab_3">8</ref> and<ref type="table" target="#tab_4">9</ref>. They highlight that the basic prompt is generally the worst prompt strategy and that the DBpedia abstract prompt is always the best one. However, for characters that do not have a DBpedia abstract, the verbalised triples prompt is better than the basic label and the plain triples prompts (with a p-value of 0.05810). Additionally, in order to understand if the number of relations and unique relation type attached to a given entity in extracted triples has an effect on the generated image quality, we compute the correlation between these variables with the CLIPscores. Results indicate that there is no such correlation (see Table <ref type="table" target="#tab_5">10</ref>). Finally, we present the results of the Student tests related to the effect of the values of the instance of properties attached to an entity on the quality of generated images in two parts. The first displays the effect of values of the instance of properties on the generated images in Table <ref type="table" target="#tab_7">11</ref> for plain triples prompts, and Table <ref type="table" target="#tab_8">13</ref> for verbalised triples prompts. We can see that characters that already have a widely known visual representation (e.g. characters from comics, cartoons, or movies) generally have low CLIPscores. On the other hand, characters that do not have a visual representation (e.g. from written works such as novels) are usually more similar to the ground-truth images. The second part of the Student tests deals with the effect of properties on the quality of the generated images. Results are shown in Table <ref type="table">12</ref> and Table <ref type="table">14</ref>. These results show that the majority of relations impact CLIPscores negatively.</p></div>
<div><head n="5.2.2.">Human Evaluation</head><p>To measure how humans evaluate the similarity of generated and ground-truth images, we ran an evaluation survey in which each participant is presented with images of ten different fictional characters chosen randomly from our dataset (shown in Figure <ref type="figure" target="#fig_4">5</ref>). For each character, four pairs of images were displayed. Each pair consisted of the ground-truth image and a generated image.</p><p>Participants were asked to rate how likely it is that both images portray the same character on a scale of 1-5, 1 being very unlikely and 5 being very likely. Figure <ref type="figure" target="#fig_2">3</ref> shows the distribution of participant replies for all ten characters based on prompt types. We immediately notice that images generated based on the three prompt types of basic labels, plain triples, and verbalised triples are more likely to be evaluated as not similar to the ground-truth image (i.e. the most frequent response for all three prompt types is one). On the other hand, images generated with DBpedia abstract prompts are most frequently rated as 3 and 4, both having the same number of responses. When examining the high numbers on the scale that indicate a high similarity between the ground truth and the generated images (i.e. 4 and 5), we notice a specific trend. The least frequent prompt type for those numbers is the basic label, followed by plain triples, verbalised triples, and DBpedia abstracts. Additionally, we analyse participant responses to the open question of which criteria they consider when giving their responses. Figure <ref type="figure" target="#fig_3">4</ref> presents the top ten criteria mentioned by participants. The analysis was done by extracting nouns and adjectives, and filtering out stop words and generic terms such as 'character'. We also manually grouped synonymous concepts such as clothes, clothing, and outfit. In total, our survey had 101 participants ranging between the ages of 17-59 with an average age of 30. About 57% of participants were male, 41% female, and 2% non-binary. 48% of the participants had a master's education level. We did not target any specific group since we wanted to receive general responses regarding the similarity of images. Thus, we distributed the survey among friends and colleagues both from within and outside the research community. The cultural backgrounds of participants ranged from South and North America (ca. 8%) to Europe (ca. 63%), East Asia (ca. 10%), and the Middle East (ca. 20%). That being said, ca. 25% of participants were Italian. We received 37 responses for the open question of listing relevant criteria. Finally, we measured the agreement of participants using Krippendorff's alpha. The global score is equal to 0.17, meaning that no concrete agreement was found. The same conclusion could be drawn at the level of the images presented to the participants and at the level of the prompt types used for the image generation (cf. Table <ref type="table">2</ref> and<ref type="table">Table 3</ref>).</p></div>
<div><head n="5.2.3.">Automatic and Human Evaluation Alignment</head><p>As a last step in our evaluation, we want to assess if there is an alignment between the score of the automatic metrics (UQI, CLIPscore, and FID) and the human evaluation. In order to be able to normalise the participant evaluations, we standardise the score given by each participant. For ùëñ ‚àà {1, .., 101}, the unique ID of a given participant, and ùëó ‚àà {0, 39}, a given generated image, the standardised score is computed as follows:</p><formula xml:id="formula_0">ùë• ùë†ùë°ùëéùëõùëë ùëñ,ùëó = ùë• ùëñ,ùëó -ùúá ùëñ ùúé ùëñ</formula><p>The alignment between automatic metrics and human evaluation scores is shown in figure <ref type="figure" target="#fig_5">6</ref>. We see that CLIPscores seem to be most correlated to the human scores with a Pearson correlation of 0.5 for the plain triples prompt, 0.6 for the verbalised triples prompt, and 0.7 for the basic label prompt. Concerning the DBpedia abstract prompt, none of the metrics seem to be correlated with the human evaluation. UQI and FID are not correlated to human evaluation, results, both having scores close to zero.</p></div>
<div><head n="6.">Discussion</head><p>Results of most automatic evaluation approaches we used (CLIPscores, ANOVA, and Tukey's HSD) as well as the human evaluation results suggest a clear trend: images generated using DBpedia abstracts as prompts were rated as most similar to the ground truth images, followed respectively by verbalised triples prompts, plain triples prompts, and basic label prompts. This implies that DBpedia abstracts, which are written by human editors and contain more natural, diverse, and fluent text, enable text-to-image generators to produce better results. The fact that verbalised triple prompts produce the second-best results further emphasises the importance of fluent text on the quality of the generated image. These results directly answer RQ1.</p><p>When further analysing the obtained CLIPscores, we observe that the maximum CLIPscore occurred for an image generated by using a basic label prompt, possibly indicating that the text-to-image model had "seen" this character during training. This enabled it to create a similar image to the ground-truth one without adding any additional context. However, the lowest CLIPscore also occurred when using the prompt type of basic label, further emphasising that for some characters, generating an image based only on their label is not enough. We conclude that in order to automatically generate images for fictional characters that correctly portray them, using natural text descriptions is the best option. When this text is available (e.g. in DBpedia abstracts), it is best to use it, however, as we have observed when creating our dataset, many entities of fictional characters (See Table <ref type="table">1</ref>) do not have a DBpedia abstract. To create images for those characters, the best method seems to be extracting knowledge about them in the form of triples, verbalising those triples using a large language model, and giving the verbalised text as input to a text-to-image generative model. In this case, the content of the triples is crucial for generating high-quality images. However, the quality of images is not related to the number of triples or the number of unique relations contained in the triples. But is highly dependent on object values, highlighting the impact of the value of instance of property of fictional characters on the quality of generated images. Answering RQ2, generated images can then be leveraged for completing missing images in Wikidata entities. Finally, addressing RQ3, when comparing the three automatic evaluation metrics we see that only the CLIPscores align with the human evaluation scores. This is because, unlike the human and CLIP evaluations which assess semantic similarity, the UQI and FID metrics only focus on image quality. This limitation in evaluating semantic content likely explains the discrepancy in results between the three automatic evaluation metrics.</p></div>
<div><head n="7.">Limitations and Risks</head><p>Our work is limited in many aspects. First, we are currently dealing only with English data due to the limitations of the verbalisation model and the <software>Stable Diffusion</software> model we used. Future work will consider dealing with multilingual datasets as well.</p><p>Additionally, when designing prompts based on Wikidata triples, we had to make decisions such as extracting triples based on subjects without considering objects. We also treated all triples equally with no emphasis on properties or types of entities. As shown by the open question in our human evaluation survey, it is evident that some properties are more important than others when generating images to portray a specific character. Future work can potentially explore in more depth which properties lead to better representations of characters. Further, when encountered with triples that have the same predicate, we selected the one with the longest object assuming it would contain more information. We are aware that this decision might have removed important information for characters, and this can be addressed in future work by concatenating object strings or summarising them automatically.</p><p>Our usage of the <software>Stable Diffusion</software> generative model means that our method is inheriting its biases as well. Although directly leveraging information about each character from its triples is supposed to limit biases when generating images, this cannot always be controlled (e.g., for some female entities, the model generated images of male characters). Additionally, using a predefined set of negative prompts for all characters (which includes terms such as mutilated and disfigured) is a considerable limitation of the model to correctly portray characters. A possible solution for this could be to design specific negative prompts for each individual character in a semi-automatic manner or to use another type of text-to-image model that does not require negative prompting.</p><p>Our work is also limited in terms of the ground-truth dataset constructed based on Wikidata entities that already have images. This is because, for some of these entities, the images are not reliably portraying the character, but the actor depicting the character. Finally, in order to mitigate any copyright and/or privacy risks, we stress that our method is not suggested to be directly deployed into Wikidata, as we think that using AI-generated images can potentially be very harmful. Should this method be used for image completion, we encourage clearly watermarking images as AI-generated.</p></div>
<div><head n="8.">Conclusion</head><p>In this paper, we investigate four different methods for generating prompts based on extracting knowledge in the form of triples. We then generate images based on each prompt using <software ContextAttributes="used">Stable Diffusion</software>, a generative text-to-image model. We evaluate the different prompt types by automatic as well as human evaluation approaches and conclude that the best-generated images are based on natural language text that includes the context and background of the character. When possible, this text can be extracted from a human-edited source such as DBpedia abstracts, however, most characters do not have a DBpedia entity. This brings to light the need to verbalise triples (i.e. transform them into natural text based on large language models) and use them as prompts in order to receive the best visual representation of their corresponding fictional characters. To the best of our knowledge, our work is novel in terms of utilising triples for prompt engineering in order to complete missing information on Wikidata. Possible future work includes finetuning the last <software ContextAttributes="used">Stable Diffusion</software> model via a Lora adaptation <ref type="bibr" target="#b34">[35]</ref>, trying other text-to-image models that rely on different architectures, and modifying prompts to include the most significant triples by investigating which properties affect image quality the most. Our approach is not intended to directly complete entities on Wikidata with AI-generated images, rather it can be used by editors to further enrich entities such as fictional characters, fictional places, or landscapes. Alternatively, instead of directly using the output of generative models, they could be given to artists who can use them as inspiration to create depictions of entities.     </p></div><figure xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The pipeline of our proposed KG completion process.</figDesc><graphic coords="5,130.96,84.19,333.36,88.12" type="bitmap" /></figure>
<figure xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Images for the character of Harlequin. (a) Ground truth from Wikidata. (b) Generation from the basic label prompt. (c) Generation from the plain triples prompt. (d) Generation from the verbalised triples prompt. (e) Generation from the DBpedia abstract prompt.</figDesc><graphic coords="7,162.21,340.70,270.86,170.08" type="bitmap" /></figure>
<figure xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Distribution of the human evaluation survey results for all four prompt types.</figDesc><graphic coords="10,151.79,319.91,291.69,170.08" type="bitmap" /></figure>
<figure xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Results of the human evaluation survey related to the question about the key elements that have influenced user's evaluation.</figDesc><graphic coords="11,151.80,84.19,291.68,184.25" type="bitmap" /></figure>
<figure xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: The 10 random generated images used for the Human Evaluation</figDesc><graphic coords="19,151.80,118.22,291.69,498.19" type="bitmap" /></figure>
<figure xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Correlation plots of the automatic and the human evaluation</figDesc><graphic coords="24,89.29,217.03,416.70,300.57" type="bitmap" /></figure>
<figure type="table" xml:id="tab_0"><head>Table 5</head><label>5</label><figDesc>FID metrics computed on the human evaluation subset (note that lower numbers mean higher similarity)</figDesc><table><row><cell>Fictionnal Character</cell><cell cols="4">Basic label Plain prompt Verbalised prompt DBpedia abstract</cell></row><row><cell>Lancelot (Q215681)</cell><cell>126.35087</cell><cell>54.766038</cell><cell>96.49390</cell><cell>67.89936</cell></row><row><cell>F√´anor (Q716794)</cell><cell>119.39887</cell><cell>118.62600</cell><cell>125.80051</cell><cell>123.55941</cell></row><row><cell>John Sheppard (Q923684)</cell><cell>241.30466</cell><cell>236.92710</cell><cell>274.41479</cell><cell>139.81330</cell></row><row><cell>Hoshi Sato (Q1055776)</cell><cell>228.71975</cell><cell>203.96307</cell><cell>225.44356</cell><cell>161.90406</cell></row><row><cell>Puck (Q1248616)</cell><cell>118.86526</cell><cell>157.89964</cell><cell>137.24375</cell><cell>145.40420</cell></row><row><cell>Harry Potter (Q3244512)</cell><cell>190.61544</cell><cell>217.70724</cell><cell>197.86598</cell><cell>188.29662</cell></row><row><cell>Agramante (Q3606846)</cell><cell>125.21297</cell><cell>67.034159</cell><cell>132.19362</cell><cell>110.24826</cell></row><row><cell cols="2">Mariner Moose (Q5353616) 73.558398</cell><cell>174.05515</cell><cell>104.00699</cell><cell>76.30708</cell></row><row><cell>Octobriana (Q7077012)</cell><cell>78.224097</cell><cell>209.75924</cell><cell>173.35200</cell><cell>204.09481</cell></row><row><cell>Phanuel (Q7180638)</cell><cell>164.80482</cell><cell>75.619137</cell><cell>49.82890</cell><cell>57.28145</cell></row><row><cell>Average</cell><cell>146.70551</cell><cell>151.63568</cell><cell>151.66440</cell><cell>127.48086</cell></row></table></figure>
<figure type="table" xml:id="tab_1"><head>Table 6</head><label>6</label><figDesc>Analysis of variance focused on entity with abstracts (N=914) regarding the distribution of UQI and the CLIPscore, with the prompt strategy as the main fixed effects.</figDesc><table><row><cell>Metric</cell><cell cols="3">df sum of squares mean of squares</cell><cell>F value</cell><cell>signifiance</cell></row><row><cell cols="2">CLIPscore 3</cell><cell>4.77477</cell><cell>1.59159</cell><cell cols="2">100.29899 2.18256e-62</cell></row><row><cell>UQI</cell><cell>3</cell><cell>0.02104</cell><cell>0.00701</cell><cell>0.53132</cell><cell>0.66078</cell></row></table></figure>
<figure type="table" xml:id="tab_2"><head>Table 7</head><label>7</label><figDesc>Analysis of variance focused on entity without abstracts (N=586) regarding the distribution of UQI and the CLIPscore, with the prompt strategy as the main fixed effects.</figDesc><table><row><cell>Metric</cell><cell cols="5">df sum of squares mean of squares F value signifiance</cell></row><row><cell cols="2">CLIPscore 2</cell><cell>2.29885</cell><cell>1.1494</cell><cell cols="2">77.94544 4.47908e-33</cell></row><row><cell>UQI</cell><cell>2</cell><cell>0.022125</cell><cell>0.00737</cell><cell>0.51729</cell><cell>0.59622</cell></row></table></figure>
<figure type="table" xml:id="tab_3"><head>Table 8</head><label>8</label><figDesc>Pairwise tests using Tukey HSD related to the effect of the prompt on the CLIPscore, on the subset of images having DBpedia abstracts</figDesc><table><row><cell>prompt1</cell><cell>prompt2</cell><cell>diff</cell><cell>lower</cell><cell cols="2">upper q-value p-value</cell></row><row><cell>basic prompt</cell><cell>plain prompt</cell><cell cols="3">0.05848 0.04334 0.07363 14.03695</cell><cell>0.001</cell></row><row><cell>basic prompt</cell><cell>verbalised prompt</cell><cell cols="3">0.06724 0.05209 0.08238 16.13806</cell><cell>0.001</cell></row><row><cell>basic prompt</cell><cell cols="4">dbpedia abstract prompt 0.10023 0.08508 0.11537 24.05521</cell><cell>0.001</cell></row><row><cell>plain prompt</cell><cell>verbalised prompt</cell><cell cols="3">0.00875 -0.00639 0.02390 2.10110</cell><cell>0.44767</cell></row><row><cell>plain prompt</cell><cell cols="4">dbpedia abstract prompt 0.04174 0.02659 0.05688 10.01826</cell><cell>0.001</cell></row><row><cell cols="5">verbalised prompt dbpedia abstract prompt 0.03298 0.01784 0.04813 7.91715</cell><cell>0.001</cell></row></table></figure>
<figure type="table" xml:id="tab_4"><head>Table 9</head><label>9</label><figDesc>Pairwise tests using Tukey HSD related to the effect of the prompt on the CLIPscore, on the subset of images do not having DBpedia abstracts</figDesc><table><row><cell>prompt1</cell><cell>prompt2</cell><cell>diff</cell><cell>lower</cell><cell cols="2">upper q-value p-value</cell></row><row><cell cols="2">basic prompt plain prompt</cell><cell cols="3">0.06934 0.05220 0.08649 13.41698</cell><cell>0.001</cell></row><row><cell cols="5">basic prompt verbalised prompt 0.08605 0.06890 0.10320 16.6495</cell><cell>0.001</cell></row><row><cell cols="5">plain prompt verbalised prompt 0.01670 -0.00043 0.03385 3.23256</cell><cell>0.05810</cell></row></table></figure>
<figure type="table" xml:id="tab_5"><head>Table 10</head><label>10</label><figDesc>Correlation scores between number of relation and CLIP score for triple-based prompts</figDesc><table /></figure>
<figure type="table" xml:id="tab_6"><head>var plain triple verbalised triples CLIP score Vs. Number of relations</head><label /><figDesc /><table><row><cell /><cell>0.10433</cell><cell>0.15312</cell></row><row><cell>CLIP score Vs. Number of unique relations</cell><cell>0.16794</cell><cell>0.17828</cell></row></table></figure>
<figure type="table" xml:id="tab_7"><head>Table 11</head><label>11</label><figDesc>Student tests on plain triples on the values of the "instance of" relations appearing more than 100 times</figDesc><table><row><cell>instance of</cell><cell cols="5">sample size mean with mean without t student p-value</cell></row><row><cell>graphic novel character</cell><cell>120</cell><cell>0.53759</cell><cell>0.60367</cell><cell>-4.85939</cell><cell>0.0</cell></row><row><cell>fictional character in comics</cell><cell>120</cell><cell>0.54465</cell><cell>0.60367</cell><cell>-4.14029</cell><cell>5e-05</cell></row><row><cell>comic character</cell><cell>120</cell><cell>0.54554</cell><cell>0.60367</cell><cell>-4.12248</cell><cell>5e-05</cell></row><row><cell>comics characters</cell><cell>120</cell><cell>0.54714</cell><cell>0.60367</cell><cell>-3.80711</cell><cell>0.00018</cell></row><row><cell>comic strip character</cell><cell>120</cell><cell>0.54894</cell><cell>0.60367</cell><cell>-3.78012</cell><cell>0.0002</cell></row><row><cell>cartoon character</cell><cell>151</cell><cell>0.54186</cell><cell>0.59345</cell><cell>-3.66697</cell><cell>0.00029</cell></row><row><cell>comic characters</cell><cell>120</cell><cell>0.55257</cell><cell>0.60367</cell><cell>-3.53789</cell><cell>0.00048</cell></row><row><cell>fictional man</cell><cell>604</cell><cell>0.56867</cell><cell>0.54659</cell><cell>3.25443</cell><cell>0.00117</cell></row><row><cell>fictional character appearing in</cell><cell /><cell /><cell /><cell /><cell /></row><row><cell>a film</cell><cell>146</cell><cell>0.5415</cell><cell>0.58396</cell><cell>-3.05048</cell><cell>0.0025</cell></row><row><cell>human being that only</cell><cell /><cell /><cell /><cell /><cell /></row><row><cell>exists in fictional works</cell><cell>604</cell><cell>0.56749</cell><cell>0.54659</cell><cell>2.99562</cell><cell>0.00279</cell></row><row><cell>human fictional character</cell><cell>604</cell><cell>0.5668</cell><cell>0.54659</cell><cell>2.95531</cell><cell>0.00318</cell></row><row><cell>comic book character</cell><cell>120</cell><cell>0.5613</cell><cell>0.60367</cell><cell>-2.90949</cell><cell>0.00396</cell></row><row><cell>fictional person</cell><cell>604</cell><cell>0.56611</cell><cell>0.54659</cell><cell>2.86978</cell><cell>0.00418</cell></row><row><cell>character in a book</cell><cell>311</cell><cell>0.57115</cell><cell>0.54416</cell><cell>2.81644</cell><cell>0.00501</cell></row><row><cell>animation character</cell><cell>151</cell><cell>0.55279</cell><cell>0.59345</cell><cell>-2.77323</cell><cell>0.0059</cell></row><row><cell>fictional persons</cell><cell>604</cell><cell>0.56374</cell><cell>0.54659</cell><cell>2.50207</cell><cell>0.01248</cell></row><row><cell>fictional character</cell><cell /><cell /><cell /><cell /><cell /></row><row><cell>who appears in</cell><cell /><cell /><cell /><cell /><cell /></row><row><cell>animated films, television, and</cell><cell /><cell /><cell /><cell /><cell /></row><row><cell>other animated works</cell><cell>151</cell><cell>0.55864</cell><cell>0.59345</cell><cell>-2.43599</cell><cell>0.01543</cell></row><row><cell>television show character</cell><cell>207</cell><cell>0.54562</cell><cell>0.57471</cell><cell>-2.42813</cell><cell>0.0156</cell></row><row><cell>fictional woman</cell><cell>604</cell><cell>0.5623</cell><cell>0.54659</cell><cell>2.27222</cell><cell>0.02325</cell></row><row><cell>fictional character who appears</cell><cell /><cell /><cell /><cell /><cell /></row><row><cell>in a television series</cell><cell>207</cell><cell>0.54721</cell><cell>0.57471</cell><cell>-2.27788</cell><cell>0.02325</cell></row><row><cell>human fictional characters</cell><cell>604</cell><cell>0.56201</cell><cell>0.54659</cell><cell>2.26597</cell><cell>0.02363</cell></row><row><cell>comics character</cell><cell>120</cell><cell>0.57095</cell><cell>0.60367</cell><cell>-2.22542</cell><cell>0.02699</cell></row><row><cell>fictional human</cell><cell>604</cell><cell>0.56125</cell><cell>0.54659</cell><cell>2.16103</cell><cell>0.03089</cell></row><row><cell>animated character</cell><cell>151</cell><cell>0.56282</cell><cell>0.59345</cell><cell>-2.16122</cell><cell>0.03147</cell></row><row><cell>cartoon characters</cell><cell>151</cell><cell>0.5668</cell><cell>0.59345</cell><cell>-1.91238</cell><cell>0.05678</cell></row><row><cell>TV show character</cell><cell>207</cell><cell>0.55195</cell><cell>0.57471</cell><cell>-1.8769</cell><cell>0.06124</cell></row><row><cell>TV character</cell><cell>207</cell><cell>0.55358</cell><cell>0.57471</cell><cell>-1.77022</cell><cell>0.07743</cell></row><row><cell>cinematic character</cell><cell>146</cell><cell>0.55876</cell><cell>0.58396</cell><cell>-1.7426</cell><cell>0.08246</cell></row><row><cell>fictional character appearing in</cell><cell /><cell /><cell /><cell /><cell /></row><row><cell>written works</cell><cell>311</cell><cell>0.56126</cell><cell>0.54416</cell><cell>1.72544</cell><cell>0.08495</cell></row><row><cell>character in literature</cell><cell>311</cell><cell>0.56107</cell><cell>0.54416</cell><cell>1.71958</cell><cell>0.08601</cell></row><row><cell>movie character</cell><cell>146</cell><cell>0.55973</cell><cell>0.58396</cell><cell>-1.69479</cell><cell>0.09119</cell></row><row><cell>book character</cell><cell>311</cell><cell>0.56006</cell><cell>0.54416</cell><cell>1.63574</cell><cell>0.1024</cell></row><row><cell>literary character</cell><cell>311</cell><cell>0.55985</cell><cell>0.54416</cell><cell>1.62197</cell><cell>0.10532</cell></row><row><cell>novel character</cell><cell>311</cell><cell>0.55851</cell><cell>0.54416</cell><cell>1.43847</cell><cell>0.15081</cell></row><row><cell>literature character</cell><cell>311</cell><cell>0.55855</cell><cell>0.54416</cell><cell>1.42897</cell><cell>0.15352</cell></row><row><cell>film character</cell><cell>146</cell><cell>0.56439</cell><cell>0.58396</cell><cell>-1.38642</cell><cell>0.16668</cell></row><row><cell>TV series character</cell><cell>207</cell><cell>0.55938</cell><cell>0.57471</cell><cell>-1.25831</cell><cell>0.20899</cell></row><row><cell>television series character</cell><cell>207</cell><cell>0.56432</cell><cell>0.57471</cell><cell>-0.90818</cell><cell>0.36431</cell></row><row><cell>character in a novel</cell><cell>311</cell><cell>0.553</cell><cell>0.54416</cell><cell>0.90722</cell><cell>0.36464</cell></row><row><cell>human</cell><cell /><cell /><cell /><cell /><cell /></row><row><cell>(as opposed to supernatural)</cell><cell /><cell /><cell /><cell /><cell /></row><row><cell>character in the</cell><cell /><cell /><cell /><cell /><cell /></row><row><cell>Old Testament/Hebrew Bible</cell><cell /><cell /><cell /><cell /><cell /></row><row><cell>or New Testament</cell><cell>112</cell><cell>0.53849</cell><cell>0.55244</cell><cell>-0.85578</cell><cell>0.39304</cell></row><row><cell>human biblical figure</cell><cell>112</cell><cell>0.54104</cell><cell>0.55244</cell><cell>-0.7692</cell><cell>0.44259</cell></row><row><cell>television character</cell><cell>207</cell><cell>0.56607</cell><cell>0.57471</cell><cell>-0.75946</cell><cell>0.44801</cell></row><row><cell>biblical human</cell><cell>112</cell><cell>0.54532</cell><cell>0.55244</cell><cell>-0.47913</cell><cell>0.63232</cell></row><row><cell>biblical human character</cell><cell>112</cell><cell>0.54976</cell><cell>0.55244</cell><cell>-0.17741</cell><cell>0.85935</cell></row><row><cell>human in the Bible</cell><cell>112</cell><cell>0.55174</cell><cell>0.55244</cell><cell>-0.04366</cell><cell>0.96521</cell></row></table></figure>
<figure type="table" xml:id="tab_8"><head>Table 13</head><label>13</label><figDesc>Student tests on verbalised triples on the values of the "instance of" relations appearing more than 100 times</figDesc><table><row><cell>instance of</cell><cell cols="5">sample size mean with mean without t student p-value</cell></row><row><cell>fictional person</cell><cell>604</cell><cell>0.55721</cell><cell>0.52212</cell><cell>52155</cell><cell>0</cell></row><row><cell>human being that only exists in</cell><cell /><cell /><cell /><cell /><cell /></row><row><cell>fictional works</cell><cell>604</cell><cell>0.55742</cell><cell>0.52212</cell><cell>58938</cell><cell>0</cell></row><row><cell>fictional man</cell><cell>604</cell><cell>0.56348</cell><cell>0.52212</cell><cell>5.93812</cell><cell>0</cell></row><row><cell>fictional persons</cell><cell>604</cell><cell>0.5636</cell><cell>0.52212</cell><cell>62202</cell><cell>0</cell></row><row><cell>fictional woman</cell><cell>604</cell><cell>0.56756</cell><cell>0.52212</cell><cell>6.67694</cell><cell>0</cell></row><row><cell>human fictional character</cell><cell>604</cell><cell>0.56339</cell><cell>0.52212</cell><cell>5.96221</cell><cell>0</cell></row><row><cell>human fictional characters</cell><cell>604</cell><cell>0.56621</cell><cell>0.52212</cell><cell>6.46184</cell><cell>0</cell></row><row><cell>fictional human</cell><cell>604</cell><cell>0.56144</cell><cell>0.52212</cell><cell>5.6847</cell><cell>0</cell></row><row><cell>human</cell><cell /><cell /><cell /><cell /><cell /></row><row><cell>(as opposed to supernatural)</cell><cell /><cell /><cell /><cell /><cell /></row><row><cell>character in the</cell><cell /><cell /><cell /><cell /><cell /></row><row><cell>Old Testament/Hebrew</cell><cell /><cell /><cell /><cell /><cell /></row><row><cell>Bible or New Testament</cell><cell>112</cell><cell>0.51176</cell><cell>0.59839</cell><cell>-5.70389</cell><cell>0</cell></row><row><cell>comics character</cell><cell>120</cell><cell>0.52673</cell><cell>0.58356</cell><cell>-4.15193</cell><cell>5e-05</cell></row><row><cell>human biblical figure</cell><cell>112</cell><cell>0.5395</cell><cell>0.59839</cell><cell>-3.79607</cell><cell>00019</cell></row><row><cell>fictional character who appears</cell><cell /><cell /><cell /><cell /><cell /></row><row><cell>in animated films, television,</cell><cell /><cell /><cell /><cell /><cell /></row><row><cell>and other animated works</cell><cell>151</cell><cell>0.52863</cell><cell>0.58071</cell><cell>-3.76422</cell><cell>0002</cell></row><row><cell>fictional character in comics</cell><cell>120</cell><cell>0.5314</cell><cell>0.58356</cell><cell>-3.77533</cell><cell>0002</cell></row><row><cell>fictional character appearing in</cell><cell /><cell /><cell /><cell /><cell /></row><row><cell>written works</cell><cell>311</cell><cell>0.56562</cell><cell>0.52885</cell><cell>3.72153</cell><cell>00022</cell></row><row><cell>biblical human character</cell><cell>112</cell><cell>0.54161</cell><cell>0.59839</cell><cell>-3.57517</cell><cell>00043</cell></row><row><cell>comic book character</cell><cell>120</cell><cell>0.5355</cell><cell>0.58356</cell><cell>-3.41365</cell><cell>00075</cell></row><row><cell>graphic novel character</cell><cell>120</cell><cell>0.53706</cell><cell>0.58356</cell><cell>-3.2513</cell><cell>00132</cell></row><row><cell>comic characters</cell><cell>120</cell><cell>0.54027</cell><cell>0.58356</cell><cell>-3.24203</cell><cell>00136</cell></row><row><cell>human in the Bible</cell><cell>112</cell><cell>0.55056</cell><cell>0.59839</cell><cell>-3.18845</cell><cell>00164</cell></row><row><cell>cartoon character</cell><cell>151</cell><cell>0.53912</cell><cell>0.58071</cell><cell>-38265</cell><cell>00224</cell></row><row><cell>comic character</cell><cell>120</cell><cell>0.54249</cell><cell>0.58356</cell><cell>-2.9583</cell><cell>00341</cell></row><row><cell>biblical human</cell><cell>112</cell><cell>0.55704</cell><cell>0.59839</cell><cell>-2.80093</cell><cell>00555</cell></row><row><cell>comic strip character</cell><cell>120</cell><cell>0.54284</cell><cell>0.58356</cell><cell>-2.76604</cell><cell>00612</cell></row><row><cell>character in a novel</cell><cell>311</cell><cell>0.553</cell><cell>0.52885</cell><cell>2.53108</cell><cell>01162</cell></row><row><cell>character in literature</cell><cell>311</cell><cell>0.5534</cell><cell>0.52885</cell><cell>2.5077</cell><cell>01241</cell></row><row><cell>cartoon characters</cell><cell>151</cell><cell>0.54863</cell><cell>0.58071</cell><cell>-2.43456</cell><cell>01549</cell></row><row><cell>novel character</cell><cell>311</cell><cell>0.55218</cell><cell>0.52885</cell><cell>2.37615</cell><cell>0178</cell></row><row><cell>comics characters</cell><cell>120</cell><cell>0.54978</cell><cell>0.58356</cell><cell>-2.32572</cell><cell>02087</cell></row><row><cell>animated character</cell><cell>151</cell><cell>0.54876</cell><cell>0.58071</cell><cell>-2.26202</cell><cell>02441</cell></row><row><cell>cinematic character</cell><cell>146</cell><cell>0.5397</cell><cell>0.57174</cell><cell>-2.25485</cell><cell>02489</cell></row><row><cell>literature character</cell><cell>311</cell><cell>0.55055</cell><cell>0.52885</cell><cell>2.19677</cell><cell>02841</cell></row><row><cell>movie character</cell><cell>146</cell><cell>0.53912</cell><cell>0.57174</cell><cell>-2.1486</cell><cell>03249</cell></row><row><cell>literary character</cell><cell>311</cell><cell>0.54767</cell><cell>0.52885</cell><cell>1.89849</cell><cell>0581</cell></row><row><cell>TV character</cell><cell>207</cell><cell>0.56049</cell><cell>0.53791</cell><cell>1.8604</cell><cell>06354</cell></row><row><cell>character in a book</cell><cell>311</cell><cell>0.54643</cell><cell>0.52885</cell><cell>1.76267</cell><cell>07845</cell></row><row><cell>fictional character appearing in</cell><cell /><cell /><cell /><cell /><cell /></row><row><cell>a film</cell><cell>146</cell><cell>0.54601</cell><cell>0.57174</cell><cell>-1.72412</cell><cell>08575</cell></row><row><cell>book character</cell><cell>311</cell><cell>0.54493</cell><cell>0.52885</cell><cell>1.63332</cell><cell>0.10291</cell></row><row><cell>film character</cell><cell>146</cell><cell>0.54905</cell><cell>0.57174</cell><cell>-1.56495</cell><cell>0.11868</cell></row><row><cell>fictional character who appears</cell><cell /><cell /><cell /><cell /><cell /></row><row><cell>in a television series</cell><cell>207</cell><cell>0.55602</cell><cell>0.53791</cell><cell>1.47277</cell><cell>0.14158</cell></row><row><cell>animation character</cell><cell>151</cell><cell>0.56582</cell><cell>0.58071</cell><cell>-1.1429</cell><cell>0.25399</cell></row><row><cell>television character</cell><cell>207</cell><cell>0.55208</cell><cell>0.53791</cell><cell>1.14182</cell><cell>0.25419</cell></row><row><cell>TV show character</cell><cell>207</cell><cell>0.52439</cell><cell>0.53791</cell><cell>-16176</cell><cell>0.28896</cell></row><row><cell>television show character</cell><cell>207</cell><cell>0.55049</cell><cell>0.53791</cell><cell>0.99373</cell><cell>0.32094</cell></row><row><cell>TV series character</cell><cell>207</cell><cell>0.54779</cell><cell>0.53791</cell><cell>0.78829</cell><cell>0.43098</cell></row><row><cell>television series character</cell><cell>207</cell><cell>0.54323</cell><cell>0.53791</cell><cell>0.44074</cell><cell>0.65964</cell></row></table></figure>
<figure type="table" xml:id="tab_9"><head>Table 15</head><label>15</label><figDesc>Prompt examples of an entity from the generated prompt dataset. Lance Hunter instance of fictional character in comics. Lance Hunter instance of comics character. Lance Hunter instance of comic book character. Lance Hunter instance of comic character. Lance Hunter instance of comic strip character. Lance Hunter instance of comics characters. Lance Hunter instance of comic characters. Lance Hunter instance of graphic novel character. Lance Hunter instance of human being that only exists in fictional works. Lance Hunter instance of fictional human. Lance Hunter instance of fictional person. Lance Hunter instance of fictional man. Lance Hunter instance of fictional persons. Lance Hunter instance of fictional woman. Lance Hunter instance of human fictional character. Lance Hunter instance of human fictional characters. Lance Hunter instance of TV show character. Lance Hunter instance of TV character. Lance Hunter instance of television series character. Lance Hunter instance of television show character. Lance Hunter instance of TV series character. Lance Hunter instance of fictional character who appears in a television series. Lance Hunter instance of television character. Lance Hunter present in work Marvel's Agents of S.H.I.E.L.D.. Lance Hunter from narrative universe shared fictional universe of many comic books published by Marvel Comics. Lance Hunter given name male given name. Lance Hunter sex or gender to be used in sex or gender (P21) to indicate that the human subject is a male or semantic gender (P10339) to indicate that a word refers to a male person. Lance Hunter family name Hunter family name. Verbalised Triples Lance Hunter is a fictional character in Marvel's Agents of S.H.I.E.L.D. He is a character in the TV series Marvel's Agents of S.H.I.L.D. He is also a character in the comic book genre. He is also a character in the graphic novel genre. DBpedia Abstract Lancelot Lance Hunter is a fictional character appearing in American comic books published by Marvel Comics. He first appeared in Captain Britain Weekly 19 (February 16, 1977) and was created by writer Gary Friedrich and artist Herb Trimpe. Hunter is a Royal Navy Commander who became Director of S.T.R.I.K.E. before later gaining the rank of Commodore and becoming Joint Intelligence Committee Chair. The character made his live-action debut in the Marvel Cinematic Universe television series Agents of S.H.I.E.L.D., portrayed by Nick Blood.</figDesc><table><row><cell cols="2">Fictional Character Lancelot (Q215681)</cell></row><row><cell>Basic Label</cell><cell>Lance Hunter</cell></row><row><cell>Plain Triples</cell><cell /></row></table></figure>
			<note place="foot" n="1" xml:id="foot_0"><p>This query was performed in June</p></note>
			<note place="foot" n="2023" xml:id="foot_1"><p><ref type="bibr" target="#b1">2</ref> The project and dataset are available at https://github.com/helemanc/gryffindor and at https://huggingface.co/ gryffindor-ISWS, respectively.<ref type="bibr" target="#b2">3</ref> The model card is at https://huggingface.co/stabilityai/stable-diffusion-2-1</p></note>
			<note place="foot" n="4" xml:id="foot_2"><p>https://sweet-hall-e72.notion.site/A-Traveler-s-Guide-to-the-Latent-Space-85efba7e5e6a40e5bd3cae980f30235f</p></note>
			<note place="foot" n="5" xml:id="foot_3"><p>All the SPARQL queries with detailed explanations are available at https://github.com/helemanc/gryffindor/ blob/main/src/data-collection/wiki_query_service.py.</p></note>
			<note place="foot" n="6" xml:id="foot_4"><p><software>Stable Diffusion</software> v2.1 model card: https://huggingface.co/stabilityai/stable-diffusion-2-1</p></note>
			<note place="foot" n="7" xml:id="foot_5"><p>The negative prompts: https://github.com/mikhail-bot/stable-diffusion-negative-prompts</p></note>
			<note place="foot" n="8" xml:id="foot_6"><p>Compel encodes and decodes the portion of the prompt and available at https://github.com/damian0815/compel</p></note>
			<note place="foot" n="9" xml:id="foot_7"><p>The entire dataset is available at https://huggingface.co/gryffindor-ISWS</p></note>
			<note place="foot" n="10" xml:id="foot_8"><p>The data collection codes are available at https://github.com/helemanc/gryffindor/blob/main/src/ data-collection/wiki_query_service.py</p></note>
			<note place="foot" n="11" xml:id="foot_9"><p>The percentage is computed on Sept.<ref type="bibr" target="#b5">6,</ref> 2023   </p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This project is the result of a research task force team at the <rs type="institution">International Semantic Web Summer School (ISWS)</rs> 2023. We would like to thank the organisers and tutors. We especially thank our mentor <rs type="person">Albert Mero√±o-Pe√±uela</rs> for his valuable advice and input throughout this work. We made use of the central High Performance Computer system at <rs type="institution">Freie Universit√§t Berlin</rs> to conduct the data collection and image generation parts, and we would like to express our gratitude for the resources provided. The work of the author, <rs type="person">Sefika Efeoglu</rs>, is funded by the <rs type="funder">German Federal Ministry of Education and Research (BMBF)</rs> and the state of <rs type="funder">Berlin</rs> under the <rs type="funder">Excellence Strategy of the Federal Government</rs> and the <rs type="funder">L√§nder</rs> over the project. This paper has been developed within the HE project <rs type="projectName">MuseIT</rs>, which has been cofounded by the <rs type="funder">European Union</rs> under the Grant Agreement No <rs type="grantNumber">101061441</rs>. Views and opinions expressed are, however, those of the authors and do not necessarily reflect those of the <rs type="institution">European Union</rs> or <rs type="institution">European Research Executive Agency</rs>. This work has been supported by the <rs type="funder">French government</rs>, through the <rs type="funder">3IA C√¥te d'Azur Investments</rs> in the Future project managed by the <rs type="funder">National Research Agency (ANR)</rs> with the reference number <rs type="grantNumber">ANR-19-P3IA-0002</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_gp7jK3c">
					<orgName type="project" subtype="full">MuseIT</orgName>
				</org>
				<org type="funding" xml:id="_WNyFSJ7">
					<idno type="grant-number">101061441</idno>
				</org>
				<org type="funding" xml:id="_ZAD9P8S">
					<idno type="grant-number">ANR-19-P3IA-0002</idno>
				</org>
			</listOrg>
			<div type="annex">
<div><head>A. Appendix</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Wikidata: A New Platform for Collaborative Data Collection</title>
		<author>
			<persName><forename type="first">D</forename><surname>Vrandeƒçiƒá</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st international conference on world wide web</title>
		<meeting>the 21st international conference on world wide web</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1063" to="1064" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">An Analysis of Content Gaps Versus User Needs in the Wikidata Knowledge Graph</title>
		<author>
			<persName><forename type="first">D</forename><surname>Abi√°n</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mero√±o-Pe√±uela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Simperl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Semantic Web Conference</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="354" to="374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">High-Resolution Image Synthesis With Latent Diffusion Models</title>
		<author>
			<persName><forename type="first">R</forename><surname>Rombach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Blattmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lorenz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Esser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="10684" to="10695" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<ptr target="https://openai.com/blog/chatgpt" />
		<title level="m">Introducing ChatGPT</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning Transferable Visual Models From Natural Language Supervision</title>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v139/radford21a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International Conference on Machine Learning, ICML</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Meila</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</editor>
		<meeting>the 38th International Conference on Machine Learning, ICML<address><addrLine>Virtual Event; PMLR</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021-07">2021. July 2021. 2021</date>
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page" from="8748" to="8763" />
		</imprint>
	</monogr>
	<note>Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Hierarchical Text-Conditional Image Generation with CLIP Latents</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<idno>ArXiv abs/2204.06125</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Mo√ªsai: Text-to-Music Generation with Long-Context Latent Diffusion</title>
		<author>
			<persName><forename type="first">F</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Sch√∂lkopf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2301.11757</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">S</forename><surname>Kweon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.07909</idno>
		<title level="m">Text-to-image Diffusion Models in Generative AI: A Survey</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">E</forename><surname>Mansimov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Parisotto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.02793</idno>
		<title level="m">Generating Images from Captions with Attention</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Generative Adversarial Text to Image Synthesis</title>
		<author>
			<persName><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Logeswaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1060" to="1069" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">StackGAN: Text to Photo-realistic Image Synthesis with Stacked Generative Adversarial Networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5907" to="5915" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">AttnGAN: Fine-Grained Text to Image Generation with Attentional Generative Adversarial Networks</title>
		<author>
			<persName><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1316" to="1324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Zero-Shot Text-to-Image Generation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pavlov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="8821" to="8831" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Cogview: Mastering text-to-image generation via transformers</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="19822" to="19835" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">N√ºwa: Visual Synthesis Pre-training for Neural visUal World creAtion</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Duan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2022: 17th European Conference</title>
		<meeting><address><addrLine>Tel Aviv, Israel</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">October 23-27, 2022. 2022</date>
			<biblScope unit="page" from="720" to="736" />
		</imprint>
	</monogr>
	<note>Proceedings, Part XVI</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Auto-Encoding Variational Bayes</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno>CoRR abs/1312.6114</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Vahdat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.03898</idno>
		<title level="m">NVAE: A Deep Hierarchical Variational Autoencoder</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Very Deep VAEs Generalize Autoregressive Models and Can Outperform Them on Images</title>
		<author>
			<persName><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<idno>ArXiv abs/2011.10650</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mcgrew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.10741</idno>
		<title level="m">GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding</title>
		<author>
			<persName><forename type="first">C</forename><surname>Saharia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Whang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">L</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ghasemipour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">Gontijo</forename><surname>Lopes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Karagol Ayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="36479" to="36494" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">High-Resolution Image Synthesis with Latent Diffusion Models</title>
		<author>
			<persName><forename type="first">R</forename><surname>Rombach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Blattmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lorenz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Esser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="10684" to="10695" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Design Guidelines for Prompt Engineering Text-to-Image Generative Models</title>
		<author>
			<persName><forename type="first">V</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">B</forename><surname>Chilton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems</title>
		<meeting>the 2022 CHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Oppenlaender</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Linder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Silvennoinen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.13534</idno>
		<title level="m">Prompting AI Art: An Investigation into the Creative Skill of Prompt Engineering</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Oppenlaender</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.13988</idno>
		<title level="m">A Taxonomy of Prompt Modifiers for Text-to-Image Generation</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Prompt Engineering for Healthcare: Methodologies and Applications</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.14670</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">WDV: A Broad Data Verbalisation Dataset Built from Wikidata</title>
		<author>
			<persName><forename type="first">G</forename><surname>Amaral</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Rodrigues</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Simperl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Semantic Web -ISWC 2022</title>
		<editor>
			<persName><forename type="first">U</forename><surname>Sattler</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Hogan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Keet</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">V</forename><surname>Presutti</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">P A</forename><surname>Almeida</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Takeda</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Monnin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Pirr√≤</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Amato</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="556" to="574" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Investigating Pretrained Language Models for Graph-to-Text Generation</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">F R</forename><surname>Ribeiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schmitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sch√ºtze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Gurevych</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.nlp4convai-1.20</idno>
		<ptr target="https://aclanthology.org/2021.nlp4convai-1.20.doi:10.18653/v1/2021.nlp4convai-1.20" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd Workshop on Natural Language Processing for Conversational AI, Association for Computational Linguistics</title>
		<meeting>the 3rd Workshop on Natural Language Processing for Conversational AI, Association for Computational Linguistics<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="211" to="227" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">DBpedia Abstracts: A Large-Scale, Open, Multilingual NLP Training Corpus</title>
		<author>
			<persName><forename type="first">Martin</forename><surname>Br√ºmmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Milan</forename><surname>Dojchinovski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Hellmann</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/L16-1532" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC'16), European Language Resources Association (ELRA)</title>
		<meeting>the Tenth International Conference on Language Resources and Evaluation (LREC'16), European Language Resources Association (ELRA)<address><addrLine>Portoro≈æ, Slovenia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3339" to="3343" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.10843</idno>
		<title level="m">X-IQE: eXplainable Image Quality Evaluation for Text-to-Image Generation with Visual Large Language Models</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Full-Reference Image Quality Assessment Based on an Optimal Linear Combination of Quality Measures Selected by Simulated Annealing</title>
		<author>
			<persName><forename type="first">D</forename><surname>Varga</surname></persName>
		</author>
		<idno type="DOI">10.3390/jimaging8080224</idno>
		<ptr target="https://www.mdpi.com/2313-433X/8/8/224.doi:10.3390/jimaging8080224" />
	</analytic>
	<monogr>
		<title level="j">Journal of Imaging</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">CLIPScore: A Reference-free Evaluation Metric for Image Captioning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hessel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Forbes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">Le</forename><surname>Bras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.emnlp-main.595</idno>
		<ptr target="https://aclanthology.org/2021.emnlp-main.595.doi:10.18653/v1/2021.emnlp-main.595" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics, Online and</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics, Online and<address><addrLine>Punta Cana, Dominican Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="7514" to="7528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=YicbFdNTTy" />
	</analytic>
	<monogr>
		<title level="m">9th International Conference on Learning Representations, ICLR 2021</title>
		<meeting><address><addrLine>Virtual Event, Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">May 3-7, 2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.08500</idno>
		<title level="m">GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Answering the Call for a Standard Reliability Measure for Coding Data</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Hayes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Krippendorff</surname></persName>
		</author>
		<idno type="DOI">10.1080/19312450709336664</idno>
		<ptr target="https://doi.org/10.1080/19312450709336664" />
	</analytic>
	<monogr>
		<title level="j">Communication Methods and Measures</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="77" to="89" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wallis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Allen-Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.09685</idno>
		<title level="m">LoRA: Low-Rank Adaptation of Large Language Models</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>