<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Toward Generic Abstractions for Data of Any Model</title>
				<funder ref="#_JwgXRbH #_eC4c5fX">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Nelly</forename><surname>Barret</surname></persName>
							<email>nelly.barret@inria.fr</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Inria</orgName>
								<orgName type="institution" key="instit2">Institut Polytechnique de Paris</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ioana</forename><surname>Manolescu</surname></persName>
							<email>ioana.manolescu@inria.fr</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Inria</orgName>
								<orgName type="institution" key="instit2">Institut Polytechnique de Paris</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Prajna</forename><surname>Upadhyay</surname></persName>
							<email>prajna-devi.upadhyay@inria.fr</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Inria</orgName>
								<orgName type="institution" key="instit2">Institut Polytechnique de Paris</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Toward Generic Abstractions for Data of Any Model</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">3AD859BD738BF7C72C5EF374EDC47FF5</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-04-12T14:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Digital data sharing leads to unprecedented opportunities to develop data-driven systems for supporting economic activities, the social and political life, and science. Many open-access datasets are RDF graphs, but others are CSV files, Neo4J property graphs, JSON or XML documents, etc.</p><p>Potential users need to understand a dataset in order to decide if it is useful for their goal. While some datasets come with a schema and/or documentation, this is not always the case. Data summarization or schema inference tools have been proposed, specializing in XML, or JSON, or the RDF data models. In this work, we present a dataset abstraction approach, which (𝑖) applies on relational, CSV, XML, JSON, RDF or Property Graph data; (𝑖𝑖) computes an abstraction meant for humans (as opposed to a schema meant for a parser); (𝑖𝑖𝑖) integrates Information Extraction data profiling, to also classify dataset content among a set of categories of interest to the user. Our abstractions are conceptually close to an Entity-Relationship diagram, if one allows nested and possibly heterogeneous structure within entities.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Open-access data is multiplying over the Internet. This leads on one hand, to the development of new businesses, economic opportunities and applications, and on the other hand, to circulating knowledge on a variety of topics, from health to education, environment, the arts, or leisure activities.</p><p>Many of the openly available datasets follow the RDF standard, the W3C's recommendation for sharing data. The Linked Open Data Cloud portal lists thousands of such datasets containing, e.g., national or worldwide statistics, music, scientific bibliographies, and many other interesting, open RDF graphs are not listed there. However, Open Data sets are not (or not only) RDF, as demonstrated by the following examples of very popular open datasets. (𝑖) CSV files (each of which can be seen as a table) are shared on machine learning portals such as Kaggle or the French public portal data.gouv.fr; The French national transparency database HATVP (Haute Autorité pour la Transparence de la Vie Publique) also publishes CSV files; (𝑖𝑖) relational databases, comprising several interrelated tables, are used e.g. to disseminate the DBLP bibliographic data; (𝑖𝑖𝑖) XML is the format used in hundreds of million of bibliographic notices, e.g., on PubMed; DBLP and HATVP data has also been shared as XML; (𝑖𝑣) JSON has become more recently the format of choice, used e.g. to describe the complete activity of the French parliament on the websites NosDeputes.fr and NosSenateurs.fr; (𝑣) property graphs, such as pioneered by Neo4J, are oriented graphs whose nodes and edges may have labels and properties; this is the format used by the International Consortium of Investigative Journalists to share their investigative data.</p><p>Decades of data management research have shown that no new data model completely replaces the previous ones. Some models thought obsolete re-surface under new incarnations (think of nested relations vs. document stores, or object databases vs. property graphs). The model in which a dataset is produced or exported is decided by the producers, depending on what they understand/are familiar with, the system at their disposal for storing the data, and the needs of foreseeable data users. The Open Data exchange scenarios, where data users often lack any institutional connection to the producers, also forces users to cope with the data as it is, since the producers have no incentive (and, often, lack the resources) to restructure the data in a different format. Thus, we believe the data model variety in Open Data is here to stay.</p><p>Users who must decide whether to use a dataset in an application need to have a basic understanding of its content and the suitability to their need. Towards this goal, schemas may be available to describe the data structure, and/or documentation (text) may describe its content in natural language. As help to the users, schemas and documentations have some limitations: (𝑎) schemas are often unavailable, especially for semistructured data formats such as JSON, RDF or XML and documentation is also often unavailable or too terse to inform the users; (𝑏) even when they are published, or built by automated tools, e.g., <ref type="bibr">[3-5, 7-9, 15]</ref>, schemas are not helpful (or too complex) for casual users, who ignore what an "XML element", "JSON array" or "RDF property node" is; (𝑐) schemas as well as documentation describe the data according to the data producer's terminology, not according to the consumer's. For instance, in the XML dataset at the left in Figure <ref type="figure" target="#fig_0">1</ref>, a public library labels its data entries item, with the implicit knowledge that these are documents (books, magazines, etc.), whereas from the users' perspective, this dataset describes books; (𝑑) by design, schemas do not quantitatively reflect the data, whereas such information could be very useful as a first insight on the data. Towards a data model-independent dataset abstraction To facilitate the understanding of a dataset by a user, we compute a compact description of the data, focusing on its most frequent content, free of data model-specific syntactic details, and formulated in terms that interest the user. To that end, we develop a single, integrated method, applicable to any of the data models mentioned above. For example, given any of the four bibliographic datasets in Figure <ref type="figure" target="#fig_0">1</ref> and a user interested in "books", our approach would correctly identify the dataset as pertinent for the user's question. As we will explain, users can specify their categories of interest through a few hints; with the help of popular knowledge bases, our system makes suggestions to enlarge the set of hints and improve the chances of users to have an accurate description of their dataset. We proceed in several steps, which also organize the remainder of the paper.</p><p>(1.) We view each dataset as holding records, that is: objects with some internal structure, simple or complex, representing a concept or an object. For instance, the XML data in Figure <ref type="figure" target="#fig_0">1</ref> contains two item records. Further, we identify collections grouping similar records (some records may belong to no collection). For instance, the bibliography XML element in Figure <ref type="figure" target="#fig_0">1</ref> can be seen as a collection which holds the two records. We identify a set of requirements for our record and collection detection method, and formalize the problem of deriving them automatically from a dataset (Section 2).</p><p>(2.) We propose an algorithm for automatically identifying records and collections in a dataset of any of the supported data models (Section 3). For instance, given the XML dataset in Figure <ref type="figure" target="#fig_0">1</ref> (or the relational or the RDF dataset from the same figure), our algorithm understands it as a collection of records, each record corresponding to a book, and having: a title, a collection of authors, and possibly a collection of reviews.</p><p>(3.) We separate collections of Entities from collections of Relationships, in order to report to the user a structured, meaningful dataset abstraction (Section 4).</p><p>(4.) To help users understand the data in their terms, we attempt to assign each collection to a category from a predefined category set, derived from a general-purpose ontology and/or based on the concepts of interest to users, e.g., books in the above example. We provide a method for our tool to enrich its knowledge of userspecified categories by selectively gathering information from large online knowledge bases. For instance, our algorithm classifies the bibliography in Figure <ref type="figure" target="#fig_0">1</ref> as a Creative Work (a Schema.org standard type including books, paintings, songs, etc.) Deployment scenarios On one hand, data producers can use our tool to automatically derive data abstractions, to be shared next to the data; on the other hand, users can generate the abstractions after downloading a candidate dataset, in order to assess its interest.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">PROBLEM STATEMENT</head><p>We start by analyzing a set of requirements of our problem, then formally state it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Requirements</head><p>R1: data model-independent abstractions Our method needs to go beyond the syntactic details to extract semantically meaningful records, and understand if they are organized in collections. R2: structurally rich abstractions The above problem can be seen as reverse-engineering a dataset to identify its conceptual model, which generalizes the classic Entity-Relationship model behind relational databases <ref type="bibr" target="#b12">[13]</ref> by allowing records to have multivalued attributes, and to contain other records and/or collections. For instance, the second item record in Figure <ref type="figure" target="#fig_0">1</ref> contains a collection of review records. A collection uniformly represents a list, set or bag of records: data order and possible duplicates are not reflected in our abstractions. R3: see beyond the data structures In some cases, data syntax features are insufficient to distinguish records from collections. On one hand, in some data models, such as XML or RDF, the syntax does not distinguish them, e.g., in XMark <ref type="bibr" target="#b13">[14]</ref> benchmark documents, an ⟨open_auctions⟩ element is clearly a collection, while a ⟨user⟩ element describes a record. On the other hand, even when the data model distinguishes nodes that should naturally be records (e.g., JSON maps), from others that should be collections (e.g., JSON arrays), we should not make this decision purely based on syntax. Indeed, as also noted in <ref type="bibr" target="#b14">[15]</ref>, a short array could in fact designate an object (e.g., three coordinates describe a geographical point), while a map may be used to encode a list, e.g., with attributes named "1", "2", "3" etc., as in Le Monde's Decodex dataset. It is important that such variations in data design do not confuse our abstraction. R4: implicit or explicit collections In our motivating example, each of the four datasets holds a collection of books. In XML, the collection is explicit (materialized by the ⟨bibliography⟩ node), and the table node labeled Book plays the same role in the relational dataset. In contrast, in the RDF dataset, there is no common parent of the books; we say that the collection here is implicit. Note that whether collections are explicit or implicit does not depend on the original data model: in the relational dataset, the book collection is explicit but the review collection is not; similarly, the RDF dataset could have included a common parent to all the book nodes, which would have made that collection explicit. Availability and role of schemas and types What schema information can we expect to have, and how should we treat it?</p><p>Relational databases always have a schema, describing elementary data types, the attributes of each table, and possible integrity constraints. In a CSV file, the number of attributes can be identified easily; their names may or may not be present, and data types are not explicitly declared; data profiling <ref type="bibr" target="#b0">[1]</ref> is needed to infer their domains. XML documents may or may not have a schema, expressed as a Document Type Description (DTD) or XML Schema Description (XSD); these specify the allowed children for each type of element. JSON documents usually come without a schema, but recent methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b14">15]</ref> derive schemas from the documents. RDF graphs are often schemaless, but they may be endowed with: (𝑖) an ontology, expressed in RDF Schema or OWL, describing relationships between the types and properties present in the graph, e.g., Any Student is a Person, or Anyone taking a Course is a Student; (𝑖𝑖) a SHACL (Shapes Constraint Language) schema specification, against which a graph may be validated or not. Schemas for property graphs are being investigated actively <ref type="bibr" target="#b9">[10]</ref>, although no standard has emerged yet.</p><p>Data types are basic components of schemas. When present, types encapsulate valuable insights into the data organization and semantics. Thus, we formulate the following requirement: R5: explicit types When available, types should guide our identification of records and collections, even though our approach should not depend on them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Abstraction approach</head><p>To satisfy requirement R1, we leverage the ConnectionLens system <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b5">6]</ref> which models the information from any relational database, CSV, XML, JSON, RDF document, or property graph, as a graph 𝐺 = (𝑁 , 𝐸) where 𝐸 ⊆ 𝑁 × 𝑁 is a set of directed edges, and 𝜆 𝑁 , 𝜆 𝐸 are two functions labeling each node (respectively, edge) with a label (a string), that could in particular be 𝜖 (the empty label). Figure <ref type="figure" target="#fig_1">2</ref> illustrates this; for now, just focus on the nodes and edges (their colors and the shaded areas will be explained later).</p><p>Relational data A relational table or a CSV dataset can be seen as a set of tuples, each with the same attributes. This can be turned into a graph by modeling the dataset as a table node, having one child tuple node for each tuple (or line in the CSV file); this child has one child attribute node for each attribute. Figure <ref type="figure" target="#fig_1">2</ref> illustrates this for the sample relational dataset in Figure <ref type="figure" target="#fig_0">1</ref>. Following R5, we leverage foreign key constraints expressed in a relational database schema as follows. Whenever relation 𝑆 includes a foreign key to relation 𝑅, an edge is created leading from each tuple in 𝑆, to the respective 𝑅 tuple node.</p><p>XML and JSON data are naturally converted into trees. RDF data Each triple (𝑠, 𝑝, 𝑜) from an RDF graph is converted into an edge between the (single) node labeled 𝑠 to the (single) node labeled 𝑜; the edge is labeled 𝑝. We denote by 𝜏 the special RDF type property used to explicitly connect an URI to its type (which is also a node in the graph).</p><p>Property graphs (PG) In this rich, directed graph data model: (𝑖) each node may have a set of attributes with a name and a value; (𝑖𝑖) each edge can similarly have attributes; (𝑖𝑖𝑖) zero or more labels may be attached to each node and/or edge, playing roughly the role of a type. In our graphs, each node/edge has at most one label. Therefore, we transform a property graph as follows. Each PG node becomes a node 𝑛 ∈ 𝑁 , labeled 𝜖. Each label 𝑙 of a PG node 𝑛 becomes an edge 𝑛 𝜏 -→ 𝑛 𝑙 ∈ 𝐸, where 𝑛 𝑙 is a leaf node labeled 𝑙 and 𝜏 is the RDF type property mentioned above. Each attribute of 𝑛, named 𝑎 and whose value is 𝑏, becomes an edge 𝑛 𝑎 -→ 𝑛 𝑏 ∈ 𝐸 where 𝑛 𝑏 is a leaf node labeled 𝑏. Each PG edge 𝑒 is turned into a node 𝑛 𝑒 , labeled 𝜖, plus two edges, connecting it to its source and target nodes in the original PG; 𝑛 𝑒 also has outgoing edges modeling the attributes of 𝑒, similarly to PG nodes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Extracted entities</head><p>The graph built by ConnectionLens out of any dataset is enriched through entity extraction, applied on each value (string, leaf) node present in the dataset <ref type="bibr" target="#b1">[2]</ref>. In our example, Alice, Bob, Carole and David are recognized as Person entities. The set 𝑇 𝐸 of entity types also includes: Location, Organization, Date, URIs, emails, hashtags, etc. Entities will be used to categorize collections (Section 5).</p><p>Problem statement Given the graph 𝐺 = (𝑁 , 𝐸) obtained as above, our goal is to:</p><p>(1) Identify records and collections, that is: find (𝑖) a set R ⊆ 𝑁 of nodes which we call records; (𝑖𝑖) a set of sets C = {𝐶 1 , 𝐶 Together with edges connecting them to each other and to a record 𝑟 , sub-records form the record content sought in (3b) below.</p><p>As shown above, for now, only collections of entities are classified (not those of relationships). The reason not to classify relationships is that entities ("things") seem even easier to understand for non-IT users, and it is easier for them to provide hints (see later) about entities, than about relationships.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">IDENTIFYING RECORDS AND COLLECTIONS</head><p>We start by a convenient transformation on the graph 𝐺. Some of its edges have labels (e.g., RDF triples, edges in JSON maps) while others carry the empty label 𝜖. For uniformity, we transform 𝐺 into an unlabeled graph 𝐺 ′ , replacing each labeled edge 𝑛 1 an intermediary node labeled 𝑙, and connected to 𝑛 1 , 𝑛 2 as follows:</p><formula xml:id="formula_0">𝑙 - → 𝑛 2 with</formula><formula xml:id="formula_1">𝑛 1 𝜖 -→ 𝑛 𝑙 𝜖 -→ 𝑛 2 .</formula><p>From now on, we will work on 𝐺 ′ , whose nodes and edges will be simply denoted as (𝑁 ′ , 𝐸 ′ ). Then:</p><p>(1) First, we compute a quotient summary of 𝐺 ′ , that is: we identify a partition P = {𝑁 𝑖 } 𝑖 of its nodes 𝑁 ′ , such that 𝑖 𝑁 𝑖 = 𝑁 ′ and the 𝑁 𝑖 are pairwise disjont. We say the nodes from a given set 𝑁 𝑖 are equivalent, and call 𝑁 𝑖 an equivalence class. Then, the quotient summary of 𝐺 ′ is a graph whose nodes are the equivalence classes, and such that whenever 𝐺 ′ contains an edge 𝑛 1 → 𝑛 2 , its summary contains the edge 𝐸𝐶 (𝑛 1 ) → 𝐸𝐶 (𝑛 2 ), where 𝐸𝐶 (𝑛 𝑖 ) denotes the equivalence class of 𝑛 𝑖 for 𝑖 ∈ {1, 2}. Many quotient summarization techniques have been proposed <ref type="bibr" target="#b4">[5]</ref>; we discuss some of them below. Note that while the quotient summary guides the abstraction, it may still differ from it quite substantially: a set 𝑁 𝑖 may contain records from multiple collections; it may contain an explicit collection node; finally, it may contain nodes which turn out to be in SR.</p><p>(2) Next, we compute the signature of each equivalence class, i.e.</p><p>an object reflecting the entities extracted out of the nodes of that equivalence class. This signature holds such statistics for every entity type in 𝑇 𝐸 (Section 2.2). For instance, given the XML document presented in Figure <ref type="figure" target="#fig_1">2</ref>, the signature of the equivalence class representing the nodes ⟨author⟩ is: {"total_length":14, "PERSON": {"occurrences":3, "extracted_length":14}} because the three authors have been recognised as Person entities.</p><p>(3) We consider that each 𝑁 𝑖 is a union of one or more collections, plus possibly (more rarely) a few records. To separate these, we proceed as follows: (a) We cluster the nodes in each 𝑁 𝑖 according to their structure.</p><p>(i) We transform 𝑁 𝑖 into a set of transactions D, by turning each node into a transaction, whose item set is the set of labels of the node's non-leaf children. Thus, the first book from the XML bibliography in Figure <ref type="figure" target="#fig_1">2</ref>    <ref type="figure" target="#fig_1">2</ref>. For instance, in the XML bibliography, the second ⟨book⟩ record includes the ⟨title⟩, the two ⟨author⟩, and the two ⟨review⟩. Similarly, for the RDF example, the second ⟨book⟩ record consists of its ⟨title⟩, plus few ⟨authors⟩ and ⟨reviews⟩. For the relational database, the ⟨book⟩ record consists of the ⟨title⟩ and the ⟨id⟩ of the book. Note that the content of a record is extensive using transitivity (e.g. the ⟨author⟩ collection is part of the ⟨book⟩).</p><p>In ( <ref type="formula" target="#formula_2">1</ref>), knowledge about possible node types should be injected in P, thus satisfying R5. R2 is met by including in a record many nodes reachable from it, in step (3b). Finally, step (3a) satisfies R3 by operating on the graph content (not on the original syntax), and R4 by detecting both implicit and explicit collections.</p><p>We now discuss possible choices for the quotient summary technique. The most general method, applicable to arbitrary graphs, consists of building a quotient graph summary <ref type="bibr" target="#b4">[5]</ref>, such as those described in <ref type="bibr" target="#b7">[8]</ref> which can be built in linear time in the input size. In particular, a type-first quotient summary <ref type="bibr" target="#b7">[8]</ref> uses type information when available to group nodes by their set of most general types (an RDF node may have several types, and a PG node may have several labels), while partitioning untyped nodes according to their incoming and outgoing nodes. Alternatively, in the particular case of tree data models, such as XML or JSON, P may be a Dataguide <ref type="bibr" target="#b8">[9]</ref>, which can also be constructed in linear time in the size of the input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">DISTINGUISHING ENTITIES FROM RELATIONSHIPS</head><p>We now analyse the collections to separate C into C 𝐸 , the set of entity collections, from C 𝑅 , the set of relationships collection. For instance, the ⟨authors⟩ node in the XML data describes entities while the ⟨wrote⟩ nodes in the property graph describe relationships. We say a collection 𝑐 is in C 𝑅 iff there exist two collections 𝑎 and 𝑏 such that: ∀𝑟 𝑐 ∈ 𝑐, ∃!𝑟 𝑎 ∈ 𝑎, ∃!𝑟 𝑏 ∈ 𝑏 such that 𝑟 𝑎 , 𝑟 𝑐 are connected by an edge in 𝐸 ′ and similarly 𝑟 𝑏 , 𝑟 𝑐 are connected by an 𝐸 ′ edge. If this is not the case, we consider that 𝑐 is a collection of entities. For instance, in the property graph in Figure <ref type="figure" target="#fig_1">2</ref>, each record in the implicit collection ⟨wrote⟩ links one ⟨author⟩ and one ⟨book⟩, therefore the collection ⟨wrote⟩ contains relationships; the ⟨author⟩ and ⟨book⟩ collections are collections of entities. This check can be sped up by exploiting connection statistics that can be gathered while computing the quotient summary of 𝐺 ′ .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CLASSIFYING COLLECTIONS</head><p>Our next step is to classify each collection in C 𝐸 into a given set K of categories of interest to the user, or Other if no category is pertinent. In our example, we consider the categories Person, Organization, Location, Event and Creative Work. As input to the classification process, we are also given a set of hints H . A hint ℎ ∈ H is a tuple (𝐴, 𝑙, 𝐵) where 𝐴 ⊆ K, 𝑙 is a label and 𝐵 is a signature pattern, which is matched (satisfied) by an individual signature, or not. Such a hint states that a node which has a child labeled 𝑙 and its signature matches 𝐵, should be classified as one of the types in 𝐴. For instance, the hint ({𝑂𝑟𝑔𝑎𝑛𝑖𝑧𝑎𝑡𝑖𝑜𝑛}, ℎ𝑎𝑠𝐶𝐸𝑂, {𝑃𝑒𝑟𝑠𝑜𝑛}) states that a record having a property hasCEO, whose signature matches Person, should be assigned to the category Organization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Classification algorithm</head><p>Algorithm 1 details our classification.</p><p>(1) For each record 𝑟 ∈ 𝑐, we initialize K 𝑟 , a multiset of candidate categories for 𝑟 , and a vector of scores of 𝑟 for each hint. K 𝑟 is a multiset to accomodate the possibility that a given category may be suggested by several hints. (2) If the record 𝑟 has a label semantically close to one of the categories in K, this category is stored as a candidate category in K 𝑟 , and the similarity score recorded in scores. (3) For each child 𝑛𝑐 of the record 𝑟 , we create a pair 𝜋 containing the label of 𝑛𝑐 and the signature of 𝑛𝑐. (4) Next, we compute the similarity of 𝜋 with each hint ℎ in H according to Equation <ref type="formula" target="#formula_2">1</ref>. This equation gives the similarity between a node and a hint, based on the label and the signature of both elements:</p><p>𝑠𝑖𝑚(𝜋, ℎ) = 𝑠𝑖𝑔_𝑠𝑖𝑚(𝜋 .𝑆, ℎ.𝐵)</p><formula xml:id="formula_2">+ 𝑘 𝑖 ∈𝐾 1 ,𝑘 𝑗 ∈𝐾 2 𝑐𝑜𝑠𝑖𝑛𝑒_𝑠𝑖𝑚(𝑉 (𝑘 𝑖 ), 𝑉 (𝑘 𝑗 ))<label>(1)</label></formula><p>where 𝐾 1 is the set of keywords present in 𝜋 .𝑙𝑎𝑏𝑒𝑙, 𝐾 2 is the set of keywords present in ℎ.𝑙, and 𝑠𝑖𝑔_𝑠𝑖𝑚(𝜋 .𝑆, ℎ.𝐵) is the similarity between the individual signature 𝜋 .𝑆 and the signature pattern ℎ.𝐵, 𝑉 (𝑘 𝑖 ) is a vector representation (embedding) of keyword 𝑘 𝑖 in a multidimensional space. Such embeddings enable detecting that a node labeled writer is close to a hint labeled author, even if they are different words.</p><p>We currently use the Word2Vec model <ref type="bibr" target="#b10">[11]</ref> for this task. (5) For each 𝜋, we choose the hint ℎ leading to the highest similarity score for 𝜋. Each category indicated by the domain of ℎ is added to K 𝑟 . (6) We classify the record 𝑟 in the category that is the most frequent in K 𝑟 , if one category is more frequent than 50%; otherwise, we classify it as Other. (7) Finally, we classify the collection 𝑐 in the most popular category among its records.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Constructing hints</head><p>The classification of the collections depends on the quality of hints provided as input. Users looking for a concept, e.g., creative work, may have in mind a few properties that creative works have, such as title or author, and may provide them as hints. Our approach works better if there are many hints, in order to obtain a decisive category vote. To overcome burdening human experts, we use Classify 𝑟 in the most frequent 𝑘 ∈ K 𝑟 , or Other 14 Classify 𝑐 with the most frequent category of its records knowledge bases like Wikidata <ref type="bibr" target="#b15">[16]</ref> and Yago <ref type="bibr" target="#b11">[12]</ref> to enhance an existing set of hints, as follows.</p><p>A knowledge base 𝐾𝐵 consists of triples of the form ⟨a,r,b⟩, where r is the relationship between the entities a and b. For example, the triple ⟨Albert Wessels, spouse, Elisabeth Eybers⟩ states that Albert Wessels' spouse is Elisabeth Eybers. It also makes statements about the entity type, such as ⟨Albert Wessels, type, Person⟩, from which we can obtain the category an entity belongs to.</p><p>Let 𝑘 ∈ K be the category we want to enhance hints for and 𝐾𝐵 be the knowledge base. The set of properties 𝑃 𝑘 that are likely to be associated with 𝑘 can be acquired using the following equation:</p><formula xml:id="formula_3">𝑃 𝑘 = {𝑟 | ⟨a,r,b⟩ ∈ 𝐾𝐵 ∧ ⟨a,type,k⟩ ∈ 𝐾𝐵}<label>(2)</label></formula><p>For example, the triple ⟨Albert Wessels, type, Person⟩ exists in YAGO, and Albert Wessels participates in triples such as ⟨Albert Wessels, nationality, South Africa⟩ and ⟨Albert Wessels, spouse, Elisabeth Eybers⟩ among many others. So, nationality and spouse would be added to the set 𝑃 𝑃𝑒𝑟𝑠𝑜𝑛 .</p><p>The acquired set of properties may contain some inaccurate information, e.g., for the category Organization, some properties such as date of birth were retrieved. This happens because knowledge bases such as Wikidata are collaboratively created, which can lead to errors, as evident from the triples ⟨Steven Shankman, type, Organization⟩ and ⟨Steven Shankman, date of birth, 1947⟩. We avoid such errors by scoring each property in the set 𝑃 𝑘 , i.e. errors such as the one reported above will lead to a low score. Formally:</p><p>Category score. This score is set proportional to the number of instances of the category the property was participating with. 𝑐_𝑠𝑐𝑜𝑟𝑒 (𝑝, 𝑘) for each 𝑝 ∈ 𝑃 𝑘 is computed as follows:</p><p>𝑐_𝑠𝑐𝑜𝑟𝑒 (𝑝, 𝑘) = |{𝑎 | ⟨a,p,b⟩ ∈ 𝐾𝐵 ∧ ⟨a,type,k⟩ ∈ 𝐾𝐵}| (3) While this prunes away errors, we might still get some properties which are common for all the categories in K, thus are not very useful in distinguishing between these categories. For example, the property Google Knowledge Graph ID appears for all the different categories. We introduce another score to penalize such properties: Inverse category score. This score quantifies how unique a property is for distinguishing between categories. Let 𝑃 𝑎𝑙𝑙 = {𝑃 𝑘 1 , 𝑃 𝑘 2 , . . . , 𝑃 𝑘 |K | } be the set of sets of properties retrieved for each category 𝑘 𝑖 ∈ K according to Equation <ref type="formula" target="#formula_3">2</ref>. The inverse category score of a property 𝑝 is computed as follows:</p><formula xml:id="formula_4">𝑖𝑣𝑐_𝑠𝑐𝑜𝑟𝑒 (𝑝) = 𝑙𝑜𝑔 1 + |K | 1 + |{𝑃 | 𝑝 ∈ 𝑃, 𝑃 ∈ 𝑃 𝑎𝑙𝑙 }|<label>(4)</label></formula><p>A property 𝑝 that appears in all sets 𝑃 𝑘 𝑖 , ∀𝑖 ∈ 1, 2, ..., |K | will get 𝑖𝑣𝑐_𝑠𝑐𝑜𝑟𝑒 (𝑝) = 0 since 𝑙𝑜𝑔(1) = 0. A property 𝑝 which appears in only one of the sets 𝑃 𝑘 1 , 𝑃 𝑘 2 , ..., 𝑃 𝑘 |K | will get the highest score. Total score. The total score of a property is given by a product of category score and inverse category score, as shown by Equation <ref type="formula">5</ref>.</p><p>𝑡𝑜𝑡_𝑠𝑐𝑜𝑟𝑒 (𝑝) = 𝑐_𝑠𝑐𝑜𝑟𝑒 (𝑝) * 𝑖𝑣𝑐_𝑠𝑐𝑜𝑟𝑒 (𝑝) + 1</p><p>(5)</p><p>We score the properties in decreasing order of 𝑡𝑜𝑡_𝑠𝑐𝑜𝑟𝑒 and retain the top-𝑧 𝑃 𝑘 𝑧 properties for each category 𝑘. For each 𝑘 ∈ K and 𝑝 ∈ 𝑃 𝑘 𝑧 , we create a hint ({𝐴}, 𝑝, 𝑟𝑎𝑛𝑔𝑒 (𝑝)), where 𝐴 = {𝑘 : 𝑝 ∈ 𝑃 𝑘 𝑧 } and 𝑟𝑎𝑛𝑔𝑒 (𝑝) is the range of the property 𝑝 if available from 𝐾𝐵, and ∅ if not present. We are currently working to make the signature patterns 𝑟𝑎𝑛𝑔𝑒 (𝑝) more specific, by exploiting the knowledge the 𝐾𝐵 may have about the domain of the property 𝑝.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION AND PERSPECTIVES</head><p>The approach described above aims at producing expressive abstraction of datasets organized in a variety of data models. This goes through their transformation in graphs, identifying records and collections, analyzing and classifying collections. The implementation and fine-tuning of our system is ongoing.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Motivating example: four bibliographic datasets, which we view as Collections of CreativeWork records.bibliography XML element in Figure1can be seen as a collection which holds the two records. We identify a set of requirements for our record and collection detection method, and formalize the problem of deriving them automatically from a dataset (Section 2).(2.) We propose an algorithm for automatically identifying records and collections in a dataset of any of the supported data models (Section 3). For instance, given the XML dataset in Figure1(or the relational or the RDF dataset from the same figure), our algorithm understands it as a collection of records, each record corresponding to a book, and having: a title, a collection of authors, and possibly a collection of reviews.(3.) We separate collections of Entities from collections of Relationships, in order to report to the user a structured, meaningful dataset abstraction (Section 4).(4.) To help users understand the data in their terms, we attempt to assign each collection to a category from a predefined category set, derived from a general-purpose ontology and/or based on the concepts of interest to users, e.g., books in the above example. We provide a method for our tool to enrich its knowledge of userspecified categories by selectively gathering information from large online knowledge bases. For instance, our algorithm classifies the bibliography in Figure1as a Creative Work (a Schema.org standard type including books, paintings, songs, etc.) Deployment scenarios On one hand, data producers can use our tool to automatically derive data abstractions, to be shared next to the data; on the other hand, users can generate the abstractions after downloading a candidate dataset, in order to assess its interest.</figDesc><graphic coords="3,79.02,75.18,453.96,251.76" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Graph representations of the four bibliographic datasets shown in Figure 1.</figDesc><graphic coords="5,77.90,83.68,453.96,295.37" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>has the items title, id, reviews and authors while the second has title, id and authors. (ii) Given an item set 𝑋 ∈ D, we denote by 𝑆 (𝑋 ) the support of 𝑋 , defined by 𝑆 (𝑋 ) = |{𝑌 ∈ D | 𝑋 ⊆ 𝑌 }|. We also define the transferred support of an item set 𝑋 , denoted 𝑇 (𝑋 ), based on the set of itemsets Y obtained as follows. Y is initialized with all the proper subsets of 𝑋 (not 𝑋 and not the empty set) that appear in D. Then, we traverse Y in the decreasing order of the itemset size, and remove all subsets of 𝑌 from Y. For a given 𝑋 and Y, the transferred support 𝑇 (𝑋 ) is defined as: 𝑇 (𝑋 ) = 𝑆 (𝑋 ) + 𝑌 |𝑌 ∈Y 𝑇 (𝑌 ). Our clustering algorithm starts by computing 𝑆 (𝑋 ) and 𝑇 (𝑋 ) for each 𝑋 in D. Then, it proceeds in a greedy manner, choosing the 𝑋 ∈ D with the highest value of 𝑇 (𝑋 ), and creating a collection 𝑐 containing 𝑋 and all the D transactions whose items are all included in those of 𝑋 . We then remove 𝑐 and the previously selected transactions from D and repeat the procedure. Each 𝑐 of more than 𝑡 elements (where 𝑡 is a threshold,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>𝑛 𝐶 𝑖 is a parent (in 𝐺) of all the nodes from 𝐶 𝑖 ; (𝑖𝑖𝑖) for each 𝑟 ∈ R, a set of nodes and edges of 𝐺 which we view as part of the record 𝑟 .(2) Separate the collections in C into C 𝐸 and C 𝑅 , respectively collections of entities and collections of relationships. (3) Classify the collections of entities: given a set of categories, and a set of hints (see Section 5), assign to each collection the category it is closest to (or none if does not fit the given categories).The nodes 𝑁 \ R \ {𝑛 𝐶 𝑖 | 𝐶 𝑖 ∈ C} which are neither records nor collection nodes are called sub-records and their set is denoted SR.</figDesc><table /><note><p>2 , . . .}, where each 𝐶 𝑖 is a set of elements from R, the 𝐶 𝑖 's are pairwise disjoint, and for each 𝐶 𝑖 , there may exist a node 𝑛 𝐶 𝑖 such that (𝑛 𝐶 𝑖 , 𝑟 𝑗 𝑖 ) ∈ 𝐸 for every 𝑟 𝑗 𝑖 ∈ 𝐶 𝑖 , in other words:</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>e.g., 𝑡 = 2) is considered a collection; if the 𝑐 elements have a common parent, that becomes the collection node, otherwise, the collection is implicit. For each 𝑐, every child 𝑟 ∈ 𝑐 is considered a record, part of 𝑐. The elements of 𝑐 that are not considered collections nor records are considered sub-records. For instance, in Figure2, the XML sample describes 4 collections (blue nodes): a ⟨bibliography⟩ containing ⟨item⟩ records (orange nodes), two sets of ⟨authors⟩ containing ⟨author⟩ records and a set of ⟨reviews⟩ containing ⟨review⟩ records. The sub-records are the green nodes. (b) For each record 𝑟 ∈ R, we build its content, i.e. a directed acyclic graph (DAG) 𝑑 𝑟 by following edges outgoing from 𝑟 , until we reach leaf nodes, or another record node 𝑟 ′ , or a collection node. The content of each record is represented by a light yellow box in Figure</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Algorithm 1: Classifying a collection 𝑐 Input: a collection 𝑐, hints H , categories K 1 foreach 𝑟 ∈ C do</figDesc><table><row><cell>2</cell><cell>K 𝑟 ← ∅</cell></row><row><cell>3</cell><cell>scores ← ∅</cell></row><row><cell>4</cell><cell>foreach 𝑘 ∈ K do</cell></row><row><cell>5</cell><cell>if the similarity between 𝑘 and the label of 𝑟 is higher</cell></row><row><cell></cell><cell>than a threshold then</cell></row><row><cell>6</cell><cell>K 𝑟 ← K 𝑟 ∪ {𝑘 }</cell></row><row><cell>7</cell><cell>foreach 𝑛𝑐 ∈ 𝑟 .children do</cell></row><row><cell>8</cell><cell>𝜋 ← (𝑛𝑐.𝑙𝑎𝑏𝑒𝑙, 𝑛𝑐.𝑠𝑖𝑔𝑛𝑎𝑡𝑢𝑟𝑒)</cell></row><row><cell>9</cell><cell>foreach ℎ ∈ H do</cell></row><row><cell>10</cell><cell>scores ← scores (ℎ, 𝑠𝑖𝑚(𝜋, ℎ))</cell></row><row><cell>11</cell><cell>bestHint ← argmax(scores)</cell></row></table><note><p><p>12</p>K 𝑟 ← K 𝑟 ∪ bestHint.domain 13</p></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgments. This work is funded by <rs type="grantNumber">DIM RFSI PHD 2020-01</rs> and <rs type="projectName">AI Chair SourcesSay</rs> project (<rs type="grantNumber">ANR-20-CHIA-0015-01</rs>) grants.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_JwgXRbH">
					<idno type="grant-number">DIM RFSI PHD 2020-01</idno>
					<orgName type="project" subtype="full">AI Chair SourcesSay</orgName>
				</org>
				<org type="funding" xml:id="_eC4c5fX">
					<idno type="grant-number">ANR-20-CHIA-0015-01</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Data Profiling</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Abedjan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Golab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Naumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Papenbrock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Synthesis Lectures on Data Management</title>
		<imprint>
			<publisher>Morgan &amp; Claypool Publishers</publisher>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Graph integration of structured, semistructured and unstructured data for data journalism</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Anadiotis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Balalau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Conceicao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Galhardas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Y</forename><surname>Haddad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Manolescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Merabti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>You</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021-07">July 2021</date>
		</imprint>
		<respStmt>
			<orgName>Information Systems</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Human-in-the-loop schema inference for massive JSON datasets</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Baazizi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Berti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Colazzo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ghelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sartiani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EDBT</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Parametric schema inference for massive JSON datasets</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Baazizi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Colazzo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ghelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sartiani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">VLDB J</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Summarizing Semantic Graphs: A Survey</title>
		<author>
			<persName><forename type="first">S</forename><surname>Cebiric</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Goasdoué</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kondylakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kotzinos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Manolescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Troullinou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zneika</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The VLDB Journal</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2019-06">June 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">ConnectionLens: Finding connections across heterogeneous data sources (demonstration)</title>
		<author>
			<persName><forename type="first">C</forename><surname>Chanial</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Dziri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Galhardas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leblay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Manolescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PVLDB</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">12</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Schemas for safe and efficient XML processing</title>
		<author>
			<persName><forename type="first">D</forename><surname>Colazzo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ghelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sartiani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDE</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">RDF graph summarization for first-sight structure discovery</title>
		<author>
			<persName><forename type="first">F</forename><surname>Goasdoué</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Guzewicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Manolescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The VLDB Journal</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2020-04">Apr. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Dataguides: Enabling query formulation and optimization in semistructured databases</title>
		<author>
			<persName><forename type="first">R</forename><surname>Goldman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Widom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VLDB</title>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Schema inference for property graphs</title>
		<author>
			<persName><forename type="first">H</forename><surname>Lbath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bonifati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Harmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EDBT. OpenProceedings.org</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Yago 4: A reason-able knowledge base</title>
		<author>
			<persName><forename type="first">T</forename><surname>Pellissier Tanon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Weikum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Suchanek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ESWC</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Database Management Systems</title>
		<author>
			<persName><forename type="first">R</forename><surname>Ramakhrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gehrke</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
			<publisher>McGraw-Hill</publisher>
		</imprint>
	</monogr>
	<note>3rd edition</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Xmark: A benchmark for XML data management</title>
		<author>
			<persName><forename type="first">A</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Waas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Kersten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Carey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Manolescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Busse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PVLDB</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Reducing ambiguity in JSON schema discovery</title>
		<author>
			<persName><forename type="first">W</forename><surname>Spoth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">A</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hammerschmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Wikidata: A free collaborative knowledgebase</title>
		<author>
			<persName><forename type="first">D</forename><surname>Vrandečić</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Krötzsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">10</biblScope>
			<date type="published" when="2014-09">Sept. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
