<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Self-Concordant Analysis of Generalized Linear Bandits with Forgetting</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yoan</forename><surname>Russac</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Louis</forename><surname>Faury</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Olivier</forename><surname>Cappé</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Aurélien</forename><surname>Garivier</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="laboratory">DI ENS</orgName>
								<orgName type="institution" key="instit1">CNRS</orgName>
								<orgName type="institution" key="instit2">Inria</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="laboratory" key="lab1">ENS</orgName>
								<orgName type="laboratory" key="lab2">Université PSL Criteo AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="laboratory">LTCI TélécomParis DI ENS</orgName>
								<orgName type="institution" key="instit1">CNRS</orgName>
								<orgName type="institution" key="instit2">Inria</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="laboratory">ENS</orgName>
								<orgName type="institution" key="instit1">Université PSL UMPA</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution" key="instit1">Inria</orgName>
								<orgName type="institution" key="instit2">ENS Lyon</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Self-Concordant Analysis of Generalized Linear Bandits with Forgetting</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">FC8FFB44B34DFA664B27B68E1680A397</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-04-12T14:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Algorithm Setting Projection Regret Upper Bound GLM-UCB Filippi et al Stationary Logistic Non-convex</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Contextual sequential decision problems with categorical or numerical observations are ubiquitous and Generalized Linear Bandits (GLB) offer a solid theoretical framework to address them. In contrast to the case of linear bandits, existing algorithms for GLB have two drawbacks undermining their applicability. First, they rely on excessively pessimistic concentration bounds due to the non-linear nature of the model. Second, they require either nonconvex projection steps or burn-in phases to enforce boundedness of the estimators. Both of these issues are worsened when considering non-stationary models, in which the GLB parameter may vary with time. In this work, we focus on self-concordant GLB (which include logistic and Poisson regression) with forgetting achieved either by the use of a sliding window or exponential weights. We propose a novel confidence-based algorithm for the maximum-likehood estimator with forgetting and analyze its perfomance in abruptly changing environments. These results as well as the accompanying numerical simulations highlight the potential of the proposed approach to address non-stationarity in GLB.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>In recent years, linear bandits <ref type="bibr">(Abbasi-Yadkori et al., 2011;</ref><ref type="bibr">Chu et al., 2011;</ref><ref type="bibr">Dani et al., 2008;</ref><ref type="bibr">Rusmevichientong and Tsitsiklis, 2010)</ref> have become the go-to paradigm to balance exploration and exploitation in contextual sequential decision making problems. Linear bandits have typically found applications for contentbased recommendations <ref type="bibr">(Li et al., 2010;</ref><ref type="bibr">Valko et al., 2014)</ref>, real-time bidding <ref type="bibr">(Flajolet and Jaillet, 2017)</ref> and even mobile-health interventions <ref type="bibr">(Tewari and Murphy, 2017)</ref>. Concurrently, Generalized linear bandits (GLB) have been introduced as a generalization of linear bandits, able to describe broader reward models of considerable practical relevance, in particular binary or categorical rewards <ref type="bibr">(Filippi et al., 2010;</ref><ref type="bibr">Li et al., 2017)</ref>. GLB are for instance a natural option in online advertising applications where the rewards take the form of clicks <ref type="bibr">(Chapelle and Li, 2011)</ref>. In this work, we focus on deterministic algorithms and refer to <ref type="bibr">(Chapelle and Li, 2011;</ref><ref type="bibr">Kveton et al., 2020)</ref> for randomized algorithms applicable to GLB. Compared to the linear bandits case, there are two distinctive drawbacks of GLB algorithms. The first is (1) the presence of a problem-dependent constant, imposed by the non-linear nature of the model, that is possibly prohibitively large and has a negative impact both on the design of algorithms and on their analysis. The second is (2) the need to modify the Maximum Likelihood Estimator (MLE) to ensure that it has a bounded norm. Usually this is achieved by resorting to an additional non-convex projection program applied to the MLE <ref type="bibr">(Filippi et al., 2010)</ref>. These distinctions correspond to a fundamental difference between the models, and explain why methods developed for linear bandits may fail in the case of GLB.</p><p>The first drawback (1) was recently addressed by <ref type="bibr">Faury et al. (2020)</ref>, in the specific case of logistic bandits. They showed that in this particular setting, the regret bounds of carefully designed algorithms could be significantly improved only at the cost of minor algorithmic modifications. Their analysis tightens the gap with the linear case, and takes a significant step towards the development of efficient GLB algorithms.</p><p>The second drawback (2) has seen little treatment in the literature, except for the work of <ref type="bibr">Li et al. (2017)</ref> who proved that the projection step of <ref type="bibr">Filippi et al. (2010)</ref> could be avoided by resorting to random initialization phases. However, a careful examination of the required conditions shows that these initialization phases can be prohibitively long to be deployed in scenarios of practical interest.</p><p>The aforementioned improvements to the original GLB algorithm of <ref type="bibr">Filippi et al. (2010)</ref> were developed under a stationarity assumption. However, non-stationary environments are ubiquitous in real-world applications of contextual bandits. In the linear bandits literature, this has motivated the development of adequate algorithms, able to handle changes in the structure of the reward signal <ref type="bibr">(Cheung et al., 2019b;</ref><ref type="bibr">Russac et al., 2019;</ref><ref type="bibr">Zhao et al., 2020)</ref>. <ref type="bibr">Russac et al. (2020)</ref> generalized such approaches to GLB, but without addressing neither (1) nor (2). As a result, the practical relevance of their approach remains questionable and the development of efficient and non-stationary GLB algorithms stands incomplete.</p><p>This paper aims at closing this gap. We study a broad family of GLB, known as self-concordant (which includes for instance the logistic and Poisson bandits), in environments where the parameter is allowed to switch arbitrarily over time. Under this setting, we answer (1) by providing a non-trivial extension of the concentration results from <ref type="bibr">Faury et al. (2020)</ref>. We also leverage the self-concordance property to remove the projection step, henceforth overcoming (2). This is made possible by an improved characterization of the, possibly weighted, MLE in (self-concordant) generalized linear models. Combined together, these two contributions lead to the design of efficient GLB algorithms, with improved regret bounds and which do not require to solve hard (i.e. non-convex) optimization programs. In doing so, we also answer the long-standing issue of providing proper confidence regions centered around the pristine MLE in GLB.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">BACKGROUND</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Setting and Assumptions</head><p>At each time step, the environment provides a timedependent action set A t and the agent plays a ddimensional action a t ∈ A t . We will assume that the reward's distribution belongs to a canonical exponential family with respect to a reference measure ν, such that dP θ (r|a) = exp(ra θ -b(a θ) + c(r))dν(r). Here, the function c(•) is real-valued and b(•) is assumed to be twice continuously differentiable. Thanks to the properties of exponential families, b is convex and can be related to the function µ = ḃ, itself referred to as the inverse link or mean function. A key feature of this description is that given a ground-truth parameter θ , selecting an action a t at time t yields a reward r t+1 conditionally independent on the past and such that E[r t+1 |a t ] = µ(a t θ ).</p><p>The non-stationary nature of the considered environ-ments is characterized as follows: the bandit parameter θ is allowed to change in an arbitrary fashion up to Γ T times within the horizon T . In the following, θ will be indexed by t to clearly exhibit its dependency w.r.t round t, and the reward signal will follow E[r t+1 |a t ] = µ(a t θ t ) .</p><p>The focus of this paper is the dynamic regret defined as</p><formula xml:id="formula_0">R T = T t=1 max a∈At µ a θ t -µ a t θ t .</formula><p>Note that in this setting, there is no fixed best arm, both due to the non-stationarity of the environment and to the fact that the action set A t may vary with time. We will work under the following assumptions.</p><p>Assumption 1 (Bounded actions and bandit parameters).</p><p>∀t ≥ 1, θ t 2 ≤ S and ∀a ∈ A t , a 2 ≤ 1 .</p><p>We define the admissible parameter space Θ = θ ∈ R d , θ 2 ≤ S . Assumption 2 (Bounded rewards).</p><p>∃m ∈ R + such that ∀t ≥ 1, 0 ≤ r t ≤ m .</p><p>Assumption 3. The mean function µ : R → R is continuously differentiable, Lipschitz with constant k µ and such that</p><formula xml:id="formula_1">c µ = inf θ∈Θ, a 2≤1 μ a θ &gt; 0 .</formula><p>The quantity c µ is crucial in the analysis, as it represents the (worst case) sensitivity of the mean function.</p><p>Our last assumption differs from most of existing works as we focus here on self-concordant GLMs. This assumption on the curvature of the mean function is rather mild, and covers for instance the logistic and Poisson models.</p><p>Assumption 4 (Generalized self-concordance). The mean function verifies |μ| ≤ μ .</p><p>In order to estimate the unknown bandit parameter θ t , we will adopt a weighted regularized maximumlikelihood principle. Formally, we define θt for λ &gt; 0 and γ ∈ (0, 1] as the solution of the strictly convex program θt = arg min</p><formula xml:id="formula_2">θ∈R d - t-1 s=1 γ t-1-s log P θ (r s+1 |a s ) + λ 2 θ 2 2 .</formula><p>(1) Equivalently, θt may be defined as the minimizer of t-1 s=1 γ -s log P θ (r s+1 |a s ) + λγ -(t-1) 2 θ 2 2 , with timeindependent increasing weights γ -s and time-varying regularization λγ -(t-1) , which is more handy for analysis purposes, see <ref type="bibr">(Russac et al., 2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Stationary GLB</head><p>GLB were first considered in the seminal work of <ref type="bibr">Filippi et al. (2010)</ref> who proposed GLM-UCB, an optimistic algorithm with a regret upper bound of the form Õ(c -1 µ d</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>√ T ).</head><p>A key characteristic of GLM-UCB is a projection step, used to map the MLE onto the set of admissible parameters Θ. Formally, when the MLE θt is not in Θ, it needs to be replaced by (2) where V t is an invertible d × d square matrix.</p><p>With GLM-UCB, both the size of the confidence set (thus the exploration bonus) and the regret bound scale as c -1 µ . However, this constant can be prohibitively large. In the cases of the logistic and Poisson bandits, one has c -1 µ ≥ e S , revealing an exponential dependency on S. If we consider the example of click prediction in online advertising with the logistic GLB, c -1 µ is of the order 10 3 , corresponding to typical click rates of less than a percent. This critical dependency was addressed by <ref type="bibr">Faury et al. (2020)</ref> for the logistic bandit. They introduce LogUCB1 and LogUCB2 for which they respectively prove O(c</p><formula xml:id="formula_3">-1/2 µ d √ T ) and O(d √ T + c -1 µ )</formula><p>regret upper bounds. Their analysis relies on the self-concordance property of the logistic log-likelihood. Self-concordance offers a refined way to control the curvature of the log-likelihood, and has been used in batch statistical learning <ref type="bibr">(Bach, 2010)</ref> and online optimization (Bach and Moulines, 2013) (see also <ref type="bibr">(Boyd and Vandenberghe, 2004, Section 9.6</ref>) for a broader picture). However, the analysis of <ref type="bibr">Faury et al. (2020)</ref> does not use the self-concordance to its fullest and a projection step is still required, as detailed in Section 5.</p><p>Since the mean function µ can be non-convex (as for example in the case of logistic regression), the projection step defined in Equation (2) generally involves the minimization of a non-convex function. Solving this program can be arduous and finding ways to bypass it is desirable. This was achieved by Li et al. ( <ref type="formula">2017</ref>) using a burn-in phase corresponding to an initial number of rounds during which the agent plays randomly. This ensures that θt stays in Θ for subsequent rounds and therefore avoids the projection step. This technique was re-used in other recent works, such as <ref type="bibr">(Kveton et al., 2020;</ref><ref type="bibr">Zhou et al., 2019)</ref>. A major drawback of this approach however is the length of this burn-in phase, which typically grows with c -2 µ (Kveton et al., 2020, Section 4.5). In the previously cited example of click-prediction, this would lead the agent to act randomly for approximately 10 6 rounds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Forgetting in Non-Stationary Environments</head><p>Motivated by the non-stationary nature of most real-life applications of contextual bandits, a consequent theory for linear bandits in non-stationary environments has been recently developed <ref type="bibr">(Cheung et al., 2019a;</ref><ref type="bibr">Russac et al., 2019;</ref><ref type="bibr">Zhao et al., 2020)</ref>. We focus here on forgetting policies, a broader perspective is discussed in Section 5. In <ref type="bibr">(Cheung et al., 2019a)</ref>, a sliding window is used and the estimator is constructed based on the most recent observations only. In <ref type="bibr">(Russac et al., 2019)</ref> exponentially increasing weights are used to give more importance to most recent observations. In <ref type="bibr">(Zhao et al., 2020)</ref> the algorithm is restarted on a regular basis. The non-stationary nature of the problem rules out the use of burning phases as changes in the GLB parameter can lead θt to leave Θ, even when well initialized. This also accentuates the inconveniences brought by the projection step, as θt leaving Θ is more likely to happen. This is why finding alternatives without projection is even more attractive in this particular setting. Furthermore, a generalization of the improvements brought by <ref type="bibr">Faury et al. (2020)</ref> to non-stationary world is missing, and it is unclear if the dependency in c µ can still be reduced in this harder setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Contributions</head><p>The present paper addresses these challenges, focusing on the use of exponential weights to adapt to changes in the model. First, we extend in Theorem 3 the Bernsteinlike tail-inequality of (Faury et al., 2020, Theorem 1) to weighted self-normalized martingales. We then leverage the self-concordance property (Assumption 4) to provide an improved characterization of the maximumlikelihood estimator (Proposition 1). This allows to provide concentration guarantees without projecting θt back to Θ. Combining these results leads to the SC-D-GLUCB strategy (Algorithm 1), which does not resort to a non-convex projection step and enjoys an Õ(c</p><formula xml:id="formula_4">-1/3 µ d 2/3 Γ 1/3 T T 2/3 ) worst case regret upper bound (Theorem 2). A O(c -1/2 µ ∆ -1 d √ Γ T T</formula><p>) regret bound is also obtained (Theorem 1) under an additional minimal gap ∆ &gt; 0 assumption (Assumption 5). A summary of our contributions and comparison with prior work are given in Table <ref type="table">1</ref>.</p><p>Table <ref type="table">1</ref>: Comparison of regret guarantees for different algorithms in the GLM setting with respect to the degree of non-linearity c µ , the dimension d, the horizon T and the number Γ T of abrupt changes. In the table SC stands for self-concordant. Regret guarantees for SC-SW-GLUCB are the same than for SC-D-GLUCB.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">ALGORITHM AND RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Algorithms</head><p>In this section, we consider the abruptly changing environments defined in Section 2. We propose two algorithms: SC-D-GLUCB, which is based on discount factors, and SC-SW-GLUCB using a sliding window. Due to space limitation constraints, the pseudo-code of SC-SW-GLUCB and the corresponding theoretical results are reported in Appendix C. Associated with the weighed MLE defined in Equation (1), define the weighted design matrix as</p><formula xml:id="formula_5">V t = t-1 s=1 γ t-1-s a s a s + λ c µ I d .<label>(3)</label></formula><p>The SC-D-GLUCB algorithm proceeds as follows. First, based on the previous rewards and actions, θt is computed. After receiving the action set A t , the action a t is chosen optimistically as the maximizer of the current estimate µ(a θt ) of each arm's reward inflated by the confidence bonus c</p><formula xml:id="formula_6">-1/2 µ β δ T a V -1 t .</formula><p>Finally, the reward r t+1 is received and the matrix V t is updated. The expression of β δ T is a consequence of our novel concentration result and is defined in Equation (4). A pseudo-code of the algorithm is presented in Algorithm 1.</p><p>There are two differences between SC-D-GLUCB and the algorithm proposed in <ref type="bibr">Russac et al. (2020)</ref>. First, we directly use θt to make predictions about the arms' performances, whether it belongs to Θ or not. Second, the exploration term scales as c -1/2 µ (instead of c -1 µ ), as in <ref type="bibr">Faury et al. (2020)</ref>. The latter has a direct impact on the regret-bound of SC-D-GLUCB, to be stated below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 SC-D-GLUCB</head><p>Input: Probability δ, dimension d, regularization λ, upper bound for bandit parameters S, discount factor γ. Initialize:</p><formula xml:id="formula_7">V 0 = (λ/c µ )I d , θ0 = 0 R d . for t = 1 to T do Receive A t , compute θt according to (1)</formula><p>Play a t = arg max a∈At µ(a θt )+</p><formula xml:id="formula_8">β δ T √ cµ a V -1 t with β δ T defined in Equation (4) Receive reward r t+1 Update: V t+1 ← a t a t + γV t + λ cµ (1 -γ)I d end for</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Regret Upper Bounds</head><p>We detail in this section the performance guarantees for SC-D-GLUCB. Define</p><formula xml:id="formula_9">β δ T = k µ √ λ 1 + S + 1 + S λ ρ δ T + ρ δ T √ λ 2 3/2<label>(4)</label></formula><p>with</p><formula xml:id="formula_10">S = S + 2Sk µ + m T λ(1 -γ) ,<label>(5)</label></formula><p>and where</p><formula xml:id="formula_11">ρ δ T = √ λ 2m + 2m √ λ log T δ + 2m √ λ d log(2) + dm √ λ log 1 + k µ (1 -T -2 ) dλ(1 -γ 2 ) .</formula><p>The latter expression is a direct consequence of the concentration result presented in Theorem 3 below. The difference between S and S is a bias term due to the non-stationarity.</p><p>Before stating our first theorem, we add an additional assumption on the minimal gap. This assumption is discussed in Section 5 and is only used in Theorem 1.</p><p>Assumption 5. The reward gaps ∆ t = min a∈At,µ(a θ t )&lt;µ(a θ t ) µ(a θ t ) -µ(a θ t ) satisfies ∀t ≤ T, ∆ t ≥ ∆ &gt; 0 .</p><p>Theorem 1. Under Assumption 5, the regret of the SC-D-GLUCB algorithm is bounded for all γ ∈ (1/2, 1) with probability at least 1 -δ by</p><formula xml:id="formula_12">R T ≤ C 1 Γ T 1 -γ + C 2 1 T (1 -γ) 2 ∆ + C 3 β δ T √ dT √ c µ ∆ T log(1/γ) + log 1 + 1 dλ(1 -γ) + C 4 d(β δ T ) 2 c µ ∆ T log(1/γ) + log 1 + 1 dλ(1 -γ) ,</formula><p>where C 1 , C 2 , C 3 , C 4 are universal constants independent of c µ , γ with only logarithmic terms in T .</p><p>In particular, setting γ = 1 -</p><formula xml:id="formula_13">√ cµΓ T d √ T and λ = d log(T ) leads to R T = O ∆ -1 c -1/2 µ d Γ T T .</formula><p>There is a strong link between the cost of nonstationarity in the K-arm setting and the one observed in the more general GLB setting. In the K-arm setting, any sub-optimal arm i is played at most O(∆ -2 i log(T )) times (e.g (Munos, 2014, Proposition 1.1)), whereas in any abruptly changing environment, forgetting policies play a sub-optimal arm i at most O((∆ T (i)) -2 √ Γ T T ) (Garivier and Moulines, 2011). ∆ T (i) is the minimum distance between the mean of the optimal arm and the mean of the suboptimal arm i over the entire time horizon. For GLBs, in the stationary case Filippi et al. (2010, Theorem 1) give a gap-dependent bound on the regret scaling as O(∆ -1 c -2 µ d 2 log(T )). Here, the bound of Theorem 1 is of order</p><formula xml:id="formula_14">O(∆ -1 c -1/2 µ d √ Γ T T ).</formula><p>The reduced dependency in c µ in the latter bound is a direct consequence of the use of self-concordance. Also note that when the inverse link function is the identity and the action set is the canonical basis, our analysis recovers the results of Garivier and Moulines (2011).</p><p>We give an upper bound for the worst case regret of Algorithm 1 in the following theorem; its proof is deferred to the appendix.</p><p>Theorem 2. The regret of the SC-D-GLUCB algorithm is bounded for all γ ∈ (1/2, 1) with probability at least 1 -δ by</p><formula xml:id="formula_15">R T ≤ C 1 Γ T 1 -γ + C 2 β δ T √ dT √ c µ T log 1 γ + log 1 + 1 dλ(1 -γ) ,</formula><p>where C 1 and C 2 are universal constants independent of c µ and γ with only logarithmic terms in T .</p><p>In particular, setting γ = 1 -</p><formula xml:id="formula_16">c 1/2 µ Γ T dT 2/3 and λ = d log(T ) leads to R T = O c -1/3 µ d 2/3 Γ 1/3 T T 2/3 .</formula><p>As ). This cannot be used to improve the result of Theorem 2, as one doesn't know in advance for which rounds the condition will be satisfied, but this minor modification of Algorithm 1 is most often advisable in practice. See Section B.4 in Appendix for more details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">KEY ARGUMENTS</head><p>In this section, we detail some key elements of our analysis. First, we describe the concentration result in its most generic form. Then, we explain the main steps to derive the upper bound of the regret of SC-D-GLUCB.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">A Tail-Inequality for Self-Normalized Weighted Martingales</head><p>To reduce the dependency in c µ , it is essential to take into account the actual conditional variance of the generalized linear model <ref type="bibr">(Faury et al., 2020)</ref>. With exponentially increasing weights, we also need timedependent regularization parameters to avoid a vanishing effect of the regularization <ref type="bibr">(Russac et al., 2019)</ref>.</p><p>Carefully combining these two elements yields the following concentration result.</p><p>Theorem 3. Let t be a fixed time instant. Let {F u } t u=1 be a filtration. Let {a u } t u=1 be a stochastic process on R d such that a u is F u measurable and a u 2 ≤ 1. Let { u } t u=2 be a martingale difference sequence such that u+1 is F u+1 measurable. Assume that the weights are non-decreasing, strictly positive and the time horizon is known. Furthermore, assume that conditionally on F u we have | u+1 | ≤ m a.s. Let {λ u } t u=1 be a deterministic sequence of regularization terms and denote</p><formula xml:id="formula_17">σ 2 t = E 2 t+1 |F t . Let H t = t-1 s=1 w 2 s σ 2 s a s a s + λ t-1 I d and S t = t-1</formula><p>s=1 w s s+1 a s , then for any δ ∈ (0, 1],</p><formula xml:id="formula_18">S t H -1 t ≥ λ t-1 2mw t-1 + 2mw t-1 λ t-1 log det( H t ) 1/2 δλ d/2 t + 2mw t-1 λ t-1 d log(2)</formula><p>with probability smaller than δ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Upper Bounding the Regret of SC-D-GLUCB</head><p>In a non-stationary environment, each change in the parameter will necessarily result in a number of rounds where the bias of the weighted MLE estimator cannot be controlled. This gives rise to the first term in the upper bound in Theorem 2. To make this observation more explicit, for D ≥ 1, define T (γ) = {1 ≤ t ≤ T, such that θ s = θ t for t -D ≤ s ≤ t -1} the set of time instants that are at least D steps away from the previous closest breakpoint. Central in the analysis of weighted GLBs is the matrix</p><formula xml:id="formula_19">G t ( θt , θ t ) = t-1 s=1 γ t-1-s α(a s , θt , θ t )a s a s + λI d ,</formula><p>where</p><formula xml:id="formula_20">α(a s , θt , θ t ) = 1 0 μ(a s ((1 -v)θ t + v θt ))dv.</formula><p>As in the linear case, we define its analogue with squared exponential weights,</p><formula xml:id="formula_21">G t ( θt , θ t ) = t-1 s=1 γ 2(t-1-s) α(a s , θt , θ t )a s a s + λI d .</formula><p>We add the subscript t -D : t to a quantity when the sum is for time instants between t -D and t -1. In this subsection, for space constraints, we will denote equivalently G t ( θt , θ t ) (resp. G t ( θt , θ t )) by G t (resp. G t ). As for linear bandits, the exploration bonus is designed to mitigate the impact of prediction errors. We focus below on upper bounding the prediction error in θt defined as ∆ t (a, θt ) = |µ(a θt ) -µ(a θ t )|. The exact link between the regret and this quantity is made explicit in Proposition 9 in the appendix. By defining g t (θ) =</p><p>t-1 s=t-D γ t-1-s µ(a s θ)a s + λθ, when t ∈ T (γ) one can upper bound the prediction error in θt .</p><formula xml:id="formula_22">∆ t (a, θt ) ≤ cγ D 1 -γ + k µ g t ( θt ) -g t (θ t ) G -1 t-D:t 1 a G -1 t 2</formula><p>The first term corresponds to the bias due to nonstationarity. 1 is a measure of the deviation of θt from θ t adapted to the non-linear nature of the problem. Note that g t ( θt ) -g t (θ t ) involves a martingale difference sequence (thanks to the optimality condition of the MLE) that can be controlled using Theorem 3. However, to bound 1 using Theorem 3 one needs to link the matrix G t-D:t with H t-D:t , the self-concordance allows exactly to do this.</p><p>Self-Concordance More precisely, the use of selfconcordance offers a sharp relation (independent of c µ ) between the first derivative of the mean function evaluated at different points. Using Lemma 4 reported in Appendix D, standard calculations yield:</p><formula xml:id="formula_23">G t-D:t ≥ 1 + C + 1 √ λ g t ( θt ) -g t (θ t ) G -1 t-D:t H t-D:t</formula><p>(6) Note that Equation (6) involves the deviation term that we want to control. Here, C is a residual bias due to the non-stationarity of the environment.</p><p>Better Characterization of the MLE By leveraging Equation (6) to bound the deviation g t ( θt ) -g t (θ t ) in the G -1 t-D:t -norm, one obtains an implicit equation. Solving it leads to the following proposition.</p><p>Proposition 1. When t ∈ T (γ), the following holds,</p><formula xml:id="formula_24">g t ( θt )-g t (θ t ) G -1 t-D:t ( θt,θ t ) ≤ √ 1 + Cρ δ T + 1 √ λ ρ δ T 2 ,</formula><p>where C is a residual term due to non-stationarity.</p><p>Remark. In stark contrast with previously existing works (see <ref type="bibr">(Filippi et al., 2010</ref>, Proposition 1)), deviations from the true parameter θ t are characterized uniquely by the MLE (and not by its projected counterpart). This can be done whether θt belongs to Θ or not and without any projection. This is not specific to the non-stationary nature of the problem but fundamentally relies on an improved analysis of the MLE. Similar guarantees can be obtained in any stationary environment. See Section 5 for a more detailed comparison of the possible uses of the self-concordance property.</p><p>1 can be upper bounded using Proposition 1. To upper bound 2 we use the following inequality.</p><formula xml:id="formula_25">G t ≥ 1 + C + 1 √ λ g t ( θt ) -g t (θ t ) G -1 t-D:t -1 c µ V t .</formula><p>(7) Combining Proposition 1 with Equation ( <ref type="formula">7</ref>) gives the upper bound for 2 . Putting everything together, we obtain the form of β δ T given in Equation ( <ref type="formula" target="#formula_9">4</ref>). The regret bound is then obtained by summing the exploration bonus for the different time instants. Applying the socalled elliptical lemma (see <ref type="bibr">(Lattimore and Szepesvári, 2019, Chap. 19</ref>)) and letting D = log(T )/ log(1/γ) completes the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">DISCUSSION</head><p>Assumption on the Gaps. Assumptions similar to our Assumption 5 requiring a minimum gap are frequent in non-stationary bandits. First, note that ∆ is not required for the algorithm but only for the theoretical analysis. Second, similar assumptions can be found for K-arm bandits in several works to obtain the optimal O( √ Γ T T ) regret bound. This is in particular the case for change-points detection methods:  <ref type="formula">2019</ref>)) achieve the optimal O( √ Γ T T ) regret bound. Yet, their analysis does not apply to the GLB framework. Furthermore, both works rely on replaying phases that are incompatible with time-dependent action sets as considered here. <ref type="bibr">Additionally, in (Chen et al., 2019)</ref> the regret is defined with respect to the best policy in some finite class, whereas our results apply to the general setting where actions can change over time and the regret benchmark is the ground-truth of the environment. The best lower-bound for forgetting policies in abruptly changing environments with time-dependent action sets remains unknown. While it is known that forgetting policies are minimax optimal when non-stationarity is measured through the so-called variational budget (see <ref type="bibr">Cheung et al. (2019b)</ref>; Russac et al. ( <ref type="formula">2019</ref>)), whether such methods are optimal in abruptly changing environments is unclear. Nonetheless, the bound obtained by Garivier and Moulines (2011) in the K-arm setting yields a worst case regret bound that can be shown to be of order O(Γ  <ref type="bibr">(Auer et al., 2019, Remark 2)</ref>). Similarly, in the absence of Assumption 5 an upper bound of order O(c</p><formula xml:id="formula_26">1/3 T T 2/3 ) (see Appendix E).</formula><formula xml:id="formula_27">(∆ -1 dc -1/2 µ T max(Γ T , T 1/2 )) (see</formula><formula xml:id="formula_28">-1/3 µ d 2/3 T 2/3 max(Γ T , d -1/2 T 1/4 ) 1/3</formula><p>) can be achieved (see <ref type="bibr">(Zhao et al., 2020, Theorem 4)</ref>).</p><p>Self-Concordance The analysis of <ref type="bibr">Faury et al. (2020)</ref> does not use self-concordance to its fullest. We present an improved analysis valid in any stationary time frame, proving that a better treatment of the selfconcordance removes the need for the inconvenient projection. Informally, the self-concordance links µ(x θt ) to µ(x θ ) without resorting to global bounds on μ (e.g k µ and c µ ). In <ref type="bibr">Faury et al. (2020)</ref>, this takes the form of a Taylor-like expansion:</p><formula xml:id="formula_29">µ(x θ t ) ≤ µ(x θ ) + |x (θ -θ t )| 1 + 2S μ(x θ ) ,</formula><p>where θ t is a projected version of θt in Θ. The denominator of the r.h.s. is reminiscent of this projection step.</p><p>We show here that a finer analysis yields the following, more implicit but powerful bound:</p><formula xml:id="formula_30">µ(x θt ) ≤ µ(x θ ) + |x (θ -θt )| 1 + |x (θ -θt )| μ(x θ ) .</formula><p>Note that when θt ∈ Θ (i.e there is no need for a projection), our bound implies the one of <ref type="bibr">Faury et al. (2020)</ref>.</p><p>The kind of relationship displayed in the above equation allows us to derive a tail inequality for the deviation from θt to θ without projecting θt , by solving an implicit equation. We believe that this new approach is of interest in other settings involving self-concordant GLBs. The self-concordance assumption (Assumption 4) is not particularly restrictive and goes beyond logistic functions. Under the classical Assumption 1 (i.e. bounded features) all GLMs are self-concordant (cf. Sec. 2 of Bach ( <ref type="formula">2014</ref>)) with constants that depend on the link function.</p><formula xml:id="formula_31">(a) c -1 µ = 400 (b) c -1 µ = 1000</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">EXPERIMENTS</head><p>In this section, we illustrate the empirical performance of SC-D-GLUCB in a simulated, abruptly changing environment with a logistic link function µ(x) = 1/(1 + exp(-x)). In this two-dimensional problem, there is a switch in the reward distribution at t = 4000 (red dashed line on Figure <ref type="figure" target="#fig_3">1</ref>). In Fig. <ref type="figure" target="#fig_3">1a</ref>, θ starts on the circle of radius S = 6 (corresponding to c -1 µ = exp(S) ≈ 400) with an angle of 2π/3 and jumps at t = 4000 to an angle of 4π/3. The experiment reported on Fig. <ref type="figure" target="#fig_3">1b</ref> is identical with a radius S = 7 corresponding to a c -1 µ ≈ 1000. As previously discussed, using such values of S is required in situation where the actions return binary rewards with expected values in the range 10 -3 -10 -2 , which is typically the case in web advertising or recommendation applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SC-D-GLUCB</head><p>For both experiments, at every time steps, 50 randomly generated actions in the unit circle are proposed to the learner. For SC-D-GLUCB and D-GLUCB the asymptotically optimal choice of the discount factors is used: γ = 1 -(Γ T /(d × T )) 2/3 with d = 2, Γ T = 2 and T = 8000. To speed up the learning that is hard with those values of c µ , all the algorithms have their exploration bonus divided by 5.</p><p>As expected, the algorithms tuned for non stationary situations (SC-D-GLUCB, D-GLUCB) perform worse than their stationary counterparts (LogUCB1 and GLM-UCB) during the first stationary phase. More precisely, with the choice made for γ the estimation of θt for algorithms that use exponential weights is roughly based on the 1/(1 -γ) ≈ 400 most recent observations. In contrast, LogUCB1 and GLM-UCB use all the observations from the start to compute the MLE, which eventually leads to a more precise estimation. Right after the change, the bias caused by the non-stationarity results in a significant increase in regret. Unweighted algorithms are affected much more deeply by this phenomenon that will eventually cause large losses in performance due to the persistence of obsolete information.</p><p>The theoretical analysis of Section 3.2 suggests that the advantage of SC-D-GLUCB is all the more significant in strongly non-linear (large c -1 µ ) non-stationary environments. This is obvious in Figure <ref type="figure" target="#fig_3">1</ref>, particularly when comparing Fig. <ref type="figure" target="#fig_3">1a</ref> and Fig. <ref type="figure" target="#fig_3">1b</ref>, which differ by the range on which the logistic function is used for making reward predictions. Note that, on average, for these two simulated scenarios the fact that the MLE θt does not belong to Θ happens for several hundred of rounds. All the algorithms except SC-D-GLUCB would require non convex projection steps at these instants, or equivalently, one should inflate S (and thus c -1 µ ) to ensure the compliance of these algorithms with the associated theory. In producing Figure <ref type="figure" target="#fig_3">1</ref>, this projection step was simply bypassed, which provides an optimistic evaluation of the performance of the competitors of SC-D-GLUCB. Interestingly, the observation that the dispersion of performance of SC-D-GLUCB is slightly higher than that of D-GLUCB can be traced back to the use of Remark 1 in these simulations: SC-D-GLUCB adapts to the events { θt / ∈ Θ} (rather than pretending that these did not happen) and thus its performance is made somewhat dependent on the actual occurrence of these events.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION</head><p>In this paper, we design GLB algorithms for piecewise stationary environments by resorting to forgetting mechanisms. We improve existing solutions by circumventing important drawbacks affecting their applicability in real-life scenarios. More precisely, under a generic self-concordance assumption, we remove the need for burdensome non-convex projections and leverage refined and exponentially deflated confidence regions. At the heart of our approach are a refined characterization of the maximum-likelihood estimator and an extension of a Bernstein-like tail inequality to weighted selfnormalized martingales. We believe that both can be of independent interest and leveraged in other settings (e.g drifting environments). We can see two natural extensions of our work; the first involves achieving similar success for other nature of non-stationarity such as drifting environments. The second could try to replicate the recent progress of Abeille et al. ( <ref type="formula">2020</ref>) in the stationary logistic case, and provide minimax rates w.r.t c µ in non-stationary self-concordant GLBs. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>References</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Self-Concordant Analysis of Generalized Linear Bandits with Forgetting: Supplementary Material</head><p>The Appendix is structured as follows. In Section A, our new concentration result for self-normalized weighted martingales with time dependent regularization parameters is presented. In Section A.3, similar concentration results are established when a sliding window is used. Section B studies the regret with discount factors through our improved characterization of the MLE. Section C gives similar results with a sliding window. Section D gathers some technical results, in particular the main properties resulting from the self-concordance assumption.</p><p>Finally in Section E, a worst case bound for a sliding window policy in the K-arm setting is presented.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A TAIL-INEQUALITY FOR SELF-NORMALIZED WEIGHTED MARTINGALES</head><p>While keeping in mind our objective of obtaining a deviation inequality with exponentially increasing weights, we give more generic results under two assumptions on the weights.</p><p>Assumption 6. The time horizon T is known in advance.</p><p>Assumption 7. The weights are deterministic, strictly positive and non-decreasing, i.e,</p><formula xml:id="formula_32">∀1 ≤ t ≤ T, 0 &lt; w 1 ≤ w t ≤ w t+1 ≤ w T .</formula><p>We recall the statement of the corresponding concentration result.</p><p>Theorem 3. Let t be a fixed time instant. Let {F u } t u=1 be a filtration. Let {a u } t u=1 be a stochastic process on R d such that a u is F u measurable and a u 2 ≤ 1. Let { u } t u=2 be a martingale difference sequence such that u+1 is F u+1 measurable. Assume that the weights are non-decreasing, strictly positive and the time horizon is known. Furthermore, assume that conditionally on F u we have | u+1 | ≤ m a.s. Let {λ u } t u=1 be a deterministic sequence of regularization terms and denote</p><formula xml:id="formula_33">σ 2 t = E 2 t+1 |F t . Let H t = t-1 s=1 w 2 s σ 2 s a s a s + λ t-1 I d and S t = t-1</formula><p>s=1 w s s+1 a s , then for any δ ∈ (0, 1],</p><formula xml:id="formula_34">S t H -1 t ≥ λ t-1 2mw t-1 + 2mw t-1 λ t-1 log det( H t ) 1/2 δλ d/2 t + 2mw t-1 λ t-1 d log(2)</formula><p>with probability smaller than δ.</p><p>Theorem 3 is a non-trivial extension of Faury et al. (2020, Theorem 1) allowing for the use of time-dependent regularization parameters and weights. We now state several lemmas that are useful for establishing Theorem 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Useful Lemmas</head><p>As a first step we fix a time instant t. Let M t u (ξ) for ξ ∈ R d and 1 ≤ u ≤ t be defined as</p><formula xml:id="formula_35">M t u (ξ) = exp 1 mw t-1 ξ S u - 1 m 2 w 2 t-1 ξ H u (0)ξ ,<label>(8)</label></formula><p>with</p><formula xml:id="formula_36">S u = u-1 s=1 w s s+1 a s and H u (0) = u-1 s=1 w 2 s σ 2 s a s a s where σ 2 s = E[ 2 s+1 |F s ].</formula><p>We prefer the notation M t u to M u to clearly indicate the dependency on the weight w t-1 . When u = t, we prefer the notation M t to M t t . For the entire appendix, we use the notation B 2 (d) = {a ∈ R d , a 2 ≤ 1}. Lemma 1. For all ξ ∈ B 2 (d) and 2 ≤ u ≤ t, under Assumption 6 and 7, we have</p><formula xml:id="formula_37">E M t u (ξ)|F u-1 ≤ M t u-1 (ξ) . a.s</formula><p>Proof.</p><formula xml:id="formula_38">E M t u (ξ)|F u-1 = M t u-1 (ξ) exp - 1 m 2 w 2 t-1 ξ w 2 u-1 σ 2 u-1 a u-1 a u-1 ξ × E exp 1 mw t-1 ξ w u-1 u a u-1 |F u-1 .</formula><p>The equality holds because a u-1 is F u-1 measurable and u-1 is F u-1 measurable. With ˜ u = u /m and v = wu-1 wt-1 ξ a u-1 , the conditions of Lemma 3 (stated below) are met and we have,</p><formula xml:id="formula_39">E exp 1 mw t-1 ξ w u-1 u a u-1 |F u-1 = E [exp(v˜ u )|F u-1 ] ≤ 1 + v 2 m 2 σ 2 u-1 .</formula><p>|v| ≤ 1 holds because of Assumption 7 and both ξ and a u-1 ∈ B 2 (d). Therefore,</p><formula xml:id="formula_40">E M t u (ξ)|F u-1 ≤ M t u-1 (ξ) exp - 1 m 2 w 2 t-1 ξ w 2 u-1 σ 2 u-1 a u-1 a u-1 ξ × 1 + w 2 u-1 m 2 w 2 t-1 σ 2 u-1 ξ a u-1 a u-1 ξ ≤ M t u-1 (ξ) (a.s) ,</formula><p>where the last inequality uses 1 + x ≤ exp(x).</p><p>Hence, for all</p><formula xml:id="formula_41">1 ≤ u ≤ t and ξ ∈ B 2 (d), E [M t (ξ)] ≤ E [M t u (ξ)] ≤ E [M t 1 (ξ)] = 1. For 1 ≤ u ≤ t we define, M t u = ξ M t u (ξ)dh u (ξ) .<label>(9)</label></formula><p>Here, h u is the density of an isotropic normal distribution of precision 2λu-1</p><formula xml:id="formula_42">m 2 w 2 t-1</formula><p>truncated on B 2 (d). We will denote N (h u ) its normalization constant.</p><p>Lemma 2. Let t be a fixed time instant, for all 1 ≤ u ≤ t, under assumptions 6 and 7, with {h u } t u=1 the density of an isotropic normal distribution of precision 2λu-1</p><formula xml:id="formula_43">m 2 w 2 t-1 truncated on B 2 (d) we have, E M t u ≤ 1 .</formula><p>Proof.</p><formula xml:id="formula_44">E M t u = Ω M t u dP(w) = Ω R d M t u (ξ)dh u (ξ) dP(w) ≤ R d Ω M t u (ξ)dP(w) dh u (ξ) (Fubini) ≤ R d Ω 1dP(w) dh u (ξ) (Lemma 1 + h u defined on B 2 (d)) ≤ R d dh u (ξ) = 1 . (h u is a probability density function)</formula><p>Remark 2. Allowing time-dependent regularization parameters is essential in our analysis to avoid the vanishing effect of the regularization with exponentially increasing weights for example. This is a fundamental difference with the deviation result provided in <ref type="bibr">Faury et al. (2020)</ref>. Furthermore, allowing the regularization parameters to be time-dependent comes at a cost here, we loose the property E M t u |F u-1 ≤ M t u-1 that would hold with a fixed regularization parameter (as in Faury et al. ( <ref type="formula">2020</ref>)). In the linear bandit setting, this issue was discussed in Lemma 2 in <ref type="bibr">Russac et al. (2019)</ref>.</p><p>In particular, applying Lemma 2 for u = t gives,</p><formula xml:id="formula_45">E Mt = E M t t ≤ 1 .<label>(10)</label></formula><p>Lemma 3 (Lemma 7 of Faury et al. ( <ref type="formula">2020</ref>)). Let ε be a centered random variable of variance σ 2 and such that |ε| ≤ 1 almost surely. Then for all v ∈ [-1, 1],</p><formula xml:id="formula_46">E [exp(vε)] ≤ 1 + v 2 σ 2 .</formula><p>Remark 3. We stress out that v ∈ [-1, 1] is required for Lemma 3 to hold. It has strong consequences in our setting with the weights as the normalization 1/w t-1 and 1/w 2 t-1 in the definition of M t u are needed to ensure that v = (w u-1 /w t-1 )ξ a u that appears in the proof of Lemma 1 will be smaller than 1. As a consequence, the stopping trick presented in <ref type="bibr">Abbasi-Yadkori et al. (2011)</ref> can not be applied to M t u because of its dependency on t. For this reason, the deviation result presented in Theorem 3 is only valid for a fixed time instant t. To obtain a deviation result on the entire trajectory an union bound is required.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Proof of Theorem 3</head><p>The proof of this theorem follows the line of proof of <ref type="bibr">Faury et al. (2020)</ref>. The main differences are the timedependent regularization parameters and the presence of weights. We recall that in Equation (9) h t is the density of an isotropic normal distribution of precision 2λt-1</p><formula xml:id="formula_47">m 2 w 2 t-1 truncated on B 2 (d) and denote N (h t ) its normalization constant.</formula><p>The following holds,</p><formula xml:id="formula_48">Mt = 1 N (h t ) R d 1 [ξ ∈ B 2 (d)] exp 1 mw t-1 ξ S t - 1 m 2 w 2 t-1 ξ H t ξ dξ . (<label>11</label></formula><formula xml:id="formula_49">) Let f t : R d → R be defined as f t (ξ) = 1 mwt-1 ξ S t - 1 m 2 w 2 t-1 ξ H t ξ. As a quadratic function, f t can be rewritten for ξ = arg max ξ 2≤1/2 f t (ξ), f t (ξ) = f t (ξ ) + ∇f t (ξ ) (ξ -ξ ) + 1 2 (ξ -ξ ) ∇ 2 f t (ξ )(ξ -ξ ) . Using ∀ξ ∈ B 2 (d), ∇ 2 f t (ξ) = -2 m 2 w 2 t-1 H t , Mt = e ft(ξ ) N (h t ) R d 1 [ ξ 2 ≤ 1] exp ∇f t (ξ ) (ξ -ξ ) - 1 m 2 w 2 t-1 ξ -ξ 2 Ht dξ = e ft(ξ ) N (h t ) R d 1 [ ξ + ξ 2 ≤ 1] exp ∇f t (ξ ) ξ - 1 m 2 w 2 t-1 ξ 2 Ht dξ ≥ e ft(ξ ) N (h t ) R d 1 [ ξ 2 ≤ 1/2] exp ∇f t (ξ ) ξ - 1 m 2 w 2 t-1 ξ 2 Ht dξ ≥ e ft(ξ ) N (g t ) N (h t ) E ξ∼gt exp ∇f t (ξ ) ξ .</formula><p>The second equality is obtained after a change of variable ξ → ξ -ξ . In the last inequality, g t is the density of a d-dimensional normal distribution with precision matrix</p><formula xml:id="formula_50">2 m 2 w 2 t-1 H t truncated on {a ∈ R d , a 2 ≤ 1/2}. Mt ≥ e ft(ξ ) N (g t ) N (h t ) exp E ξ∼gt ∇f t (ξ ) ξ . (Jensen's inequality) g t is symmetric which implies E ξ∼gt [ξ] = 0. Hence, Mt ≥ e ft(ξ ) N (g t ) N (h t ) .<label>(12)</label></formula><p>Therefore,</p><formula xml:id="formula_51">δ ≥ P Mt ≥ 1 δ (Equation (10) + Markov's Inequality) ≥ P f t (ξ ) ≥ log 1 δ + log N (h t ) N (g t ) (Equation (12)) = P max ξ 2≤1/2 f t (ξ) ≥ log 1 δ + log N (h t ) N (g t ) ≥ P f t (ξ 0 ) ≥ log 1 δ + log N (h t ) N (g t ) .</formula><p>In the last inequality ξ 0 is defined as</p><formula xml:id="formula_52">ξ 0 = √ λt 2 H -1 t St St H -1 t</formula><p>, such that ξ 0 2 ≤ 1/2 holds. This can be seen by using</p><formula xml:id="formula_53">H t ≥ λ t-1 I d .</formula><p>We also have,</p><formula xml:id="formula_54">f t (ξ 0 ) = 1 mw t-1 ξ 0 S t - 1 m 2 w 2 t-1 ξ 0 H t ξ 0 = λ t-1 2mw t-1 S t H -1 t - λ t-1 4m 2 w 2 t-1 . Therefore, P S t H -1 t ≥ λ t-1 2mw t-1 + 2mw t-1 λ t-1 log(1/δ) + 2mw t-1 λ t-1 log N (h t ) N (g t ) ≤ δ . (<label>13</label></formula><formula xml:id="formula_55">)</formula><p>We conclude using Proposition 2.</p><p>Proposition 2. Let h t be the density of a d-dimensional isotropic normal distribution of precision 2λt-1 m 2 w 2 t-1 truncated on B 2 (d). Let g t be the density of a d-dimensional normal distribution with precision matrix</p><formula xml:id="formula_56">2 m 2 w 2 t-1 H t truncated on {a ∈ R d , a 2 ≤ 1/2}. The following inequality holds, log N (h t ) N (g t ) ≤ log det( H t ) λ d/2 t-1 + d log(2) .<label>(14)</label></formula><p>Proof.</p><formula xml:id="formula_57">N (h t ) = R d 1 [ ξ 2 ≤ 1] exp - 1 2 2λ t-1 m 2 w 2 t-1 ξ 2 2 dξ = m 2 w 2 t-1 2λ t-1 d/2 R d 1 ξ 2 ≤ 2λ t-1 mw t-1 exp - 1 2 ξ 2 2 dξ . N (g t ) = R d 1 [ ξ 2 ≤ 1/2] exp - 1 2 2 m 2 w 2 t-1 ξ H t ξ dξ = 1 det √ 2 mwt-1 H 1/2 t R d 1 ξ 2 ≤ 1 2 2λ t-1 mw t-1 exp - 1 2 ξ 2 2 dξ ≥ m 2 w 2 t-1 2 d/2 det( H t ) -1/2 R d 1 ξ 2 ≤ 1 2 2λ t-1 mw t-1 exp - 1 2 ξ 2 2 dξ .</formula><p>Therefore,</p><formula xml:id="formula_58">N (h t ) N (g t ) ≤ det( H t ) λ d/2 t-1 R d 1 ξ 2 ≤ √ 2λt-1 mwt-1 exp -1 2 ξ 2 2 dξ R d 1 ξ 2 ≤ 1 2 √ 2λt-1 mwt-1 exp -1 2 ξ 2 2 dξ R . (<label>15</label></formula><formula xml:id="formula_59">)</formula><p>The last step consists in upper bounding the ratio of the integrals R. Following, (Faury et al., 2020, Lemma 6), one gets R = 2 d .</p><p>We conclude by using this equality in Equation ( <ref type="formula" target="#formula_58">15</ref>) and applying the logarithm on both sides.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 A Unifying Concentration Result for Discount Factors and Sliding-Window</head><p>In this section, we explain how Theorem 3 can be used with self-concordant GLBs to obtain a concentration inequality that encapsulates the analysis for both discount-factors and the sliding-window.</p><p>Up to now, we have stated the results in the most generic way. Actually, in our analysis we will use a weaker version of the concentration inequality established in Theorem 3. Theorem 4. Let t be a fixed time instant. Let {F u } t u=1 be a filtration. Let {a u } t u=1 be a stochastic process on R d such that a u is F u measurable and a u 2 ≤ 1. Let { u } t u=2 be a martingale difference sequence such that u+1 is F u+1 measurable. Assume that the weights are non-decreasing, positive and the time horizon is known. Furthermore, assume that conditionally on F u we have | u+1 | ≤ m a.s. Let {λ u } t u=1 be a deterministic sequence of regularization terms and denote</p><formula xml:id="formula_60">σ 2 t = E 2 t+1 |F t . Let H t-t0:t = t-1 s=t-t0 w 2 s σ 2 s a s a s + λ t-1 I d and S t-t0:t = t-1</formula><p>s=t-t0 w s s+1 a s . Then for any δ ∈ (0, 1],</p><formula xml:id="formula_61">P S t-t0:t H -1 t-t 0 :t ≥ λ t-1 2mw t-1 + 2mw t-1 λ t-1 log det( H t-t0:t ) 1/2 δλ d/2 t-1 + 2mw t-1 λ t-1 d log(2) ≤ δ .</formula><p>Proof. The arguments used to establish Theorem 4 are the same than for Theorem 3. We only give the main term that differs from the proof of Theorem 3.</p><p>With t a fixed time instant, for any u such that t -t 0 ≤ u ≤ t, M t u is defined as</p><formula xml:id="formula_62">M t u (ξ) = exp 1 mw t-1 ξ S t-t0:u - 1 m 2 w 2 t-1 ξ u-1 s=t-t0 w 2</formula><p>s a s a s ξ , with S t-t0:u = u-1 s=t-t0 w s s+1 a s . Following the steps of the proof of Theorem 3 with these slight differences gives the result.</p><p>Discount Factors Let t 0 = D be the equivalent of the sliding window length with exponential weights, w t = γ -t and λ t = λγ -2t for 0 &lt; γ &lt; 1. Even when γ depends on T , the weights satisfy the assumptions 6 and 7. We can obtain:</p><p>Corollary 1 (Concentration result with discount factors). Under the same assumption than Theorem 4, when defining H t-D:t = t-1 s=t-D γ 2(t-1-s) μ(a s θ s )a s a s + λI d and S t-D:t = t-1 s=t-D γ -s s+1 a s . For any δ ∈ (0, 1],</p><formula xml:id="formula_63">P γ t-1 S t-D:t H -1 t-D:t ≥ √ λ 2m + 2m √ λ log det( H t-D:t ) 1/2 δλ d/2 + 2m √ λ d log(2) ≤ δ .</formula><p>Sliding Window With t 0 = τ the length of the sliding window, with the weights satisfying w t = 1 for t -τ ≤ s ≤ t -1 and λ t = λ, we have:</p><p>Corollary 2 (Concentration result with a sliding window). Under the same assumption than Theorem 4, when defining</p><formula xml:id="formula_64">H t = t-1 s=max(1,t-τ ) μ(a s θ s )a s a s + λI d and S t = t-1</formula><p>s=max(1,t-τ ) s+1 a s . For any δ ∈ (0, 1],</p><formula xml:id="formula_65">P S t H -1 t ≥ √ λ 2m + 2m √ λ log det(H t ) 1/2 δλ d/2 + 2m √ λ d log(2) ≤ δ .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B REGRET ANALYSIS WITH DISCOUNT FACTORS</head><p>In this section we detail the regret analysis of SC-D-GLUCB. First we recall the main notation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Notation</head><p>For any θ ∈ R d ,</p><formula xml:id="formula_66">H t (θ) = t-1 s=1 γ 2(t-1-s) μ(a s θ)a s a s + λI d . (<label>16</label></formula><formula xml:id="formula_67">)</formula><formula xml:id="formula_68">H t (θ) = t-1 s=1 γ t-1-s μ(a s θ)a s a s + λI d . (<label>17</label></formula><formula xml:id="formula_69">) V t = t-1 s=1 γ 2(t-1-s) a s a s + λ c µ I d . (<label>18</label></formula><formula xml:id="formula_70">) V t = t-1 s=1 γ t-1-s a s a s + λ c µ I d .<label>(19)</label></formula><formula xml:id="formula_71">g 1:t (θ) = t-1 s=1 γ t-1-s µ(a s θ)a s + λθ . (<label>20</label></formula><formula xml:id="formula_72">) S t = t-1 s=1 γ -s s+1 a s . (<label>21</label></formula><formula xml:id="formula_73">) For any θ 1 , θ 2 ∈ R d , α(a, θ 1 , θ 2 ) = 1 0 μ(va θ 2 + (1 -v)a θ 1 )dv . G t (θ 1 , θ 2 ) = t-1 s=1 γ t-1-s α(a s , θ 1 , θ 2 )a s a s + λI d . G t (θ 1 , θ 2 ) = t-1 s=1 γ 2(t-1-s) α(a s , θ 1 , θ 2 )a s a s + λI d .<label>(22)</label></formula><p>Let H t be defined as</p><formula xml:id="formula_74">H t = t-1 s=1 γ 2(t-1-s) μ(a s θ s )a s a s + λI d .<label>(23)</label></formula><p>Let us define T (γ) as</p><formula xml:id="formula_75">T (γ) = {1 ≤ t ≤ T, such that ∀s, t -D ≤ s ≤ t -1, θ s = θ t } . (<label>24</label></formula><formula xml:id="formula_76">)</formula><p>Remark. t ∈ T (γ) when t is a least D steps away from the closest previous breakpoint. On the contrary to the analysis with the sliding window (see Appendix C) the bias does not completely cancel out when we are far enough from a breakpoint.</p><p>D is an analysis parameter and will be specified later in the different theorems. For the entire section we will use the notation t -D : t when the sum concerns time instants s such that t -D ≤ s ≤ t -1. In the weighted setting, we construct an estimator based on a weighted penalized log-likelihood. θt is defined as the unique maximizer of</p><formula xml:id="formula_77">t-1 s=1 γ t-1-s log P θ (r s+1 |a s ) - λ 2 θ 2 2 .</formula><p>By using the definition of the GLM and thanks to the concavity of this equation in θ, θt is the unique solution of</p><formula xml:id="formula_78">t-1 s=1 γ t-1-s (r s+1 -µ(a s θ))a s -λθ = 0 .</formula><p>This can be summarized with</p><formula xml:id="formula_79">g 1:t ( θt ) = t-1 s=1 γ t-1-s r s+1 a s = γ t-1 S t + t-1 s=1 γ t-1-s µ(a s θ s )a s .<label>(25)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Analysis of the Regret of SC-D-GLUCB</head><p>In this section, we present the main ideas to obtain an analysis of the regret of the SC-D-GLUCB algorithm when the projection step is avoided.</p><p>We define</p><formula xml:id="formula_80">ρ δ T = √ λ 2m + 2m √ λ log T δ + dm √ λ log 1 + k µ (1 -γ 2D ) dλ(1 -γ 2 ) + 2m √ λ d log(2) ,<label>(26)</label></formula><p>and also,</p><formula xml:id="formula_81">S = S + γ D (2Sk µ + m) λ(1 -γ) . (<label>27</label></formula><formula xml:id="formula_82">)</formula><p>The expression of ρ δ T and S given here coincide with the expression in the main paper when D = log(T )/ log(1/γ). ρ δ</p><p>T is defined such that thanks to Corollary 1 with high probability for all t in T (γ),</p><formula xml:id="formula_83">γ t-1 S t-D:t H -1 t-D:t ≤ ρ δ T holds.</formula><p>The next result uses the self-concordance to relate the first derivative of the link function evaluated at different points. This relation is independent of c µ and only depends on the distance between the parameters.</p><p>Proposition 3. When θt is the maximum likelihood as defined in Equation (1) and t ∈ T (γ), we have</p><formula xml:id="formula_84">α(a, θ t , θt ) ≥ 1 + S + 1 √ λ γ t-1 S t-D:t G -1 t-D:t (θ t , θt) -1 μ(a θ t ) ,</formula><p>where S is defined in Equation (27).</p><p>Proof. In the proof, we will replace the notation G t-D:t (θ t , θt ) with G t-D:t and G t (θ t , θt ) with G t but also G t (θ t , θt ) with G t . Using Lemma 4 we have,</p><formula xml:id="formula_85">α(a, θ t , θt ) ≥ 1 + a ( θt -θ t ) -1 μ(a θ t ) .</formula><p>Combining this with the mean value theorem gives</p><formula xml:id="formula_86">α(a, θ t , θt ) ≥ 1 + a G -1 t g 1:t ( θt ) -g 1:t (θ t ) -1 μ(a θ t ) .</formula><p>Next, it is possible to upper bound |a G -1 t g 1:t ( θt ) -g 1:t (θ t ) | using the triangle inequality and Equation ( <ref type="formula" target="#formula_79">25</ref>).</p><formula xml:id="formula_87">a G -1 t g 1:t ( θt ) -g 1:t (θ t ) ≤ a G -1 t t-1 s=1 γ t-1-s (µ(a s θ s ) -µ(a s θ t ))a s b1,t(a) + a G -1 t -λθ t + t-D-1 s=1 γ t-1-s s+1 a s b2,t(a) + a G -1 t γ t-1 S t-D:t b3,t(a)</formula><p>The first term is controlled as follows,</p><formula xml:id="formula_88">b 1,t (a) = a G -1 t t-1 s=1 γ t-1-s (µ(a s θ s ) -µ(a s θ t ))a s ≤ a G -1 t t-1 s=1 γ t-1-s (µ(a s θ s ) -µ(a s θ t ))a s G -1 t (Cauchy-Schwarz ineq.) ≤ 1 √ λ t-D-1 s=1 γ t-1-s (µ(a s θ s ) -µ(a s θ t ))a s G -1 t (G t ≥ λI d and t ∈ T (γ)) ≤ 1 λ t-D-1 s=1 γ t-1-s |α(a s , θ s , θ t )| × |a s (θ t -θ s )| × a s 2 (Triangle ineq. + G t ≥ λI d ) ≤ 2Sk µ λ t-D-1 s=1 γ t-1-s (θ s and θ t ∈ Θ) ≤ 2Sk µ λ γ D 1 -γ .</formula><p>Using similar arguments, one can upper bound b 2,t (a).</p><formula xml:id="formula_89">b 2,t (a) = a G -1 t -λθ t + t-D-1 s=1 γ t-1-s s+1 a s ≤ S + t-D-1 s=1 γ t-1-s s+1 a s G -2 t ≤ S + m λ γ D 1 -γ . (| s+1 | ≤ m)</formula><p>Before upper bounding, b 3,t (a), we need the following relation.</p><p>When 0 &lt; γ &lt; 1, γ 2(t-1-s) ≤ γ t-1-s for s smaller than t -1 which implies</p><formula xml:id="formula_90">∀θ 1 , θ 2 ∈ R d , G t (θ 1 , θ 2 ) ≤ G t (θ 1 , θ 2 ) . (<label>28</label></formula><formula xml:id="formula_91">)</formula><p>We have,</p><formula xml:id="formula_92">b 3,t (a) = |a G -1 t G 1/2 t G -1/2 t γ t-1 S t-D:t | ≤ a G -1 t GtG -1 t γ t-1 S t-D:t G -1 t (Cauchy-Schwarz ineq.) ≤ a G -1 t γ t-1 S t-D:t G -1 t (Equation (28)) ≤ 1 √ λ γ t-1 S t-D:t G -1 t-D:t . (G t ≥ λI d )</formula><p>By combining all the results we have,</p><formula xml:id="formula_93">α(a, θ t , θt ) ≥ 1 + S + 1 √ λ γ t-1 S t-D:t G -1 t-D:t -1</formula><p>μ(a θ t ) .</p><p>Corollary 3. When θt is the maximum likelihood as defined in Equation (1), and t ∈ T (γ), we have</p><formula xml:id="formula_94">G t-D:t (θ t , θt ) ≥ 1 + S + 1 √ λ γ t-1 S t-D:t G -1 t-D:t (θ t , θt) -1 H t-D:t .</formula><p>This proposition establishes a useful link between G t-D:t (θ t , θt ) and H t-D:t .</p><p>Proof. Thanks to Proposition 3,</p><formula xml:id="formula_95">α(a s , θ t , θt ) ≥ 1 + S + 1 √ λ γ t-1 S t-D:t G -1 t ( θt,θ t ) -1 μ(a s θ t ) .</formula><p>Therefore,</p><formula xml:id="formula_96">t-1 s=t-D γ 2(t-1-s) α(a s , θ t , θt )a s a s ≥ 1 + S + 1 √ λ γ t-1 S t-D:t G -1 t ( θt,θ t ) -1 × t-1 s=t-D γ 2(t-1-s) μ(a s θ t )a s a s .</formula><p>We obtain the announced result by using θ s = θ t for t -D ≤ s ≤ t -1 because t ∈ T (γ) and by adding the regularization terms.</p><p>Using Proposition 3 and Corollary 3, we can now prove Proposition 1. The proposition establishes an upper bound for the deviation of the MLE (through γ t-1 S t-D:t ) that only depends on ρ δ T the high probability upper bound obtained using Corollary 1.</p><p>Proposition 1. For any δ ∈ (0, 1], with probability higher than 1 -δ,</p><formula xml:id="formula_97">∀t ∈ T (γ), γ t-1 S t-D:t G -1 t-D:t ( θt,θ t ) ≤ 1 + Sρ δ T + 1 √ λ ρ δ T 2 ,</formula><p>where ρ δ T is defined in Equation (26). Remark. Here, note that the left-hand side is controlled under the norm G -1 t-D:t ( θt , θ t ), whereas the right hand side is the consequence of the upper bound of the same term controlled in the H -1 t-D:t -norm (Corollary 1). Linking those two matrices independently from c µ is not-straightforward. The self-concordance is the key ingredient to obtain this bound.</p><p>Proof. Applying Corollary 3,</p><formula xml:id="formula_98">γ t-1 S t-D:t 2 G -1 t-D:t ( θt,θ t ) ≤ 1 + S + 1 √ λ γ t-1 S t-D:t G -1 t-D:t ( θt,θ t ) γ t-1 S t-D:t 2 H -1 t-D:t . Let X = γ t-1 S t-D:t G -1 t-D:t ( θt,θ t ) , it gives the following constraint, ∀X, X 2 - 1 √ λ γ t-1 S t-D:t 2 H -1 t-D:t X -1 + S γ t-1 S t-D:t 2 H -1 t-D:t ≤ 0 .</formula><p>Solving this polynomial inequality yields</p><formula xml:id="formula_99">γ t-1 S t-D:t G -1 t (θ t , θt) ≤ 1 √ λ γ t-1 S t-D:t 2 H -1 t-D:t + 1 + S γ t-1 S t-D:t H -1 t-D:t .</formula><p>The result is then obtained by applying Corollary 1.</p><p>Corollary 4. When θt is the maximum likelihood as defined in Equation (1) and t ∈ T (γ), we have</p><formula xml:id="formula_100">G t (θ t , θt ) ≥ 1 + S + 1 √ λ γ t-1 S t-D:t G -1 t-D:t (θ t , θt) -1 c µ V t .</formula><p>Proof. Similar to the proof of Corollary 3.</p><p>In the next proposition, we give an upper bound for ∆ t (a, θt ) the prediction error in θt which is directly connected to the instantaneous regret.</p><p>Here, β δ T is defined as in the main paper in Equation ( <ref type="formula" target="#formula_9">4</ref>) but we replace ρ δ T and S with the expressions stated Equation ( <ref type="formula" target="#formula_80">26</ref>) and ( <ref type="formula" target="#formula_81">27</ref>).</p><p>Proposition 4. For any δ ∈ (0, 1], with probability higher than 1 -δ,</p><formula xml:id="formula_101">∀t ∈ T (γ), ∆ t (a, θt ) ≤ k µ λ γ D 1 -γ (2Sk µ + m) + β δ T √ c µ a V -1 t .</formula><p>Proof. We denote G t = G t (θ t , θt ) and we have,</p><formula xml:id="formula_102">∆ t (a, θt ) = |µ(a θ t ) -µ(a θt )| ≤ k µ |a (θ t -θt )| = k µ |a G -1 t (g 1:t (θ t ) -g 1:t ( θt ))| (Mean-Value Theorem) = k µ a G -1 t t-1 s=1 γ t-1-s (µ(a s θ t ) -µ(a s θ s ))a s + λθ t -γ t-1 S t .</formula><p>In the last equality, we have used the characterization of the MLE (Equation ( <ref type="formula" target="#formula_79">25</ref>)).</p><formula xml:id="formula_103">∆ t (a, θt ) ≤ k µ a G -1 t t-1 s=1 γ t-1-s (µ(a s θ t ) -µ(a s θ s ))a s c1,t(a) + k µ a G -1 t t-D-1 s=1 γ t-1-s s+1 a s c2,t(a) +k µ a G -1 t γ t-1 S t-D:t -λθ t c3,t(a)</formula><p>.</p><p>We will bound the different terms.</p><p>c 1,t (a) can be bounded like b 1,t (a) in the proof of Proposition 3.</p><formula xml:id="formula_104">c 1,t (a) ≤ 2Sk µ λ γ D 1 -γ .</formula><p>c 2,t (a) can be bounded like b 2,t (a) in the proof of the same proposition.</p><formula xml:id="formula_105">c 2,t (a) ≤ m λ γ D 1 -γ .</formula><p>The last term requires more work. G t (θ t , θt ) will be denoted G t for simplicity.</p><formula xml:id="formula_106">c 3,t (a) = a G -1 t γ t-1 S t-D:t -λθ t = a G -1 t G 1/2 t G -1/2 t γ t-1 S t-D:t -λθ t ≤ a G -1 t GtG -1 t γ t-1 S t-D:t -λθ t G -1 t ≤ a G -1 t γ t-1 S t-D:t -λθ t G -1 t (Equation (28)) ≤ a G -1 t √ λS + γ t-1 S t-D:t G -1 t ( G t ≥ λI d and Assumption 1) ≤ a V -1 t √ c µ 1 + S + 1 √ λ γ t-1 S t-D:t G -1 t-D:t √ λS + γ t-1 S t-D:t G -1 t-D:t .</formula><p>In the last inequality we used Corollary 4. The next step consists in upper bounding γ t-1 S t-D:t G -1 t-D:t with Proposition 1 and to combine this with the high probability upper bound from Corollary 1. Therefore, with probability higher than 1 -δ,</p><formula xml:id="formula_107">c 3,t (a) ≤ a V -1 t √ c µ 1 + S + 1 + S λ ρ δ T + 1 λ (ρ δ T ) 2 √ λS + γ t-1 S t-D:t G -1 t-D:t ≤ √ λ √ c µ a V -1 t 1 + S + 1 + S λ ρ δ T + 1 λ (ρ δ T ) 2 S + 1 + S λ ρ δ T + 1 λ (ρ δ T ) 2 ≤ √ λ √ c µ a V -1 t 1 + S + 1 + S λ ρ δ T + 1 λ (ρ δ T ) 2 3/2</formula><p>.</p><p>The first term of the right hand side of Proposition 4 is a bias term resulting from the non-stationarity of the environment. The second term results from the concentration results we have established in Section A combined with the self-concordance assumption.</p><p>With β δ T defined in Equation ( <ref type="formula" target="#formula_9">4</ref>), the algorithm SC-D-GLUCB selects the action at time t as follows,</p><formula xml:id="formula_108">a t = arg max a∈At µ(a θt ) + β δ T √ c µ a V -1 t + k µ λ γ D 1 -γ (2Sk µ + m) = arg max a∈At µ(a θt ) + β δ T √ c µ a V -1 t . (<label>29</label></formula><formula xml:id="formula_109">)</formula><p>Note that the bias term is independent of the action. Nevertheless, this term will appear in the upper bound for the regret. Equation ( <ref type="formula" target="#formula_108">29</ref>) explains how the actions are chosen in Algorithm 1.</p><p>We can now give the main theorem.</p><p>Theorem 2. The regret of the SC-D-GLUCB algorithm is bounded for all γ ∈ (1/2, 1) with probability at least 1 -δ by</p><formula xml:id="formula_110">R T ≤ 2 log(T ) 1 -γ Γ T + 2k µ (2Sk µ + m) λ 1 1 -γ + 2β δ T √ c µ √ dT 2 max 1, 1 λ T log(1/γ) + log 1 + 1 dλ(1 -γ) .</formula><p>In particular, setting γ = 1 -</p><formula xml:id="formula_111">c 1/2 µ Γ T dT 2/3</formula><p>and λ = d log(T ) leads to</p><formula xml:id="formula_112">R T = O c -1/3 µ d 2/3 Γ 1/3 T T 2/3 .</formula><p>Proof. Using Proposition 4, we obtain a high probability upper bound for ∆ t (a, θt ). We recall that the exploration bonus of SC-D-GLUCB is defined as,</p><formula xml:id="formula_113">1 √ c µ β δ T a t V -1 t + k µ λ γ D 1 -γ (2Sk µ + m) .</formula><p>Furthermore, the estimator used by SC-D-GLUCB is the MLE θt as defined in Equation ( <ref type="formula">1</ref>), all the conditions required for applying Proposition 9 are met. Hence when t ∈ T (γ),</p><formula xml:id="formula_114">r t ≤ 2 √ c µ β δ T a t V -1 t + 2k µ λ γ D 1 -γ (2Sk µ + m) .</formula><p>The dynamic regret can then be upper bounded by,</p><formula xml:id="formula_115">R T = T t=1 r T = t∈T (γ) r t + t / ∈T (γ) r t ≤ Γ T D + t∈T (γ) r t ≤ Γ T D + 2k µ λ γ D 1 -γ (2Sk µ + m)T + 2β δ T √ c µ t∈T (γ) a t V -1 t ≤ Γ T D + 2k µ λ γ D 1 -γ (2Sk µ + m)T + 2β δ T √ c µ √ T t∈T (γ) a t 2 V -1 t (Cauchy-Schwarz ineq.) ≤ Γ T D + 2k µ λ γ D 1 -γ (2Sk µ + m)T + 2β δ T √ c µ √ T T t=1 a t 2 V -1 t ≤ Γ T D + 2k µ λ γ D 1 -γ (2Sk µ + m)T + 2β δ T √ c µ √ T 2 max 1, 1 λ log det(V T +1 ) γ dT λ d .</formula><p>The last inequality uses Lemma 7. Next, we use Corollary 8 to upper bound the determinant,</p><formula xml:id="formula_116">det(V T +1 ) γ dT λ d ≤ γ -dT 1 + 1 -γ T λd(1 -γ) d .</formula><p>Applying the logarithm function on both sides yields</p><formula xml:id="formula_117">R T ≤ Γ T D + 2k µ (2Sk µ + m) λ γ D 1 -γ T + 2β δ T √ c µ √ dT 2 max 1, 1 λ T log(1/γ) + log 1 + 1 dλ(1 -γ)</formula><p>.</p><p>With the additional constraint 1/2 &lt; γ &lt; 1, by setting D = log(T )/ log(1/γ), noticing that 0 &lt; 1/γ -1 &lt; 1 and using log(1 + x) ≥ x/2 for 0 &lt; x &lt; 1, we have</p><formula xml:id="formula_118">log(1/γ) = log(1 + 1/γ -1) ≥ 1 -γ 2γ .</formula><p>Therefore, we have D ≤ 2γ log(T ) 1-γ . By properly balancing the bias term due to the non-stationarity and the rate at which the weighted MLE approaches the true bandit parameter, the asymptotic behavior of SC-D-GLUCB can be characterized as follows:</p><formula xml:id="formula_119">By setting γ = 1 - c 1/2 µ Γ T dT 2/3</formula><p>and λ = d log(T ), we have:</p><formula xml:id="formula_120">• 2 log(T ) 1-γ Γ T scales as O(c -1/3 µ d 2/3 Γ 1/3 T T 2/3 ). • 2kµ(2Skµ+m) λ 1 1-γ scales as O(c -1/3 µ d 2/3 Γ -2/3 T T 2/3 ). • 2β δ T √ cµ √ dT 2 max 1, 1 λ T log(1/γ) + log 1 + 1 dλ(1-γ)</formula><p>scales as 1 √ cµ dT log(1/γ) when omitting logarithmic factors and constant terms.</p><formula xml:id="formula_121">Using -log(1 -x) ≤ x-1</formula><p>x for 0 ≤ x &lt; 1, we also have</p><formula xml:id="formula_122">log(1/γ) = -log(1 -(1 -γ)) ≤ 1 -γ γ ≤ 2(1 -γ) .</formula><p>log(1/γ) scales as O(c</p><formula xml:id="formula_123">1/6 µ d -1/3 Γ 1/3 T T -1/3 ). Hence scales c -1/2 µ dT log(1/γ) as O(c -1/3 µ d 2/3 Γ 1/3 T T 2/3</formula><p>). Combining the different terms concludes the proof.</p><p>Using Assumption 5, we can obtain refined regret bounds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 Gap-Dependent Bound</head><p>Theorem 1. Under Assumption 5, the regret of the SC-D-GLUCB algorithm is bounded for all γ ∈ (1/2, 1) with probability at least 1 -δ by</p><formula xml:id="formula_124">R T ≤ C 1 Γ T 1 -γ + C 2 1 T (1 -γ) 2 ∆ + C 3 β δ T √ dT √ c µ ∆ T log(1/γ) + log 1 + 1 dλ(1 -γ) + C 4 d(β δ T ) 2 c µ ∆ T log(1/γ) + log(1 + 1 dλ(1 -γ) ) ,</formula><p>where C 1 , C 2 , C 3 , C 4 are universal constants independent of c µ , γ with only logarithmic terms in T .</p><p>In particular, setting γ = 1 -</p><formula xml:id="formula_125">√ cµΓ T d √ T</formula><p>and λ = d log(T ) leads to</p><formula xml:id="formula_126">R T = O ∆ -1 c -1/2 µ d Γ T T .</formula><p>Proof. First note that for any suboptimal action a ∈ A t , µ(a ,t θ t ) -µ(a θ t ) ≥ ∆ .</p><p>This implies</p><formula xml:id="formula_127">r t = µ(a ,t θ t ) -µ(a t θ t ) ≤ µ(a ,t θ t ) -µ(a t θ t ) 2 ∆ = r 2 t ∆ .<label>(30)</label></formula><p>Using Proposition 9 one has,</p><formula xml:id="formula_128">r t ≤ 2 √ c µ β δ T a t V -1 t + 2k µ λ γ D 1 -γ (2Sk µ + m) .</formula><p>This implies in particular,</p><formula xml:id="formula_129">r 2 t ≤ 4 c µ (β δ T ) 2 a t 2 V -1 t r1,t + 4k 2 µ λ 2 γ 2D (1 -γ) 2 (2Sk µ + m) 2 r2,t + 8k µ λ β δ T √ c µ γ D 1 -γ (2Sk µ + m) a t V -1 t r3,t .<label>(31)</label></formula><p>The dynamic regret can then be upper bounded by,</p><formula xml:id="formula_130">R T = T t=1 r T = t∈T (γ) r t + t / ∈T (γ) r t ≤ Γ T D + t∈T (γ) (µ(a ,t θ t ) -µ(a t θ t )) ≤ Γ T D + 1 ∆ t∈T (γ) r 2 t . (Equation (30))</formula><p>By applying Equation ( <ref type="formula" target="#formula_129">31</ref>), the regret can be separated in 4 different terms.</p><p>When summing for the different time instants r 1,t becomes</p><formula xml:id="formula_131">T t=1 r 1,t ≤ 8 c µ (β δ T ) 2 max 1, 1 λ log det(V T +1 ) γ dT λ d (Lemma 7) ≤ 8d c µ (β δ T ) 2 max 1, 1 λ T log(1/γ) + log 1 + 1 dλ(1 -γ)</formula><p>. (Corollary 8)</p><p>For r 2,t , we have</p><formula xml:id="formula_132">T t=1 r 2,t ≤ 4k 2 µ λ 2 γ 2D T (1 -γ) 2 (2Sk µ + m) 2 .</formula><p>Furthermore, r 3,t is treated as follows:</p><formula xml:id="formula_133">T t=1 r 3,t ≤ 8k µ λ β δ T √ c µ γ D 1 -γ (2Sk µ + m) T t=1 a t V -1 t ≤ 8k µ λ β δ T √ c µ γ D 1 -γ (2Sk µ + m) √ T T t=1 a t 2 V -1 t ≤ 8k µ β δ T λ √ c µ γ D 1 -γ (2Sk µ + m) 2dT max 1, 1 λ T log 1 γ + log 1 + 1 dλ(1 -γ)</formula><p>.</p><formula xml:id="formula_134">When λ = d log(T ), D = log(T ) log(1/γ) and γ = 1 - √ cµΓ T d √</formula><p>T , we can upper bound the different terms following the proof of Theorem 2.</p><p>With those choices,</p><formula xml:id="formula_135">1. Γ T D scales as O(c -1/2 µ dΓ 1/2 T T 1/2 ) 2. T t=1 r 1,t scales as O(c -1/2 µ dΓ 1/2 T T 1/2 ) 3. T t=1 r 2,t scales as O(c -1 µ Γ -1 T ) 4. T t=1 r 3,t scales as O(d 1/4 c -3/4 µ Γ -1/4 T T 1/4 )</formula><p>Keeping the highest order term in T and dividing by ∆ yields the announced result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4 Refined Exploration Bonus when θt ∈ Θ</head><p>As briefly explained in Remark 1 in the main paper, when the MLE is an admissible parameter ( θt ∈ Θ) it is possible to obtain a usually tighter concentration result. In this section, we explain exactly how this can be done. Note that this improvement is mostly useful for the design of the algorithm and has no impact on the regret guarantees.</p><p>We define</p><formula xml:id="formula_136">βδ T = k µ √ 1 + 2S √ λS + ρ δ T ,<label>(32)</label></formula><p>where ρ δ T is defined in Equation ( <ref type="formula" target="#formula_80">26</ref>). Proposition 5. For any δ ∈ (0, 1], with probability higher than 1 -δ,</p><formula xml:id="formula_137">∀t ∈ T (γ) s.t θt ∈ Θ, ∆ t (a, θt ) ≤ k µ λ γ D 1 -γ (2Sk µ + m) + βδ T √ c µ a V -1 t .</formula><p>Proof. We use the notation G t (respectively G t ) instead of G t (θ t , θt ) (respectively G t (θ t , θt )). Following the same steps as for the proof of Proposition 4, one gets</p><formula xml:id="formula_138">∆ t (a, θt ) ≤ k µ λ γ D 1 -γ (2Sk µ + m) + k µ |a G -1 t (γ t-1 S t-D:t -λθ t )| ≤ k µ λ γ D 1 -γ (2Sk µ + m) + a G -1 t GtG -1 t γ t-1 S t-D:t -λθ t G -1 t ≤ k µ λ γ D 1 -γ (2Sk µ + m) + a G -1 t γ t-1 S t-D:t -λθ t G -1 t . (Equation (28))</formula><p>Here, with the additional assumption θt ∈ Θ, the self-concordance can be used to obtain an easier relation between G t and H t as stated in Lemma 6.</p><formula xml:id="formula_139">∆ t (a, θt ) ≤ k µ λ γ D 1 -γ (2Sk µ + m) + √ 1 + 2S a G -1 t γ t-1 S t-D:t -λθ t H -1 t (Lemma 6) ≤ k µ λ γ D 1 -γ (2Sk µ + m) + √ 1 + 2S a G -1 t γ t-1 S t-D:t -λθ t H -1 t-D:t .</formula><p>The last inequality uses H t-D:t ≤ H t . Now by applying Corollary 1, ∆ t (a, θt ) can be further upper bounded.</p><formula xml:id="formula_140">∆ t (a, θt ) ≤ k µ λ γ D 1 -γ (2Sk µ + m) + √ 1 + 2S a G -1 t √ λS + ρ δ T .</formula><p>The final step consists in using G t = G t (θ t , θt ) ≥ c µ V t which holds because both θt and θ t are in Θ.</p><p>Consequently, when θt ∈ Θ, the action a t at time t can be chosen according to:</p><formula xml:id="formula_141">a t = arg max a∈At µ(a θt ) + βδ T √ c µ a V -1 t + k µ λ γ D 1 -γ (2Sk µ + m) = arg max a∈At µ(a θt ) + βδ T √ c µ a V -1 t .<label>(33)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C REGRET ANALYSIS WITH A SLIDING WINDOW</head><p>In the main paper only the analysis with discount factors is discussed. However as in the linear bandit literature, the analysis with exponential weights and a sliding window share similarities, in particular they have the same form of guarantees for the regret. For the sake of completeness, we give a detailed analysis of the results achievable with a sliding window.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 Notation</head><p>Let us first introduce the main notations. For any value of θ ∈ R d , we define,</p><formula xml:id="formula_142">H t (θ) = t-1 s=max(1,t-τ ) μ(a s θ)a s a s + λI d . (<label>34</label></formula><formula xml:id="formula_143">) V t = t-1 s=max(1,t-τ ) a s a s + λ c µ I d . (<label>35</label></formula><formula xml:id="formula_144">)</formula><formula xml:id="formula_145">g t (θ) = t-1 s=max(1,t-τ ) µ(a s θ)a s + λθ . (<label>36</label></formula><formula xml:id="formula_146">) S t = t-1 s=max(1,t-τ ) s+1 a s .<label>(37)</label></formula><p>For any</p><formula xml:id="formula_147">θ 1 , θ 2 ∈ R d , α(a, θ 1 , θ 2 ) = 1 0 μ(va θ 2 + (1 -v)a θ 1 )dv . G t (θ 1 , θ 2 ) = t-1 s=max(1,t-τ ) α(a s , θ 1 , θ 2 )a s a s + λI d .<label>(38)</label></formula><p>Let H t be defined as</p><formula xml:id="formula_148">H t = t-1 s=max(1,t-τ ) μ(a s θ s )a s a s + λI d .<label>(39)</label></formula><p>Let us define T (τ ) as</p><formula xml:id="formula_149">T (τ ) = {1 ≤ t ≤ T, ∀s, such that t -τ ≤ s ≤ t -1, θ s = θ t } .<label>(40)</label></formula><p>t ∈ T (τ ) when t is a least τ steps away from the closest previous breakpoint. When focusing on time instants in T (τ ) the bias due to non-stationarity disappears. In the sliding window setting, we construct an estimator based on a truncated penalized log-likelihood. In this section, θt is defined as the unique maximizer of</p><formula xml:id="formula_150">t-1 s=max(1,t-τ ) log P θ (r s+1 |a s ) - λ 2 θ 2 2 .<label>(41)</label></formula><p>By using the definition of the GLM and thanks to the concavity of this equation in θ, θt is the unique solution of</p><formula xml:id="formula_151">t-1 s=max(1,t-τ ) (r s+1 -µ(a s θ))a s -λθ = 0 .</formula><p>This can be summarized with</p><formula xml:id="formula_152">g t ( θt ) = t-1 s=max(1,t-τ ) r s+1 a s = S t + t-1 s=max(1,t-τ ) µ(a s θ s )a s .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Algorithm</head><p>The SC-SW-GLUCB algorithm proceeds as follows. First, based on the τ last rewards and actions, θt is computed using Equation (41). Then, after receiving the action set A t the action a t is chosen optimistically. Finally, by proposing this action a reward r t+1 is received and the design matrix is updated. The pseudo code of SC-SW-GLUCB is reported in Algorithm 2.</p><p>Algorithm 2 SC-SW-GLUCB Input: Probability δ, dimension d, regularization λ, upper bound for bandit parameters S, sliding window τ . Initialize:</p><formula xml:id="formula_153">V 0 = (λ/c µ )I d , θ0 = 0 R d .</formula><p>for t = 1 to T do Receive A t , compute θt according to (41)</p><p>Play a t = arg max a∈At µ(a θt ) +</p><formula xml:id="formula_154">β δ t √ cµ a V -1 t with β δ t defined in Equation (43) Receive reward r t+1 Update: if t &lt; τ then V t+1 ← a t a t + V t else V t+1 ← a t a t -a t-τ a t-τ + V t end if end for</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 Analysis of the Regret of SC-SW-GLUCB</head><p>In Section B, the self-concordance is the key tool to obtain an analysis without using a projection step. In the next proposition, we link the matrix G t ( θt , θ t ) with H t (θ t ) independently from c µ . Proposition 6. When θt is the maximum likelihood estimator as defined in Equation (41) and t ∈ T (τ ), we have:</p><formula xml:id="formula_155">α(a, θ t , θt ) ≥ 1 + S + 1 √ λ S t G -1 t (θ t , θt) -1</formula><p>μ(a θ t ) .</p><p>Note that the main difference with Proposition 3 is that S is now replaced by S. This is due to the fact that the bias disappears when using a sliding window for t ∈ T (τ ).</p><p>Proof. Thanks to Lemma 4, we have:</p><formula xml:id="formula_156">α(a, θ t , θt ) ≥ 1 + a (θ t -θt ) -1 μ(a θ t ) ≥ 1 + a G -1 t (θ t , θt )(g t (θ t ) -g t ( θt )) -1 μ(a θ t ) (Mean-Value Theorem) ≥ 1 + a G -1 t (θ t , θt) g t (θ t ) -g t ( θt ) G -1 t (θ t , θt) -1 μ(a θ t ) (Cauchy-Schwarz) ≥ 1 + λ -1/2 g t (θ t ) -g t ( θt ) G -1 t (θ t , θt) -1 μ(a θ t ) (G t (θ t , θt ) ≥ λI d ) ≥ 1 + λ -1/2 S t -λθ t G -1 t (θ t , θt) -1 μ(a θ t ) (t ∈ T (τ )) ≥ 1 + S + λ -1/2 S t G -1 t (θ t , θt) -1</formula><p>μ(a θ t ) .</p><p>Corollary 5. When θt is the maximum likelihood estimator as defined in Equation (41), when t ∈ T (τ ) and H t is defined in Equation (39), we have,</p><formula xml:id="formula_157">G t (θ * t , θt ) ≥ 1 + S + 1 √ λ S t G -1 t (θ * t , θt) -1 H t . Furthermore, ∀t ≤ T, S t G -1 t (θ t , θt) ≤ √ 1 + S S t H -1 t + 1 √ λ S t 2 H -1 t .</formula><p>Proof. Using Proposition 6 and summing for time instants</p><formula xml:id="formula_158">s such that max(1, t -τ ) ≤ s ≤ t -1, t-1 s=t-τ α(a s , θ t , θt )a s a s ≥ 1 + S + λ -1/2 S t G -1 t (θ t , θt) -1 t-1 s=t-τ μ(a s θ s )a s a s .</formula><p>Where we use θ s = θ t for t -τ ≤ s ≤ t -1 thanks to the assumption t ∈ T (τ ). The next step consists in adding the regularization term on both sides. Note that 1 + S + λ -1/2 S t G -1 t (θ t , θt) λ ≥ λ and obtain,</p><formula xml:id="formula_159">G t (θ t , θt ) ≥ 1 + S + λ -1/2 S t G -1 t (θ t , θt) -1 H t .</formula><p>This in turn implies,</p><formula xml:id="formula_160">S t 2 G -1 t (θ t , θt) ≤ 1 + S + λ -1/2 S t G -1 t (θ t , θt) S t 2 H -1 t ⇐⇒ S t 2 G -1 t (θ t , θt) -λ -1/2 S t 2 H -1 t S t G -1 t (θ t , θt) -(1 + S) S t 2 H -1 t ≤ 0 .</formula><p>Solving this polynomial inequality (in S t G -1 t (θ t , θt) ) finally gives,</p><formula xml:id="formula_161">S t G -1 t (θ t , θt) ≤ √ 1 + S S t H -1 t + 1 √ λ S t 2 H -1 t .</formula><p>Using this technique, we have established an explicit link between G t (θ t , θt ) and H t without the need to project θt on Θ when t ∈ T (τ ).</p><p>We define</p><formula xml:id="formula_162">ρ δ t = √ λ 2m + 2m √ λ log T δ + dm √ λ log 1 + k µ min(t, τ ) dλ + 2m √ λ d log(2) ,<label>(42)</label></formula><p>and</p><formula xml:id="formula_163">β δ t = k µ √ λ 1 + S + 1 + S λ ρ δ t + ρ δ t √ λ 2 3/2 . (<label>43</label></formula><formula xml:id="formula_164">)</formula><p>In the next proposition, we give an upper bound for ∆ t (a, θt ).</p><p>Proposition 7. For any δ ∈ (0, 1], with probability higher than 1 -δ,</p><formula xml:id="formula_165">∀t ∈ T (τ ), ∆ t (a, θt ) ≤ β δ t √ c µ a V -1 t . Proof. ∆ t (a, θt ) = |µ(a θ t ) -µ(a θt )| ≤ k µ |a (θ t -θt )| = k µ |a G -1 t (θ t , θt )(g t (θ t ) -g t ( θt ))| (Mean-Value Theorem) ≤ k µ a G -1 t (θ t , θt) g t (θ t ) -g t ( θt ) G -1 t (θ t , θt) (Cauchy-Schwarz ineq.) ≤ k µ a G -1 t (θ t , θt) S t -λθ t G -1 t (θ t , θt) . (t ∈ T (τ ))</formula><p>We can use Corollary 5 to link a G -1 t (θ t , θt) with a H -1</p><formula xml:id="formula_166">t . ∆ t (a, θt ) ≤ k µ 1 + S + 1 √ λ S t G -1 t (θ t , θt) a H -1 t √ λS + S t G -1 t (θ t , θt) ≤ k µ √ λ 1 + S + 1 √ λ S t G -1 t (θ t , θt) a H -1 t S + 1 √ λ S t G -1 t (θ t , θt) ≤ k µ √ λ 1 + S + 1 √ λ S t G -1 t (θ t , θt) 3/2 a H -1 t .</formula><p>Then, using Corollary 5 we can upper bound S t G -1 t (θ t , θt) with a combination of terms depending on S t H -1 t . Recall that Corollary 2 gives with probability higher than 1 -δ , for all t in T (τ ),</p><formula xml:id="formula_167">S t H -1 t ≤ ρ δ t . ∆ t (a, θt ) ≤ k µ √ λ 1 + S + 1 + S λ ρ δ t + 1 λ (ρ δ t ) 2 3/2 a H -1 t .</formula><p>The proof is completed using H t ≥ c µ V t , which holds thanks to Assumption 1 on the bandit parameters.</p><p>Finally, we give an upper bound for the regret enjoyed by SC-SW-GLUCB.</p><p>Theorem 5. The regret of the SC-SW-GLUCB algorithm is bounded with probability at least 1 -δ by,</p><formula xml:id="formula_168">R T ≤ Γ T τ + 2β δ T √ c µ √ dT T /τ 2 max 1, 1 λ log 1 + τ dλ ,</formula><p>where β δ t is defined in Equation (43).</p><p>Proof. The proof essentially follows the steps of the proof of Theorem 2. The main difference is that β δ t from Equation ( <ref type="formula" target="#formula_163">43</ref>) is used and the elliptical lemma is different because the design matrix is designed with a sliding window instead of weights.</p><p>Applying Proposition 9 when t ∈ T (τ ), with probability higher than 1 -δ,</p><formula xml:id="formula_169">r t ≤ 2 √ c µ β δ t a t V -1 t . (<label>44</label></formula><formula xml:id="formula_170">)</formula><p>The dynamic regret can then be upper bounded by, </p><formula xml:id="formula_171">R T = T t=1 r T = t∈T (τ ) r t + t / ∈T (τ ) r t ≤ Γ T τ + t∈T (τ ) r t ≤ Γ T τ + 2β δ T √ c µ t∈T (τ ) a t V -1 t (Equation (44)) ≤ Γ T τ + 2β δ T √ c µ √ T t∈T (τ ) a t 2 V -1 t (Cauchy-Schwarz ineq.) ≤ Γ T τ + 2β δ T √ c µ √ T T t=1 a t 2 V -1 t ≤ Γ T τ + + 2β δ T √ c µ √ dT T /τ 2 max 1, 1 λ log 1 + τ dλ . (<label>Lemma</label></formula><formula xml:id="formula_172">R T = O(c -1/3 µ d 2/3 Γ 1/3 T T 2/3 ) . If Γ T is unknown, by choosing τ = dT c 1/2 µ 2/3</formula><p>, the regret of SC-SW-GLUCB scales as</p><formula xml:id="formula_173">R T = O(c -1/3 µ d 2/3 Γ T T 2/3 ) . Proof. When Γ T is known, we set λ = d log(T ) and τ = dT √ cµΓ T 2/3</formula><p>. With those choices, 1. β δ T scales as d log(T ).</p><p>2. Γ T τ scales as O(c</p><formula xml:id="formula_174">-1/3 µ d 2/3 Γ 2/3</formula><p>T T 2/3 ).</p><p>3.</p><formula xml:id="formula_175">β δ T √ cµ √ T d T τ scales as O(c -1/3 µ d 2/3 Γ 1/3 T T 2/3 ).</formula><p>The proof is similar when Γ T is unknown.</p><p>When the reward gaps are bounded from below we can obtain the following gap-dependent upper bound:</p><p>Lemma 5 (Self-concordance and sliding window). For all θ 1 , θ 2 ∈ Θ, with G t defined in Equation ( <ref type="formula" target="#formula_147">38</ref>) and H t defined in Equation (34) the following inequalities hold</p><formula xml:id="formula_176">G t (θ 1 , θ 2 ) ≥ (1 + 2S) -1 H t (θ 1 ) , G t (θ 1 , θ 2 ) ≥ (1 + 2S) -1 H t (θ 2 ) .</formula><p>Proof. Applying Lemma 4, for any</p><formula xml:id="formula_177">θ 1 , θ 2 ∈ R d , α(a, θ 1 , θ 2 ) ≥ μ(a θ 1 ) 1 + |a (θ 1 -θ 2 )| and α(a, θ 1 , θ 2 ) ≥ μ(a θ 2 ) 1 + |a (θ 1 -θ 2 )| . Furthermore, if θ 1 and θ 2 ∈ Θ, then |a (θ 1 -θ 2 )| ≤ 2S .</formula><p>Lemma 6 (Self-concordance and discount factors). For all θ 1 , θ 2 ∈ Θ, with H t (θ 1 ) defined in Equation ( <ref type="formula" target="#formula_66">16</ref>) and G t (θ 1 , θ 2 ) defined in Equation ( <ref type="formula" target="#formula_73">22</ref>) the following inequalities hold:</p><formula xml:id="formula_178">G t (θ 1 , θ 2 ) ≥ (1 + 2S) -1 H t (θ 1 ) , G t (θ 1 , θ 2 ) ≥ (1 + 2S) -1 H t (θ 2 ) .</formula><p>Proof. Proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>det(H</head><formula xml:id="formula_179">t ) = d i=1 l i (l i are the eigenvalues) ≤ 1 d d i=1 l i d (AM-GM inequality) ≤ 1 d trace(H t ) d ≤ 1 d t-1 s=1 w 2 s σ 2 s trace(a s a s ) + λ t-1 d ≤ 1 d t-1 s=1 w 2 s σ 2 s a s 2 2 + λ t-1 d ≤ λ t-1 + k µ d t-1 s=1 w 2 s d .</formula><p>Corollary 7. In the specific case where the weights are given by w t = γ -t with 0 &lt; γ &lt; 1, under the same assumptions than Proposition 8, with</p><formula xml:id="formula_180">H t = t-1 s=t-t0 γ 2(t-1-s) σ 2 s a s a s + λI d , one has det( H t ) ≤ λ + k µ (1 -γ 2t0 ) d(1 -γ 2 ) d .</formula><p>Corollary 8. In the specific case where the weights are given by w t = γ -t with 0 &lt; γ &lt; 1, under Assumption 1 with</p><formula xml:id="formula_181">V t = t-1 s=1 γ t-1-s a s a s + λI d , one has det(V t ) ≤ λ + 1 -γ t-1 d(1 -γ) d .</formula><p>Corollary 9. In the specific case where the weights are given by w t = 1 when t ≥ t -τ and 0 before. With</p><formula xml:id="formula_182">H t = t-1 s=max(1,t-τ ) σ 2 s a s a s + λI d , one has det(H t ) ≤ λ + k µ min(t, τ ) d d .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3 Elliptical Lemma</head><p>The following lemma is a version of the Elliptical Lemma when discount factors are used. It comes from Proposition 4 in <ref type="bibr">(Russac et al., 2019)</ref> and is stated here for the sake of completeness.</p><p>Lemma 7 (Elliptical potential with discount factors (based on Proposition 4 in Russac et al. ( <ref type="formula">2019</ref>))). Let {a s } ∞ s=1 a sequence in R d such that a s 2 ≤ 1 for all s ∈ N, and let λ be a non-negative scalar. For t ≥ 1 define</p><formula xml:id="formula_183">V t = t-1 s=1 γ t-1-s a s a s + λI d , the following inequality holds T t=1 a t 2 V -1 t ≤ 2 max 1, 1 λ log det(V T +1 ) λ d γ dT .</formula><p>Proof. In the proof we introduce the matrix</p><formula xml:id="formula_184">W t = t-1 s=1 γ -s a s a s + γ -(t-1) λI d such that V t = γ t-1 W t . We have, W t = t-1 s=1 γ -s a s a s + γ -(t-1) λI d = γ -(t-1) a t-1 a t-1 + t-2 s=1 γ -s a s a s + γ -(t-2) λI d + γ -(t-1) λI d -γ -(t-2) λI d = γ -(t-1) a t-1 a t-1 + γ -(t-1) (1 -γ)λI d + W t-1 ≥ γ -(t-1) a t-1 a t-1 + W t-1 ≥ W 1/2 t-1 (I d + γ -(t-1) W -1/2 t-1 a t-1 a t-1 W -1/2 t-1 )W 1/2 t-1 . This implies, det(W t+1 ) ≥ det(W t ) det I d + (γ -t/2 W -1/2 t a t )(γ -t/2 W -1/2 t a t ) ≥ det(W t ) 1 + γ -t a t 2 W -1 t (det(I d + xx ) = 1 + x 2 2 ) .</formula><p>This in turn gives,</p><formula xml:id="formula_185">det(W T +1 ) det(W 1 ) = T t=1 det(W t+1 ) det(W t ) ≥ T t=1 1 + γ -t a t 2 W -1 t .</formula><p>Taking the logarithm on both sides gives:</p><formula xml:id="formula_186">log det(W T +1 ) λ d ≥ T t=1 log(1 + γ -t a t 2 W -1 t ) ≥ T t=1 log(1 + γ -(t-1) a t 2 W -1 t ) ≥ T t=1 log   1 + γ -(t-1) a t 2 W -1 t max 1, 1 λ   .</formula><p>Next, by using W t ≥ γ -(t-1) λI d , we see that</p><formula xml:id="formula_187">γ -(t-1) a t 2 W -1 t ≤ 1 λ . Which ensures that 0 ≤ γ -(t-1) a t 2 W -1 t max 1, 1 λ ≤ 1 .</formula><p>Finally, with log(1 + x) ≥ x/2 valid when 0 ≤ x ≤ 1. We get,</p><formula xml:id="formula_188">log det(W T +1 ) λ d ≥ 1 2 max 1, 1 λ T t=1 γ -(t-1) a t 2 W -1 t .</formula><p>The following lemma is a version of the Elliptical Lemma when a sliding window is used and can be extracted from <ref type="bibr">(Russac et al., 2019, Proposition 9)</ref>. The proof is included here for the sake of completeness.</p><p>Lemma 8 (Elliptical potential with sliding window (Proposition 9 in Russac et al. ( <ref type="formula">2019</ref>))). Let {a s } ∞ s=1 a sequence in R d such that a s 2 ≤ 1 for all s ∈ N, and let λ be a non-negative scalar. For t ≥ 1 define V t = t-1 s=max(1,t-τ ) a s a s + λI d . The following inequality holds:</p><formula xml:id="formula_189">T t=1 a t 2 V -1 t ≤ 2d max 1, 1 λ T /τ log 1 + τ λd .</formula><p>Proof. We start by rewriting the sum as follows.</p><formula xml:id="formula_190">T t=1 a t 2 V -1 t = T /τ -1 k=0 (k+1)τ t=kτ +1 a t 2 V -1 t .</formula><p>For the k-th block of length τ we define the matrix W</p><formula xml:id="formula_191">(k) t = t-1 s=kτ +1 a s a s + λI d . We also have ∀t ∈ [[kτ + 1, (k + 1)τ ]], V t ≥ W (k) t as every term in W (k) t</formula><p>is contained in V t and the extra-terms in V t correspond to positive definite matrices.</p><formula xml:id="formula_192">T /τ -1 k=0 (k+1)τ t=kτ +1 a t 2 V -1 t ≤ T /τ -1 k=0 (k+1)τ t=kτ +1 a t 2 (W (k) t ) -1 . Furthermore, ∀t ∈ [[kτ + 1, (k + 1)τ ]] we have, det(W (k) t+1 ) = det(W (k) t ) 1 + a t 2 (W (k) t ) -1 .</formula><p>With positive definitive matrices whose determinants are strictly positive, this implies that</p><formula xml:id="formula_193">det(W (k) (k+1)τ +1 ) det(W (k) kτ +1 ) = (k+1)τ t=kτ +1 det(W (k) t+1 ) det(W (k) t ) = (k+1)τ t=kτ +1 1 + a t 2 (W (k) t ) -1</formula><p>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>By definition we have W</head><formula xml:id="formula_194">(k) kτ +1 = kτ t=kτ +1 a t a t + λI d = λI d . log   det W (k) (k+1)τ +1 λ d   = (k+1)τ t=kτ +1 log 1 + a t 2 (W (k) t ) -1 ≥ (k+1)τ t=kτ +1 log 1 + 1 max(1, 1/λ) a t 2 (W (k) t ) -1</formula><p>.</p><p>In the next step we use, ∀0</p><formula xml:id="formula_195">≤ x ≤ 1, log(1 + x) ≥ x/2. log   det W (k) (k+1)τ +1 λ d   ≥ 1 2 max(1, 1/λ) (k+1)τ t=kτ +1 a t 2 (W (k) t ) -1 .</formula><p>By summing, over the different blocks, we obtain</p><formula xml:id="formula_196">T /τ -1 k=0 (k+1)τ t=kτ +1 a t 2 V -1 t ≤ T /τ -1 k=0 (k+1)τ t=kτ +1 a t 2 (W (k) t ) -1 ≤ 2 max(1, 1/λ) T /τ -1 k=0 log   det W (k) (k+1)τ +1 λ d   .</formula><p>Then, we upper bound det(W (k) (k+1)τ +1 ) using similar arguments than for Corollary 9,</p><formula xml:id="formula_197">det(W (k) (k+1)τ +1 ) ≤ λ + τ d d .</formula><p>Applying the logarithm function on both sides concludes the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.4 Link Between ∆ t and the Instantaneous Regret</head><p>For any optimistic algorithm, even in a non-stationary environment the instantaneous regret can be directly related to ∆ t (a, θ) defined as For any optimistic algorithm with an exploration bonus of β t (.) and such that the upper confidence bound of the action a at time t is given by µ(a θ t ) + β t (a), by definition for all a ∈ A t µ(a θ t ) + β t (a) ≤ µ(a t θ t ) + β t (a t ) .</p><formula xml:id="formula_198">∆ t (a, θ) = |µ(a θ) -µ(a θ t )| .</formula><p>In particular, this is also true for the action a t, . Therefore, plugging this inequality in the expression of the instantaneous regret gives</p><formula xml:id="formula_199">r t ≤ ∆ t (a t , θ t ) + ∆ t (a t, , θ t ) + β(a t ) -β(a t ) .</formula><p>Under the additional assumption that ∆ t (a, θ) ≤ β t (a), we obtain the announced result.</p><p>This proposition shows that any improvement in an upper bound of ∆ t (a, θ t ) will result in an improvement of the regret, as long as the exploration bonus satisfies the assumption stated in the proposition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E ON THE WORST CASE REGRET IN THE K-ARM SETTING</head><p>In this section, we build upon the analysis from Garivier and Moulines (2011) to provide a worst case regret bound for the sliding window policy in the K-arm setting. Even if a proper lower bound is missing, the results we provide here suggest that in some cases sliding window policies can suffer a regret of order O(Γ 1/3 T T 2/3 ) in the simpler K-arm setting. In particular, this would mean that the T 2/3 dependency is not a sub-optimality from our setting but can already be seen for forgetting policies in the non-contextual setting. Worst-case regret bounds (i.e. gap independent) for forgetting policies in non-stationary environments have seen little treatment in the literature.</p><p>Setting. The setting considered in this section is the one from Garivier and Moulines (2011). At each time t, the player chooses an arm I t ∈ {1, ..., K} based on the previous rewards and actions. Upon selecting I t a reward X t (I t ) is observed. We consider abruptly changing environments as in other sections, where the distribution of the rewards remains constant during phases and changes at unknown time instants. At time t, the arm i has a mean reward µ t (i). As before, Γ T denote the number of abrupt changes in the reward distributions before time T . Following the notation from <ref type="bibr">Trovo et al. (2020)</ref>, we denote the Γ T breakpoints B = {b 1 , ..., b Γ T }. We can associate Γ T stationary phases {φ 1 , ..., φ Γ T } with these breakpoints, where φ i = {t ∈ {1, ..., T } s.t b i-1 ≤ t &lt; b i } and b 0 = 1. It is further assumed that for all arms and all time instants the means of the reward distributions lie in [0, B]. In this section the focus is on the forgetting policy using a sliding window but the same arguments can be used with exponentially increasing weights.</p><p>Improving the problem dependent bound. In (Garivier and Moulines, 2011, Theorem 2), the number of times the arm i is played before time T while being sub-optimal is upper bounded in expectation as</p><formula xml:id="formula_200">E [N T (i)] ≤ C(τ ) (∆µ T (i)) 2 T log(τ ) τ + τ Γ T + log 2 (τ ) ,<label>(46)</label></formula><p>where ∆µ T (i) = min{µ t (i t ) -µ t (i) : t ∈ {1, ..., T }, µ t (i) &lt; µ t (i t )} .</p><p>This result has a worst case flavor in the sense that ∆µ T (i) is the minimum distance between the mean of the optimal arm and the mean of the i-th arm when i is sub-optimal over the entire time horizon. We obtain a less pessimistic bound by decomposing the regret into the Γ T different stationary phases and upper-bounding the number of times a sub-optimal arm is drawn in each of these phases φ. The upper-bound naturally depends on ∆ φ i , the difference between the mean of the optimal arm and the i-th arm in the φ-th stationary phase rather than ∆µ T (i). This is of utmost importance as for some phases ∆ φ i can be significantly larger than ∆µ T (i). During the φ-th stationary phase, let µ φ i denote the mean of the i-th arm and N φ i denote the number of times the arm i is selected. The regret can be decomposed as follows:</p><formula xml:id="formula_201">E [R T ] = T t=1 (µ t -µ t (i t )) = K i=1 Γ T φ=1 ∆ φ i E[N φ i ] .<label>(47)</label></formula><p>A worst-case bound. The bound from Equation ( <ref type="formula" target="#formula_200">46</ref>) is problem dependent and depends explicitly on the minimum gap. It is interesting to study the worst case regret. In particular when ∆µ T (i) goes to 0 the upper bound from Equation (46) becomes uninformative. At the same time, with a small gap ∆ φ i the cost of selecting the i-th arm rather than the optimal one diminishes. The trade-off between these two opposite effects is made explicit in the following result.</p><p>Theorem 7. The worst case regret of the sliding window policy from <ref type="bibr">(Garivier and Moulines, 2011)</ref>, can be upper-bounded by</p><formula xml:id="formula_202">E[R T ] ≤ C 1 √ K T √ τ + C 2 √ Kτ Γ T + C 3 K T τ ,</formula><p>with C 1 , C 2 and C 3 universal constants that depends only on the logarithm of τ .</p><p>In particular, setting τ = T 2/3 K 1/3 Γ 2/3 T yields:</p><formula xml:id="formula_203">E[R T ] = O(K 2/3 Γ 1/3</formula><p>T T 2/3 ) .</p><p>Proof.</p><formula xml:id="formula_204">E[R T ] = K i=1 Γ T φ=1 ∆ φ i E[N φ i ] = i,φ:∆ φ i &gt;∆ ∆ φ i E[N φ i ] + i,φ:∆ φ i ≤∆ ∆ φ i E[N φ i ] ≤ i,φ:∆ φ i &gt;∆ ∆ φ i E[N φ i ] + ∆ K i=1 Γ T φ=1 E[N φ i ] ≤ i,φ:∆ φ i &gt;∆ ∆ φ i E[N φ i ] + ∆T .</formula><p>The next step consists in upper bounding the expected number of times the arm i is selected in the φ-th phase. We recall that N φ i is defined as</p><formula xml:id="formula_205">N φ i = t∈φ 1(I t = i = i t ) = b φ t=b φ-1 1(I t = i = i t ) .</formula><p>We introduce N t (τ, i) = t s=t-τ +1 1(I s = i), the number of times the arm i was selected in the τ steps preceding t. We have the following:</p><formula xml:id="formula_206">N φ i = b φ-1 +τ -1 t=b φ-1 1(I t = i = i t ) + b φ t=b φ-1 +τ 1(I t = i = i t ) ≤ τ + b φ t=b φ-1 +τ 1(I t = i = i t ) ≤ τ + b φ t=b φ-1 +τ 1(I t = i = i t , N t (τ, i) ≤ A φ i ) + b φ t=b φ-1 +τ 1(I t = i = i t , N t (τ, i) &gt; A φ i ) .</formula><p>The first term can be bounded using (Garivier and Moulines, 2011, Lemma 1) that is restated here.</p><p>Lemma 9 (Lemma 1 in (Garivier and Moulines, 2011)). Let i ∈ {1, ..., K}. For any positive integer τ and any positive m, T t=K+1 1(I t = i, N t (τ, i) ≤ m) ≤ T /τ m .</p><p>Lemma 9 can be adapted to our setting and by introducing T φ the length of the φ-th stationary phase, one has:</p><formula xml:id="formula_207">b φ t=b φ-1 +τ 1(I t = i = i t , N t (τ, i) ≤ A φ i ) ≤ T φ /τ A φ i .</formula><p>This in turn gives,</p><formula xml:id="formula_208">N φ i ≤ τ + T φ /τ A φ i + b φ t=b φ-1 +τ 1(I t = i = i t , N t (τ, i) &gt; A φ i ) .</formula><p>We recall that the upper confidence bound for the sliding-window strategy has the following form in the K arm setting (Garivier and Moulines, 2011):</p><formula xml:id="formula_209">U CB i (t) = Xt (τ, i) + c t (τ, i) , with Xt (τ, i) = 1 N t (τ, i) t s=t-τ +1</formula><p>X s (i)1(I s = i) and c t (τ, i) = B ξ log(min(t, τ )) N t (τ, i) .</p><p>Following the same arguments than Garivier and Moulines (2011) when the event {I t = i = i t , N t (τ, i) &gt; A φ i } holds, at least one of the three following events E 1 , E 2 , E 3 must be true where: E 1 = { Xt (τ, i) &gt; µ t (i) + c t (τ, i)} the case where µ t (i) is over-estimated.</p><p>E 2 = { Xt (τ, i t ) &lt; µ t -c t (τ, i t )} the case where the best arm at time t is under-estimated. E 3 = {µ t -µ t (i) ≤ 2c t (τ, i), N t (τ, i) &gt; A φ i } the case where the means are too close to each others. From now on, we set</p><formula xml:id="formula_210">A φ i = 4B 2 ξ log(τ ) (∆ φ i ) 2</formula><p>.</p><p>In doing so, on the event E 3 the following holds: c t (τ, i) = B ξ log(min(t, τ )) N t (τ, i) &lt; B ξ log(min(t, τ ))</p><formula xml:id="formula_211">A φ i &lt; ∆ φ i 2 log(min(t, τ )) log(τ ) &lt; ∆ φ i 2 .</formula><p>Therefore, this choice of A φ i ensures that the event E 3 never occurs. Bounding the probability of the events E 1 and E 2 can be done with the concentration inequality established in <ref type="bibr">(Garivier and Moulines, 2011)</ref>. For any η &gt; 0, by selecting a specific value of ξ one can obtain, Plugging this in the regret's upper bound gives: Hence,</p><formula xml:id="formula_212">P</formula><formula xml:id="formula_213">E[R T ] ≤ i,φ:∆ φ i &gt;∆ ∆ φ i   τ + T φ /τ 4B 2 ξ log(τ ) (∆ φ i ) 2</formula><formula xml:id="formula_214">E[R T ] ≤ 4B 2 ξ log(τ )K ∆ T τ + ∆T + τ KΓ T B + 2KB log(τ ) log(1 + η) + 1 T τ .</formula><p>By differentiating with respect to ∆, the right hand side is maximized when setting ∆ = 2B ξ log(τ )K τ . With this value of ∆,</p><formula xml:id="formula_215">E[R T ] ≤ 4B ξ log(τ ) √ K T √ τ + BKτ Γ T + 2BK log(τ ) T τ .</formula><p>Now by selecting τ = T 2/3 K 1/3 Γ 2/3 T , we obtain the announced scaling.</p><p>Remark 4. The term T / √ τ that can be seen in the worst case bound proposed in Theorem 7 also appears in the gap independent bound of SC-SW-GLUCB (Theorem 5). When focusing on gap dependent bounds, there is also a strong similarity. In the K-arm setting, Equation (46) has a T /τ dependency. This term can also be seen in the GLB setting in Theorem 6 using an analogous assumption on the gap. This analogy explains why the upper-bounds have the same scaling in the K-arm and in the GLB setting. Going from T / √ τ to T /τ when adding the assumption on the gaps is the key step allowing a scaling of the regret of order O( √ T Γ T ).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>θ -µ a s θt a s Vt -1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>(Cao et al.,  2019, Corollary 1) and(Zhou et al., 2020, Corollary  4.3) is proved under an assumption on the minimal gap. This remains true for forgetting strategies: the bound of Garivier and Moulines (2011) is gap-dependent, Trovo et al. (2020) achieve a O(∆ -1 √ T Γ T ) regret. More demanding, the LM-DSEE and SW-UCB# algorithms from Wei and Srivatsva (2018) require the minimum gap as an input of the algorithm. Generally speaking, none of those works provide an analysis when the minimum gap can depend on the time horizon T and when the mean of different arms can be arbitrarily close. We suspect that forgetting policies would obtain a O(Γ 1/3 T T 2/3 ) worst case dependency as in Theorem 2 and that changepoint detection methods are likely to fail in such a case. Tightness of the Bound. For problems with a finite number of actions, Auer et al. (2018) have developed an algorithm that does not require the knowledge of the number of breakpoints nor assumption on the gaps. This was extended to the K-arm setting by Auer et al. (2019) and to the more general contextual bandits by Chen et al. (2019). Both works (Auer et al. (2019); Chen et al. (</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Knowledge of Γ T Optimizing the choice of the forgetting parameter γ (w.r.t. the regret bound) requires the knowledge of Γ T . The Bandit over Bandit (BOB) framework introduced by Cheung et al. (2019b) can be used to circumvent this requirement. When the assumption 5 is satisfied, following the proof from Cheung et al. (2019a) one would obtain a regret bound of order O</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Regret of the different algorithms in a 2D abruptly changing environment averaged on 200 independent experiments and the 25% associated quantiles.</figDesc><graphic coords="9,310.86,72.00,238.15,186.76" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>(Algorithm 1) is compared with GLM-UCB from Filippi et al. (2010), LogUCB1 from Faury et al. (2020) and with D-GLUCB from Russac et al. (2020).SC-D-GLUCB (resp. D-GLUCB) is related with LogUCB1 (resp. GLM-UCB) in the sense that the exploration terms have the same scaling but the former incorporate the exponential weights making it possible to adapt to changes. The average regret of the different policies together with their central 50% quantiles, averaged on 200 independent runs, are reported in Figure1for two different parameter values.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>8 )</head><label>8</label><figDesc>Corollary 6 (Asymptotic bound). If Γ T is known, by choosing τ = λ = d log(T ), the regret of SC-SW-GLUCB scales as</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>In the last inequality we have used ∆ φ i ≤ B coming from µ i (t) ∈ [0, B] for all i and all t ≤ T . Furthermore,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>In 21st Annual Conference on Learning Theory,COLT 2008COLT  , pages 355-366, 2008.   .    </figDesc><table><row><cell></cell><cell>Peter Auer, Pratik Gajane, and Ronald Ortner. Adap-Yoan Russac, Olivier Cappé, and Aurélien Garivier.</cell></row><row><cell></cell><cell>tively tracking the best arm with an unknown number Algorithms for non-stationary generalized linear ban-</cell></row><row><cell>Louis Faury, Marc Abeille, Clément Calauzènes, and Olivier Fercoq. Improved optimistic algorithms for</cell><cell>of distribution changes. In European Workshop on dits. arXiv preprint arXiv:2003.10113, 2020. Reinforcement Learning, EWRL 2018, 2018. Ambuj Tewari and Susan A Murphy. From ads to</cell></row><row><cell>logistic bandits. In Proceedings of the 37th Interna-</cell><cell>Peter Auer, Pratik Gajane, and Ronald Ortner. Adap-interventions: Contextual bandits in mobile health.</cell></row><row><cell>tional Conference on Machine Learning, ICML 2020,</cell><cell>tively tracking the best bandit arm with an unknown In Mobile Health, pages 495-517. Springer, 2017.</cell></row><row><cell>2020. Sarah Filippi, Olivier Cappé, Aurélien Garivier, and Csaba Szepesvári. Parametric bandits: The general-ized linear case. In Advances in Neural Information Processing Systems, NeurIPS 2010, pages 586-594,</cell><cell>number of distribution changes. In Conference on Francesco Trovo, Stefano Paladino, Marcello Restelli, Learning Theory, COLT 2019, pages 138-158, 2019. Francis Bach. Self-concordant analysis for logistic re-gression. Electron. J. Statist., 4:384-414, 2010. doi: and Nicola Gatti. Sliding-window thompson sam-pling for non-stationary settings. Journal of Artificial Intelligence Research, 68:311-364, 2020. 10.1214/09-EJS521. Michal Valko, Rémi Munos, Branislav Kveton, and</cell></row><row><cell>2010. Arthur Flajolet and Patrick Jaillet. Real-time bidding with side information. In Advances in Neural In-formation Processing Systems, NeurIPS 2017, pages</cell><cell>Tomáš Kocák. Spectral bandits for smooth graph Francis Bach. Adaptivity of averaged stochastic gra-dient descent to local strong convexity for logistic regression. Journal of Machine Learning Research, 15(19):595-627, 2014. functions. In Proceedings of the 31st International Conference on Machine Learning, ICML 2014, pages 46-54, 2014.</cell></row><row><cell>5168-5178, 2017. Pratik Gajane, Ronald Ortner, and Peter Auer. A sliding-window algorithm for markov decision pro-cesses with arbitrarily changing rewards and transi-tions. arXiv preprint arXiv:1805.10066, 2018.</cell><cell>Lai Wei and Vaibhav Srivatsva. On abruptly-changing Francis Bach and Eric Moulines. Non-strongly-convex smooth stochastic approximation with convergence rate o (1/n). In Advances in Neural Information Processing Systems, NeurIPS 2013, pages 773-781, 2013. and slowly-varying multiarmed bandit problems. In 2018 Annual American Control Conference (ACC), pages 6291-6296. IEEE, 2018. Peng Zhao, Lijun Zhang, Yuan Jiang, and Zhi-Hua Zhou. A simple approach for non-stationary linear</cell></row><row><cell></cell><cell>bandits. In Proceedings of the 23rd International</cell></row><row><cell></cell><cell>Conference on Artificial Intelligence and Statistics,</cell></row><row><cell></cell><cell>AISTATS 2020, 2020.</cell></row><row><cell></cell><cell>Huozhi Zhou, Lingda Wang, Lav R Varshney, and Ee-</cell></row><row><cell>Branislav Kveton, Manzil Zaheer, Csaba Szepesvari,</cell><cell>Peng Lim. A near-optimal change-detection based al-</cell></row><row><cell>Lihong Li, Mohammad Ghavamzadeh, and Craig</cell><cell>gorithm for piecewise-stationary combinatorial semi-</cell></row><row><cell>Boutilier. Randomized exploration in generalized</cell><cell>bandits. AAAI, 2020.</cell></row><row><cell>linear bandits. Proceedings of the 23nd International Conference on Artificial Intelligence and Statistics, AISTATS 2020, 2020.</cell><cell>Zhengyuan Zhou, Renyuan Xu, and Jose Blanchet. Learning in generalized linear contextual bandits with stochastic delays. In Advances in Neural Infor-</cell></row><row><cell>Tor Lattimore and Csaba Szepesvári. Bandit Algo-</cell><cell>mation Processing Systems, NeurIPS 2019, 2019.</cell></row><row><cell>rithms. Cambridge University Press, 2019.</cell><cell></cell></row><row><cell>Lihong Li, Wei Chu, John Langford, and Robert E. Schapire. A contextual-bandit approach to personal-ized news article recommendation. In WWW, 2010. Lihong Li, Yu Lu, and Dengyong Zhou. Provably optimal algorithms for generalized linear contextual bandits. In Proceedings of the 34th International Conference on Machine Learning, ICML 2017, pages 2071-2080, 2017. Yasin Abbasi-Yadkori, Dávid Pál, and Csaba Szepesvári. Improved algorithms for linear stochastic Rémi Munos. From bandits to Monte-Carlo Tree Search:</cell><cell>Proceedings of the 32nd Conference on Learning The-ory, COLT 2019, 2019. Wang Chi Cheung, David Simchi-Levi, and Ruihao Zhu. Hedging the drift: Learning to optimize under non-stationarity. arXiv preprint arXiv:1903.01461, 2019a. Wang Chi Cheung, David Simchi-Levi, and Ruihao Zhu. Learning to optimize under non-stationarity. In Proceedings of the 22nd International Conference</cell></row><row><cell>The optimistic principle applied to optimization and bandits. In Advances in Neural Information Process-ing Systems, NeurIPS 2011, pages 2312-2320, 2011. planning. 2014.</cell><cell>on Artificial Intelligence and Statistics, AISTATS 2019, 2019b.</cell></row><row><cell>Marc Abeille, Louis Faury, and Clément Calauzènes. Paat Rusmevichientong and John N Tsitsiklis. Linearly</cell><cell>Wei Chu, Lihong Li, Lev Reyzin, and Robert Schapire.</cell></row><row><cell>parameterized bandits. Mathematics of Operations</cell><cell></cell></row><row><cell>Research, pages 395-411, 2010.</cell><cell></cell></row></table><note><p><p>Instance-wise minimax-optimal algorithms for logistic bandits. arXiv preprint arXiv:2010.12642, 2020.</p>Peter Auer, Thomas Jaksch, and Ronald Ortner. Nearoptimal regret bounds for reinforcement learning. In Advances in neural information processing systems, NeurIPS, pages 89-96, 2008. S.P. Boyd and L. Vandenberghe. Convex optimization. Cambridge Univ Press, 2004. Yang Cao, Zheng Wen, Branislav Kveton, and Yao Xie. Nearly optimal adaptive procedure with change detection for piecewise-stationary bandit. Proceedings of the 22nd International Conference on Artificial Intelligence and Statistics, AISTATS 2019, 2019. O. Chapelle and L. Li. An empirical evaluation of Thompson Sampling. In Advances in Neural Information Processing Systems, NeurIPS 2011, 2011. Yifang Chen, Chung-Wei Lee, Haipeng Luo, and Chen-Yu Wei. A new algorithm for non-stationary contextual bandits: Efficient, optimal, and parameter-free. Contextual bandits with linear payoff functions. In Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics, AIS-TATS 2011, pages 208-214, 2011. Varsha Dani, Thomas P Hayes, and Sham M Kakade. Stochastic linear optimization under bandit feed-back. Aurélien Garivier and Eric Moulines. On upperconfidence bound policies for switching bandit problems. In International Conference on Algorithmic Learning Theory, ALT 2011, pages 174-188, 2011. Yoan Russac, Claire Vernade, and Olivier Cappé. Weighted linear bandits for non-stationary environments. In Advances in Neural Information Processing Systems, NeurIPS 2019, pages 12017-12026, 2019.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Same arguments than for Lemma 5</figDesc><table><row><cell>t-1 s=1 w 2 s σ 2</cell><cell></cell></row><row><cell>k µ</cell><cell>t s=1 w 2 s</cell></row><row><cell></cell><cell>d</cell></row></table><note><p><p>D.2 Determinant Inequalities</p>Proposition 8 (Determinant inequality). Let (λ t ) t be a deterministic sequence of regularization parameters. Let H t = s a s a s + λ t-1 I d . Under the Assumption 1 and ∀t, σ 2 t ≤ k µ , the following holds det(H t ) ≤ λ t-1 + d .</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>(E 1 ) ≤</figDesc><table><row><cell cols="2">log(min(t,τ ))</cell><cell></cell><cell></cell><cell cols="2">log(min(t,τ ))</cell></row><row><cell cols="2">log(1+η) min(t, τ )</cell><cell cols="3">and P(E 2 ) ≤</cell><cell>log(1+η) min(t, τ )</cell><cell>.</cell></row><row><cell>Consequently we have,</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>E[N φ i ] ≤ τ + T φ /τ</cell><cell cols="2">4B 2 ξ log(τ ) (∆ φ i ) 2</cell><cell>+ 2</cell><cell>t=b φ-1 +τ b φ</cell><cell>log(min(t,τ )) log(1+η)</cell></row></table><note><p><p>min(t, τ )</p>.</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>Proceedings of the 24 th International Conference on Artificial Intelligence and Statistics (AISTATS) 2021, San Diego, California, USA. PMLR: Volume 130. Copyright 2021 by the author(s). *Equal contribution.</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>the regret of the SC-SW-GLUCB algorithm satisfies:</p><p>Proof. First note that for any suboptimal action a ∈ A t , µ(a ,t θ t ) -µ(a θ t ) ≥ ∆ .</p><p>This implies</p><p>Using Proposition 9 one has,</p><p>The dynamic regret can then be upper bounded by,</p><p>. With those choices, 1. β δ T scales as d log(T ).</p><p>2. Γ T τ scales as O(c</p><p>3.</p><p>Dividing by ∆ yields the announced result.</p><p>When θt is in Θ it is also possible with a sliding window to obtain a usually better concentration result. This discussion is not reported here, but can be easily adapted from Proposition 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D USEFUL RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1 Self-Concordant Properties</head><p>In this section we state the main properties and lemma that can be obtained with the self-concordance assumption.</p><p>Lemma 4 (Lemma 9 in Faury et al. ( <ref type="formula">2020</ref>)). For any z 1 , z 2 ∈ R, we have the following inequality</p><p>Thanks to the self-concordance property we have an interesting relation between G t (θ 1 , θ 2 ) and H t (θ 1 ) or H t (θ 2 ) when both θ 1 and θ 2 ∈ Θ. This relation is made explicit in the next lemma.</p></div>			</div>
			<div type="references">

				<listBibl/>
			</div>
		</back>
	</text>
</TEI>
