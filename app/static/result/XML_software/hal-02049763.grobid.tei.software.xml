<TEI xmlns="http://www.tei-c.org/ns/1.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xml:space="preserve" xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">From Cost-Sensitive Classification to Tight F-measure Bounds</title>
				<funder ref="#_ZvJb6kU">
					<orgName type="full">FUI MIVAO</orgName>
				</funder>
				<funder ref="#_BtAgn4S">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher />
				<availability status="unknown"><licence /></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Kevin</forename><surname>Bascol</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Rémi</forename><surname>Emonet</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Elisa</forename><surname>Fromont</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Amaury</forename><surname>Habrard</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Guillaume</forename><surname>Metzler</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Marc</forename><surname>Sebban</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="laboratory">Laboratoire Hubert Curien</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="laboratory">UMR 5516</orgName>
								<orgName type="institution">Univ Lyon</orgName>
								<address>
									<addrLine>UJM</addrLine>
									<postCode>F-42023</postCode>
									<settlement>Saint-Etienne</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="laboratory">Laboratoire Hubert Curien UMR 5516</orgName>
								<orgName type="institution">Bluecime inc</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Univ Lyon</orgName>
								<address>
									<addrLine>UJM</addrLine>
									<postCode>F-42023</postCode>
									<settlement>Saint-Etienne</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="department">IRISA/Inria</orgName>
								<orgName type="institution">Univ. Rennes</orgName>
								<address>
									<postCode>35042</postCode>
									<settlement>Rennes cedex</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="laboratory">Laboratoire Hubert Curien</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff6">
								<orgName type="laboratory">UMR 5516</orgName>
								<orgName type="institution">Univ Lyon</orgName>
								<address>
									<addrLine>UJM</addrLine>
									<postCode>F-42023</postCode>
									<settlement>Saint-Etienne</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff7">
								<orgName type="laboratory">Laboratoire Hubert Curien UMR 5516</orgName>
								<orgName type="institution">Univ Lyon</orgName>
								<address>
									<addrLine>UJM</addrLine>
									<postCode>F-42023</postCode>
									<settlement>Saint-Etienne</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff8">
								<orgName type="laboratory">Laboratoire Hubert Curien UMR 5516</orgName>
								<orgName type="institution">Blitz inc</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff9">
								<orgName type="institution">Univ Lyon</orgName>
								<address>
									<addrLine>UJM</addrLine>
									<postCode>F-42023</postCode>
									<settlement>Saint-Etienne</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">From Cost-Sensitive Classification to Tight F-measure Bounds</title>
					</analytic>
					<monogr>
						<imprint>
							<date />
						</imprint>
					</monogr>
					<idno type="MD5">C4B50713A4A1DE6C904A37B203A63D02</idno>
					<note type="submission">Submitted on 26 Feb 2019</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-04-12T14:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid" />
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract xml:lang="fr">
<div><p>ou non, émanant des établissements d'enseignement et de recherche français ou étrangers, des laboratoires publics ou privés.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div><head n="1">Introduction</head><p>The F-measure <ref type="bibr" target="#b20">(van Rijsbergen, 1974</ref>) is a performance measure used in classification to evaluate the ability of a classifier to predict the labels of new instances with</p></div>
<div><head>Proceedings of the 22 nd</head><p>International Conference on Artificial Intelligence and Statistics (AISTATS) 2019, Naha, Okinawa, Japan. PMLR: Volume 89. Copyright 2019 by the author(s).</p><p>a good recall and a good precision (as an harmonic mean of these two measures). It is the most commonly used measure in imbalanced settings where using the accuracy of the classifier would greatly favor the majority class <ref type="bibr" target="#b4">(Chandola et al., 2009;</ref><ref type="bibr" target="#b13">Lopez et al., 2013)</ref>. This measure is parameterized by a parameter that controls the relative importance of the precision and the recall. For &lt; 1 (resp. &gt; 1), more importance is given to the precision (resp. recall), with = 1, they are considered equally important. The F-measure can be expressed in terms of the true positive rate and true negative rate of the model. These rates are count-based measures which makes the F-measure, in addition to being non convex, unsuitable for direct optimization <ref type="bibr">(Narasimhan et al., 2015a)</ref>.</p><p>Several methods have been studied to solve the Fmeasure optimization problem. They can roughly be separated into two categories: Decision Theoretic Approaches (DTA) <ref type="bibr" target="#b6">(Dembczyński et al., 2017)</ref> which tries to find the classifier that maximizes the expectation of the F-measure. More precisely, these methods usually fit a probabilistic model during training followed by an inference procedure at prediction time <ref type="bibr" target="#b5">(Decubber et al., 2018)</ref>. The probabilistic model can be learned by optimizing a "simpler" surrogate function (e.g., <ref type="bibr" target="#b8">(Dembczynski et al., 2011;</ref><ref type="bibr" target="#b9">Jansche, 2005;</ref><ref type="bibr" target="#b21">Ye et al., 2012</ref>; P.M. <ref type="bibr" target="#b18">Chinta and Murty, 2013)</ref>). The second category consists of Empirical Utility Maximization (EUM) methods that learn multiple accurate models with different parameters and keep the model which maximizes the F-measure <ref type="bibr" target="#b3">(Busa-Fekete et al., 2015;</ref><ref type="bibr" target="#b10">Joachims, 2005;</ref><ref type="bibr" target="#b14">Musicant et al., 2003;</ref><ref type="bibr" target="#b17">Parambath et al., 2014;</ref><ref type="bibr" target="#b22">Zhao et al., 2013;</ref><ref type="bibr">Narasimhan et al., 2015b)</ref>. In this second category, the parameters can be the different classification thresholds for probabilistic models <ref type="bibr" target="#b3">(Busa-Fekete et al., 2015;</ref><ref type="bibr" target="#b10">Joachims, 2005;</ref><ref type="bibr" target="#b22">Zhao et al., 2013;</ref><ref type="bibr">Narasimhan et al., 2015b)</ref> or the costs on the classification errors for cost-sensitive methods <ref type="bibr" target="#b14">(Musicant et al., 2003;</ref><ref type="bibr" target="#b17">Parambath et al., 2014;</ref><ref type="bibr">Koyejo et al., 2014)</ref>.</p><p>EUM methods focus on estimation on a possibly infinite training set while DTA approaches are concerned with generalization performance <ref type="bibr" target="#b6">(Dembczyński et al., 2017)</ref>. <ref type="bibr" target="#b21">Ye et al. (2012)</ref> shows that both categories of methods give asymptotically the same results and propose heuristics to decide on the category to use depending on the context.</p><p>The work presented in this paper falls into the EUM based methods within a cost-sensitive classification approach. Indeed, by taking into account some per-class misclassification-costs, cost-sensitive learning aims at dealing with problems induced by class-imbalanced datasets. One of the few recent papers addressing the F-measure optimization, from a theoretical point of view, (see also <ref type="bibr" target="#b3">(Busa-Fekete et al., 2015;</ref><ref type="bibr" target="#b22">Zhao et al., 2013;</ref><ref type="bibr">Koyejo et al., 2014;</ref><ref type="bibr">Narasimhan et al., 2015b)</ref>) is the work from <ref type="bibr" target="#b17">(Parambath et al., 2014)</ref> . The authors propose a grid-based approach to find the optimal costs for which a cost-sensitive classifier would give the best F-measure. They theoretically prove that, with a sufficiently precise grid, one can be arbitrarily close to the optimal F-measure. However, this method relies on a relatively loose result which imposes to parse the whole grid leading an unnecessary computational burden. The methods proposed by <ref type="bibr">Koyejo et al. (2014)</ref> and by <ref type="bibr">Narasimhan et al. (2015b)</ref> achieve good performances with a limited time budget using a cost-sensitive approach. They roughly consist of fitting a probabilistic model then using a threshold in order to optimize the F-measure. In the first cases the threshold is tuned on a validation set while an iterative process based on the bisection algorithm <ref type="bibr" target="#b2">(Boyd and Vandenberghe, 2004</ref>). However we will see that, despite their simplicity (and the theoretical guarantees provided), it is possible to achieve higher performance by training (a few) number of models. Indeed tuning a model is not enough and we need to learn a different hyperplane to take the costs on each class into account. In this article, we propose a novel tighter theoretical result for cost-sensitive-based algorithms which allows us to derive a new efficient algorithm for F-measure optimization. Our contributions can be summarized as follows:</p><p>• we demonstrate tight theoretical guarantees on the F-measure of classifiers obtained from costsensitive learning;</p><p>• we give a geometric interpretation of the theoretical guarantees: they can be represented as unreachable regions (cones) in a 2D space where the x-axis</p><p>gives the value of a parameter t that controls the relative costs of the considered classes, and the y-axis gives the F-measure of the corresponding cost-sensitive classifier;</p><p>• going beyond traditional asymptotic analysis, we study the actual behavior of our bounds, on real datasets, showing it is much tighter than previous existing results;</p><p>• inspired by our bounds and their interpretation, we introduce an algorithm to explore the space of costs: our experiments show the relevance of (i) using our algorithm compared to other baselines (such as <ref type="bibr" target="#b17">Parambath et al. (2014)</ref>), and (ii) retraining the model iteratively compared to the previously described methods <ref type="bibr">(Koyejo et al. (2014)</ref>; <ref type="bibr">Narasimhan et al. (2015b)</ref>) that only tune an offset or threshold.</p><p>In Section 2, we introduce the notations and present our theoretical bound on the optimal F-measure based on a cost-sensitive approach and the pseudo-linear property of the F-measure. We give a geometric interpretation in Section 3 and introduce an algorithm that iteratively selects classification costs that lead to a near-optimal F-measure. Section 4 is devoted to the experiments on real datasets. These experiments show the effectiveness of the proposed bounds from a practical point of view. Furthermore, they show that it is possible to reach higher performance than a single model tuned a posteriori or much faster than grid search methods. We finally conclude in Section 5.</p></div>
<div><head n="2">Theoretical Bounds</head><p>Because of limited space, the following proofs focus on the binary classification case, the multi-class results are given in the supplementary material.</p></div>
<div><head n="2.1">Notations</head><p>Let X = (x 1 , ..., x m ), where x i 2 R n , be the set of m training instances and Y = (y 1 , ..., y m ) their corresponding label, where y 2 {0, 1}. Let H be a family of hypothesis e.g., linear separators. For a given hypothesis h 2 H learned from (X, Y), the errors that h makes can be summarized in an error profile, noted E(h), which, in the binary case can be defined as (FN(h), FP(h)).</p><p>In a binary setting, P is the proportion of positive instances and N the proportion of negative examples. We also denote by e the vector (e 1 , e 2 ) where e 1 and e 2 are respectively the proportion of False Negative (FN) examples and the proportion of False Positive (FP) ones as introduced previously. We then denote as E(H) the set of all possible error profiles for a given set of hypotheses: an error profile e = (e 1 , e 2 ) is in E(H) if there exists an hypothesis h 2 H that yields proportions of e 1 false negatives and e 2 false positives.</p><p>We first recall the definition of F-measure for any value of :</p><formula xml:id="formula_0">F = (1 + 2 )(P F N) (1 + 2 )P F N + F P .</formula><p>Using the above notations, the F-measure, F (e), defined in terms of the error profile e can be rewritten as:</p><formula xml:id="formula_1">F (e) = (1 + 2 )(P e 1 ) (1 + 2 )P e 1 + e 2 .</formula></div>
<div><head n="2.2">Pseudo linearity property</head><p>The F-measure is a linear-fractional function, i.e. it can be written as the ratio of two affine functions of the error profile. We briefly recall how to show that the F-measure is a pseudo-linear function, which is one of the main property of linear-fractional function. This property is the starting point of the demonstration of our main theoretical result. Definition 1. [from <ref type="bibr" target="#b19">Rapcsák (1991)</ref>] A real differentiable function f defined on an open convex set C ⇢ R q is said to be pseudo-convex if for every e, e 0 2 C, hrf (e), (e 0 e)i 0 =) f (e 0 ) f (e),</p><p>where rf denotes the gradient of the function f .</p><p>The pseudo-convexity is used to define the pseudolinearity as we see below.</p></div>
<div><head>Definition 2. A function f defined on an open convex</head><p>C is said to be pseudo-linear if both f and f are pseudo-convex.</p><p>It is now easy to show that the F-measure has the property of pseudo-linearity. Proposition 1. The F-measure is a pseudo-linear function.</p><p>Proof 1. See Supplementary Material.</p><p>Using this property, we are able, using a result from <ref type="bibr" target="#b0">Alberto and Laura (2009)</ref> to give a link between the F-measure and a cost-sensitive function, i.e. a function which assigns weights to each classes. Proposition 2. [Theorem 3.3.9 from Alberto and Laura ( <ref type="formula">2009</ref>)] Let f be a non-constant differentiable function on an open convex set C 2 R q , q &gt; 0. Then f is pseudo-linear on C if and only if the following properties hold: (i) each of the level sets of f is the intersection of C with a hyperplane;</p><p>(ii) rf (e) 6 = 0 for all e 2 C.</p><p>Let us consider the set of error profile {e 2 R 2 | (1 + 2 )P e 1 +e 2 &gt; 0} (which is always the case in practice with the F-measure). Then according to the previous theorem, we rewrite (i) as follows: It exists a : R ! R 2 and b : R ! R such that</p><formula xml:id="formula_2">F (e) = t () ha(t), ei + b(t) = 0,</formula><p>which can be rewritten : ha(F (e)), ei + b(F (e)) = 0.</p><p>(1)</p><p>For the F-measure, the functions a and b are defined by a(t) = (1 + 2 t, t) and b(t) = (1 + 2 )P (t 1).</p><p>The term ha(t), ei can be seen as a weighted error loss function, and thus a(t) can be seen as the costs to assign to each class.</p></div>
<div><head n="2.3">Bounds on the optimal F-measure</head><p>We now show the importance of the function a and of the parameter t to characterize the difference of F-measure between any two error profiles.</p><p>Step 1: impact of a change in the error profile</p><p>We first derive the relation between the difference in Fmeasure (F ) and the difference in error profile (e). We thus consider e and e 0 any two error profiles and denote by F (e) and F (e 0 ) the corresponding F-measures.</p><p>From the pseudo-linearity property (Eq. ( <ref type="formula" target="#formula_13">1</ref>)), we have:</p><formula xml:id="formula_3">0 = ha(F (e)), ei + b(F (e)),<label>(2)</label></formula><formula xml:id="formula_4">0 = ha(F (e 0 )), e 0 i + b(F (e 0 )).<label>(3)</label></formula><p>We now develop ha(F (e 0 )), e e 0 i and make the difference in F-measure appears in its expression.</p><formula xml:id="formula_5">ha(F (e 0 )), e e 0 i = ha(F (e 0 )), ei + b(F (e 0 )), = ha(F (e 0 )), ei ha(F (e)), ei b(F (e)) + b(F (e 0 )),</formula><p>ha(F (e 0 )), e e 0 i = (F (e 0 ) F (e))</p><formula xml:id="formula_6">• (1 + 2 )P 1 e 1 + e 2 ,</formula><p>where the first line uses the linearity of the inner product and Eq. (3). The second uses Eq. ( <ref type="formula" target="#formula_3">2</ref>) and the last line uses the definition of a and b defined in Section 2.2. Now we can rewrite the difference in F-measure as:</p><formula xml:id="formula_7">F (e 0 ) F (e) = e • ha(F (e 0 )), e e 0 i,<label>(4)</label></formula><p>where e = 1 (1 + 2 )P e 1 + e 2 .</p><p>Step 2: bounds on the F-measure F (e 0 )</p><p>We suppose that we have a value of t for which a weighted-classifier with weights a(t) has been learned. This classifier has an error profile e and a F-measure F (e). We now imagine a hypothetical classifier that is learned with weights a(t 0 ), and we denote by e 0 the error profile of this classifier. For any value of t 0 , we derive an upper bound on the on the F-measure F (e 0 ) that this hypothetical classifier can achieve.</p><p>Starting from the result obtained in Eq. 4, we have:</p><formula xml:id="formula_8">F (e 0 ) F (e)</formula><p>= e (ha(t 0 ), ei ha(t 0 ), e 0 i) , = e (ha(t), ei + ha(t 0 ) a(t), ei ha(t 0 ), e 0 i) , = e (ha(t), ei + (t 0 t)(e 2 e 1 ) ha(t 0 ), e 0 i) ,  e (ha(t), e 0 i + " 1 ha(t 0 ), e 0 i + (t 0 t)(e 2 e 1 )) ,  e ((t t 0 )(e 0 2 e 0 1 ) + " 1 + (t 0 t)(e 2 e 1 )) , </p><p>e " 1 + e • (e 2 e 1 (e 0 2 e 0 1 ))(t 0 t).</p><p>We have successively used the linearity of the inner product, introduced a(t) and its definition in the first three equalities. The first inequality uses ha(t), ei  ha(t), e best i + " 1 , the sub-optimality of the a(t)-weighted-error classifier. The value of " 1 represents the excess of risk of the classifier which aim to minimize the a(t)-weighted-error. More precisely, it represents the difference of risk between our classifier and the best classifier h best (in terms on a(t)-weighterror) in our set of hypothesis H. We denote by e best the error profile associated to h best . This inequality is still true if we replace e best by any vector e 0 . We finally apply the definition of a.</p><p>The quantities e 0 2 and e 0 1 remain unknown, but can be tightly bounded. The result of this development can be summarized into the following proposition (see the supplementary material for the derivation of the values of M min and M max ).</p><p>Proposition 1. Let e be the error profile obtained with a classifier trained with the parameter t and F (e) its associated F-measure value. Let us also consider e as defined in Eq. (4) and " 1 &gt; 0 the sub-optimality of our linear classifier. Then for all t 0 &lt; t:</p><p>F (e 0 )  F (e) + e " 1 + e With this first result, we give an upper bound on the reachable F-measures for any value of t 0 given an observed value of F-measure with the parameter t. A geometric interpretation and an illustration of this result will be provided in Section 3.1.</p><p>Corollary 1. Given the same assumptions and considering t ? the value of t for which the best cost-sensitive learning algorithm leads to a model with an error profile e ? associated to the optimal F-measure, we have: if t ? &lt; t:</p><p>F (e ? )  F (e) + e " 1 + e • (e 2 e 1 M max )(t ? t), and, if t ? &gt; t:</p><p>F (e ? )  F (e) + e " 1 + e • (e 2 e 1 M min )(t ? t).</p><p>This means that if we learn a model with a parameter t sufficiently close to t ? then, we guarantee to reach the optimal F-measure up to a constant equal to e " 1 .</p></div>
<div><head n="3">Geometric Interpretation, CONE</head><p>In this section we provide a geometric interpretation of our main result, i.e. Proposition 1 of Section 2.3 and compare it to the bound introduced in <ref type="bibr" target="#b17">Parambath et al. (2014)</ref>. We also show how this theoretical result can be an inspiration to create an algorithm, CONE, which optimizes the F-measure by wrapping a cost-sensitive learning algorithm.</p></div>
<div><head n="3.1">Unreachable regions</head><p>In Fig. <ref type="figure" target="#fig_4">1</ref> (left), we give a geometric interpretation of the result from Prop. 1 in the 2-D space where t is the x-axis and F is the y-axis. In this (t, F ) graph, the previous near-optimality result yields an upper cone of values where F (e ? ) cannot be found. More precisely, when a model is learned for a given value of t (with weights a(t)), we measure the value F (e) of this model and, given these two numbers, we are able to draw an upper cone which represents the unreachable values of F-measure for any t 0 on the x-axis. Furthermore, given " 1 , the sub-optimality of the cost-sensitive learning algorithm for the weighted-0/1 loss, e " 1 corresponds to the vertical offset of this cone, which means that the peak of the cone is located at (t, F (e) + e " 1 ).</p><p>Note that, even if the authors were focusing on asymptotic results, the bound given in <ref type="bibr" target="#b17">Parambath et al. (2014)</ref> can also be interpreted geometrically. Their bound is given as follows:</p><formula xml:id="formula_9">F (e ? )  F (e) + • (2" 0 M + " 1 ), where M = max e2E(H)</formula><p>kek 2 , = ( 2 P ) 1 and " 0 is a gap parameter defined as the `2 norm of the difference between a weighted function a and the optimal one a ? . In the supplementary material, we detail how this bound can, in fact, be rewritten for all t, t 0 2 [0, 1] as:</p><formula xml:id="formula_10">F (e 0 )  F (e) + " 1 + 4M |t 0 t|.</formula><p>This bound also defines a cone which is, this time, symmetric with a slope equal to 4 M , as illustrated in Fig. <ref type="figure" target="#fig_4">1</ref> (right). Using real datasets, Section 4.2 compares the cones produced by this bound and ours.</p></div>
<div><head n="3.2">A bound-inspired algorithm</head><p>We now leverage the geometric interpretation from Section 3.1 to design CONE (Cone-based Optimal Next Evaluation), an iterative algorithm that wraps a cost-sensitive classification algorithm (e.g., a weighted SVM). At every iteration i, CONE proposes a new value t i to be used by the cost-sensitive algorithm. CONE is illustrated in Fig. <ref type="figure">3</ref> and is explained below.</p><p>The choice of t i is based on the area Z i 1 which we define as the union of all cones obtained from previous iterations. t i is chosen to reduce the maximum value of F for which (t, F ) is not in any previous cone. Initialize L = {0, 1}, Z 0 = ? and i = 1. refine the unreachable area as Z i = Z i 1 [ V i , where V i is the cone corresponding to (t i , F i ). In the case where there are multiple values of t that maximize F max (t) (e.g., at the beginning, or when some range of t values yield F = 1), CONE selects as t opt the middle of the widest range at the first stage (i) (see the white dotted lines in Fig. <ref type="figure">3</ref>).</p><formula xml:id="formula_11">repeat t i = findNextT (Z i 1 , L) classif ier i = wLearn(1 + 2 t i , t i , S) F i = F (classif ier i , S) V i = unreachableZone(t i , F i , S, classifier i ) Z i = Z i 1 [ V i L = L [ {t i } i = i + 1 until shouldStop(i, classif ier i , Z i , L)</formula><p>From a practical perspective, Z i can be represented as a combination of linear constraints or as a very dense grid of binary values (a rasterization of [0, 1] ⇥ [0, 1], the (t, F ) space). Both approaches can be made efficient (and negligible compared to wLearn). The stopping criterion shouldStop can take different forms including a fixed number of iterations, a fixed time budget, or some rules on the current best F-measure and the current upper bound max t F max (t). While the algorithm we describe selects a single next value of t to consider, it can easily be generalized to produce multiple values of t to consider in parallel (to exploit parallel computing of multiple instances of wLearn).</p><p>By always selecting a t i that is in the middle of two restrict the values of t in the (t, F )-space that the algorithm considers. More precisely, we can limit the depth of the progressive refinement to an integer value k. In this case, and CONE will do at most 2 k 1 iterations, in order to cover all possible values on a grid with stride 1 2 k . However, as the procedure is informed by the theoretical bounds, we will see in Section 4.3 that CONE finds good models in its very first iterations.</p></div>
<div><head n="4">Experiments</head><p>The experiments from this section study the tightness of our bounds and behavior of the CONE algorithm.</p></div>
<div><head n="4.1">Datasets and experimental settings</head><p>Table <ref type="table" target="#tab_2">1</ref> describes the datasets we used for our experiments, with their Imbalance Ratio (I.R.). The higher this ratio, the more one should expect that optimizing the classification accuracy is a bad choice in terms of trade-off between precision and recall. The datasets IJCNN'01 and News20 are obtained from <software ContextAttributes="used">LIBSVM</software> 1 . The other ones come from the UCI repository 2 .</p><p>We reproduce the experimental settings from <ref type="bibr" target="#b17">Parambath et al. (2014)</ref> which we describe here. For datasets with no explicit test set, 1 4 of the data is kept for testing. The training set is split at random, keeping 1 3 as the validation set, used to select the hyper-parameters using the F 1 -measure. The penalty constraint of the classifiers (hyper-parameter C) is considered in {2 6 , 2 5 , ..., 2 6 }. In the experiments t is taken in [0, 1] as t belongs in the image space of the 1 https://www.csie.ntu.edu.tw/~cjlin/libsvm/ 2 https://archive.ics.uci.edu/ml/datasets.html F-measure. Thus the class weights a(t) belongs to [0, 1 + 2 ]. The maximal number of training iterations is set to 50000. Fitting the intercept of the classifiers is achieved by adding a constant feature with value 1. We report test-time F 1 -measure averaged over 5 experiments.</p><p>We consider two different base cost-sensitive classification algorithms (both implementations use LIBLIN-EAR): linear SVM and Logistic Regression (LR) for a fair comparison with <ref type="bibr">Koyejo et al. (2014)</ref>. We report the performance of 5 different approaches: using a single standard classification algorithm with hyperparameters tuned on the F-measure, the Grid wrapper proposed in <ref type="bibr" target="#b17">Parambath et al. (2014)</ref> that regularly splits the interval [0, 1] of t values, the algorithm derived from our theoretical study, algorithm 2 from <ref type="bibr">Narasimhan et al. (2015b)</ref> based on the bisection method, and finally, an additional baseline (with the I.R. subscript), which consists in using a cost that re-balances the classes (the cost c of a False Negative is the proportion of positive examples in the dataset and the cost of False Positive is 1 c).</p><p>About " 1 . The value of " 1 (in all presented bounds) represents the a(t)-weighted sub-optimality of the classifier, compared to the best one from the hypothesis class. This sub-optimality cannot be computed efficiently as it would require a learning algorithm that produces optimal classifiers in terms of a(t)-weighted error. We thus start by studying the impact of " 1 in Section 4.2 on our bounds. As the focus of this paper is not on estimating " 1 , we then set " 1 = 0 which is computationnaly free, and shown by the experiment to be a reasonable choice both in terms of bound analysis (the bound is most of the time respected) and in terms of overall results from the CONE algorithm.</p></div>
<div><head n="4.2">Evaluation of the tightness of the bound</head><p>In this section, we aim at illustrating and showing the tightness of our bounds. To do so, we consider the (t, F ) values obtained by 19 weighted-SVM learned on a regular grid of t values. For these same 19 models, we consider the cones obtained from our bounds and previous work (see Section 3.1 for details). Due to space limitations, we show only two illustrations, with two different datasets, but the supplementary material contains similar illustrations for all datasets.</p><p>Impact of " 1 . Both our bounds and the one from previous work are impacted by " 1 which shows up as an offset, multiplied by e for our bounds, and by in previous work. As e  , our bounds are less impacted by an increased " 1 . With the 19-SVM setting, Fig. <ref type="figure" target="#fig_1">4</ref> shows the evolution of the maximum still-  <ref type="bibr" target="#b17">Parambath et al. (2014)</ref> and the subscript I.R. is used for the classifiers trained with a cost depending on the Imbalance Ratio. The subscript B corresponds to the bisection algorithm presented by <ref type="bibr">Narasimhan et al. (2015b)</ref>. LR T and LR T I.R. are reproduced experiments of <ref type="bibr">Koyejo et al. (2014)</ref>. Finally the C stands for our wrapper CONE and SVM T C designed as a combination using the CONE + threshold. Reported F-measure values are averaged over 5 experiments (standard deviation between brackets). <ref type="table" target="#tab_26">62.5 (0.2) 64.9 (0.3) 66.4 (0.1) 66.5 (0.1) 66.4 (0.1)  66.5 (0.1) 66.5 (0.1) 66.5 (0.1) 66.6 (0.1)  Abalone10  0.0 (0.0) 30.9 (1.2) 32.4 (1.3) 32.2 (0.8) 31.8 (1.9)  30.8 (2.2) 30.7 (1.9) 30.7 (1.9) 31.6 (0</ref>  achievable F-measure depending on the value of " 1 , with a hard maximum at 1. The values of " 1 are expressed in number of points for an easier interpretation.</p><formula xml:id="formula_12">Dataset SVM SVMI.R. SVMG SVMC SVM T C LR T LR T I.R. LR T G LRB Adult</formula><p>The bound from <ref type="bibr" target="#b17">Parambath et al. (2014)</ref> gives loose guarantees and the aggregate bound is most of the time above 1. The values, before being clipped to 1 can for example start at F = 7 and end up at F = 40 (on Yeast, if plotted on the same range of " 1 values). This representation shows once again that our bounds are very tight. On Abalone10 and Letter, where the other bound starts below 1 (see supplementary), the graph also confirms the fact that our bounds aref less sensitive to the value of " 1 ( e  ).</p><p>Visualizing unreachable zones. The grayed-out areas in Fig. <ref type="figure">5</ref> are the unreachable zones. This figure shows that the guarantees obtained with our bounds are much more relevant than the ones from <ref type="bibr" target="#b17">Parambath et al. (2014)</ref>. Indeed, it is only on two datasets (Abalone10 and Letter, see supplementary) that the previously existing bound actually gives a maximum possible Fmeasure that is below 1. Our bounds give unreachable zones that go very close to the empirical points.</p><p>Looking at the cones with our tight bounds, we see that sometimes a point is in the cone generated by another point. This looks like a violation of our bounds but it rather shows that " 1 cannot be considered to be 0 in the current setting. Naturally, " 1 6 = 0 comes from the fact that the weighted-SVM is not robust and not optimal in terms of weighted-0/1 loss. Our intuition is that the SVM is less and less optimal as the weights become more extreme, such as when t gets closer to 0.</p><p>Bounds' evolution across iterations. We now study, with CONE, how the training performance and the overall bound evolve as we add more models.</p><p>In CONE adding a model means doing one more iteration, while with the grid approach Parambath et al.</p><p>(2014) it requires to re-learn all models (as all grid locations change). Fig. <ref type="figure" target="#fig_2">6</ref> (and supplementary) illustrates that CONE tends to produce better models at a lower cost. These figures also outline the fact that our upper bound is tight and goes down quickly as we add models. The baseline of using a simple SVM completely fails on half of the datasets. The improved SVM which consists in rebalancing the classes (SVM I.R. ) still performs worse than other approaches in average, and on most datasets. Even with thresholding, the approaches that learn a single model (LR T and LR T I.R. ) are still outperformed by the ones that learn multiple models with different class-weights like ours (all subscripts C ) and the grid one (subscripts G ). This last result shows that it is insufficient to solely rely on tuning the threshold of a single model.  <ref type="formula">2014</ref>), together with their respective bounds (on Adult (left) and Abalone12 (right)). We suppose " 1 = 0, which explains that we observe empirical values that are higher than our upper bound (on Abalone12). In average, both our approach and the grid approach outperform all other considered approaches, including the bisection algorithm LR B . We see that the results of CONE are very similar to the grid approach SVM G . However, looking at Fig. <ref type="figure" target="#fig_3">7</ref> at the bottom (but also in supplementary), we see that the proposed method is able to reach higher values with a limited number of iterations, i.e. after training fewer models.</p></div>
<div><head n="4.3">Performance in</head></div>
<div><head n="5">Conclusion</head><p>In this work, we have presented new bounds on the F-measure based on a cost-sensitive classification approach. These bounds have been shown to be tighter than existing ones and less sensitive to the sub-optimality of the learned classifier (" 1 ). Furthermore, we have shown that our bounds are useful from a practical point of view by deriving CONE, an algorithm which iteratively selects class weights to reduce the overall upper bound on the optimal F-measure. Finally, CONE has been shown to perform at least as well as its competitors on various datasets. If this work focuses on the F-measure, it can be generalized to any other linear-fractional performance measure. Our perspectives include estimating " 1 (for example using <ref type="bibr" target="#b1">(Bousquet et al., 2004)</ref>), refining our framework to improve the search space exploration or to adapt it to SGD-based learning algorithms, and finally deriving generalization guarantees.</p><p>-Supplementary Material -From Cost-Sensitive Classification to Tight F-measure Bounds The goal of this document is to:</p><p>• detail the proof of the results provided in the main article,</p><p>• develop the multi-class extension,</p><p>• provide illustrations and results on all considered datasets,</p><p>• give numerical values used to plot the curves (for easier reproducibility).</p><p>For the sake of clarity, we will remind each statement before giving its proof. We also recall the notations and the definitions that are used for our purpose.</p><p>In the body of the paper, the error profile of an hypothesis h as been defined as E(h) = (e 1 (h), e 2 (h)) = (F N(h), F P (h)) . In the binary setting and using the previous notations, the F-Measure is defined by:</p><formula xml:id="formula_13">F (e) = (1 + 2 )(P e 1 ) (1 + 2 )P e 1 + e 2 . (<label>1</label></formula><formula xml:id="formula_14">)</formula><p>1 Main results of the article</p><p>In this section, we provide all the proofs of the main article but only in the binary setting.</p></div>
<div><head n="1.1">Pseudo-linearity of F-Measure</head><p>We aim to prove the following proposition, which plays a key role to provide a the bound on the F-measure.</p><p>Proposition 1. The F-measure, F , is a pseudo-linear function.</p><p>Proof. We need to show that both F and F are pseudo-convex, i.e. that we have:</p><p>hrF (e), (e 0 e)i 0 =) F (e 0 ) F (e).</p><p>(2)</p><p>1</p><p>The gradient of the F-measure is defined by:</p><formula xml:id="formula_15">rF (e) = 1 + 2 ((1 + 2 )P e 1 + e 2 ) 2 ✓ 2 P + e 2 P e 1 ◆ .</formula><p>We now develop the left hand side of the implication (2):</p><p>hrF (e), (e 0 e)i 0, 1 + 2 ((1 + 2 )P e 1 + e 2 ) 2</p></div>
<div><head>⇥</head><p>( 2 P + e 2 )(e 0 1 e 1 ) + (P e 1 )(e 0 2 e 2 ) ⇤ 0, so, e 1 e 0 2 . Now we add P (e 1 + e 0 1 ) on both side of the inequality, so we have: 2 P e 0 1 + P e 2 e 0 1 e 2 P (e 1 + e 0 1 ) 2 P e 1 + P e 0 2 e 1 e 0 2 P (e 1 + e 0 1 ), (1 + 2 )P e 0 1 + P e 2 e 0 1 e 2 P e 1 (1 + 2 )P e 1 + P e 0 2 e 1 e 0 2 P e 0 1 .</p><formula xml:id="formula_16">( 2 P + e 2 )(</formula><p>Then, we add e 1 e 0 1 on both sides: (1 + 2 )P e 0 1 + P e 2 e 0 1 e 2 P e 1 + e 1 e 0</p></div>
<div><head>1</head><p>(1 + 2 )P e 1 + P e 0 2 e 1 e 0 2 P e 0 1 + e 1 e 0 1 , (1 + 2 )P e 0 1 (P e 0 1 )e 1 + (P e 0 1 )e 2 (1 + 2 )P e 1 (P e 1 )e 0 1 + (P e 1 )e 0 2 . Finally, by adding (1 + 2 )P 2 on both sides of the inequality and factorizing with the terms (1 + 2 )P e 0 1 on the left (respectively (1 + 2 )P e 1 on the right), we get: (1 + 2 )P (P e 0 1 ) (P e 0 1 )e 1 + (P e 0 1 )e 2 (1 + 2 )P (P e 1 ) (P e 1 )e 0 1 + (P e 1 )e 0 2 , (1 + 2 )P (P e 0 1 ) (P e 0 1 )e 1 + (P e 0 1 )e 2 (1 + 2 )P (P e 1 ) (P e 1 )e 0 1 + (P e 1 )e 0 2 , (P e 0 1 )((1 + 2 )P e 1 + e 2 ) (P e 1 )((1 + 2 )P e 0 1 + e 0 2 ), (1 + 2 )(P e 0 1 )((1 + 2 )P e 1 + e 2 ) (1 + 2 )(P e 1 )((1 + 2 )P e 0 1 + e 0 2 ), (P e 0 1 ) (1 + 2 )P e 0</p><p>1 + e 0</p></div>
<div><head>2</head><p>(P e 1 ) (1 + 2 )P e 1 + e 2 ,</p><p>(1 + 2 )(P e 0 1 ) (1 + 2 )P e 0</p><p>1 + e 0</p></div>
<div><head>2</head><p>(1 + 2 )(P e 1 ) (1 + 2 )P e 1 + e 2 ,</p><p>F (e 0 ) F (e).</p><p>The proof is similar for F . We have shown that both F and F are pseudo-convex so F is pseudo-linear.</p><p>We can now use this property to derive our bound. However, we have seen that the bound still depends on to other parameters M min and M max that we should compute.</p><p>1.2 Computation of the values of M min and M max .</p><p>We aim to show how we can solve the optimization problems that define M min and M max and show how it can be reduced to a simple convex optimization problem where the set of constraints is a convex polygon.</p></div>
<div><head>Computation of M max</head><p>Now, we would like to give an explicit value for M max . This value can be obtained by solving the following optimization problem:</p><formula xml:id="formula_17">max e 0 2E(H) e 0 2 e 0 1 s.t. F (e 0 ) &gt; F (e).</formula><p>In the binary case, setting e = (e 1 , e 2 ) and e 0 = (e 0 1 , e 0 2 ). We can write F (e 0 ) &gt; F (e) as:</p><formula xml:id="formula_18">(1 + 2 )(P e 0 1 ) (1 + 2 )P e 0 1 + e 0 2 &gt; (1 + 2 )(P e 1 ) (1 + 2 )P e 1 + e 2 ,</formula><p>Now we develop and reduce these expressions.</p><p>(P e 0 1 )[(1 + 2 )P e 1 + e 2 ] &gt; (P e 1 )[(1 + 2 )P e 0 1 + e 0 2 ]), (1 + 2 )P 2 (1 + 2 )P e 0 1 + (P e 0 1 )(e 2 e 1 ) &gt; (1 + 2 )P 2 (1 + 2 )P e 1 + (P e 1 )(e 0 2 e 0 1 ), (1 + 2 )P (e 1 e 0 1 ) + P (e 2 e 1 + e 0 1 e 0 2 ) &gt; e 2 e 0 1 e 1 e 0 2 + e 0 1 e 1 e 1 e 0 1 . Now, we set: e 0 1 = e 1 + ↵ 1 and e 0 2 = e 2 + ↵ 2 . In other words, we study how much we have to change e 0 from e to solve our problem. We can then write:</p><p>(1 + 2 )P ↵ 1 + P (↵ 1 ↵ 2 ) &gt; e 2 (e 1 + ↵ 1 ) e 1 (e 2 + ↵ 2 ), ↵ 1 ( (1 + 2 )P + P e 2 ) + ↵ 2 ( P + e 1 ) &gt; 0,</p><formula xml:id="formula_19">↵ 1 ( 2 P + e 2 ) &lt; ↵ 2 (P e 1 ).</formula><p>Thus, the optimization problem can be rewritten as:</p><formula xml:id="formula_20">max ↵ ↵ 2 ↵ 1 , s.t. ↵ 1 &lt; ↵ 2 (P e 1 ) 2 P + e 2 , ↵ 1 2 [ e 1 , P e 1 ], ↵ 2 2 [ e 2 , N e 2 ].</formula><p>The optimization problem consists of maximizing a difference under a polygon set of constraints. In the binary setting, the set of constraints can be represented as shown in Fig. <ref type="figure" target="#fig_4">1</ref> where the line D is defined by the following equation: To maximize the difference, we should maximize the value of ↵ 2 and minimize the value of ↵ 1 , i.e. the solution is located in the bottom right region of each figure. A quick study of these figures shows that the lowest value of ↵ 1 we can reach is e 1 .</p><formula xml:id="formula_21">↵ 1 = ↵ 2 (P e 1 ) 2 P + e 2 .<label>(3)</label></formula><p>We shall now study where the line D intersects the rectangle to have the solution with respect to ↵ 2 . If D does not intersect the line of equation ↵ 1 = e 1 in the rectangle (i.e. it intersects with the right side of the rectangle) then ↵ 2 = N e 2 . Else, it intersects with the bottom face of the rectangle, then we determine the value of ↵ 2 using Eq. (3) and ↵ 2 = ( 2 P + e 2 )e 1 P e 1 .</p><p>Finally, the solution of the optimization problem is:</p><formula xml:id="formula_22">(↵ 1 , ↵ 2 ) = ✓ e 1 , min ✓ N e 2 ,</formula><p>( 2 P + e 2 )e 1 P e 1 ◆◆ , and the optimal value M max is defined by:</p><formula xml:id="formula_23">M max = e 2 + min ✓ N e 2 ,</formula><p>( 2 P + e 2 )e 1 P e 1 ◆ .</p></div>
<div><head>Computation of M min</head><p>We now aim to solve the following optimization problem:</p><formula xml:id="formula_24">min e 0 2E(H) e 0 2 e 0 1 s.t. F (e 0 ) &gt; F (e).</formula><p>As it has been done and using the same notations as in the previous section, we can rewrite the optimization problem as follows:</p><formula xml:id="formula_25">min ↵ ↵ 2 ↵ 1 , s.t. ↵ 1 &lt; ↵ 2 (P e 1 ) 2 P + e 2 , ↵ 1 2 [ e 1 , P e 1 ], ↵ 2 2 [ e 2 , N e 2 ].</formula><p>The constraints remain unchanged. However, to minimize this difference, we have to maximize the value of ↵ 1 and minimize the value of ↵ 2 , i.e. we are interested in the upper left region of each rectangles. In each cases represented in Fig <ref type="figure" target="#fig_4">1</ref>, we see that the minimum of ↵ 2 is equal to e 2 .</p><p>If we have a look at the two figures at the bottom of Fig. <ref type="figure" target="#fig_4">1</ref>, we see that the optimal value of ↵ 1 is equal to P e 1 . However, this value is not in the image of the function of ↵ 2 defined by Eq (3). In fact, according to Eq. ( <ref type="formula" target="#formula_4">3</ref>), the image of ↵ 2 = e 2 is equal to e 2 (P e 1 ) 2 P + e 2 which is lower than P e 1 .</p><p>So the two figures at the bottom represent cases that never happen and the intersection of D with the rectangle of constraint is on left part of the rectangle.</p><p>Finally, the solution of the optimization problem is:</p><formula xml:id="formula_26">(↵ 1 , ↵ 2 ) = ✓ e 2 (P e 1 ) 2 P + e 2 , e 2 ◆ ,</formula><p>and the optimal value M min is defined by: M min = e 1 e 2 (P e 1 ) 2 P + e 2 .</p><p>Now that we have provided all the details to compute and plot our bound, it remains to explain how to compute the bound from <ref type="bibr" target="#b17">Parambath et al. (2014)</ref> with respect to any cost parameters t, t 0 for a fair comparison.</p></div>
<div><head n="1.3">Rewriting the bound of Parambath et al. (2014)</head><p>For the sake of clarity we restate the Proposition 5 of <ref type="bibr" target="#b17">Parambath et al. (2014)</ref> for our purpose:</p><p>Proposition 2. Let t, t 2 [0, 1] and " 1 0. Suppose that it exists &gt; 0 such that for all e, e 0 2 E(H) satisfying F (e 0 ) &gt; F (e), we have:</p><formula xml:id="formula_27">F (e 0 ) F (e)</formula><p>ha(t 0 ), e e 0 i.</p><p>Furthermore, suppose that we have the two following conditions ke 00 k 2 , then we have:</p><formula xml:id="formula_29">(i) ka(t) a(t 0 )k 2  2|t t 0 | (ii) ha(t</formula><formula xml:id="formula_30">F (e 0 )  F (e) + " 1 + 4M |t 0 t|.</formula><p>According to the authors, the point (i) is a consequence of a of being Lipschitz continous with Lipschtiz constant equal to 2. The point (ii) is just the expression of the sub-optimality of the learned classifier.</p><p>Proof. For all e, ẽ 2 E(H) and t, t 0 2 [0, 1], we have:</p><formula xml:id="formula_31">ha(t), ẽi = ha(t) a(t 0 ), ẽi + ha(t 0 ), ẽi,  ha(t 0 ), ẽi + 2M |t 0 t|.</formula><p>Where we have successively applied the Cauchy-Schwarz inequality and (i). Then: min e 00 2E(H) ha(t), e 00 i  min e 00 2E(H) ha(t 0 ), e 00 i + 2M |t 0 t| = ha(t 0 ), e 0 i + 2M |t 0 t|,</p><p>where e 0 denote the error profile learned by the optimal classifier trained with the cost function a(t 0 ) and is such that F (e 0 ) &gt; F (e). Then, writing ha(t 0 ), ei = ha(t 0 ) a(t), ei + ha(t), ei and applying the Cauchy-Schwarz inequality, we have:</p><formula xml:id="formula_33">ha(t 0 ), ei  ha(t), ei + 2M |t 0 t|,  min e 00 2E(H) ha(t), e 00 i + " 1 + 2M |t 0 t|,  ha(t 0 ), e 0 i + " 1 + 4M |t 0 t|,</formula><p>where the second inequality comes from (ii) and the last inequality comes from Eq. ( <ref type="formula" target="#formula_32">5</ref>). By plugging this last inequality in inequality (4), we get the result. Furthermore, the existence of the constant has been proved by the authors and is equal to</p><formula xml:id="formula_34">( 2 P ) 1</formula><p>Remark. This bound can be used in both binary and multi-class setting.</p></div>
<div><head n="2">The multi-class setting</head><p>For a given hypothesis h H learned from X, the errors that h makes can be summarized in an error profile defined as E(h) 2 R 2L :</p><formula xml:id="formula_35">E(h) = (FN 1 (h), FP 1 (h), ..., FN L (h), FP L (h)) ,</formula><p>where FN i (h) (resp. FP i (h)) is the proportion of False Negative (resp. False Positive) that h yields for class i. </p><formula xml:id="formula_36">mcF (e) = (1 + 2 )(1 P 1 P L k=2 e 2k 1 ) (1 + 2 )(1 P 1 ) P L k=2 e 2k 1 + e 1 .</formula><p>In this section, we aim to derive all the results presented in the binary case in a multi-class setting.</p></div>
<div><head n="2.1">Pseudo-linearity</head><p>Proposition 3. The multi-class-micro F-measure, mcF , is a pseudo-linear function with respect to e.</p><p>Proof. As in the binary cases, we have to prove that both mc The gradient of the multi-class-micro F-measure,mcF , is defined by: rmcF (e) =</p><p>(1 + 2 )</p><p>(1 + 2 )(1 P 1 ) P L k=2 e 2k 1 + e 1</p><p>( 1 P 1 P L k=2 e 2k 1 w.r.t. e 1 , 2 (1 P 1 ) + e 1 w.r.t. e k 8k = 2, ..., L.</p><p>The proof is similar to the proof of Proposition 1. The scheme is the same, we simply have to do the following changes of notation in the proof:</p><formula xml:id="formula_37">e 1 L X k=2 e 2k 1 ,</formula><p>e 2 e 1 , P 1 P 1 .</p></div>
<div><head n="2.2">Derivation of the bound</head><p>As it was done in the binary case, we will use the property of pseudo-linearity of mcF (e) to bound the difference of micro F-measure in terms of the parameters of our weighted function. First, we introduce the definition of our weighted function a : R ! R 2L and express the difference of micro F-measure of two error profiles in function of the two error profiles. In this section, for the sake of clarity, we will set ê = P L k=2 e 2k 1 .</p></div>
<div><head>First step: impact of a change in the error profile</head><p>Using the property pseudo-linearity, we can show that it exists two functions a : R ! R 2L and b : R ! R defined by: 0 = ha(mcF (e)), ei + b(mcF (e)),</p><p>where:</p><formula xml:id="formula_38">a(t) = 8 &gt; &lt; &gt; :</formula><p>1 + 2 t for e 2k 1 , k = 2, ..., L t for e 1 , 0 otherwise, and b(t) = (t 1)(1 + 2 )(1 P 1 ).</p><p>From these definitions we can write: We can now write the difference of micro-F-measure as:</p><p>mcF (e 0 ) mcF (e) = e • ha(t), e e 0 i,</p><p>where:</p><formula xml:id="formula_39">e = 1 (1 + 2 )(1 P 1 ) ê + e 1 ,</formula><p>Second step: a bound on the micro F-measure mcF (e)</p><p>We suppose that we have a value of t for which a weighted-classifier with weights a(t) has been learned. This classifier has an error profile e and a F-measure mcF (e). We now imagine a hypothetical classifier that is learned with weights a(t 0 ), and we denote by e 0 the error profile of this classifier. For any value of t 0 , we derive an upper bound on the on the F-measure mcF (e 0 ) that this hypothetical classifier can achieve.</p><p>mcF (e 0 ) mcF (e) = e • ha(t 0 ), e e 0 i, = e • ha(t 0 ), ei ha(t 0 ), ei , = e • ha(t 0 ) a(t), ei + ha(t), ei ha(t 0 ), e 0 i , = e • h(t 0 t, t t 0 ), ei + ha(t), ei ha(t 0 ), e 0 i , = e • (t 0 t)(e 1 ê) + ha(t), ei ha(t 0 ), e 0 i ,</p><formula xml:id="formula_40"> e • ha(t), e 0 i + " 1 ha(t 0 ), e 0 i + (t 0 t)(e 1 ê) ,  e • (t 0 t)(e 1 ê) + " 1 (t 0 t)(e 0 1 ê0 ) ,  e " 1 + e • (e 1 ê (e 0 1 ê0 ))(t 0 t).</formula><p>In the previous development, we have used the linearity of the inner product and the definition of a.</p><p>The first inequality uses the sub-optimality of the learned classifier. We then use the definition of the function a.</p><p>As in the binary cases, the quantity (e 0 ê0 ) remains unknown. However, we are looking for a vector e 0 such that mcF (e 0 ) &gt; mcF (e). So the last inequality becomes, if t 0 &lt; t: mcF (e 0 ) mcF (e)  e " 1 + e (e 2 e 1 M max )(t 0 t), and, if t 0 &gt; t:</p><p>mcF (e 0 ) mcF (e)  e " 1 + e (e 2 e 1 M min )(t 0 t).</p></div>
<div><head n="2.3">Computation of M max and M min in a multiclass setting</head><p>To compute the value of both M max and M min , we use the same development as done in the binary setting. We have to search how to modify the vector e in order to improve the F-Measure and to maximize (or minimize) the difference: e 0 1 P L k=2 e 0 2k 1 , where e 0 = e + ↵ . As in the previous section, ↵ is the solution of the following optimization problem:</p><formula xml:id="formula_41">max ↵ ↵ 1 L X k=2 ↵ 2k 1 , s.t. ↵ 1 &lt; L X k=2 ↵ 2k 1 2 (1 P 1 ) + e 1 1 P 1 P L k=2 e 2k 1 ↵ 1 2 [ e 1 , P 1 e 1 ] , ↵ 2k 1 2 [ e 2k 1 , P 2k 1 e 2k 1 ] , 8k = 2, ..., L.</formula><p>Then we add the quantity e 1 P L k=2 e 2k 1 to this result to have the value M max . Similarly, we solve the following optimization problem:</p><formula xml:id="formula_42">min ↵ ↵ 1 L X k=2 ↵ 2k 1 , s.t. ↵ 1 &lt; L X k=2 ↵ 2k 1 2 (1 P 1 ) + e 1 1 P 1 P L k=2 e 2k 1 ↵ 1 2 [ e 1 , P 1 e 1 ] , ↵ 2k 1 2 [ e 2k 1 , P 2k 1 e 2k 1 ] , 8k = 2, ..., L.</formula><p>Then we add the quantity e 1 P L k=2 e 2k 1 to this result to have the value M min .</p></div>
<div><head n="3">Extended Experiments</head><p>This section is dedicated to the experiments. We provide all graphs and tables we were not able to give in the main paper and for all datasets.</p></div>
<div><head n="3.1">Illustrations of unreachable regions</head><p>In this section we provide the unreachable regions (see Fig. <ref type="figure" target="#fig_0">2</ref>) of both presented bounds, our vs. the one obtained from <ref type="bibr" target="#b17">Parambath et al. (2014)</ref>. As it was noticed in the main paper, our result gives a tighter bound on the optimal reachable F-measure. Moreover, we see that the more the data is imbalanced, the tightest is our bound. The fact that some points lie in unreachable regions is explained by our setting. Indeed, we recall that we made the assumption that " 1 = 0, i.e. we suppose that learned classifier is the optimal one, in terms of 0 1 loss, but it is not the case in practice.  </p></div>
<div><head n="3.2">Theoretical bound versus " 1</head><p>In this section we compare our bound with the one from <ref type="bibr" target="#b17">Parambath et al. (2014)</ref> with respect to " 1 . The graphics presented in Fig. <ref type="figure">3</ref> show that the bound from Parambath et al. ( <ref type="formula">2014</ref>) is uninformative since the value of the best reachable F-measure is always equal to 1 except on Abalone10 dataset. We see that our bound increase mostly linearly with " 1 . the evolution is not exactly linear because the value of e depends on the error profile, so it depends on the value of the parameter t in our cost function a. Note that the best classifier reaches a best F-measure in some cases (on Letter dataset for instance) which emphasize the need to look for an estimation of " 1 .</p><p>Figure <ref type="figure">3</ref>: Bounds on the F-measure as a function of " 1 , the unknown sub-optimality of the SVM learning algorithm. Results are given on all datasets.  <ref type="formula">2014</ref>) with respect to the number of iteration/ the size of the grid. We also represent the evolution of both associated algorithms.</p></div>
<div><head n="3.3">Evolution of Bounds vs. iterations/grid size</head></div>
<div><head n="3.4">Test-time results and result tables of results</head><p>For the sake of clarity, only a small number of algorithms have been chosen to be represented graphically in Fig. <ref type="figure">5</ref>.</p><p>Figure <ref type="figure">5</ref>: F-measure value with respect to the number of iterations or the size of the grid of four different algorithms, all of them are based on SVM.</p><p>To complete the results given in the main article, we provide two tables below. Table <ref type="table" target="#tab_2">1</ref> gives the value of the F-measure for all experiments with SVM or Logisitic Regression based algorithms.</p><p>Because we compare our method to some which uses a threshold to predict the class of an example <ref type="bibr">(Narasimhan et al., 2015;</ref><ref type="bibr">Koyejo et al., 2014)</ref>, we also provide a thresholded version of all algorithms in Table <ref type="table" target="#tab_3">2</ref>.</p><p>Table <ref type="table" target="#tab_2">1</ref>: Classification F-Measure for = 1 with SVM algorithm. SVM G are reproduced from <ref type="bibr" target="#b17">(Parambath et al., 2014)</ref> and the subscript I.R. is used for the classifiers trained with a cost depending on the Imbalance Ratio. The subscript B corresponds to the bisection algorithm presented in <ref type="bibr">(Narasimhan et al., 2015)</ref>. Finally the C stand for our wrapper CONE. The presented values are obtained by taking the mean F-Measure over 5 experiments (standard deviation between brackets). Letter 75.4 (0.7) 74.9 (0.8) 80.8 (0.5) 81.0 (0.4) 82.9 (0.3) 82.9 (0.3) 74.9 (0.5) 82.9 (0.2) 82.9 (0.3) News20 90.9 (0.   <ref type="bibr" target="#b17">(Parambath et al., 2014)</ref> and the subscript I.R. is used for the classifiers trained with a cost depending on the Imbalance Ratio. The subscript B corresponds to the bisection algorithm presented in <ref type="bibr">(Narasimhan et al., 2015)</ref>. Finally the C stand for our wrapper CONE. The presented values are obtained by taking the mean F-Measure over 5 experiments (standard deviation between brackets). Letter 75.4 (0.7) 74.9 (0.8) 80.8 (0.5) 81.0 (0.4) 82.9 (0.3) 82.9 (0.3) 82.9 (0.2) 82.9 (0.2) News20 90.9 (0.1) 91.0 (0.2) 91. Finally, we give here exhaustive tabular results, giving test-time F-measure results obtained by different methods when varying the budget (when meaningful) from 1 to 18 call to the weight classifier learning algorithm to complete the previous graphs. 2) 64.9 (0.3) 65.0 (0.4) 65.0 (0.4) 63.1 (0.1) 66.0 (0.1) 66.6 (0.1) 66.1 (0.1) 66.1 (0.1) Abalone10 0.0 (0.0) 30.9 (1.2) 0.0 (0.0) 0.0 (0.0) 0.0 (0.0) 31.9 (1.4) 31.6 (0.6) 24.4 (1.2) 24.4 (1.3) Satimage 0.0 (0.0) 23.4 (4.3) 0.9 (1.9) 0.0 (0.0) 0.5 (0.9) 24.2 (5.3) 21.4 (4.6) 3.5 (6.9) 3.5 (6.9) IJCNN 44.5 (0.4) 53.3 (0.4) 61.6 (0.5) 61.6 (0.5)  Letter 75.4 (0.7) 74.9 (0.8) 80.7 (0.5) 80.4 (0.5) 82.9 (0.3) 82.9 (0.3) 74.9 (0.5) 82.9 (0.2) 82.8 (0.2) News20 90.9 (0.1) 91.0 (0.2) 90.9 (0.  Letter 75.4 (0.7) 74.9 (0.8) 80.5 (0.2) 80.4 (0.5) 82.9 (0.3) 82.9 (0.3) 74.9 (0.5) 82.9 (0.2) 82.9 (0.2) News20 90.9 (0.   Letter 75.4 (0.7) 74.9 (0.8) 80.5 (0.4) 80.4 (0.5) 82.9 (0.3) 82.9 (0.3) 74.9 (0.5) 82.9 (0.2) 82.9 (0.3) News20 90.9 (0.  Letter 75.4 (0.7) 74.9 (0.8) 80.8 (0.5) 80.5 (0.4) 82.9 (0.3) 82.9 (0.3) 74.9 (0.5) 82.9 (0.2) 82.9 (0.3) News20 90.9 (0.   Letter 75.4 (0.7) 74.9 (0.8) 80.6 (0.4) 80.5 (0.4) 82.9 (0.3) 82.9 (0.3) 74.9 (0.5) 82.9 (0.2) 82.9 (0.3) News20 90.9 (0.  Letter 75.4 (0.7) 74.9 (0.8) 80.8 (0.5) 80.5 (0.5) 82.9 (0.3) 82.9 (0.3) 74.9 (0.5) 82.9 (0.2) 82.9 (0.3) News20 90.9 (0.   Letter 75.4 (0.7) 74.9 (0.8) 80.7 (0.3) 80.9 (0.4) 82.9 (0.3) 82.9 (0.3) 74.9 (0.5) 82.9 (0.2) 82.9 (0.3) News20 90.9 (0.  Letter 75.4 (0.7) 74.9 (0.8) 80.8 (0.5) 80.9 (0.4) 82.9 (0.3) 82.9 (0.3) 74.9 (0.5) 82.9 (0.3) 82.9 (0.3) News20 90.9 (0.  Letter 75.4 (0.7) 74.9 (0.8) 80.8 (0.5) 80.9 (0.4) 82.9 (0.3) 82.9 (0.3) 74.9 (0.5) 82.9 (0.3) 82.9 (0.3) News20 90.9 (0.  Letter 75.4 (0.7) 74.9 (0.8) 80.8 (0.5) 80.9 (0.4) 82.9 (0.3) 82.9 (0.3) 74.9 (0.5) 82.9 (0.2) 82.9 (0.3) News20 90.9 (0.1) 91.0 (0.2) 91.  63.1 (0.1) 66.0 (0.1) 66.6 (0.1) 66.4 (0.1) 66.5 (0.1) Abalone10 0.0 (0.0) 30.9 (1.2) 31.0 (1.0) 32.2 (0.8) 0.0 (0.0) 31.9 (1.4) 31.6 (0.6) 31.9 (0.5) 30.9 (1.9) Satimage 0.0 (0.0) 23.4 (4.3) 20.4 (5.3) 20.6 (5.6) 0.5 (0.9) 24. Letter 75.4 (0.7) 74.9 (0.8) 80.8 (0.5) 80.9 (0.4) 82.9 (0.3) 82.9 (0.3) 74.9 (0.5) 82.9 (0.2) 82.9 (0.3) News20 90.9 (0.1) 91.0 (0.2) 91. Letter 75.4 (0.7) 74.9 (0.8) 80.8 (0.5) 81.0 (0.4) 82.9 (0.3) 82.9 (0.3) 74.9 (0.5) 82.9 (0.2) 82.9 (0.3) News20 90.9 (0.  Letter 75.4 (0.7) 74.9 (0.8) 80.8 (0.5) 81.0 (0.4) 82.9 (0.3) 82.9 (0.3) 74.9 (0.5) 82.9 (0.2) 82.9 (0.3) News20 90.9 (0.1) 91.0 (0.2) 91.  Letter 75.4 (0.7) 74.9 (0.8) 80.8 (0.5) 81.0 (0.4) 82.9 (0.3) 82.9 (0.3) 74.9 (0.5) 82.9 (0.2) 82.9 (0.3) News20 90.9 (0.1) 91.0 (0.2) 91. </p></div><figure xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: CONE Algorithm</figDesc></figure>
<figure xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Bounds on the F-measure as a function of " 1 , the unknown sub-optimality of the SVM learning algorithm. Results are shown on two datasets: Adult (left) and Abalone12 (right).</figDesc></figure>
<figure xml:id="fig_2"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Training performance of CONE versus the grid approach from Parambath et al. (2014), together with their respective bounds (on Adult (left) and Abalone12 (right)). We suppose " 1 = 0, which explains that we observe empirical values that are higher than our upper bound (on Abalone12).</figDesc></figure>
<figure xml:id="fig_3"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: F-measure obtained on the test set for four considered approaches on Adult (top) and Abalone12 (bottom) datasets, plotted as a function of the computing budget (number of weighted SVM to learn).</figDesc></figure>
<figure xml:id="fig_4"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Geometric representation of the optimization problem. The rectangle represents the constraint (↵ 2 , ↵ 1 ) 2 [ e 2 , N e 2 ] ⇥ [e 1 , P e 1 ]. The white area represents the set of value (↵ 2 , ↵ 1 ) for which the inequality constraint holds. the four figures represent the four possibility for the position of the line D on the rectangle. See the computation of M min to see that cases represented by the two figures at the bottom never happen.</figDesc></figure>
<figure xml:id="fig_5"><head /><label /><figDesc>In a multi-class setting with L classes P k , k = 1, ..., L denotes the proportion of examples in class k and e = (e 1 , e 2 , ..., e 2L 1 , e 2L ) denotes the proportions of misclassified examples composing the error profile. The multi-class-micro F-measure, mcF (e) with L classes is defined by:</figDesc></figure>
<figure xml:id="fig_6"><head /><label /><figDesc>ha(mcF (e 0 )), e e 0 i = ha(mcF (e 0 )), ei + b(mcF (e 0 )), = ha(mcF (e 0 )) a(mcF (e)), ei b(mcF (e)) + b(mcF (e 0 )), = (mcF (e 0 ) mcF (e))(1 + 2 )(1 P 1 ) + (mcF (e 0 ) mcF (e))e 1 + (mcF (e) mcF (e 0 ))ê, = (mcF (e 0 ) mcF (e)) (1 + 2 )(1 P 1 ) ê + e 1 .</figDesc></figure>
<figure xml:id="fig_8"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Unreachable regions obtained from the same 19 (t 1 , F i ) points corresponding to learning weighted SVM on a grid of t values. Cones are shown for all datasets. The bound from Parambath et al. (2014) is represented on the left and our bound on the right.</figDesc></figure>
<figure xml:id="fig_9"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Comparison of our bound and the one from Parambath et al. (2014) with respect to the number of iteration/ the size of the grid. We also represent the evolution of both associated algorithms.</figDesc></figure>
<figure type="table" xml:id="tab_0"><head /><label /><figDesc>• (e 2 e 1 M max )(t 0 t), •(e 2 e 1 M max ) and e •(e 2 e 1 M min ).</figDesc><table><row><cell /><cell /><cell /><cell /><cell>Figure 1: Geometric interpretation of both theoretical</cell></row><row><cell /><cell /><cell /><cell /><cell>results: our bound on the left and the one from Param-</cell></row><row><cell /><cell /><cell /><cell /><cell>bath et al. (2014) on the right. Note that our "cone" is</cell></row><row><cell /><cell /><cell /><cell /><cell>not symmetric compared to the other one. On the left,</cell></row><row><cell /><cell /><cell /><cell /><cell>the slanted values represent the slope of our cone on</cell></row><row><cell /><cell /><cell /><cell /><cell>each side :</cell></row><row><cell>where M max =</cell><cell cols="2">max s.t. F (e 00 )&gt;F (e) 2E(H) e 00</cell><cell>(e 00 2</cell><cell>e 00 1 )</cell></row><row><cell cols="2">and, for all t 0 &gt; t:</cell><cell /><cell /></row><row><cell cols="5">F (e 0 )  F (e) + e " 1 + e • (e 2 e 1 M min )(t 0 t),</cell></row><row><cell>where M min =</cell><cell>min s.t. F (e 00 )&gt;F (e) 2E(H) e 00</cell><cell cols="2">(e 00 2</cell><cell>e 00 1 ).</cell></row></table><note><p>e</p></note></figure>
<figure type="table" xml:id="tab_1"><head /><label /><figDesc>greatest value t l in L such that t l &lt; t opt and the smallest value t r such that t opt &lt; t r . (iii) take the middle of the interval [t l , t r ] as the return value,</figDesc><table><row><cell>Input: training set S,</cell></row><row><cell>Input: weighted-learning algorithm wLearn,</cell></row><row><cell>Input: stopping criterion shouldStop.</cell></row><row><cell>To achieve</cell></row><row><cell>this goal, CONE keeps track of a list L, initialized</cell></row><row><cell>with the values 0 and 1, and enriched at each iteration</cell></row><row><cell>with the values of t that have been considered. The se-</cell></row><row><cell>lection of t i is done as follows: (i) search the value t opt</cell></row><row><cell>which maximizes F max (t) = max{F, (t, F ) / 2 Z i 1 }, (ii) search for the i.e. t i = 1 2 (t l + t r ).</cell></row><row><cell>The cost sensitive classification algorithm then provides</cell></row><row><cell>a new value of F i obtained from cost t i , which is used to</cell></row></table></figure>
<figure type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Datasets details. The Imbalance Ratio (I.R.) is the ratio between negative and positive instances (or between sizes of the largest and smallest classes, in a multiclass setting).</figDesc><table><row><cell>Dataset</cell><cell cols="2">Instances Classes</cell><cell>I.R.</cell><cell>Features</cell></row><row><cell>Adult</cell><cell>48842</cell><cell>2</cell><cell>3.19</cell><cell>123</cell></row><row><cell>Abalone10</cell><cell>4174</cell><cell>2</cell><cell>5.64</cell><cell>10</cell></row><row><cell>SatImage</cell><cell>6400</cell><cell>2</cell><cell>9.3</cell><cell>36</cell></row><row><cell>IJCNN'01</cell><cell>141691</cell><cell>2</cell><cell>9.39</cell><cell>22</cell></row><row><cell>Abalone12</cell><cell>4174</cell><cell>2</cell><cell>15.18</cell><cell>10</cell></row><row><cell>PageBlocks</cell><cell>5500</cell><cell>2</cell><cell>22.7</cell><cell>10</cell></row><row><cell>Yeast</cell><cell>1484</cell><cell>2</cell><cell>27.48</cell><cell>8</cell></row><row><cell>Wine</cell><cell>1599</cell><cell>2</cell><cell>28.79</cell><cell>11</cell></row><row><cell>Letter</cell><cell>20000</cell><cell>26</cell><cell>1.32</cell><cell>16</cell></row><row><cell>News20</cell><cell>19928</cell><cell>20</cell><cell>1.12</cell><cell>62061</cell></row></table><note><p><p>previously tested t-values, CONE performs a progressive refinement of a grid. We can</p>(and do, in practice)   </p></note></figure>
<figure type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Classification F-Measures for = 1 with SVM and Logistic Regression algorithms. SVM G and LR T G are reproduced experiments of</figDesc><table /></figure>
<figure type="table" xml:id="tab_8"><head /><label /><figDesc>), ei  min</figDesc><table><row><cell>e 00 2E(H)</cell><cell>ha(t), e 00</cell><cell>i + " 1</cell></row><row><cell>Let us also set M = max e 00 2E(H)</cell><cell /><cell /></row></table></figure>
<figure type="table" xml:id="tab_11"><head>Table 2 :</head><label>2</label><figDesc>Classification F-Measure for = 1 with thresholded SVM algorithm. SVM G are reproduced from</figDesc><table /></figure>
<figure type="table" xml:id="tab_14"><head>Table 3 :</head><label>3</label><figDesc>Mean F-Measure over 5 experiments and limiting the number of iterations/grid steps to 1 (standard deviation between brackets).</figDesc><table><row><cell>Datasets</cell><cell>SVM</cell><cell>SVMI.R.</cell><cell>SVMG</cell><cell>SVMC</cell><cell>LR</cell><cell>LRI.R.</cell><cell>LRB</cell><cell>LRG</cell><cell>LRC</cell></row><row><cell>Adult</cell><cell>62.5 (0.</cell><cell /><cell /><cell /><cell /><cell /><cell /><cell /><cell /></row></table></figure>
<figure type="table" xml:id="tab_16"><head>Table 4 :</head><label>4</label><figDesc>Mean F-Measure over 5 experiments and limiting the number of iterations/grid steps to 2 (standard deviation between brackets).</figDesc><table><row><cell>Datasets</cell><cell>SVM</cell><cell>SVMI.R.</cell><cell>SVMG</cell><cell>SVMC</cell><cell>LR</cell><cell>LRI.R.</cell><cell>LRB</cell><cell>LRG</cell><cell>LRC</cell></row><row><cell>Adult</cell><cell cols="4">62.5 (0.2) 64.9 (0.3) 66.4 (0.2) 66.2 (0.3)</cell><cell cols="5">63.1 (0.1) 66.0 (0.1) 66.6 (0.1) 66.6 (0.1) 66.2 (0.1)</cell></row><row><cell>Abalone10</cell><cell cols="4">0.0 (0.0) 30.9 (1.2) 32.6 (1.4) 30.7 (1.1)</cell><cell cols="5">0.0 (0.0) 31.9 (1.4) 31.6 (0.6) 31.9 (1.7) 32.4 (1.9)</cell></row><row><cell>Satimage</cell><cell cols="2">0.0 (0.0) 23.4 (4.3)</cell><cell>6.1 (12.2)</cell><cell>5.9 (11.8)</cell><cell cols="3">0.5 (0.9) 24.2 (5.3) 21.4 (4.6)</cell><cell>6.2 (12.3)</cell><cell>6.1 (12.2)</cell></row><row><cell>IJCNN</cell><cell cols="4">44.5 (0.4) 53.3 (0.4) 60.7 (0.4) 61.6 (0.5)</cell><cell cols="5">46.2 (0.3) 51.6 (0.3) 59.2 (0.3) 56.8 (0.3) 58.3 (0.3)</cell></row><row><cell>Abalone12</cell><cell cols="2">0.0 (0.0) 16.8 (2.7)</cell><cell>0.0 (0.0)</cell><cell>0.0 (0.0)</cell><cell cols="3">0.0 (0.0) 18.0 (3.5) 17.7 (3.7)</cell><cell cols="2">2.8 (3.4) 13.3 (3.5)</cell></row><row><cell>Pageblocks</cell><cell cols="4">48.1 (5.8) 39.6 (4.7) 65.0 (7.6) 63.3 (4.1)</cell><cell cols="5">48.6 (3.3) 42.4 (5.2) 55.7 (5.7) 62.7 (7.1) 58.3 (6.8)</cell></row><row><cell>Yeast</cell><cell cols="4">0.0 (0.0) 29.4 (2.9) 30.9 (17.2) 25.4 (17.5)</cell><cell cols="5">2.5 (5.0) 29.0 (3.5) 35.4 (15.6) 27.8 (20.0) 33.0 (18.3)</cell></row><row><cell>Wine</cell><cell cols="2">0.0 (0.0) 15.6 (5.2)</cell><cell cols="2">0.0 (0.0) 11.7 (11.1)</cell><cell cols="3">0.0 (0.0) 14.6 (3.2) 18.3 (7.2)</cell><cell cols="2">8.7 (11.2) 15.6 (6.7)</cell></row></table></figure>
<figure type="table" xml:id="tab_18"><head>Table 5 :</head><label>5</label><figDesc>Mean F-Measure over 5 experiments and limiting the number of iterations/grid steps to 3 (standard deviation between brackets).</figDesc><table><row><cell>Datasets</cell><cell>SVM</cell><cell>SVMI.R.</cell><cell>SVMG</cell><cell>SVMC</cell><cell>LR</cell><cell>LRI.R.</cell><cell>LRB</cell><cell>LRG</cell><cell>LRC</cell></row><row><cell>Adult</cell><cell cols="4">62.5 (0.2) 64.9 (0.3) 66.1 (0.2) 66.2 (0.3)</cell><cell cols="5">63.1 (0.1) 66.0 (0.1) 66.6 (0.1) 66.2 (0.1) 66.2 (0.1)</cell></row><row><cell>Abalone10</cell><cell cols="4">0.0 (0.0) 30.9 (1.2) 30.7 (1.1) 31.0 (1.4)</cell><cell cols="5">0.0 (0.0) 31.9 (1.4) 31.6 (0.6) 32.5 (1.5) 31.3 (2.2)</cell></row><row><cell>Satimage</cell><cell cols="2">0.0 (0.0) 23.4 (4.3)</cell><cell cols="2">5.9 (11.8) 20.2 (4.7)</cell><cell cols="3">0.5 (0.9) 24.2 (5.3) 21.4 (4.6)</cell><cell cols="2">6.1 (12.1) 20.3 (5.1)</cell></row><row><cell>IJCNN</cell><cell cols="4">44.5 (0.4) 53.3 (0.4) 61.6 (0.5) 61.6 (0.5)</cell><cell cols="5">46.2 (0.3) 51.6 (0.3) 59.2 (0.3) 58.3 (0.3) 58.3 (0.3)</cell></row><row><cell>Abalone12</cell><cell cols="2">0.0 (0.0) 16.8 (2.7)</cell><cell cols="2">0.0 (0.0) 16.7 (2.7)</cell><cell cols="5">0.0 (0.0) 18.0 (3.5) 17.7 (3.7) 14.2 (3.0) 16.6 (3.4)</cell></row><row><cell>Pageblocks</cell><cell cols="4">48.1 (5.8) 39.6 (4.7) 65.5 (2.0) 63.3 (4.1)</cell><cell cols="5">48.6 (3.3) 42.4 (5.2) 55.7 (5.7) 60.4 (6.4) 58.3 (6.8)</cell></row><row><cell>Yeast</cell><cell cols="4">0.0 (0.0) 29.4 (2.9) 32.6 (18.3) 37.8 (7.8)</cell><cell cols="5">2.5 (5.0) 29.0 (3.5) 35.4 (15.6) 32.1 (11.9) 32.6 (12.0)</cell></row><row><cell>Wine</cell><cell cols="4">0.0 (0.0) 15.6 (5.2) 11.8 (11.1) 19.5 (5.1)</cell><cell cols="5">0.0 (0.0) 14.6 (3.2) 18.3 (7.2) 17.5 (5.8) 20.0 (3.8)</cell></row></table></figure>
<figure type="table" xml:id="tab_20"><head>Table 6 :</head><label>6</label><figDesc>Mean F-Measure over 5 experiments and limiting the number of iterations/grid steps to 4 (standard deviation between brackets).</figDesc><table><row><cell>Datasets</cell><cell>SVM</cell><cell>SVMI.R.</cell><cell>SVMG</cell><cell>SVMC</cell><cell>LR</cell><cell>LRI.R.</cell><cell>LRB</cell><cell>LRG</cell><cell>LRC</cell></row><row><cell>Adult</cell><cell cols="4">62.5 (0.2) 64.9 (0.3) 66.0 (0.2) 66.2 (0.3)</cell><cell cols="5">63.1 (0.1) 66.0 (0.1) 66.6 (0.1) 66.4 (0.1) 66.2 (0.1)</cell></row><row><cell>Abalone10</cell><cell cols="4">0.0 (0.0) 30.9 (1.2) 31.0 (1.0) 31.0 (1.4)</cell><cell cols="5">0.0 (0.0) 31.9 (1.4) 31.6 (0.6) 30.9 (1.7) 31.3 (2.2)</cell></row><row><cell>Satimage</cell><cell cols="4">0.0 (0.0) 23.4 (4.3) 16.4 (9.5) 20.6 (5.6)</cell><cell cols="5">0.5 (0.9) 24.2 (5.3) 21.4 (4.6) 17.0 (9.8) 20.5 (5.0)</cell></row><row><cell>IJCNN</cell><cell cols="4">44.5 (0.4) 53.3 (0.4) 61.5 (0.4) 61.1 (0.5)</cell><cell cols="5">46.2 (0.3) 51.6 (0.3) 59.2 (0.3) 57.8 (0.4) 58.3 (0.3)</cell></row><row><cell>Abalone12</cell><cell cols="4">0.0 (0.0) 16.8 (2.7) 16.5 (4.0) 16.9 (4.3)</cell><cell cols="5">0.0 (0.0) 18.0 (3.5) 17.7 (3.7) 17.6 (3.0) 17.6 (3.1)</cell></row><row><cell>Pageblocks</cell><cell cols="4">48.1 (5.8) 39.6 (4.7) 61.0 (6.0) 63.3 (4.1)</cell><cell cols="5">48.6 (3.3) 42.4 (5.2) 55.7 (5.7) 62.1 (7.8) 58.4 (6.7)</cell></row><row><cell>Yeast</cell><cell cols="4">0.0 (0.0) 29.4 (2.9) 35.4 (8.7) 39.0 (7.5)</cell><cell cols="5">2.5 (5.0) 29.0 (3.5) 35.4 (15.6) 31.1 (18.0) 32.5 (12.0)</cell></row><row><cell>Wine</cell><cell cols="4">0.0 (0.0) 15.6 (5.2) 11.5 (7.8) 19.5 (5.1)</cell><cell cols="5">0.0 (0.0) 14.6 (3.2) 18.3 (7.2) 17.9 (2.8) 20.0 (3.8)</cell></row><row><cell>Letter</cell><cell cols="4">75.4 (0.7) 74.9 (0.8) 80.5 (0.3) 80.4 (0.5)</cell><cell cols="5">82.9 (0.3) 82.9 (0.3) 74.9 (0.5) 82.9 (0.2) 82.9 (0.3)</cell></row><row><cell>News20</cell><cell cols="4">90.9 (0.1) 91.0 (0.2) 91.0 (0.1) 91.0 (0.2)</cell><cell cols="5">90.6 (0.1) 90.6 (0.1) 89.4 (0.2) 90.6 (0.2) 90.7 (0.1)</cell></row><row><cell>Average</cell><cell cols="4">32.1 (0.7) 44.0 (2.3) 47.1 (3.8) 48.9 (2.9)</cell><cell cols="5">33.4 (1.0) 45.1 (2.3) 47.0 (3.9) 47.4 (4.4) 47.8 (3.4)</cell></row></table></figure>
<figure type="table" xml:id="tab_21"><head>Table 7 :</head><label>7</label><figDesc>Mean F-Measure over 5 experiments and limiting the number of iterations/grid steps to 5 (standard deviation between brackets).</figDesc><table><row><cell>Datasets</cell><cell>SVM</cell><cell>SVMI.R.</cell><cell>SVMG</cell><cell>SVMC</cell><cell>LR</cell><cell>LRI.R.</cell><cell>LRB</cell><cell>LRG</cell><cell>LRC</cell></row><row><cell>Adult</cell><cell cols="4">62.5 (0.2) 64.9 (0.3) 66.4 (0.3) 66.2 (0.2)</cell><cell cols="5">63.1 (0.1) 66.0 (0.1) 66.6 (0.1) 66.6 (0.1) 66.5 (0.1)</cell></row><row><cell>Abalone10</cell><cell cols="4">0.0 (0.0) 30.9 (1.2) 32.6 (1.4) 31.7 (1.0)</cell><cell cols="5">0.0 (0.0) 31.9 (1.4) 31.6 (0.6) 31.3 (0.7) 31.2 (2.3)</cell></row><row><cell>Satimage</cell><cell cols="4">0.0 (0.0) 23.4 (4.3) 16.4 (9.5) 20.6 (5.6)</cell><cell cols="5">0.5 (0.9) 24.2 (5.3) 21.4 (4.6) 17.0 (9.8) 20.5 (5.0)</cell></row><row><cell>IJCNN</cell><cell cols="4">44.5 (0.4) 53.3 (0.4) 61.4 (0.6) 61.1 (0.5)</cell><cell cols="5">46.2 (0.3) 51.6 (0.3) 59.2 (0.3) 58.3 (0.3) 58.3 (0.3)</cell></row><row><cell>Abalone12</cell><cell cols="4">0.0 (0.0) 16.8 (2.7) 16.5 (4.0) 16.5 (4.0)</cell><cell cols="5">0.0 (0.0) 18.0 (3.5) 17.7 (3.7) 17.6 (3.0) 18.1 (2.6)</cell></row><row><cell>Pageblocks</cell><cell cols="4">48.1 (5.8) 39.6 (4.7) 67.7 (4.0) 62.1 (5.0)</cell><cell cols="5">48.6 (3.3) 42.4 (5.2) 55.7 (5.7) 61.8 (7.3) 59.6 (7.3)</cell></row><row><cell>Yeast</cell><cell cols="4">0.0 (0.0) 29.4 (2.9) 31.8 (10.5) 39.0 (7.5)</cell><cell cols="5">2.5 (5.0) 29.0 (3.5) 35.4 (15.6) 30.1 (17.2) 38.8 (8.5)</cell></row><row><cell>Wine</cell><cell cols="4">0.0 (0.0) 15.6 (5.2) 11.5 (7.8) 20.4 (5.6)</cell><cell cols="5">0.0 (0.0) 14.6 (3.2) 18.3 (7.2) 17.9 (2.8) 21.2 (5.1)</cell></row></table></figure>
<figure type="table" xml:id="tab_23"><head>Table 8 :</head><label>8</label><figDesc>Mean F-Measure over 5 experiments and limiting the number of iterations/grid steps to 6 (standard deviation between brackets).</figDesc><table><row><cell>Datasets</cell><cell>SVM</cell><cell>SVMI.R.</cell><cell>SVMG</cell><cell>SVMC</cell><cell>LR</cell><cell>LRI.R.</cell><cell>LRB</cell><cell>LRG</cell><cell>LRC</cell></row><row><cell>Adult</cell><cell cols="4">62.5 (0.2) 64.9 (0.3) 66.5 (0.1) 66.4 (0.1)</cell><cell cols="5">63.1 (0.1) 66.0 (0.1) 66.6 (0.1) 66.4 (0.2) 66.5 (0.1)</cell></row><row><cell>Abalone10</cell><cell cols="4">0.0 (0.0) 30.9 (1.2) 30.7 (1.1) 31.7 (1.0)</cell><cell cols="5">0.0 (0.0) 31.9 (1.4) 31.6 (0.6) 31.6 (1.0) 31.4 (2.2)</cell></row><row><cell>Satimage</cell><cell cols="4">0.0 (0.0) 23.4 (4.3) 20.4 (5.3) 20.6 (5.6)</cell><cell cols="5">0.5 (0.9) 24.2 (5.3) 21.4 (4.6) 20.1 (4.6) 20.5 (5.0)</cell></row><row><cell>IJCNN</cell><cell cols="4">44.5 (0.4) 53.3 (0.4) 62.1 (0.5) 61.3 (0.6)</cell><cell cols="5">46.2 (0.3) 51.6 (0.3) 59.2 (0.3) 58.0 (0.4) 58.1 (0.3)</cell></row><row><cell>Abalone12</cell><cell cols="4">0.0 (0.0) 16.8 (2.7) 16.9 (2.9) 18.2 (3.3)</cell><cell cols="5">0.0 (0.0) 18.0 (3.5) 17.7 (3.7) 15.5 (6.2) 17.7 (3.4)</cell></row><row><cell>Pageblocks</cell><cell cols="4">48.1 (5.8) 39.6 (4.7) 64.8 (3.1) 64.2 (4.6)</cell><cell cols="5">48.6 (3.3) 42.4 (5.2) 55.7 (5.7) 60.6 (9.1) 59.5 (7.4)</cell></row><row><cell>Yeast</cell><cell cols="4">0.0 (0.0) 29.4 (2.9) 32.0 (10.4) 39.0 (7.5)</cell><cell cols="5">2.5 (5.0) 29.0 (3.5) 35.4 (15.6) 33.0 (18.8) 38.8 (8.5)</cell></row><row><cell>Wine</cell><cell cols="4">0.0 (0.0) 15.6 (5.2) 19.4 (5.3) 19.0 (7.0)</cell><cell cols="5">0.0 (0.0) 14.6 (3.2) 18.3 (7.2) 19.6 (5.1) 21.1 (5.2)</cell></row></table></figure>
<figure type="table" xml:id="tab_25"><head>Table 9 :</head><label>9</label><figDesc>Mean F-Measure over 5 experiments and limiting the number of iterations/grid steps to 7 (standard deviation between brackets).</figDesc><table><row><cell>Datasets</cell><cell>SVM</cell><cell>SVMI.R.</cell><cell>SVMG</cell><cell>SVMC</cell><cell>LR</cell><cell>LRI.R.</cell><cell>LRB</cell><cell>LRG</cell><cell>LRC</cell></row><row><cell>Adult</cell><cell cols="4">62.5 (0.2) 64.9 (0.3) 66.2 (0.1) 66.4 (0.1)</cell><cell cols="5">63.1 (0.1) 66.0 (0.1) 66.6 (0.1) 66.4 (0.1) 66.5 (0.1)</cell></row><row><cell>Abalone10</cell><cell cols="4">0.0 (0.0) 30.9 (1.2) 31.0 (1.0) 32.5 (1.0)</cell><cell cols="5">0.0 (0.0) 31.9 (1.4) 31.6 (0.6) 32.2 (0.6) 31.4 (2.2)</cell></row><row><cell>Satimage</cell><cell cols="4">0.0 (0.0) 23.4 (4.3) 20.2 (4.7) 20.6 (5.6)</cell><cell cols="5">0.5 (0.9) 24.2 (5.3) 21.4 (4.6) 20.3 (5.0) 20.5 (5.0)</cell></row><row><cell>IJCNN</cell><cell cols="4">44.5 (0.4) 53.3 (0.4) 61.5 (0.4) 61.5 (0.5)</cell><cell cols="5">46.2 (0.3) 51.6 (0.3) 59.2 (0.3) 58.3 (0.3) 58.1 (0.3)</cell></row><row><cell>Abalone12</cell><cell cols="4">0.0 (0.0) 16.8 (2.7) 16.9 (2.9) 18.3 (3.3)</cell><cell cols="5">0.0 (0.0) 18.0 (3.5) 17.7 (3.7) 17.5 (3.4) 17.7 (3.4)</cell></row><row><cell>Pageblocks</cell><cell cols="4">48.1 (5.8) 39.6 (4.7) 65.7 (2.6) 62.8 (3.9)</cell><cell cols="5">48.6 (3.3) 42.4 (5.2) 55.7 (5.7) 61.3 (9.9) 59.9 (7.0)</cell></row><row><cell>Yeast</cell><cell cols="4">0.0 (0.0) 29.4 (2.9) 38.8 (7.0) 39.0 (7.5)</cell><cell cols="5">2.5 (5.0) 29.0 (3.5) 35.4 (15.6) 32.7 (11.8) 38.9 (8.6)</cell></row><row><cell>Wine</cell><cell cols="4">0.0 (0.0) 15.6 (5.2) 19.5 (6.2) 19.0 (7.0)</cell><cell cols="5">0.0 (0.0) 14.6 (3.2) 18.3 (7.2) 18.7 (4.5) 21.1 (5.2)</cell></row><row><cell>Letter</cell><cell cols="4">75.4 (0.7) 74.9 (0.8) 80.6 (0.1) 80.5 (0.4)</cell><cell cols="5">82.9 (0.3) 82.9 (0.3) 74.9 (0.5) 82.9 (0.2) 82.9 (0.3)</cell></row><row><cell>News20</cell><cell cols="4">90.9 (0.1) 91.0 (0.2) 91.1 (0.1) 91.0 (0.2)</cell><cell cols="5">90.6 (0.1) 90.6 (0.1) 89.4 (0.2) 90.6 (0.1) 90.7 (0.1)</cell></row><row><cell>Average</cell><cell cols="4">32.1 (0.7) 44.0 (2.3) 49.2 (2.5) 49.2 (3.0)</cell><cell cols="5">33.4 (1.0) 45.1 (2.3) 47.0 (3.9) 48.1 (3.6) 48.8 (3.2)</cell></row></table></figure>
<figure type="table" xml:id="tab_26"><head>Table 10 :</head><label>10</label><figDesc>Mean F-Measure over 5 experiments and limiting the number of iterations/grid steps to 8 (standard deviation between brackets).</figDesc><table><row><cell>Datasets</cell><cell>SVM</cell><cell>SVMI.R.</cell><cell>SVMG</cell><cell>SVMC</cell><cell>LR</cell><cell>LRI.R.</cell><cell>LRB</cell><cell>LRG</cell><cell>LRC</cell></row><row><cell>Adult</cell><cell cols="4">62.5 (0.2) 64.9 (0.3) 66.4 (0.1) 66.5 (0.1)</cell><cell cols="5">63.1 (0.1) 66.0 (0.1) 66.6 (0.1) 66.5 (0.1) 66.5 (0.1)</cell></row><row><cell>Abalone10</cell><cell cols="4">0.0 (0.0) 30.9 (1.2) 32.6 (1.4) 32.6 (1.0)</cell><cell cols="5">0.0 (0.0) 31.9 (1.4) 31.6 (0.6) 32.1 (0.8) 31.4 (2.2)</cell></row><row><cell>Satimage</cell><cell cols="4">0.0 (0.0) 23.4 (4.3) 20.2 (4.7) 20.6 (5.6)</cell><cell cols="5">0.5 (0.9) 24.2 (5.3) 21.4 (4.6) 20.3 (5.0) 20.5 (5.0)</cell></row><row><cell>IJCNN</cell><cell cols="4">44.5 (0.4) 53.3 (0.4) 61.9 (0.7) 61.5 (0.5)</cell><cell cols="5">46.2 (0.3) 51.6 (0.3) 59.2 (0.3) 58.0 (0.4) 58.1 (0.3)</cell></row><row><cell>Abalone12</cell><cell cols="4">0.0 (0.0) 16.8 (2.7) 16.9 (2.9) 18.3 (3.3)</cell><cell cols="5">0.0 (0.0) 18.0 (3.5) 17.7 (3.7) 17.5 (3.4) 18.1 (3.7)</cell></row><row><cell>Pageblocks</cell><cell cols="4">48.1 (5.8) 39.6 (4.7) 65.8 (4.3) 62.8 (3.9)</cell><cell cols="5">48.6 (3.3) 42.4 (5.2) 55.7 (5.7) 60.0 (8.8) 59.4 (7.5)</cell></row><row><cell>Yeast</cell><cell cols="4">0.0 (0.0) 29.4 (2.9) 33.3 (12.2) 39.0 (7.5)</cell><cell cols="5">2.5 (5.0) 29.0 (3.5) 35.4 (15.6) 39.4 (8.5) 38.9 (8.6)</cell></row><row><cell>Wine</cell><cell cols="4">0.0 (0.0) 15.6 (5.2) 19.5 (6.2) 22.4 (6.1)</cell><cell cols="5">0.0 (0.0) 14.6 (3.2) 18.3 (7.2) 18.7 (4.5) 21.1 (5.2)</cell></row></table></figure>
<figure type="table" xml:id="tab_28"><head>Table 11 :</head><label>11</label><figDesc>Mean F-Measure over 5 experiments and limiting the number of iterations/grid steps to 9 (standard deviation between brackets).</figDesc><table><row><cell>Datasets</cell><cell>SVM</cell><cell>SVMI.R.</cell><cell>SVMG</cell><cell>SVMC</cell><cell>LR</cell><cell>LRI.R.</cell><cell>LRB</cell><cell>LRG</cell><cell>LRC</cell></row><row><cell>Adult</cell><cell cols="4">62.5 (0.2) 64.9 (0.3) 66.4 (0.1) 66.5 (0.1)</cell><cell cols="5">63.1 (0.1) 66.0 (0.1) 66.6 (0.1) 66.4 (0.1) 66.5 (0.1)</cell></row><row><cell>Abalone10</cell><cell cols="4">0.0 (0.0) 30.9 (1.2) 31.0 (1.0) 32.2 (0.8)</cell><cell cols="5">0.0 (0.0) 31.9 (1.4) 31.6 (0.6) 31.5 (0.4) 31.4 (2.2)</cell></row><row><cell>Satimage</cell><cell cols="4">0.0 (0.0) 23.4 (4.3) 20.4 (5.3) 20.6 (5.6)</cell><cell cols="5">0.5 (0.9) 24.2 (5.3) 21.4 (4.6) 20.8 (4.9) 20.5 (5.0)</cell></row><row><cell>IJCNN</cell><cell cols="4">44.5 (0.4) 53.3 (0.4) 61.5 (0.4) 61.5 (0.5)</cell><cell cols="5">46.2 (0.3) 51.6 (0.3) 59.2 (0.3) 58.3 (0.3) 58.1 (0.3)</cell></row><row><cell>Abalone12</cell><cell cols="4">0.0 (0.0) 16.8 (2.7) 16.7 (4.1) 18.3 (3.3)</cell><cell cols="5">0.0 (0.0) 18.0 (3.5) 17.7 (3.7) 15.1 (5.9) 18.0 (3.6)</cell></row><row><cell>Pageblocks</cell><cell cols="4">48.1 (5.8) 39.6 (4.7) 65.4 (2.3) 62.8 (3.9)</cell><cell cols="5">48.6 (3.3) 42.4 (5.2) 55.7 (5.7) 62.7 (8.3) 59.4 (7.5)</cell></row><row><cell>Yeast</cell><cell cols="4">0.0 (0.0) 29.4 (2.9) 38.3 (3.8) 39.0 (7.5)</cell><cell cols="5">2.5 (5.0) 29.0 (3.5) 35.4 (15.6) 38.9 (10.9) 38.9 (8.6)</cell></row><row><cell>Wine</cell><cell cols="4">0.0 (0.0) 15.6 (5.2) 15.5 (6.0) 22.7 (6.0)</cell><cell cols="5">0.0 (0.0) 14.6 (3.2) 18.3 (7.2) 20.7 (6.0) 21.1 (5.2)</cell></row></table></figure>
<figure type="table" xml:id="tab_30"><head>Table 12 :</head><label>12</label><figDesc>Mean F-Measure over 5 experiments and limiting the number of iterations/grid steps to 10 (standard deviation between brackets).</figDesc><table><row><cell>Datasets</cell><cell>SVM</cell><cell>SVMI.R.</cell><cell>SVMG</cell><cell>SVMC</cell><cell>LR</cell><cell>LRI.R.</cell><cell>LRB</cell><cell>LRG</cell><cell>LRC</cell></row><row><cell>Adult</cell><cell cols="4">62.5 (0.2) 64.9 (0.3) 66.5 (0.1) 66.4 (0.1)</cell><cell cols="5">63.1 (0.1) 66.0 (0.1) 66.6 (0.1) 66.5 (0.1) 66.5 (0.1)</cell></row><row><cell>Abalone10</cell><cell cols="4">0.0 (0.0) 30.9 (1.2) 32.6 (1.4) 32.2 (0.8)</cell><cell cols="5">0.0 (0.0) 31.9 (1.4) 31.6 (0.6) 31.8 (1.0) 31.1 (2.0)</cell></row><row><cell>Satimage</cell><cell cols="4">0.0 (0.0) 23.4 (4.3) 20.4 (5.3) 20.6 (5.6)</cell><cell cols="5">0.5 (0.9) 24.2 (5.3) 21.4 (4.6) 20.8 (4.9) 20.5 (5.0)</cell></row><row><cell>IJCNN</cell><cell cols="4">44.5 (0.4) 53.3 (0.4) 61.9 (0.7) 61.5 (0.5)</cell><cell cols="5">46.2 (0.3) 51.6 (0.3) 59.2 (0.3) 58.0 (0.4) 58.1 (0.3)</cell></row><row><cell>Abalone12</cell><cell cols="4">0.0 (0.0) 16.8 (2.7) 16.7 (4.1) 18.3 (3.3)</cell><cell cols="5">0.0 (0.0) 18.0 (3.5) 17.7 (3.7) 15.1 (5.9) 17.8 (3.4)</cell></row><row><cell>Pageblocks</cell><cell cols="4">48.1 (5.8) 39.6 (4.7) 65.6 (4.1) 62.8 (3.9)</cell><cell cols="5">48.6 (3.3) 42.4 (5.2) 55.7 (5.7) 61.3 (7.3) 59.4 (7.5)</cell></row><row><cell>Yeast</cell><cell cols="4">0.0 (0.0) 29.4 (2.9) 32.5 (10.4) 39.0 (7.5)</cell><cell cols="5">2.5 (5.0) 29.0 (3.5) 35.4 (15.6) 38.9 (10.9) 39.5 (9.3)</cell></row><row><cell>Wine</cell><cell cols="4">0.0 (0.0) 15.6 (5.2) 15.5 (6.0) 22.7 (6.0)</cell><cell cols="5">0.0 (0.0) 14.6 (3.2) 18.3 (7.2) 20.7 (6.0) 21.1 (5.2)</cell></row><row><cell>Letter</cell><cell cols="4">75.4 (0.7) 74.9 (0.8) 80.8 (0.5) 80.7 (0.4)</cell><cell cols="5">82.9 (0.3) 82.9 (0.3) 74.9 (0.5) 82.9 (0.2) 82.9 (0.3)</cell></row><row><cell>News20</cell><cell cols="4">90.9 (0.1) 91.0 (0.2) 91.0 (0.1) 91.0 (0.2)</cell><cell cols="5">90.6 (0.1) 90.6 (0.1) 89.4 (0.2) 90.6 (0.2) 90.6 (0.1)</cell></row><row><cell>Average</cell><cell cols="4">32.1 (0.7) 44.0 (2.3) 48.4 (3.3) 49.5 (2.8)</cell><cell cols="5">33.4 (1.0) 45.1 (2.3) 47.0 (3.9) 48.7 (3.7) 48.8 (3.3)</cell></row></table></figure>
<figure type="table" xml:id="tab_31"><head>Table 13 :</head><label>13</label><figDesc>Mean F-Measure over 5 experiments and limiting the number of iterations/grid steps to 11 (standard deviation between brackets).</figDesc><table><row><cell>Datasets</cell><cell>SVM</cell><cell>SVMI.R.</cell><cell>SVMG</cell><cell>SVMC</cell><cell>LR</cell><cell>LRI.R.</cell><cell>LRB</cell><cell>LRG</cell><cell>LRC</cell></row><row><cell>Adult</cell><cell cols="4">62.5 (0.2) 64.9 (0.3) 66.4 (0.1) 66.5 (0.1)</cell><cell cols="5">63.1 (0.1) 66.0 (0.1) 66.6 (0.1) 66.5 (0.1) 66.5 (0.1)</cell></row><row><cell>Abalone10</cell><cell cols="4">0.0 (0.0) 30.9 (1.2) 32.4 (1.3) 32.2 (0.8)</cell><cell cols="5">0.0 (0.0) 31.9 (1.4) 31.6 (0.6) 31.9 (0.7) 30.9 (1.9)</cell></row><row><cell>Satimage</cell><cell cols="4">0.0 (0.0) 23.4 (4.3) 20.2 (4.7) 20.6 (5.6)</cell><cell cols="5">0.5 (0.9) 24.2 (5.3) 21.4 (4.6) 20.7 (4.8) 20.5 (5.0)</cell></row><row><cell>IJCNN</cell><cell cols="4">44.5 (0.4) 53.3 (0.4) 61.4 (0.5) 61.8 (0.5)</cell><cell cols="5">46.2 (0.3) 51.6 (0.3) 59.2 (0.3) 58.3 (0.3) 58.1 (0.4)</cell></row><row><cell>Abalone12</cell><cell cols="4">0.0 (0.0) 16.8 (2.7) 16.7 (4.1) 18.3 (3.3)</cell><cell cols="5">0.0 (0.0) 18.0 (3.5) 17.7 (3.7) 17.2 (3.1) 17.8 (3.4)</cell></row><row><cell>Pageblocks</cell><cell cols="4">48.1 (5.8) 39.6 (4.7) 66.4 (3.5) 62.8 (3.9)</cell><cell cols="5">48.6 (3.3) 42.4 (5.2) 55.7 (5.7) 62.6 (8.0) 59.4 (7.5)</cell></row><row><cell>Yeast</cell><cell cols="4">0.0 (0.0) 29.4 (2.9) 38.4 (7.1) 39.0 (7.5)</cell><cell cols="5">2.5 (5.0) 29.0 (3.5) 35.4 (15.6) 38.7 (8.1) 39.5 (9.3)</cell></row><row><cell>Wine</cell><cell cols="4">0.0 (0.0) 15.6 (5.2) 16.4 (5.9) 22.7 (6.0)</cell><cell cols="5">0.0 (0.0) 14.6 (3.2) 18.3 (7.2) 20.5 (6.0) 21.1 (5.2)</cell></row></table></figure>
<figure type="table" xml:id="tab_33"><head>Table 14 :</head><label>14</label><figDesc>Mean F-Measure over 5 experiments and limiting the number of iterations/grid steps to 12 (standard deviation between brackets).</figDesc><table><row><cell>Datasets</cell><cell>SVM</cell><cell>SVMI.R.</cell><cell>SVMG</cell><cell>SVMC</cell><cell>LR</cell><cell>LRI.R.</cell><cell>LRB</cell><cell>LRG</cell><cell>LRC</cell></row><row><cell>Adult</cell><cell cols="4">62.5 (0.2) 64.9 (0.3) 66.4 (0.1) 66.5 (0.1)</cell><cell cols="5">63.1 (0.1) 66.0 (0.1) 66.6 (0.1) 66.4 (0.1) 66.5 (0.1)</cell></row><row><cell>Abalone10</cell><cell cols="4">0.0 (0.0) 30.9 (1.2) 31.0 (1.0) 32.2 (0.8)</cell><cell cols="5">0.0 (0.0) 31.9 (1.4) 31.6 (0.6) 32.0 (0.7) 30.9 (1.9)</cell></row><row><cell>Satimage</cell><cell cols="4">0.0 (0.0) 23.4 (4.3) 20.4 (5.3) 20.6 (5.6)</cell><cell cols="5">0.5 (0.9) 24.2 (5.3) 21.4 (4.6) 20.3 (5.0) 20.5 (5.0)</cell></row><row><cell>IJCNN</cell><cell cols="4">44.5 (0.4) 53.3 (0.4) 61.8 (0.4) 61.6 (0.6)</cell><cell cols="5">46.2 (0.3) 51.6 (0.3) 59.2 (0.3) 58.0 (0.4) 58.2 (0.3)</cell></row><row><cell>Abalone12</cell><cell cols="4">0.0 (0.0) 16.8 (2.7) 16.9 (2.9) 18.3 (3.3)</cell><cell cols="5">0.0 (0.0) 18.0 (3.5) 17.7 (3.7) 17.5 (3.4) 17.8 (3.4)</cell></row><row><cell>Pageblocks</cell><cell cols="4">48.1 (5.8) 39.6 (4.7) 64.7 (3.2) 62.8 (3.9)</cell><cell cols="5">48.6 (3.3) 42.4 (5.2) 55.7 (5.7) 61.5 (10.0) 59.4 (7.5)</cell></row><row><cell>Yeast</cell><cell cols="4">0.0 (0.0) 29.4 (2.9) 38.1 (7.6) 39.0 (7.5)</cell><cell cols="5">2.5 (5.0) 29.0 (3.5) 35.4 (15.6) 39.1 (10.1) 39.5 (9.3)</cell></row><row><cell>Wine</cell><cell cols="4">0.0 (0.0) 15.6 (5.2) 20.0 (6.4) 22.7 (6.0)</cell><cell cols="5">0.0 (0.0) 14.6 (3.2) 18.3 (7.2) 18.7 (4.5) 21.1 (5.2)</cell></row></table></figure>
<figure type="table" xml:id="tab_35"><head>Table 15 :</head><label>15</label><figDesc>Mean F-Measure over 5 experiments and limiting the number of iterations/grid steps to 13 (standard deviation between brackets).</figDesc><table><row><cell>Datasets</cell><cell>SVM</cell><cell>SVMI.R.</cell><cell>SVMG</cell><cell>SVMC</cell><cell>LR</cell><cell>LRI.R.</cell><cell>LRB</cell><cell>LRG</cell><cell>LRC</cell></row><row><cell>Adult</cell><cell cols="4">62.5 (0.2) 64.9 (0.3) 66.4 (0.1) 66.5 (0.1)</cell><cell cols="5">63.1 (0.1) 66.0 (0.1) 66.6 (0.1) 66.5 (0.1) 66.5 (0.1)</cell></row><row><cell>Abalone10</cell><cell cols="4">0.0 (0.0) 30.9 (1.2) 32.6 (1.4) 32.2 (0.8)</cell><cell cols="5">0.0 (0.0) 31.9 (1.4) 31.6 (0.6) 32.3 (1.1) 30.9 (1.9)</cell></row><row><cell>Satimage</cell><cell cols="4">0.0 (0.0) 23.4 (4.3) 20.4 (5.3) 20.6 (5.6)</cell><cell cols="5">0.5 (0.9) 24.2 (5.3) 21.4 (4.6) 20.3 (5.0) 20.5 (5.0)</cell></row><row><cell>IJCNN</cell><cell cols="4">44.5 (0.4) 53.3 (0.4) 61.9 (0.7) 61.6 (0.6)</cell><cell cols="5">46.2 (0.3) 51.6 (0.3) 59.2 (0.3) 58.2 (0.2) 58.2 (0.3)</cell></row><row><cell>Abalone12</cell><cell cols="4">0.0 (0.0) 16.8 (2.7) 16.9 (2.9) 18.3 (3.3)</cell><cell cols="5">0.0 (0.0) 18.0 (3.5) 17.7 (3.7) 17.5 (3.4) 17.8 (3.4)</cell></row><row><cell>Pageblocks</cell><cell cols="4">48.1 (5.8) 39.6 (4.7) 66.6 (3.1) 62.8 (3.9)</cell><cell cols="5">48.6 (3.3) 42.4 (5.2) 55.7 (5.7) 60.2 (9.0) 59.4 (7.5)</cell></row><row><cell>Yeast</cell><cell cols="4">0.0 (0.0) 29.4 (2.9) 33.3 (12.2) 39.0 (7.5)</cell><cell cols="5">2.5 (5.0) 29.0 (3.5) 35.4 (15.6) 39.1 (10.1) 39.5 (9.3)</cell></row><row><cell>Wine</cell><cell cols="4">0.0 (0.0) 15.6 (5.2) 20.0 (6.4) 22.7 (6.0)</cell><cell cols="5">0.0 (0.0) 14.6 (3.2) 18.3 (7.2) 18.7 (4.5) 21.1 (5.2)</cell></row></table></figure>
<figure type="table" xml:id="tab_37"><head>Table 16 :</head><label>16</label><figDesc>Mean F-Measure over 5 experiments and limiting the number of iterations/grid steps to 14 (standard deviation between brackets). (0.0) 30.9 (1.2) 32.4 (1.3) 32.2 (0.8) 0.0 (0.0) 31.9 (1.4) 31.6 (0.6) 31.4 (0.5) 30.9 (1.9) Satimage 0.0 (0.0) 23.4 (4.3) 20.4 (5.3) 20.6 (5.6) 0.5 (0.9) 24.2 (5.3) 21.4 (4.6) 20.8 (4.9) 20.5 (5.0) IJCNN 44.5 (0.4) 53.3 (0.4) 61.6 (0.6) 61.6 (0.6) 46.2 (0.3) 51.6 (0.3) 59.2 (0.3) 58.0 (0.4) 58.2 (0.3) Abalone12 0.0 (0.0) 16.8 (2.7) 16.8 (4.2) 18.3 (3.3) 0.0 (0.0) 18.0 (3.5) 17.7 (3.7) 15.1 (5.9) 17.8 (3.4) Pageblocks 48.1 (5.8) 39.6 (4.7) 65.5 (4.2) 62.8 (3.9) 48.6 (3.3) 42.4 (5.2) 55.7 (5.7) 62.8 (8.2) 59.4 (7.5) Yeast 0.0 (0.0) 29.4 (2.9) 38.0 (4.4) 39.0 (7.5) 2.5 (5.0) 29.0 (3.5) 35.4 (15.6) 38.2 (11.2) 39.5 (9.3) Wine 0.0 (0.0) 15.6 (5.2) 19.1 (6.9) 22.7 (6.0) 0.0 (0.0) 14.6 (3.2) 18.3 (7.2) 18.9 (4.6) 21.1 (5.2)</figDesc><table><row><cell>Datasets</cell><cell>SVM</cell><cell>SVMI.R.</cell><cell>SVMG</cell><cell>SVMC</cell><cell>LR</cell><cell>LRI.R.</cell><cell>LRB</cell><cell>LRG</cell><cell>LRC</cell></row><row><cell>Adult</cell><cell cols="4">62.5 (0.2) 64.9 (0.3) 66.5 (0.1) 66.5 (0.1)</cell><cell cols="5">63.1 (0.1) 66.0 (0.1) 66.6 (0.1) 66.5 (0.1) 66.5 (0.1)</cell></row><row><cell>Abalone10</cell><cell>0.0</cell><cell /><cell /><cell /><cell /><cell /><cell /><cell /><cell /></row></table></figure>
<figure type="table" xml:id="tab_38"><head /><label /><figDesc>1 (0.1) 91.0 (0.2) 90.6 (0.1) 90.6 (0.1) 89.4 (0.2) 90.6 (0.2) 90.6 (0.2) Average 32.1 (0.7) 44.0 (2.3) 49.2 (2.8) 49.6 (2.8) 33.4 (1.0) 45.1 (2.3) 47.0 (3.9) 48.5 (3.6) 48.7 (3.3)</figDesc><table /></figure>
<figure type="table" xml:id="tab_39"><head>Table 17 :</head><label>17</label><figDesc>Mean F-Measure over 5 experiments and limiting the number of iterations/grid steps to 15 (standard deviation between brackets). (0.2) 64.9 (0.3) 66.4 (0.1) 66.5 (0.1)</figDesc><table><row><cell>Datasets</cell><cell>SVM</cell><cell>SVMI.R.</cell><cell>SVMG</cell><cell>SVMC</cell><cell>LR</cell><cell>LRI.R.</cell><cell>LRB</cell><cell>LRG</cell><cell>LRC</cell></row><row><cell>Adult</cell><cell>62.5</cell><cell /><cell /><cell /><cell /><cell /><cell /><cell /><cell /></row></table></figure>
<figure type="table" xml:id="tab_40"><head /><label /><figDesc>(0.0) 15.6 (5.2) 20.0 (6.4) 22.7 (6.0) 0.0 (0.0) 14.6 (3.2) 18.3 (7.2) 18.7 (4.5) 21.1 (5.2)</figDesc><table><row><cell /><cell /><cell>2 (5.3) 21.4 (4.6) 20.7 (4.8) 20.5 (5.0)</cell></row><row><cell>IJCNN</cell><cell>44.5 (0.4) 53.3 (0.4) 61.8 (0.4) 61.6 (0.6)</cell><cell>46.2 (0.3) 51.6 (0.3) 59.2 (0.3) 58.2 (0.2) 58.2 (0.3)</cell></row><row><cell>Abalone12</cell><cell>0.0 (0.0) 16.8 (2.7) 16.8 (4.2) 18.3 (3.3)</cell><cell>0.0 (0.0) 18.0 (3.5) 17.7 (3.7) 17.2 (3.1) 18.4 (2.3)</cell></row><row><cell>Pageblocks</cell><cell>48.1 (5.8) 39.6 (4.7) 65.7 (2.1) 62.8 (3.9)</cell><cell>48.6 (3.3) 42.4 (5.2) 55.7 (5.7) 62.7 (8.3) 59.4 (7.5)</cell></row><row><cell>Yeast</cell><cell>0.0 (0.0) 29.4 (2.9) 39.0 (6.8) 39.0 (7.5)</cell><cell>2.5 (5.0) 29.0 (3.5) 35.4 (15.6) 39.1 (10.1) 39.5 (9.3)</cell></row><row><cell>Wine</cell><cell>0.0</cell><cell /></row></table></figure>
<figure type="table" xml:id="tab_41"><head>Table 18 :</head><label>18</label><figDesc>1 (0.1) 91.0 (0.1) 90.6 (0.1) 90.6 (0.1) 89.4 (0.2) 90.6 (0.2) 90.6 (0.2) Average 32.1 (0.7) 44.0 (2.3) 49.3 (2.7) 49.6 (2.8) 33.4 (1.0) 45.1 (2.3) 47.0 (3.9) 48.8 (3.2) 48.8 (3.2)20 Mean F-Measure over 5 experiments and limiting the number of iterations/grid steps to 16 (standard deviation between brackets). (0.2) 64.9 (0.3) 66.4 (0.1) 66.5 (0.1)63.1 (0.1) 66.0 (0.1) 66.6 (0.1) 66.5 (0.1) 66.5 (0.1) Abalone10 0.0 (0.0) 30.9 (1.2) 32.4 (1.3) 32.2 (0.8) 0.0 (0.0) 31.9 (1.4) 31.6 (0.6) 31.7 (0.7) 30.9 (1.9) Satimage 0.0 (0.0) 23.4 (4.3) 20.4 (5.3) 20.6 (5.6) 0.5 (0.9) 24.2 (5.3) 21.4 (4.6) 20.7 (4.8) 20.5 (5.0) IJCNN 44.5 (0.4) 53.3 (0.4) 61.6 (0.6) 61.6 (0.6) 46.2 (0.3) 51.6 (0.3) 59.2 (0.3) 58.0 (0.4) 58.2 (0.3) Abalone12 0.0 (0.0) 16.8 (2.7) 16.8 (4.2) 18.3 (3.3) 0.0 (0.0) 18.0 (3.5) 17.7 (3.7) 17.2 (3.1) 18.4 (2.3) Pageblocks 48.1 (5.8) 39.6 (4.7) 65.5 (4.2) 62.8 (3.9) 48.6 (3.3) 42.4 (5.2) 55.7 (5.7) 62.8 (8.2) 59.4 (7.5) Yeast 0.0 (0.0) 29.4 (2.9) 38.6 (7.1) 39.0 (7.5) 2.5 (5.0) 29.0 (3.5) 35.4 (15.6) 39.1 (10.1) 39.5 (9.3) Wine 0.0 (0.0) 15.6 (5.2) 20.0 (6.4) 22.7 (6.0) 0.0 (0.0) 14.6 (3.2) 18.3 (7.2) 18.7 (4.5) 21.1 (5.2)</figDesc><table><row><cell>Datasets</cell><cell>SVM</cell><cell>SVMI.R.</cell><cell>SVMG</cell><cell>SVMC</cell><cell>LR</cell><cell>LRI.R.</cell><cell>LRB</cell><cell>LRG</cell><cell>LRC</cell></row><row><cell>Adult</cell><cell>62.5</cell><cell /><cell /><cell /><cell /><cell /><cell /><cell /><cell /></row></table></figure>
<figure type="table" xml:id="tab_42"><head /><label /><figDesc>1) 91.0 (0.2) 91.1 (0.1) 91.0 (0.1) 90.6 (0.1) 90.6 (0.1) 89.4 (0.2) 90.6 (0.2) 90.6 (0.2) Average 32.1 (0.7) 44.0 (2.3) 49.4 (3.0) 49.6 (2.8) 33.4 (1.0) 45.1 (2.3) 47.0 (3.9) 48.8 (3.2) 48.8 (3.2)</figDesc><table /></figure>
<figure type="table" xml:id="tab_43"><head>Table 19 :</head><label>19</label><figDesc>Mean F-Measure over 5 experiments and limiting the number of iterations/grid steps to 17 (standard deviation between brackets). (0.2) 64.9 (0.3) 66.4 (0.1) 66.5 (0.1)63.1 (0.1) 66.0 (0.1) 66.6 (0.1) 66.5 (0.1) 66.5 (0.1) Abalone10 0.0 (0.0) 30.9 (1.2) 32.4 (1.3) 32.2 (0.8) 0.0 (0.0) 31.9 (1.4) 31.6 (0.6) 31.7 (0.7) 30.9 (1.9) Satimage 0.0 (0.0) 23.4 (4.3) 20.4 (5.3) 20.6 (5.6) 0.5 (0.9) 24.2 (5.3) 21.4 (4.6) 20.7 (4.8) 20.5 (5.0) IJCNN 44.5 (0.4) 53.3 (0.4) 61.6 (0.6) 61.6 (0.6) 46.2 (0.3) 51.6 (0.3) 59.2 (0.3) 58.2 (0.2) 58.2 (0.3) Abalone12 0.0 (0.0) 16.8 (2.7) 16.8 (4.2) 18.3 (3.3) 0.0 (0.0) 18.0 (3.5) 17.7 (3.7) 17.2 (3.1) 18.4 (2.3) Pageblocks 48.1 (5.8) 39.6 (4.7) 66.4 (3.2) 62.8 (3.9) 48.6 (3.3) 42.4 (5.2) 55.7 (5.7) 62.8 (8.2) 59.4 (7.5) Yeast 0.0 (0.0) 29.4 (2.9) 38.6 (7.1) 39.0 (7.5) 2.5 (5.0) 29.0 (3.5) 35.4 (15.6) 39.1 (10.1) 39.5 (9.3) Wine 0.0 (0.0) 15.6 (5.2) 20.0 (6.4) 22.7 (6.0) 0.0 (0.0) 14.6 (3.2) 18.3 (7.2) 18.7 (4.5) 21.1 (5.2)</figDesc><table><row><cell>Datasets</cell><cell>SVM</cell><cell>SVMI.R.</cell><cell>SVMG</cell><cell>SVMC</cell><cell>LR</cell><cell>LRI.R.</cell><cell>LRB</cell><cell>LRG</cell><cell>LRC</cell></row><row><cell>Adult</cell><cell>62.5</cell><cell /><cell /><cell /><cell /><cell /><cell /><cell /><cell /></row></table></figure>
<figure type="table" xml:id="tab_44"><head /><label /><figDesc>1 (0.1) 91.0 (0.1) 90.6 (0.1) 90.6 (0.1) 89.4 (0.2) 90.6 (0.2) 90.6 (0.2) Average 32.1 (0.7) 44.0 (2.3) 49.5 (2.9) 49.6 (2.8) 33.4 (1.0) 45.1 (2.3) 47.0 (3.9) 48.8 (3.2) 48.8 (3.2)</figDesc><table /></figure>
<figure type="table" xml:id="tab_45"><head>Table 20 :</head><label>20</label><figDesc>Mean F-Measure over 5 experiments and limiting the number of iterations/grid steps to 18 (standard deviation between brackets). (0.2) 64.9 (0.3) 66.4 (0.1) 66.5 (0.1)63.1 (0.1) 66.0 (0.1) 66.6 (0.1) 66.5 (0.1) 66.5 (0.1) Abalone10 0.0 (0.0) 30.9 (1.2) 32.4 (1.3) 32.2 (0.8) 0.0 (0.0) 31.9 (1.4) 31.6 (0.6) 31.7 (0.7) 30.9 (1.9) Satimage 0.0 (0.0) 23.4 (4.3) 20.4 (5.3) 20.6 (5.6) 0.5 (0.9) 24.2 (5.3) 21.4 (4.6) 20.7 (4.8) 20.5 (5.0) IJCNN 44.5 (0.4) 53.3 (0.4) 61.6 (0.6) 61.6 (0.6) 46.2 (0.3) 51.6 (0.3) 59.2 (0.3) 58.2 (0.2) 58.2 (0.3) Abalone12 0.0 (0.0) 16.8 (2.7) 16.8 (4.2) 18.3 (3.3) 0.0 (0.0) 18.0 (3.5) 17.7 (3.7) 17.2 (3.1) 18.4 (2.3) Pageblocks 48.1 (5.8) 39.6 (4.7) 66.4 (3.2) 62.8 (3.9) 48.6 (3.3) 42.4 (5.2) 55.7 (5.7) 62.8 (8.2) 59.4 (7.5) Yeast 0.0 (0.0) 29.4 (2.9) 38.6 (7.1) 39.0 (7.5) 2.5 (5.0) 29.0 (3.5) 35.4 (15.6) 39.1 (10.1) 39.5 (9.3) Wine 0.0 (0.0) 15.6 (5.2) 20.0 (6.4) 22.7 (6.0) 0.0 (0.0) 14.6 (3.2) 18.3 (7.2) 18.7 (4.5) 21.1 (5.2)</figDesc><table><row><cell>Datasets</cell><cell>SVM</cell><cell>SVMI.R.</cell><cell>SVMG</cell><cell>SVMC</cell><cell>LR</cell><cell>LRI.R.</cell><cell>LRB</cell><cell>LRG</cell><cell>LRC</cell></row><row><cell>Adult</cell><cell>62.5</cell><cell /><cell /><cell /><cell /><cell /><cell /><cell /><cell /></row></table></figure>
<figure type="table" xml:id="tab_46"><head /><label /><figDesc>1 (0.1) 91.0 (0.1) 90.6 (0.1) 90.6 (0.1) 89.4 (0.2) 90.6 (0.2) 90.6 (0.2)</figDesc><table><row><cell>Average</cell><cell>32.1 (0.7) 44.0 (2.3) 49.5 (2.9) 49.6 (2.8)</cell><cell>33.4 (1.0) 45.1 (2.3) 47.0 (3.9) 48.8 (3.2) 48.8 (3.2)</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>This work is supported by the <rs type="projectName">TADALoT</rs> project (<rs type="grantNumber">Pack Ambition 2017</rs>, <rs type="grantNumber">17 011047 01</rs>) and the <rs type="funder">FUI MIVAO</rs>.<rs type="projectName"><software>TADALoT</software></rs> project (<rs type="grantNumber">Pack Ambition 2017</rs>, <rs type="grantNumber">17 011047 01</rs>) and the <rs type="funder">FUI MIVAO</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_BtAgn4S">
					<idno type="grant-number">Pack Ambition 2017</idno>
					<orgName type="project" subtype="full">TADALoT</orgName>
				</org>
				<org type="funding" xml:id="_ZvJb6kU">
					<idno type="grant-number">17 011047 01</idno>
				</org>
			</listOrg>
			<div type="annex">
<div><p>Systems 27, pages 2744-2752. Curran <ref type="bibr">Associates, Inc. Narasimhan, H., Ramaswamy, H., Saha, A., and Agarwal, S. (2015)</ref> </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Generalized Convexity and Optimization: Theory and Applications</title>
		<author>
			<persName><forename type="first">Alberto</forename></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Laura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Lecture Notes in Economics and Mathematical Systems</title>
		<imprint>
			<biblScope unit="volume">616</biblScope>
			<date type="published" when="2009">2009</date>
			<publisher>Springer-Verlag</publisher>
			<pubPlace>Berlin Heidelberg</pubPlace>
		</imprint>
	</monogr>
	<note>1 edition</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Introduction to Statistical Learning Theory</title>
		<author>
			<persName><forename type="first">O</forename><surname>Bousquet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Boucheron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lugosi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
			<publisher>Springer</publisher>
			<biblScope unit="page" from="169" to="207" />
			<pubPlace>Berlin Heidelberg, Berlin, Heidelberg</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Vandenberghe</surname></persName>
		</author>
		<title level="m">Convex Optimization</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Online f-measure optimization</title>
		<author>
			<persName><forename type="first">R</forename><surname>Busa-Fekete</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Szörényi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Dembczynski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hüllermeier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Anomaly detection: A survey</title>
		<author>
			<persName><forename type="first">V</forename><surname>Chandola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Comput. Surv</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Deep f-measure maximization in multi-label classification: A comparative study</title>
		<author>
			<persName><forename type="first">S</forename><surname>Decubber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mortier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Dembczynski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Waegeman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Consistency analysis for binary classification revisited</title>
		<author>
			<persName><forename type="first">K</forename><surname>Dembczyński</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Kotłowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Koyejo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Natarajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<title level="s">Proceedings of Machine Learning Research</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Precup</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</editor>
		<meeting>the 34th International Conference on Machine Learning<address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>ternational Convention Centre</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="961" to="969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title />
		<author>
			<persName><surname>Pmlr</surname></persName>
		</author>
		<imprint />
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">An exact algorithm for fmeasure maximization</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Dembczynski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Waegeman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hüllermeier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Maximum expected f-measure training of logistic regression models</title>
		<author>
			<persName><forename type="first">M</forename><surname>Jansche</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A support vector method for multivariate performance measures</title>
		<author>
			<persName><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Consistent binary classification with generalized performance metrics</title>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">O</forename><surname>Koyejo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Natarajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">K</forename><surname>Ravikumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">S</forename><surname>Dhillon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Cortes</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="2744" to="2752" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title />
		<author>
			<persName><forename type="first">Curran</forename><surname>Associates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Inc</forename></persName>
		</author>
		<imprint />
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">An insight into classification with imbalanced data: Empirical results and current trends on using data intrinsic characteristics</title>
		<author>
			<persName><forename type="first">V</forename><surname>Lopez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fernandez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Palade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Herrera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Sciences</title>
		<imprint>
			<biblScope unit="volume">250</biblScope>
			<biblScope unit="page" from="113" to="141" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Optimizing f-measure with support vector machines</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Musicant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ozgur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FLAIRS</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Optimizing non-decomposable performance measures: A tale of two classes</title>
		<author>
			<persName><forename type="first">H</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning, ICML 2015</title>
		<meeting>the 32nd International Conference on Machine Learning, ICML 2015<address><addrLine>Lille, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-06-11">2015. 6-11 July 2015</date>
			<biblScope unit="page" from="199" to="208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Consistent multiclass algorithms for complex performance measures</title>
		<author>
			<persName><forename type="first">H</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ramaswamy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Saha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning</title>
		<title level="s">Proceedings of Machine Learning Research</title>
		<editor>
			<persName><forename type="first">F</forename><surname>Bach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Blei</surname></persName>
		</editor>
		<meeting>the 32nd International Conference on Machine Learning<address><addrLine>Lille, France</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="2398" to="2407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Optimizing f-measures by cost-sensitive classification</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Parambath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Grandvalet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="2123" to="2131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Optimizing f-measure with non-convex loss and sparse linear classifiers</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Chinta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Balamurugan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Murty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCNN</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">On pseudolinear functions</title>
		<author>
			<persName><forename type="first">T</forename><surname>Rapcsák</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">European Journal of Operational Research</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="353" to="360" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Further experiments with hierarchic clustering in document retrieval</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Van Rijsbergen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Storage and Retrieval</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="1974">1974</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Optimizing f-measure: A tale of two approaches</title>
		<author>
			<persName><forename type="first">N</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M A</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">L</forename><surname>Chieu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th International Conference on Machine Learning</title>
		<meeting>the 29th International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>ICML</publisher>
			<date type="published" when="2012">2012. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Beyond fano's inequality: Bounds on the optimal f-score, ber, and cost-sensitive risk and their implications</title>
		<author>
			<persName><forename type="first">M.-J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Edakunni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pocock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR. References</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Consistent binary classification with generalized performance metrics</title>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">O</forename><surname>Koyejo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Natarajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">K</forename><surname>Ravikumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">S</forename><surname>Dhillon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing</title>
		<editor>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Cortes</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>