<TEI xmlns="http://www.tei-c.org/ns/1.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xml:space="preserve" xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Overview of BirdCLEF 2019: Large-Scale Bird Recognition in Soundscapes</title>
				<funder>
					<orgName type="full">Xeno-canto Foundation</orgName>
				</funder>
				<funder ref="#_ggTACWG">
					<orgName type="full">French CNRS</orgName>
				</funder>
				<funder ref="#_24e8t43">
					<orgName type="full">EADM GDR CNRS MADICS</orgName>
				</funder>
				<funder>
					<orgName type="full">European Union</orgName>
				</funder>
				<funder>
					<orgName type="full">European Social Fund</orgName>
					<orgName type="abbreviated">ESF</orgName>
				</funder>
				<funder>
					<orgName type="full">BRILAAM STIC-AmSud</orgName>
				</funder>
				<funder>
					<orgName type="full">Floris'Tic</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher />
				<availability status="unknown"><licence /></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Stefan</forename><surname>Kahl</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Chemnitz University of Technology</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Fabian-Robert</forename><surname>Stöter</surname></persName>
							<email>fabian-robert.stoter@inria.fr</email>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Inria</orgName>
								<orgName type="institution" key="instit2">LIRMM ZENITH team</orgName>
								<address>
									<settlement>Montpellier</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hervé</forename><surname>Goëau</surname></persName>
							<email>herve.goeau@cirad.fr</email>
							<affiliation key="aff2">
								<orgName type="laboratory">CIRAD</orgName>
								<orgName type="institution">UMR AMAP</orgName>
								<address>
									<settlement>Montpellier</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hervé</forename><surname>Glotin</surname></persName>
							<email>herve.glotin@univ-tln.fr</email>
							<affiliation key="aff3">
								<orgName type="institution" key="instit1">Université de Toulon</orgName>
								<orgName type="institution" key="instit2">Aix Marseille Univ</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<orgName type="institution" key="instit4">LIS</orgName>
								<orgName type="institution" key="instit5">DYNI team</orgName>
								<address>
									<settlement>Marseille</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Robert</forename><surname>Planqué</surname></persName>
							<affiliation key="aff4">
								<orgName type="department">Xeno-canto Foundation</orgName>
								<address>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Willem-Pier</forename><surname>Vellinga</surname></persName>
							<affiliation key="aff4">
								<orgName type="department">Xeno-canto Foundation</orgName>
								<address>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Alexis</forename><surname>Joly</surname></persName>
							<email>alexis.joly@inria.fr</email>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Inria</orgName>
								<orgName type="institution" key="instit2">LIRMM ZENITH team</orgName>
								<address>
									<settlement>Montpellier</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Overview of BirdCLEF 2019: Large-Scale Bird Recognition in Soundscapes</title>
					</analytic>
					<monogr>
						<imprint>
							<date />
						</imprint>
					</monogr>
					<idno type="MD5">90A8F7DBDFD1D6BACEBFDC934308B4BF</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-04-12T14:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid" />
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>LifeCLEF</term>
					<term>bird</term>
					<term>song</term>
					<term>call</term>
					<term>species</term>
					<term>retrieval</term>
					<term>audio</term>
					<term>collection</term>
					<term>identification</term>
					<term>fine-grained classification</term>
					<term>evaluation</term>
					<term>benchmark</term>
					<term>bioacoustics</term>
					<term>ecological monitoring</term>
				</keywords>
			</textClass>
			<abstract>
<div><p>The <software>BirdCLEF</software> challenge-as part of the 2019 <software ContextAttributes="used">LifeCLEF</software> Lab [7]-offers a large-scale proving ground for system-oriented evaluation of bird species identification based on audio recordings. The challenge uses data collected through Xeno-canto, the worldwide community of bird sound recordists. This ensures that <software ContextAttributes="used">BirdCLEF</software> is close to the conditions of real-world application, in particular with regard to the number of species in the training set (659). In 2019, the challenge was focused on the difficult task of recognizing all birds vocalizing in omni-directional soundscape recordings. Therefore, the dataset of the previous year was extended with more than 350 hours of manually annotated soundscapes that were recorded using 30 field recorders in Ithaca (NY, USA). This paper describes the methodology of the conducted evaluation as well as the synthesis of the main results and lessons learned.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div><head n="1">Introduction</head><p>Accurate knowledge of the identity, the geographic distribution and the evolution of bird species is essential for a sustainable development of humanity as well as for biodiversity conservation. The general public, especially so-called 'birders' as well as professionals such as park rangers, ecological consultants and of course ornithologists are potential users of an automated bird sound identification system in the context of wider initiatives related to ecological surveillance or biodiversity conservation. The <software ContextAttributes="used">BirdCLEF</software> challenge -as part of the 2019 <software ContextAttributes="used">LifeCLEF</software> Lab <ref type="bibr" target="#b6">[7]</ref>evaluates the state-of-the-art of audio-based bird identification systems at a very large scale. Before <software ContextAttributes="used">BirdCLEF</software> started in 2014, three previous initiatives on the evaluation of acoustic bird species identification took place, including two from the SABIOD<ref type="foot" target="#foot_0">6</ref> group <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b1">2]</ref>. In collaboration with the organizers of these previous challenges, the <software ContextAttributes="used">BirdCLEF</software> challenges went one step further by (i) significantly increasing the species number by an order of magnitude, (ii) working on realworld social data built from thousands of recordists, and (iii) moving to a more usage-driven and system-oriented benchmark by allowing the use of metadata and defining information retrieval oriented metrics. Overall, these tasks were much more difficult than previous benchmarks because of the higher confusion risk between the classes, the higher background noise and the higher diversity in the acquisition conditions (different recording devices, contexts diversity, etc.).</p><p>The main novelty of the 2017 and 2018 editions of the challenge with respect to the previous years was the inclusion of soundscape recordings containing timecoded bird species annotations. Usually Xeno-canto recordings focus on a single foreground species and result from using mono-directional recording devices. Soundscapes, on the other hand, are generally based on omnidirectional recording devices that monitor a specific environment continuously over a long period. This new kind of recording reflects passive acoustic monitoring scenarios that could soon augment the number of collected sound recordings by several orders of magnitude. Despite the technological progress in recent years, the results of the previous editions on this challenging soundscape task were quite low. We decided to shift the focus of the 2019 challenge to soundscape analysis only. We extend the previous dataset with North American bird species for which more annotated data was available. In particular, we built a dataset of 350 hours of soundscapes that were recorded and annotated by expert birders of the Cornell Lab of Ornithology in Ithaca, NY, USA (see Figure <ref type="figure">1</ref>). This large volume of data allowed us to share a fully-annotated, three-day validation dataset to enable participants to thoroughly evaluate their systems.</p></div>
<div><head n="2">Task description</head><p>The 2019 <software>BirdCLEF</software> challenge featured the largest, fully-annotated collection of soundscape recordings. With respect to real-world use cases, labels and metrics were chosen to reflect the vast diversity of bird vocalizations and high ambient noise levels in omnidirectional recordings.</p></div>
<div><head n="2.1">Goal and evaluation protocol</head><p>The goal of the task is to localize and identify all audible birds within the provided soundscape test set. Each soundscape is divided into segments of 5 Fig. <ref type="figure">1</ref>: Example of an annotated soundscape recording. Expert birders provided more than 80,000 bounding box annotations using the <software ContextAttributes="used">Raven Pro</software> analysis software. For reasons of better comparability, those annotations were condensed into label lists for 5-second intervals. seconds, and a list of species associated to probability scores had to be returned for each segment. The used evaluation metric is the classification mean Average Precision (cmAP ), considering each class c of the ground truth as a query. This means that for each class c, all predictions with ClassId = c are extracted from the run file and ranked by decreasing probability in order to compute the average precision for that class. The mean across all classes is computed as the main evaluation metric. More formally:</p><formula xml:id="formula_0">cmAP = C c=1 AveP (c) C</formula><p>where C is the number of classes (species) in the ground truth and AveP (c) is the average precision for a given species c computed as:</p><formula xml:id="formula_1">AveP (c) = nc k=1 P (k) × rel(k) n rel (c) .</formula><p>where k is the rank of an item in the list of the predicted segments containing c, n c is the total number of predicted segments containing c, P (k) is the precision at cut-off k in the list, rel(k) is an indicator function equaling 1 if the segment at rank k is a relevant one (i.e. is labeled as containing c in the ground truth) and n rel (c) is the total number of relevant segments for class c.</p></div>
<div><head n="2.2">Dataset</head><p>The 2019 dataset contains about 350 hours of manually annotated soundscapesmost of which were recorded using field recorders between January and June of 2017 in Ithaca, NY, USA. We used SWIFT recording units provided by the Bioacoustics Research Program<ref type="foot" target="#foot_1">7</ref> of the Cornell Lab of Ornithology (Figure <ref type="figure" target="#fig_1">2</ref>). These omnidirectional recorders capture audio over an array of 30 units spanning  one square mile of diverse vegetation and water bodies. We randomly selected one file for each hour of the day recorded with one of the 30 recorders to compile a data collection of 15 days. Each hour-long recording was then annotated by experts who provided more than 80,000 bounding boxes-one for each audible bird vocalization. For the sake of comparability with previous editions, these annotations were condensed into label lists for 5-second segments of audio.</p><p>In addition, we also re-used the soundscape data from the previous years of <software ContextAttributes="used">BirdCLEF</software>. More specifically, this concerns about 4,5 hours of soundscapes recorded in Columbia by Paula Caycedo Rosales, ornithologist from the Biodiversa Foundation of Colombia and an active member of Xeno-canto. More details about this soundscape data (locations, authors, etc.) can be found in the overview working note of <software ContextAttributes="used">BirdCLEF</software> 2018 <ref type="bibr" target="#b5">[6]</ref>.</p><p>As for training data, we provided an newly composed Xeno-Canto subset covering 659 species from South and North America (including all species annotated in the soundscapes). The vast collection of recordings provided by the Xeno-canto community often features multiple hundreds of recordings per (common) bird species. Especially North American species are well represented in the collection. Therefore, we limited the amount of audio files to 100 recordings per species. This way, we decreased data imbalance and provided a manageable amount of data. In total, the training data featured 50,153 files with a total run length of 608 hours. We selected recordings based on their community rating to preserve a high quality for most species. Each recording contained weak labels that state the presence of fore and background species.</p><p>Recordings are associated to various metadata such as the type of sound (call, song, alarm, flight, etc.), the date of recording, the location, textual comments of the authors, multilingual common names and collaborative quality ratings. Additionally, we provided <software>eBird</software>.org frequency lists to enable participants to decide which species are plausible for a given time, date and location. Frequency estimations of bird species occurrences were compiled using <software ContextAttributes="used">eBird</software> checklist data for the soundscape recording locations in the US and Colombia provided by the <software ContextAttributes="used">eBird API</software> 1.1 (which was unfortunately discontinued in March 2019).</p><p>The shift in acoustic domains between mono-species, high quality recordings and omnidirectional soundscapes with high ambient noise levels is one of the major challenges in bird sound recognition for avian activity monitoring. Participants were required to submit at least one run that used training data only. Aside from that, participants were allowed to use validation data for training (despite the fact that this would require extensive annotation when switching recording locations in real-world applications).</p></div>
<div><head n="3">Results</head><p>103 participants registered for the <software ContextAttributes="used">BirdCLEF</software> 2019 challenge and downloaded the dataset. Five of them succeeded in submitting runs, but only four teams published their approaches. Details of the methods and systems used in the runs are synthesized in the individual working notes of the participants and are summarized in this section. In Figure <ref type="figure">3</ref> we report the performance achieved by the 25 collected runs, Table <ref type="table">1</ref> provides more detailed insights and additional scores for each of the two soundscape recording locations.</p></div>
<div><head n="3.1">MfN [11], Best overall</head><p>Lasseck managed to achieve top scores in most of the past editions in Bird<software ContextAttributes="used">-CLEF</software>. Most notably, his 2018 performance topped all previous results in the mono-species recording domain <ref type="bibr" target="#b9">[10]</ref> and led to the observation that this task can be considered solved. This year, MfN build upon the results of past editions and managed to outperform all other participating teams with his very deep Inception and ResNet architectures that were pre-trained on ImageNet, as a continuation of <ref type="bibr" target="#b11">[12]</ref>. Lasseck used 5-second spectrograms with mel-compressed frequency and dB amplitude scale. Sophisticated data augmentation methods lead to consistent improvements and can be considered a major contribution to the field of bioacoustics. Additionally, the use of validation data to fine-tune the pre-trained networks has a significant effect on the overall scores. Considering this, annotating soundscapes to adapt neural networks to specific recording conditions appears to be well worth the costs. Fig. <ref type="figure">3</ref>: Scores achieved by all systems evaluated within the bird identification task of <software ContextAttributes="used">LifeCLEF</software> 2019. MfN scored best overall with outstanding results in both soundscape domains. The difference between runs that only used training data and those which also used validation samples for training is significant and raises the question whether local adaption to recording conditions is worth the manual annotation effort.</p></div>
<div><head n="3.2">ASAS [9]</head><p>This team also used Inception and ResNet architectures to conduct their experiments. Again, task-specific data augmentation was key to achieve higher scores. ASAS followed the spectrogram extraction approach of the 2018 Baseline Repository <ref type="bibr" target="#b7">[8]</ref>. The results however-although very competitive-do not outperform the approach of Lasseck despite the similarities in deep neural network design. This leads to the assumption that sophisticated augmentation strategies are of particular importance since they provide the needed variance to the input data distribution which prevents overfitting. The authors state that pre-processing of training data could have a significant impact on the overall performance due to the difficulties of weakly labeled data.</p></div>
<div><head n="3.3">NWPU [1]</head><p>Consistent with all other approaches, these participants used mel-scale spectrogram extracted from the provided training data to build a Inception-v3 feature extractor and classifier. The model was pre-trained on ImageNet and a number data augmentation methods were applied. However, training deep neural networks is costly and subsequently, the participants were not able to conduct the amount of experiments needed to achieve higher scores. The results state once again the most notable observation across all submission: An elaborate training regime is key to good overall performance. It appears that this applies independent of the underlying network architecture.</p><p>Table <ref type="table">1</ref>: Detailed results of runs submitted by the participants. Lasseck submitted the best performing run based on our primary metric (MfN run 5). In both domains, Colombia and North America, results show string improvements compared to previous years. Team names shortened for brevity. </p></div>
<div><head n="3.4">MIHAI [3]</head><p>The submission results of this participant confirm this thought. The author states that he was able to confirm that deeper architectures do not necessarily lead to better performance, especially when computational constraints limit the choice of hyperparameters. Despite very low scores across all runs, the observation that task-specific training and model layouts matter was consistent with the submissions of other teams.</p><p>In this edition of the <software>BirdCLEF</software> challenge, participants built on established systems from previous years, all submitted runs featured a CNN classifier trained on spectrograms-very deep networks once again performed best. Participants were able to significantly improve the detection performance. In fact, we saw an increase of more than 180% for the best performing runs (2018: 0.193 -2019: 0.356). This result is probably largely due to the high number of North American soundscapes that are less complex than their South American counterparts. However, the recognition performance for South American soundscapes also increased significantly compared to 2018 with a cmAP of 0.293 in 2019 over 0.222 from last year. Participants were allowed to use any publicly available metadata and even the provided validation data to improve the performance of their systems. Although expert annotations are not an adequate (or even easy-to-acquire) addition for the training of a recognition system for unseen habitats, the increase in overall performance is considerable. The highest scoring run submitted by MfN achieved a sample-wise mean average precision (our secondary metric) of 0.446 without the use of validation samples and 0.745 when validation data was used for training. These scores imply that domain adaption to new acoustic environments (and recorder characteristics) plays a crucial role and should be subject of investigation in future editions.</p></div><figure xml:id="fig_0"><head /><label /><figDesc>(a) SWIFT recorder assembly line (b) SWIFT recorder in the field</figDesc></figure>
<figure xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Fig. 2: Autonomous recording units are a widely used sampling tool in ecological research. The SWIFT recorder provided by the Bioacoustics Research Program (BRP) of the Cornell Lab of Ornithology allows to record up to 30 consecutive days of audio. Optimizing the assembly of these weatherproof recorders reduces the costs per unit significantly. Images provided by the BRP.</figDesc><graphic coords="4,134.77,125.80,164.24,123.18" type="bitmap" /></figure>
<figure><head /><label /><figDesc /><graphic coords="6,134.77,115.83,345.82,154.52" type="bitmap" /></figure>
			<note place="foot" n="6" xml:id="foot_0"><p>Scaled Acoustic Biodiversity http://sabiod.univ-tln.fr</p></note>
			<note place="foot" n="7" xml:id="foot_1"><p>http://www.birds.cornell.edu/brp/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements The organization of the <software ContextAttributes="used">BirdCLEF</software> task is supported by the <rs type="funder">Xeno-canto Foundation</rs>, the <rs type="funder">European Union</rs> and the <rs type="funder">European Social Fund (ESF) for Germany</rs>, as well as by the <rs type="funder">French CNRS</rs> project <rs type="projectName">SABIOD</rs>.<rs type="projectName">ORG</rs> and <rs type="funder">EADM GDR CNRS MADICS</rs>, <rs type="funder">BRILAAM STIC-AmSud</rs>, and <rs type="funder">Floris'Tic</rs>. The annotations of some soundscapes were prepared by the wonderful <rs type="person">Lucio Pando</rs> of <rs type="person">Explorama Lodges</rs>, with the support of <rs type="person">Pam Bucur</rs>, <rs type="person">H. Glotin</rs> and <rs type="person">Marie Trone</rs>. We want to thank all expert birders who annotated North American soundscapes with incredible effort: <rs type="person">Cullen Hanks</rs>, <rs type="person">Jay McGowan</rs>, <rs type="person">Matt Young</rs>, <rs type="person">Randy Little</rs>, and <rs type="person">Sarah Dzielski</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_ggTACWG">
					<orgName type="project" subtype="full">SABIOD</orgName>
				</org>
				<org type="funded-project" xml:id="_24e8t43">
					<orgName type="project" subtype="full">ORG</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div><head>Overall</head><p>North America Colombia TEAM <ref type="bibr">RUN</ref>  </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Inception-v3 based method of lifeclef 2019 bird recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CLEF working notes 2019</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The 9th mlsp competition: New methods for acoustic classification of multiple simultaneous bird species in noisy environment</title>
		<author>
			<persName><forename type="first">F</forename><surname>Briggs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Raich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Eftaxias</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Workshop on Machine Learning for Signal Processing (MLSP)</title>
		<imprint>
			<biblScope unit="page" from="1" to="8" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Bird species identification using neural networks</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Costandache</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CLEF working notes</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page">2019</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Bioacoustic challenges in icml4b</title>
		<author>
			<persName><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dugan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Halkias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sueur</surname></persName>
		</author>
		<ptr target="http://sabiod.org/ICML4B2013_proceedings.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proc. of 1st workshop on Machine Learning for Bioacoustics</title>
		<meeting>of 1st workshop on Machine Learning for Bioacoustics<address><addrLine>USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Overview of the 2nd challenge on acoustic bird classification</title>
		<author>
			<persName><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Dufour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bas</surname></persName>
		</author>
		<ptr target="http://sabiod.univ-tln.fr/nips4b" />
	</analytic>
	<monogr>
		<title level="m">Proc. Neural Information Processing Scaled for Bioacoustics. NIPS Int. Conf</title>
		<meeting>Neural Information essing Scaled for Bioacoustics. NIPS Int. Conf<address><addrLine>Halkias X., USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Overview of birdclef 2018: monophone vs. soundscape bird identification</title>
		<author>
			<persName><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Planqué</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">P</forename><surname>Vellinga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CLEF working notes</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page">2018</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Overview of lifeclef 2019: Identification of amazonian plants, south &amp; north american birds, and niche prediction</title>
		<author>
			<persName><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Botella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Servajean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">P</forename><surname>Vellinga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Planqué</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">R</forename><surname>Stöter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CLEF 2019</title>
		<meeting>CLEF 2019</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Kahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wilhelm-Stein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Klinck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kowerko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Eibl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.07177</idno>
		<title level="m">Recognizing birds from sound -the 2018 birdclef baseline system</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Bird sound classification using convolutional neural networks</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Y</forename><surname>Koh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">H</forename><surname>Hsieh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CLEF working notes</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page">2019</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Audio-based bird species identification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Lasseck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Working Notes of CLEF 2018 (Cross Language Evaluation Forum</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Bird species identification in soundscapes</title>
		<author>
			<persName><forename type="first">M</forename><surname>Lasseck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CLEF working notes 2019</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Audio bird classification with inception v4 joint to an attention mechanism</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sevilla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Working Notes of CLEF 2017 (Cross Language Evaluation Forum</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>