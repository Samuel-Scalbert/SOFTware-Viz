<TEI xmlns="http://www.tei-c.org/ns/1.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xml:space="preserve" xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Parallel Quotient Summarization of RDF Graphs</title>
			</titleStmt>
			<publicationStmt>
				<publisher />
				<availability status="unknown"><licence /></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Paweł</forename><surname>Guzewicz</surname></persName>
							<email>pawel.guzewicz@inria.fr</email>
							<affiliation key="aff0">
								<orgName type="laboratory" key="lab1">Inria and LIX (</orgName>
								<orgName type="laboratory" key="lab2">UMR 7161</orgName>
								<orgName type="institution">CNRS and Ecole Polytechnique</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ioana</forename><surname>Manolescu</surname></persName>
							<email>ioana.manolescu@inria.fr</email>
							<affiliation key="aff0">
								<orgName type="laboratory" key="lab1">Inria and LIX (</orgName>
								<orgName type="laboratory" key="lab2">UMR 7161</orgName>
								<orgName type="institution">CNRS and Ecole Polytechnique</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Parallel Quotient Summarization of RDF Graphs</title>
					</analytic>
					<monogr>
						<imprint>
							<date />
						</imprint>
					</monogr>
					<idno type="MD5">BA15E49C31173B476E40EC9405C58D55</idno>
					<idno type="DOI">10.1145/3323878.3325809</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-04-12T14:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid" />
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Information systems → Database performance evaluation; Resource Description Framework (RDF) RDF graphs</term>
					<term>summarization</term>
					<term>parallel computations</term>
					<term>Spark</term>
				</keywords>
			</textClass>
			<abstract>
<div><p>Discovering the structure and content of an RDF graph is hard for human users, due to its heterogeneity, complexity, and possibly large size. One class of tools for this task are structural RDF graph summaries, which allow users to grasp the different connections between RDF graph nodes. RDFQuotient graph summaries are a brand of structural summaries we developed. They are usually very compact, making them good for first-sight visual discovery. Existing algorithms for building these summaries are centralized, and require the graph to fit in memory.</p><p>Going beyond, in this work we present novel algorithms for building RDFQuotient summaries in a parallel, shared-nothing architecture. We instantiate our algorithms to <software>Apache</software> <software ContextAttributes="used">Spark</software> platform; our experiments demonstrate the merit of our approach.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div><head n="1">INTRODUCTION</head><p>The structural heterogeneity inherent to RDF graphs makes it hard for the casual users to get acquainted with a graph's structure. To help solve this problem, many RDF summarization techniques have been proposed [CGK + 18]. Each of them builds out of a given RDF graph, a compact structure which conveys the essential information of the graph, all that while being much more compact. Among these techniques, we have studied in recent years quotient structural summarization of RDF graphs <ref type="bibr" target="#b7">[ ČGM15b,</ref><ref type="bibr" target="#b8">ČGM17,</ref><ref type="bibr" target="#b11">GGM19]</ref>. A quotient RDF graph summary is a graph obtained by grouping RDF nodes in equivalence classes, each class being represented by a summary node; quotient summary edges similarly "represent" potentially many RDF edges. While quotient summarization has been studied for many equivalence relations, in particular for bisimilarity <ref type="bibr" target="#b15">[HHK95]</ref> equivalence relations [MS99, CLO03, KBNK02, CMRV10, SNLPZ13, TLR13], the interest of the equivalence relations we defined in <ref type="bibr" target="#b11">[GGM19,</ref><ref type="bibr" target="#b3">ČGGM18]</ref> is to be very tolerant of heterogeneity, thus leading to very compact RDF graph summaries, which are particularly suited for human users seeking to get an idea of an RDF graph structure. This is illustrated for many popular RDF graphs in our online gallery <ref type="foot" target="#foot_0">1</ref> .</p><p>On the algorithmic side, we have previously proposed scalable algorithms for centralized summary construction, including incremental ones, capable of reflecting a newly added triple in a summary without the need to re-summarize the graph <ref type="bibr" target="#b11">[GGM19]</ref>. A limitation of these algorithms is that their memory needs grow linearly with the size of the graph, which is problematic for very large graphs in memory-constrained environments.</p><p>This work presents novel, parallel algorithms for building our summaries out of an input RDF graph. We have implemented our algorithms using <software>Spark</software>; our experiments confirm that parallelism allows to distribute work and speed up execution.</p><p>Below, Section 2 recalls RDFQuotient summaries (most examples are borrowed from <ref type="bibr" target="#b11">[GGM19]</ref>). Section 3 presents our contribution, namely our parallel summarization algorithms. Section 4 describes our experiments. We then discuss related work and conclude.</p></div>
<div><head n="2">RDFQUOTIENT SUMMARIES</head><p>Let U be a set of URIs, L be a set of literals and B be a set of blank nodes as per the RDF specification. An RDF graph G is a set of triples of the form (s, p, o) where s ∈ U ∪ B ∪ L, p ∈ U and o ∈ U ∪ B ∪ L. The special URI type, part of the RDF standard, is used to attach types to nodes. An RDF graph may contain ontology (schema) triples; while there are interesting interactions between summarization and ontologies <ref type="bibr" target="#b3">[ ČGGM18]</ref>, below we only focus on summarizing the non-schema triples, which make up the vast majority of all RDF graphs we encountered. Thus, we consider G consists exclusively of type triples and/or data triples (all those whose property is not type; we call these data properties).</p><p>An RDF equivalence relation, denoted ≡, is a binary relation over the nodes of an RDF graph that is reflexive, symmetric and transitive. Given an equivalence relation ≡, an RDF graph quotient is an RDF graph having (i) one node for each equivalence class of nodes; (ii) for each edge n 1 a → n 2 , a summary edge n ≡ 1 a → n ≡ 2 , where n ≡ i , i ∈ {1, 2}, is the summary node corresponding to the equivalence class of n i , also called representative of n i . We call representation function a function that maps each graph node n i to its representative    (n ≡ i ). As stated above, the literature comprises many quotient graph summaries, which differ by their equivalence relations.</p><p>The equivalence relations we use are based on the concept of property cliques, which encodes a transitive relation of edge label (property) co-occurrence on graph nodes. Given an RDF graph G, two data properties a,b are in the same source clique iff: (i) there exists a G node n which is the source of a and b (i.e., (n, a, x) ∈ G and (n, b,y) ∈ G for some x and y), or (iii) there exists a data property c such that c is in the same source clique as a, and c is in the same source clique as b. Symmetrically, a and b are in the same target clique if there exists a G node which is the target of a and b, or a data property c which is in the same target clique as a and b. In Figure <ref type="figure" target="#fig_0">1</ref> (disregard the colored areas for now, their role will be explained later), the properties advises and teaches are in the same source clique due to p 4 . The same holds for advises and wrote due to p 1 ; consequently, advises and wrote are also in the same source clique. Further, the graduate student p 2 teaches a course and takes another, thus teaches, advises, wrote and takes are all part of the same source clique. In this example, p 1 , p 2 , p 3 , p 4 , p 5 have the source clique SC 1 = {advises, takes, teaches, wrote}, c 1 , c 2 , c 3 have the source clique SC 2 = {coursedescr} and a 1 , a 2 have the empty source clique SC 3 = ∅. Similarly, the target cliques are, respectively; TC 1 = {advises} for p 2 , p 5 , TC 2 = {teaches, takes} for c 1 , c 2 , c 3 , TC 3 = {coursedescr} for d 1 , d 2 , TC 4 = {wrote} for a 1 , a 2 and TC 5 = ∅ for p 1 , p 3 , p 4 .</p><p>It is easy to see that the set of non-empty source (or target) cliques is a partition over the data properties of an RDF graph G. Moreover, if a G node n is source of some data properties, they are all in the same source clique; similarly, all the properties of which n is a target are in the same target clique. Based on these cliques, for any nodes n 1 , n 2 of G, we define:</p><formula xml:id="formula_0">• n 1 is weakly equivalent to n 2 , denoted n 1 ≡ W n 2 , iff n 1 , n 2</formula><p>have the same source clique or the same target clique;</p><formula xml:id="formula_1">• n 1 is strongly equivalent to n 2 , denoted n 1 ≡ S n 2 , iff n 1 , n 2</formula><p>have the same source clique and the same target clique.</p><p>Furthermore, we decide that in any RDF equivalence relation, any class node, i.e., a URI c appearing in a triple of the form (n, type, c), and any property node, i.e., a URI p appearing as a subject or object of subproperty triple, is (i) only equivalent to itself and (ii) represented by itself in any RDFQuotient summary. This ensures that RDF types (classes), which (when present) denote an important information that data producers added to help understand their RDF graphs, are preserved in the summary.</p><p>The equivalence relations ≡ W and ≡ S lead to the weak, respectively strong summaries, defined as quotients of G through ≡ W , denoted G ⇑W , respectively, through ≡ S , denoted G ⇑S . Figures <ref type="figure" target="#fig_1">2</ref> and<ref type="figure" target="#fig_2">3</ref> illustrate these on the sample graph in Figure <ref type="figure" target="#fig_0">1</ref>. For brevity, in the figures we use a, w, te, ta, cd to denote respectively the properties advises, writes, teaches, and coursedescr.</p><p>A different flavor of RDFQuotient summaries can be defined to take into account in the first place the types attached to different nodes. We term type-then-data RDFQuotient summary, a summary that first groups the nodes that have the same set of types, and then applies an equivalence relation, such as weak or strong, to classify the remaining, untyped nodes. Formally:</p><p>• n 1 is typed weakly equivalent to n 2 , denoted n 1 ≡ TW n 2 , iff n 1 , n 2 are typed nodes and they have the same set of types or n 1 , n 2 are untyped nodes and they are weakly equivalent;</p><p>• n 1 is typed strongly equivalent to n 2 , denoted n 1 ≡ TS n 2 , iff n 1 , n 2 are typed nodes and they have the same set of types or n 1 , n 2 are untyped nodes and they are strongly equivalent.</p><p>Analogously, typed weak and typed strong summaries are defined as quotients through ≡ TW , denoted G ⇑TW , and ≡ TS , denoted G ⇑TS , respectively. Figure <ref type="figure" target="#fig_3">4</ref> shows the typed weak summary of sample graph in Figure <ref type="figure" target="#fig_0">1</ref>. In our example, the G ⇑TW and G ⇑TS summaries coincide.</p></div>
<div><head n="3">PARALLEL ALGORITHMS</head><p>A first idea is to simply partition G among the available nodes (machines), summarize each slice of the graph on its node, and then summarize again the union of these partial summaries to get the summary of G. Unfortunately, this may be incorrect. Assume that G is the graph in Figure <ref type="figure" target="#fig_0">1</ref> that is partitioned on two machines into G 1 and G 2 , as shown with orange and blue regions, respectively. Notice that all teaches triples are in G 2 . Summarizing the two subgraphs separately, (G 1 ) ⇑W has a wrote edge, (G 2 ) ⇑W has a teaches edge. However, the information that wrote and teaches had common sources in G is lost: using only (G 1 ) ⇑W and (G 2 ) ⇑W , one cannot compute G ⇑W . A similar reasoning holds for G ⇑S , G ⇑TW and G ⇑TS . Therefore, we devised new parallel algorithms for building our summaries (Sections 3.1 to 3.3); they all assume a distributed storage and a MapReduce-like framework. Section 3.4 shows how to tailor our algorithms to the <software ContextAttributes="used">Spark</software> framework used in our implementation. We assume the graph holds ⋃︀G⋃︀ triples and we have M machines at our disposal. All the algorithms perform two preprocessing steps: (i) build the sets of class and property nodes of G, which must be preserved by summarization; (ii) dictionary-encode the RDF URIs and literals into integers, to manipulate less voluminous data. (This is quite standard in RDF data management works.)</p></div>
<div><head n="3.1">Parallel computation of the strong summary</head><p>We compute the strong summary through a sequence of parallel processing jobs as follows.</p><p>(S1) We distribute all (data and type) triples of input graph equally among all the machines, e.g. using round robin approach, so that each The data and type triples initially distributed to each machine m 1 , . . . ,m M are kept (persisted) on that machine throughout the computation. All other partial results produced are discarded after they are processed, unless otherwise specified. (S3) In the corresponding Reduce job, for each resource r ∈ G, all the data triples whose subject or object is r are on a same machine m i .</p><formula xml:id="formula_2">m i , 1 ≤ i ≤ M holds at most ⌊︂ ⋃︀ G⋃︀ M }︂ triples. (<label>S2</label></formula><p>For each such r , m i can infer the relationships (same source clique, same target clique) that hold between the data properties of G appearing on incoming and outgoing edges of r .</p><p>Formally, a property relation information (or PRI, in short) between two properties a, b of G states that they are in the same source clique, or that they are in the same target clique. For instance, if m i hosts the blue triples from Figure <ref type="figure" target="#fig_0">1</ref>, where a = teaches and b = takes, the triples p2 teaches c1 and p2 takes c2 lead to a PRI of the form (teaches, takes, source).</p><p>We also emit PRIs for each property with itself, in order to prepare the necessary information so that all cliques are correctly computed in the steps below (even those consisting of a single data property). The PRIs resulting from all the data triples hosted on m i are de-duplicated locally at m i . (S4) Each machine broadcasts its PRIs to all other machines while also keeping its own PRIs. Observe that for k properties having, for instance, the same source,</p><formula xml:id="formula_3">k (k -1) 2</formula><p>PRIs can be produced. However, it suffices to broadcast k -1 among them (the others will be inferred by transitivity in step (S6)). (S5) Based on this broadcast, each machine has the necessary information to compute the source and target cliques of G locally, and actually computes them<ref type="foot" target="#foot_1">2</ref> . At the end of this stage, the cliques are known and will persist on each machine until the end of the algorithm, but we still need to compute: (i) all the (source clique, target clique) pairs which actually occur in G nodes, and (ii) the representation function and (iii) the summary edges. (S6) The representation function can now be locally computed on each machine as follows:</p><p>• For a given pair of source and target cliques (SC,TC), let N T C SC be an URI uniquely determined by SC and TC, such that a different URI is assigned to each distinct clique pairs: N T C SC will be the URI of the G ⇑S node corresponding these source and target cliques. by Nr such that the type triple r type c is on m i , the machine outputs the summary triple Nr type c. The above process may generate the same summary triple more than once (and at most M times). Thus, a final duplicateelimination step may be needed.</p><p>Algorithm correctness. The following observations ensure the correctness of the above algorithm's stages.</p><p>• Steps (S1) to (S4) ensure that each machine has the complete information concerning data properties being sourceor target-related. Thus, each machine correctly computes the source and target cliques. • Step (S3) ensures that each machine can correctly identify the source and target clique of the resources r which end up on that machine. • The split of the triples in Step (S1) and the broadcast of source and target clique ensure that the last steps (computation of representation function and of the summary triples) yield the expected results.</p></div>
<div><head n="3.2">Parallel computation of the weak summary</head><p>The algorithm for weak summarization starts with the steps (S1) -(S3) as above, then continues as shown below. In a nutshell, this algorithm exploits the observation that by definition of the weak summary, each data property occurs only once.</p><p>(W4) Instead of PRIs, the machines emit Unification Decisions. A unification decision between two data properties a, b, is of one of the following forms: (i) a,b have the same source node in G ⇑W ; (ii) a,b have the same target node in G ⇑W ; (iii) the source of a is the same as the target of b. For instance, in the blue region of Figure <ref type="figure" target="#fig_0">1</ref>, two triples p2 takes c2, c2 coursedescr d1 lead to the UD "the target of takes is the same as the source of coursedescr "; similarly, p4 teaches c2, p4 advises p5 lead to the UD "the source of teaches is the same as the source of advises" etc. In the above, just like for the PRIs, a and b can be the same or they can be different. (W5) Each machine broadcasts its unique set of UDs while also keeping its own. The number of UDs is bound by the number of properties pairs in G. Not all of the combinations are sent; a transitive closure is applied in step (W6).</p><p>(W6) Each machine has the necessary information to compute the nodes and edges of G ⇑W as follows:</p><p>• Assume that for two sets IP, OP of incoming, respectively, outgoing data properties, we are able to compute a unique URI W O P I P , which is different for each distinct pair of sets. • Build G ⇑W with an edge for each distinct data property p in G; the source of this edge for property is W {p} ∅ , while its target is W ∅ {p} . All edges are initially disconnected, that is, there are initially 2 × P nodes in G ⇑W .</p><p>• Apply each UD on the graph thus obtained, gradually fusing the nodes which are the source(s) and target(s) of the various data properties. This entails replacing each W node with one reflecting all its incoming and outgoing data properties known so far. At the end of this process, each node has G ⇑W . We still need to compute the representation function. (W7) On each machine holding a triple r 1 p r 2 , we identify the W nodes Wp , W p in G ⇑W which contain p in their outgoing, respectively, incoming property set. We output the G ⇑W triple Wp p W p . (W8) The type summary triples are built exactly as in step (S7b).</p></div>
<div><head n="3.3">Parallel computation of the typed strong and typed weak summaries</head><p>We now present the changes needed by the above algorithms to compute the typed counterparts of weak and strong summaries. The changes are needed in order to reflect the different treatment of the type triples. In particular, we introduce a new constant token type to be sent in step (S2). We emit pairs corresponding to type triples only in the forward direction, e.g., we send (s, (type, p, o)) but not (o, (type, p, s)). We do not emit pairs with tokens source nor target for type triples, as typed nodes do contribute to property cliques (recall Section 2). The type triples are then cached (kept) at each machine, and not used until the step (S6) in the strong summarization algorithm, respectively (W6) in the weak one.</p><p>To determine the representative of a typed node, each machine that received some type triples groups them by the subject and creates a temporary class set IDs based on the types it knows for each subject. Then, a step similar to (S7a) is needed to disseminate information about each such typed nodes, say n at machine m. Any machine m ′ which has received some triples of whom n is a subject/object, but has no type triples about n, needs to know it is typed, thus omit it from the clique computation.</p></div>
<div><head n="3.4">Apache Spark implementation specifics</head><p>We used <software>Spark</software> 2.3.0 with Hadoop's <software ContextAttributes="used">YARN</software> scheduler 2.9.0. We implemented our algorithms in <software ContextAttributes="used">Scala</software> 2.11.</p><p>A <software>Spark</software> cluster has a set of worker nodes and a driver, which coordinates the run of an application and communicates with cluster manager (e.g. <software ContextAttributes="used">YARN</software> scheduler). A <software ContextAttributes="used">Spark</software> executor is a process working on a piece of data in a worker node. Each <software ContextAttributes="used">Spark</software> application consists of jobs that are divided into stages, each divided into tasks. Basic terminology. <software ContextAttributes="used">Spark</software> relies on Resilient Distributed Datasets (RDDs), which are fault-tolerant and immutable collections of data distributed among the cluster nodes. <software ContextAttributes="used">Spark</software> also supports broadcast variables, i.e., collections of the data that are first gathered at the driver and then are broadcast (copies shipped over network) to all the cluster nodes. These collections are immutable and can only be used for local lookups. Adapting our algorithms to <software ContextAttributes="used">Spark</software>. We adapt our distributed algorithms to <software ContextAttributes="used">Spark</software>'s RDD-based computation model as follows. Each step of an algorithm consumes an RDD and builds another one. First of all, we load the input graph into an RDD called graph. Then, we preprocess it in order to create the RDDs: dictionary, reverseDictionary, nonData-NodesBlacklist and encodedTriples. dictionary maps G nodes to their integer encodings, reverseDictionary is its reverse map and encodedTriples are the iG triples encoded into integers. We collect the class and property nodes in schema-Nodes RDD; this (very small) collection is broadcast to all nodes.</p><p>Then, we create an RDD called nodesGrouped that is a map from G nodes to the set of their incoming and outgoing edges. Next, in the weak algorithm we create unificationDecisions RDD that is a collection of unification decisions, in the strong algorithm we create respectively a propertyRelationInformation RDD. For those two RDDs we exclude (pre-filter) class and property nodes, and in case of typed summaries we exclude typed nodes too.</p><p>The RDDs unificationDecisions and property-RelationInformation are gathered at the driver, which computes the source and target cliques of G. The driver then broadcasts the source and target clique IDs of each property to all the nodes. This allows to create the summary nodes and to create the summary edges. The algorithms that build G ⇑S , G ⇑TW and G ⇑TS also use an RDD called representationFunction, which stores a mapping between the input graph node and its summary representative. This map is filled by each algorithm, and then used on each machine to emit, for each triple of the form n p m that it stores, the corresponding summary triple nr ep p mr ep , where nr ep ,mr ep are the representatives of n and m respectively. The weak summarization algorithm does not need this map; as explained in Section 3.2 (step (W6) and below), it can create the summary triple representing n p m directly based on p.</p><p>Finally, in all the algorithms we need to decode the properties (edge labels). We do it by joining the encoded summary triples with the reverseDictionary in order to replace each encoded property with its full value from G.</p></div>
<div><head n="4">EXPERIMENTAL EVALUATION</head><p>Cluster setup. We are using a cluster of 6 machines, each of which is equipped with an Intel Xeon CPU E5-2640 v4 @2.40GHz and 124GB RAM. Each machine has 20 physical CPU cores. However, we use the cluster with a hyper-threading option enabled, which gives an operating system effectively 40 cores for resource allocation. All machines in this cluster are connected to a switch using 10 Gigabit Ethernet. We give to <software ContextAttributes="used">Spark</software> and <software ContextAttributes="used">YARN</software> 100GB of RAM and 36 cores at each machine. We leave some fraction of the memory (remaining 24GB) and 4 CPU cores for the operating system. RDF graphs. To compare algorithm behavior for different data sizes, we used synthetic benchmark graphs from the BSBM <ref type="bibr" target="#b0">[BS09]</ref> benchmark graph, of 1M, 10M, respectively 100M triples. We found that in these graphs, 61% to 68% of the nodes were untyped, while the others have at least one type. On the one hand, this justifies the need for summaries (such as G ⇑W and G ⇑S ) which do not require all nodes to be typed; on the other hand, there is also a sizable share of typed nodes, which makes the computation of G ⇑TW and G ⇑TW significantly different, on these graphs, than that of G ⇑W and G ⇑S . For the BSBM graphs, as for many other RDF graphs we experimented with and whose summaries are depicted online 1 , the RDFQuotient summaries are quite small: from 179 triples (G ⇑W , BSBM1M) to 3325 triples (G ⇑TS , BSBM100M) 3 . Thus, the information (PRIs and UDs) broadcast by our algorithms is also quite compact. Configuration. We recall here that M denotes the number of machines (<software ContextAttributes="used">Spark</software> workers). <software ContextAttributes="used">Spark</software> parameters relevant for our performance analysis were set as follows:</p><p>• The number of cores per machine C M : we picked C M = 36 (all available cluster resources). We pick all the available resources in order to maximize the performance.</p><p>• The number of cores per executor C E . Following <software>Spark</software> guidelines, unless otherwise specified, we pick C E = 4.</p><p>• The available memory per machine R M : we set it to 100 GB.</p><p>• The amount of memory per executor R E ≥ 2.78 GB. There can be at most C M executors per machine, so they will use at least</p><formula xml:id="formula_4">R M C M , in general R E = R M ⋅C E C M .</formula><p>We set the lower bound for R E as a minimal memory of the <software>YARN</software> container, within the memory limit for executor we need to hold out around 1GB for a memory overhead (memory for JVM in <software ContextAttributes="used">YARN</software>).</p><p>• The number of executors per machine E M , typically 9.</p><p>• The total number of executors E is computed as E M ⋅ M.</p><p>• The number of partitions P = αE. Following existing recommendations 4 , we set α = 4. Speed up through parallelism. We fix the following parameter values M = 5, C E = 4, E M = 9, R E = 11GB, P = 180 and we vary E, the number of executors, by using more or less machines.</p><p>Figure <ref type="figure" target="#fig_6">5</ref> shows the computation time (in minutes) of the parallel algorithms with respect to the number of executors, on a BSBM graph of 10M triples. The time here includes loading, preprocessing, summarization and saving the output file containing the graph summary. We can see that parallelization helps decrease the running time: there is a big gain from 9 to 18 executors, and smaller gains as the parallelism increases (in our setting, benefits basically disappear/amortize going from 36 to 45).</p><p>Figure <ref type="figure" target="#fig_7">6</ref> zooms to show only the summarization time (in seconds) for the same set of computations. We see that the algorithm is overall efficient, i.e., summarization itself is quite fast (seconds as opposed to minutes for the overall computation). We investigated and found large parts of the computation time in Figure <ref type="figure" target="#fig_7">6</ref> are spent: (i) encoding the RDF triples into triples of integers; this is by far the dominant-cost operation. To do this, we need to identify the (duplicate-free) set of node labels from G, operation which we implemented using <software ContextAttributes="used">Spark</software>'s distinct() function, which eliminates 3 Our online summary visualizations 1 also apply two post-processing steps to make summaries even smaller: group nodes by their most general type (à la <ref type="bibr" target="#b12">[GM18]</ref>) and draw leaf nodes as attributes of their parents. This is how the typed strong summary of the BSBM 100M graph is depicted with only 10 edges! 4 http://spark.apache.org/docs/latest/tuning.html#level-of-parallelism, https:// stackoverflow.com/questions/31359219/relationship-between-rdd-partitions-and-nodes  duplicates from an RDD that is (as usual) spread across the nodes. It involves communication between nodes, thus its high latency. We note, however, that our initial implementation, which lacked the encoding and worked directly with URIs and literals from G, was way slower (and encountered memory issues). Thus, we believe the encoding cost is worth paying; (ii) the pre-computation of the RDF class and property nodes from a given graph G. This is also implemented by each machine adding its schema nodes to an RDD and then calling distinct().</p><p>Coming back to Figure <ref type="figure" target="#fig_7">6</ref>, we see that despite the need for a global synchronization step, parallelism (increasing the number of executors) clearly shortens the summarization time. A few data points are counter-intuitive, e.g., using 45 executors takes slightly more than using 36 for weak summarization, or using 36 takes more than using 27 for strong summarization. We believe these points are due to the (random) way in which the data is distributed among the executors, which in turn determines when the broadcast operation (steps (S4) and (W5)) is finished. While this distribution is simply made in round-robin fashion, we find that overall parallelism clearly helps, which can be seen e.g. by comparing the times for the lowest parallelism degrees (9, 18) and the highest (36, 45); this holds even more when put back in the perspective of the overall time (Figure <ref type="figure" target="#fig_6">5</ref>). Scaling in the dataset size. Our next experiment studies the impact of the RDF graph size on the summarization time. We have repeated the above experiments for BSBM graphs of 1M and 100M triples, leading to time measures for 1M, 10M, and 100M triples, that we analyze together to determine how the algorithms scale. Figure <ref type="figure" target="#fig_8">7</ref> shows the total computation time (in minutes) while Figure <ref type="figure" target="#fig_9">8</ref> zooms in just on the time to compute the summary. Note the logarithmic scale of the y axis in both graphs.</p><p>The total time (Figure <ref type="figure" target="#fig_8">7</ref>) which is dominated by the data preprocessing and thus I/O-bound, grows linearly with the data size.  The summarization time alone (Figure <ref type="figure" target="#fig_9">8</ref>) grows almost linearly with the data size for weak, typed weak and typed strong summarization, whereas the growth of the summarization time is super-linear for the strong summary. We have not been able to determine precisely the cause. Recalling also Figure <ref type="figure" target="#fig_7">6</ref>, we believe there is some variability in <software ContextAttributes="created">Spark</software> in-memory execution performance, that we were not able to control precisely. However, considering (also) the fact that summarization itself takes a relatively small part of the total time, we can conclude that overall our parallel algorithms scale up well (basically linearly) with the data size.</p></div>
<div><head n="5">RELATED WORK AND CONCLUSION</head><p>Graph summarization has a long history and many applications; a recent survey dedicated to RDF summarization is [CGK + 18]. Closest to us are existing quotient summaries of RDF graphs, notably those based on bisimulation [MS99, CLO03, KBNK02, CMRV10, SNLPZ13, TLR13], possibly bounded to some distance k; [CDT13, KC15, CFKP15] parallelize the computation of bisimulation summaries. In <ref type="bibr" target="#b1">[CDT13]</ref>, a summary groups RDF nodes in a distinct group: by their type set or by their outgoing properties (these two are not quotients, since a node may belong to multiple groups); or by types and properties. Grouping by the type set (which collapses all untyped nodes) leads to the smallest summaries (most of which still have from 30 to 10 5 edges), but it can lead to loss of information. All the other summaries in <ref type="bibr" target="#b1">[CDT13]</ref> have thousands of edges. Characteristic sets, proposed as support for RDF cardinality estimation <ref type="bibr" target="#b19">[NM11,</ref><ref type="bibr" target="#b13">GN14]</ref>, can also be cast as a form of quotient summary, albeit much less compact than ours. Even though [NM11, GN14, CDT13] ignore incoming properties (useful structural information about the nodes), they lead to summaries impractical for visualization due to the lack of the transitive aspect built into our property cliques (Section 2). Dataguides <ref type="bibr" target="#b14">[GW97]</ref> are non-quotient structural summaries, which may be larger than the original graph and may take exponential time to build. We had demonstrated <ref type="bibr" target="#b7">[ ČGM15b]</ref> and (informally) presented G ⇑W and G ⇑TW in a short "work in progress" paper <ref type="bibr" target="#b6">[ ČGM15a]</ref>, with procedural definitions (not as quotients). <ref type="bibr" target="#b8">[ ČGM17]</ref> discusses the interplay between summarization and RDF graph saturation. In <ref type="bibr" target="#b12">[GM18]</ref>, we define a type-then-data summarization variant based on type generalization; it is orthogonal (and complementary) to this work. Our centralized algorithms for incrementally summarizing RDF graphs has been demonstrated recently <ref type="bibr" target="#b11">[GGM19]</ref>.</p><p>In this work we presented novel parallel algorithms for computing RDFQuotient graph summaries that let the computations scale up to large graphs. We studied the design of the algorithms dedicated to a distributed setting, and we implemented them in the <software>Spark</software> framework. We assessed the performance and showed benefits of the parallel algorithms by experimenting with datasets of size different by orders of magnitude, and by varying the degree of parallelism.</p></div><figure xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Sample RDF graph.</figDesc><graphic coords="3,65.81,83.68,216.22,84.40" type="bitmap" /></figure>
<figure xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Weak summary of the graph in Figure 1.</figDesc><graphic coords="3,101.85,182.41,144.14,53.51" type="bitmap" /></figure>
<figure xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Strong summary of the graph in Figure 1.</figDesc><graphic coords="3,101.85,254.49,144.14,54.63" type="bitmap" /></figure>
<figure xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Typed weak summary of the graph in Figure 1.</figDesc><graphic coords="3,71.82,327.69,204.21,66.74" type="bitmap" /></figure>
<figure xml:id="fig_4"><head /><label /><figDesc>) In a Map job, each machine m i for a given data triple t = s p o emits two pairs: (s, (source, p, o)) and (o, (target, p, s)), where source and target are two constant tokens (labels).</figDesc></figure>
<figure xml:id="fig_5"><head>•</head><label /><figDesc>For each resource r stored on m i , the machine identifies the source clique SCr and target clique TCr of r , and creates (or retrieves, if already created) the URI N T C r SC r of the node representing r in G ⇑S . (S7) Finally, we need to build the edges of G ⇑S . (a) To summarize data triples, for each resource r whose representative Nr is known by m i , and each triple (hosted on m i ) of the form r p o, m i emits (o, (p, Nr )). This triple arrives on the machine m j which hosts o and thus already knows No . The machine outputs the G ⇑S triple Nr p No . (b) To summarize type triples, for each resource r represented</figDesc></figure>
<figure xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Total computation time for various number of executors.</figDesc><graphic coords="6,341.98,221.62,192.20,114.87" type="bitmap" /></figure>
<figure xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Summarization time for various number of executors.</figDesc></figure>
<figure xml:id="fig_8"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Total computation time for datasets of different size.</figDesc><graphic coords="7,77.82,211.21,192.20,107.92" type="bitmap" /></figure>
<figure xml:id="fig_9"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Summarization time for datasets of different size.The summarization time alone (Figure8) grows almost linearly with the data size for weak, typed weak and typed strong summarization, whereas the growth of the summarization time is super-linear for the strong summary. We have not been able to determine precisely the cause. Recalling also Figure6, we believe there is some variability in Spark in-memory execution performance, that we were not able to control precisely. However, considering (also) the fact that summarization itself takes a relatively small part of the total time, we can conclude that overall our parallel algorithms scale up well (basically linearly) with the data size.</figDesc></figure>
			<note place="foot" n="1" xml:id="foot_0"><p>https://project.inria.fr/rdfquotient</p></note>
			<note place="foot" n="2" xml:id="foot_1"><p>In practice: (i) this can be implemented e.g., using Union-Find; (ii) this is redundant as only one of them could have done it and broadcast the result.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The Berlin SPARQL Benchmark</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Bizer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Schultz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Semantic Web Inf. Syst</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Efficiency and precision trade-offs in graph summary algorithms</title>
		<author>
			<persName><forename type="first">Stéphane</forename><surname>Campinas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Renaud</forename><surname>Delbru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giovanni</forename><surname>Tummarello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IDEAS</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">S+EPPs: Construct and explore bisimulation summaries + optimize navigational queries; all on existing SPARQL systems (demonstration)</title>
		<author>
			<persName><forename type="first">Mariano</forename><forename type="middle">P</forename><surname>Consens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Valeria</forename><surname>Fionda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shahan</forename><surname>Khatchadourian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giuseppe</forename><surname>Pirrò</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PVLDB</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">12</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Compact Summaries of Rich Heterogeneous Graphs</title>
		<author>
			<persName><forename type="first">Šejla</forename><surname>Čebirić</surname></persName>
		</author>
		<author>
			<persName><forename type="first">François</forename><surname>Goasdoué</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pawel</forename><surname>Guzewicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ioana</forename><surname>Manolescu</surname></persName>
		</author>
		<ptr target="https://hal.inria.fr/hal-01325900" />
		<imprint>
			<date type="published" when="2018-07">July 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">Inria Research Report</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title />
		<author>
			<persName><surname>Cgk +</surname></persName>
		</author>
		<imprint />
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Summarizing Semantic Graphs: A Survey</title>
		<author>
			<persName><forename type="first">Sejla</forename><surname>Cebiric</surname></persName>
		</author>
		<author>
			<persName><forename type="first">François</forename><surname>Goasdoué</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haridimos</forename><surname>Kondylakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitris</forename><surname>Kotzinos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ioana</forename><surname>Manolescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georgia</forename><surname>Troullinou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mussab</forename><surname>Zneika</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The VLDB Journal</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Query-oriented summarization of RDF graphs</title>
		<author>
			<persName><forename type="first">Šejla</forename><surname>Čebirić</surname></persName>
		</author>
		<author>
			<persName><forename type="first">François</forename><surname>Goasdoué</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ioana</forename><surname>Manolescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BICOD</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Query-oriented summarization of RDF graphs (demonstration)</title>
		<author>
			<persName><forename type="first">Šejla</forename><surname>Čebirić</surname></persName>
		</author>
		<author>
			<persName><forename type="first">François</forename><surname>Goasdoué</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ioana</forename><surname>Manolescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PVLDB</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">12</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A framework for efficient representative summarization of RDF graphs</title>
		<author>
			<persName><forename type="first">Šejla</forename><surname>Čebirić</surname></persName>
		</author>
		<author>
			<persName><forename type="first">François</forename><surname>Goasdoué</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ioana</forename><surname>Manolescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISWC (poster)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">D(K )-index: An adaptive structural summary for graph-structured data</title>
		<author>
			<persName><forename type="first">Qun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kian</forename><forename type="middle">Win</forename><surname>Ong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Exploring XML web collections with DescribeX</title>
		<author>
			<persName><forename type="first">Mariano</forename><forename type="middle">P</forename><surname>Consens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Renée</forename><forename type="middle">J</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Flavio</forename><surname>Rizzolo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alejandro</forename><forename type="middle">A</forename><surname>Vaisman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TWEB</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Incremental structural summarization of RDF graphs (demo)</title>
		<author>
			<persName><forename type="first">François</forename><surname>Goasdoué</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pawel</forename><surname>Guzewicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ioana</forename><surname>Manolescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EDBT 2019</title>
		<meeting><address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-03">March 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Quotient RDF Summaries Based on Type Hierarchies</title>
		<author>
			<persName><forename type="first">Pawel</forename><surname>Guzewicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ioana</forename><surname>Manolescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">DESWeb (Data Engineering meets the Semantic Web) Workshop</title>
		<meeting><address><addrLine>Paris, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-04">April 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Exploiting the query structure for efficient join ordering in SPARQL queries</title>
		<author>
			<persName><forename type="first">Andrey</forename><surname>Gubichev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Neumann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EDBT</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Dataguides: Enabling query formulation and optimization in semistructured databases</title>
		<author>
			<persName><forename type="first">Roy</forename><surname>Goldman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennifer</forename><surname>Widom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VLDB</title>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Computing simulations on finite and infinite graphs</title>
		<author>
			<persName><forename type="first">Monika</forename><forename type="middle">Rauch</forename><surname>Henzinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">A</forename><surname>Henzinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">W</forename><surname>Kopke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FOCS</title>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Covering indexes for branching path queries</title>
		<author>
			<persName><forename type="first">Raghav</forename><surname>Kaushik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Bohannon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><forename type="middle">F</forename><surname>Naughton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henry</forename><forename type="middle">F</forename><surname>Korth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Constructing bisimulation summaries on a multi-core graph processing framework</title>
		<author>
			<persName><forename type="first">Shahan</forename><surname>Khatchadourian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mariano</forename><forename type="middle">P</forename><surname>Consens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">GRADES</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Index structures for path expressions</title>
		<author>
			<persName><forename type="first">Tova</forename><surname>Milo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Suciu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDT</title>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Characteristic sets: Accurate cardinality estimation for RDF queries with multiple joins</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guido</forename><surname>Moerkotte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDE</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Large-scale bisimulation of RDF graphs</title>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Schätzle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antony</forename><surname>Neu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georg</forename><surname>Lausen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Przyjaciel-Zablocki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SWIM</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Managing structured and semistructured RDF data using structure indexes</title>
		<author>
			<persName><forename type="first">Thanh</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Günter</forename><surname>Ladwig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Rudolph</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TKDE</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">9</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>