<TEI xmlns="http://www.tei-c.org/ns/1.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xml:space="preserve" xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Capturing Provenance to Improve the Model Training of PINNs: first handon experiences with Grid5000</title>
				<funder>
					<orgName type="full">CNPq</orgName>
				</funder>
				<funder ref="#_GFhfyb8">
					<orgName type="full">FAPERJ (Center of Excellence in Digital Transformation and Artificial Intelligence of Rio de Janeiro State</orgName>
				</funder>
				<funder ref="#_rerXnSE">
					<orgName type="full">Coordenac ¸ão de Aperfeic ¸oamento de Pessoal de Nível Superior -Brasil (CAPES) -Finance</orgName>
				</funder>
				<funder>
					<orgName type="full">Inria</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher />
				<availability status="unknown"><licence /></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Rômulo</forename><forename type="middle">M</forename><surname>Silva</surname></persName>
							<email>romulo.silva@coc.ufrj.br</email>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Civil Engineering</orgName>
								<orgName type="institution">Federal University of Rio de Janeiro Av. Athos</orgName>
								<address>
									<addrLine>da Silveira Ramos, 149, Room B101, Rio de Janeiro</addrLine>
									<postCode>1941-909</postCode>
									<region>CT, RJ</region>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Débora</forename><surname>Pina</surname></persName>
							<email>dbpina@cos.ufrj.br</email>
							<affiliation key="aff1">
								<orgName type="department">Dept. of Computer and Systems Engineering</orgName>
								<orgName type="institution">Federal University of Rio de Janeiro</orgName>
								<address>
									<addrLine>Avenida Horácio Macedo 2030, Room H319, Rio de Janeiro</addrLine>
									<postCode>21941-914</postCode>
									<region>CT, RJ</region>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Liliane</forename><surname>Kunstmann</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Dept. of Computer and Systems Engineering</orgName>
								<orgName type="institution">Federal University of Rio de Janeiro</orgName>
								<address>
									<addrLine>Avenida Horácio Macedo 2030, Room H319, Rio de Janeiro</addrLine>
									<postCode>21941-914</postCode>
									<region>CT, RJ</region>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Daniel</forename><surname>De Oliveira</surname></persName>
							<email>danielcmo@ic.uff.br</email>
							<affiliation key="aff2">
								<orgName type="department">Institute of Computing</orgName>
								<orgName type="institution">Fluminense Federal University Rua Passo da Pátria</orgName>
								<address>
									<postCode>156</postCode>
									<settlement>Niterói</settlement>
									<region>RJ</region>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Patrick</forename><surname>Valduriez</surname></persName>
							<email>patrick.valduriez@inria.fr</email>
							<affiliation key="aff3">
								<orgName type="institution" key="instit1">Inria</orgName>
								<orgName type="institution" key="instit2">University of Montpellier</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<orgName type="institution" key="instit4">LIRMM Campus Saint</orgName>
								<address>
									<addrLine>Priest -Bâtiment 5, 860 Rue de St Priest</addrLine>
									<postCode>34095, Cedex 5</postCode>
									<settlement>Montpellier</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Alvaro</forename><forename type="middle">L G A</forename><surname>Coutinho</surname></persName>
							<email>alvaro@coc.ufrj.br</email>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Civil Engineering</orgName>
								<orgName type="institution">Federal University of Rio de Janeiro Av. Athos</orgName>
								<address>
									<addrLine>da Silveira Ramos, 149, Room B101, Rio de Janeiro</addrLine>
									<postCode>1941-909</postCode>
									<region>CT, RJ</region>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Marta</forename><surname>Mattoso</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Dept. of Computer and Systems Engineering</orgName>
								<orgName type="institution">Federal University of Rio de Janeiro</orgName>
								<address>
									<addrLine>Avenida Horácio Macedo 2030, Room H319, Rio de Janeiro</addrLine>
									<postCode>21941-914</postCode>
									<region>CT, RJ</region>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Capturing Provenance to Improve the Model Training of PINNs: first handon experiences with Grid5000</title>
					</analytic>
					<monogr>
						<imprint>
							<date />
						</imprint>
					</monogr>
					<idno type="MD5">BD9F1411EC0F4A5B8A9989FE4A6CCA96</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-04-12T14:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid" />
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Physics-informed Neural Networks</term>
					<term>Eikonal Equation</term>
					<term>Provenance</term>
					<term>Hybrid Computing</term>
				</keywords>
			</textClass>
			<abstract>
<div><p>The growing popularity of Neural Networks in computational science and engineering raises several challenges in configuring training parameters and validating the models. Machine learning has been used to approximate costly computational problems in computational mechanics, discover equations by coefficient estimation, and build surrogates. Those applications are outside of the common usage of neural networks. They require a different set of techniques generally encompassed by Physics-Informed Neural Networks (PINNs), which appear to be a good alternative for solving forward and inverse problems governed by PDEs in a small data regime, especially when it comes to Uncertainty Quantification. PINNs have been successfully applied for solving problems in fluid dynamics, inference of hydraulic conductivity, velocity inversion, phase separation, and many others. Nevertheless, we still need to investigate its computational aspects, especially its scalability, when running in large-scale systems. Several hyperparameter configurations have to be evaluated to reach a trained model, often requiring fine-tuning hyperparameters, despite the existence of a few setting recommendations. In PINNs, this fine-tuning requires analyzing configurations and how they relate to the loss function evaluation. We propose provenance data capture and analysis techniques to improve the model training of PINNs. We also report our first experiences on running PINNs in <software>Grid5000</software> using hybrid CPU-GPU computing.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div><head n="1">Introduction</head><p>Hey et al. <ref type="bibr" target="#b0">[1]</ref> draw attention to the growing popularity of Neural Networks (NNs) in Computational Science and Engineering (CSE). In this area, machine learning (ML) has been used to approximate costly computational problems, discover equations by coefficient estimation, and for building surrogates <ref type="bibr" target="#b1">[2]</ref>. Those are outside the common usage of NNs, requiring a different set of techniques. Moreover, the standard rules for hyperparameter tuning usually do not apply to such problems <ref type="bibr" target="#b2">[3]</ref>. An emerging framework for NNs in CSE is the Physics-Informed Neural Networks (PINNs). PINNs are Deep Neural Networks (DNN) that aim to solve forward and inverse problems characterized by Partial Differential Equations (PDEs) by inserting the governing equations of the underlying physics in the loss function.</p><p>One of the challenges when working with PINNs is to reduce the training cost. For this scenario, it is essential to verify the evolution of the loss function over the epochs in many different situations. Moreover, it is necessary to associate the loss metrics to the hyperparameters' configurations, for instance, learning rate, batch size, number of epochs, activation function, and optimizer. Improving the selection of these parameters is essential due to the time it takes to train a PINN, the search space of hyperparameters' combinations, and the difficulty of tuning them, even with auto-tuning tools <ref type="bibr" target="#b3">[4]</ref>. Another challenge is to make explicit the metrics that guided the model's choice towards explainability.</p><p>Provenance has been added to ML to assist hyperparameter analysis in environments like <software ContextAttributes="created">ModelHub</software> <ref type="bibr" target="#b4">[5]</ref>, and <software ContextAttributes="created">ModelKB</software> <ref type="bibr" target="#b5">[6]</ref>. However, these solutions require that the ML algorithm runs in specific platforms other than those being used in PINNs. ML frameworks like <software ContextAttributes="created">TensorFlow</software> and <software ContextAttributes="created">Keras</software> are prevalent, and they can support PINNs, but they have limited provenance data, which are stored in log files. Logs are difficult to query, limiting their ability for hyperparameter analysis at runtime. In addition, PINNs have specific loss function parameters, important for the fine-tuning, not captured in current solutions. This paper presents <software ContextAttributes="created">DNNProv</software> as an alternative to generating provenance in PINNs. <software ContextAttributes="created">DNNProv</software> captures provenance from NN coded in environments like <software ContextAttributes="created">TensorFlow</software> and <software ContextAttributes="created">Keras</software> with low overhead <ref type="bibr" target="#b6">[7]</ref>. Provenance in <software ContextAttributes="created">DNNProv</software> captures the metrics associated with the hyperparameters along the epochs during the training execution. Provenance captured is sent asynchronously to be ingested in a database ready for online queries. Initial experiments with <software ContextAttributes="created">DNNProv</software> using a PINN for the Factored Eikonal Equation in the <software ContextAttributes="created">Grid5000</software> using hybrid computing (CPU-GPU) provide evidence of the flexibility, the efficiency of data capture, and data analysis.</p><p>The remainder of this paper is structured as follows. Section 2 details the PINN scheme for solving an inverse problem governed by the Factored Eikonal Equation, Section 3 presents provenance in PINNs. Section 4 details the case study, and the experimental evaluation. Section 5 concludes.</p><p>2 PINNs for the Factored Eikonal Equation <ref type="formula" target="#formula_1">2</ref>.</p></div>
<div><head n="1">PINNs in a Nutshell</head><p>Consider the problem presented in Eqs. ( <ref type="formula" target="#formula_0">1</ref>)-( <ref type="formula" target="#formula_2">3</ref>) whose solution is given by u(x) with x ∈ R N and u ∈ R S with its physical parameters λ <ref type="bibr" target="#b7">[8]</ref>,</p><formula xml:id="formula_0">f x; u; ∂u ∂x 1 , ..., ∂u ∂x N ; ∂u 2 ∂x 1 x 1 , ..., ∂u 2 ∂x 1 x N ; λ = 0, ∀x ∈ Ω, Ω ⊂ R N ,<label>(1)</label></formula><formula xml:id="formula_1">u(x) = g(x), ∀x ∈ Γ g , Γ g ⊂ R N -1 ,<label>(2)</label></formula><formula xml:id="formula_2">h x; ∂u ∂x 1 , ..., ∂u ∂x N ; λ = 0, ∀x ∈ Γ h , Γ h ⊂ R N -1<label>(3)</label></formula><p>in which Eq. ( <ref type="formula" target="#formula_0">1</ref>) is the governing PDE, Eq. ( <ref type="formula" target="#formula_1">2</ref>) represents the Dirichlet boundary conditions, while Eq. ( <ref type="formula" target="#formula_2">3</ref>) encapsulates both, Neumann and Robin boundary conditions. Furthermore, Ω denotes the internal domain, Γ h and Γ g denote the natural and essential boundaries, respectively. In general, PDEs are difficult to solve and require approximation to be solved numerically. One can try to approximate the differential operator, but it is also possible to approximate the function space of the solution u. The solution can be approximated using bases composed of monomial functions, piece-wise linear functions, or even by the space of the Neural Networks H N N .</p><p>If we choose to approximate the solution u(x) by û(x; θ) ∈ H N N in Eqs. ( <ref type="formula" target="#formula_0">1</ref>)-(3) we obtain</p><formula xml:id="formula_3">f x; ∂û(x; θ) ∂x 1 , ..., ∂û(x; θ) ∂x N ; ∂û 2 (x; θ) ∂x 1 x 1 , ..., ∂û 2 (x; θ) ∂x 1 x N ; λ = R(x; θ; λ)<label>(4)</label></formula><p>where R(x; θ; λ) is called the residual of the PDE and θ are the parameters of the neural network. For solving Eq. (1) using û(x; θ) as an approximation for its solution u(x), one should minimize the residual (Eq. ( <ref type="formula" target="#formula_3">4</ref>)) at a set of residual points T R = {x 1 , x 2 , ..., x R } with T R ⊂ Ω and also satisfy the boundary conditions with a acceptable accuracy at a set of points T B ⊂ Γ. To measure how good is the approximation of the residual of the PDE we write,</p><formula xml:id="formula_4">L R (θ; T R ) = M SE{R(x; θ; λ), 0},<label>(5)</label></formula><p>and for measuring the quality of the approximation of the boundary conditions</p><formula xml:id="formula_5">L BC (θ; T BC ) = M SE h x; ∂u ∂x 1 , ..., ∂u ∂x N ; λ , 0 + M SE{û(x; θ), g(x)},<label>(6)</label></formula><p>where M SE{y true (x), y pred (x)} denotes the Mean Squared Error between the vectors y true (x) and y pred (x).</p><p>In addition, if one want to solve an inverse problem it may be required to incorporate the data through</p><formula xml:id="formula_6">L D (θ; T D ) = M SE{û pred (x), ûdata (x)},<label>(7)</label></formula><p>where T D is the set of points from which we can take measurements from our system, ûdata denotes the measurements and ûpred denotes the predictions at x ∈ T D .</p><p>Notice that, Equation ( <ref type="formula" target="#formula_5">6</ref>) involves not only the information about the boundary conditions, but also the initial conditions for time-dependent problems since the components of x denotes not only the spatial dimensions but also time. With these measures in hand, we can now write the final loss function for informing the physical laws to a neural network during its training phase,</p><formula xml:id="formula_7">L(θ; T ) = w R L R (θ; T R ) + w BC L BC (θ; T BC ) + w D L D (θ; T D )<label>(8)</label></formula><p>where w R , w BC and w D are the weights of the loss function, which play an essential role in the minimization of the loss function.</p><p>Finally, for the solution of inverse problems, one may want to estimate the parameters λ as a set of physical constants or even in the form of a function λ(x; θ λ ), which is easily achievable and implemented thanks to computing libraries such as <software>TensorFlow</software> and <software ContextAttributes="created">PyTorch</software>.</p></div>
<div><head n="2.2">Factored Eikonal Equation</head><p>The Eikonal equation is a first-order nonlinear equation relevant in many fields. It describes phenomena like wave propagation for acoustic and elastic media, as well as electromagnetic media <ref type="bibr" target="#b8">[9]</ref>. Therefore, the Eikonal equation plays an important role in problems like optics, shortest path problems, image segmentation, and seismic and medical imaging. The Eikonal equation is given by,</p><formula xml:id="formula_8">∥∇ϕ(x)∥ 2 = 1 v(x) , ∀x ∈ Ω. (<label>9</label></formula><formula xml:id="formula_9">)</formula><p>where Ω ⊂ R n sd , is the domain, n sd = 1, 2, 3 is the number of space dimensions, x = {x, y, z} is the position vector, and ∥.∥ stands for the L 2 -norm. Solving this equation for ϕ(x) for a given velocity field, v(x), and proper boundary conditions become feasible employing numerical methods as, for example, the Fast Marching Method <ref type="bibr" target="#b9">[10]</ref>, and the Fast Sweeping Method <ref type="bibr" target="#b10">[11]</ref>. However, when point singularities are present, as, in seismic imaging, the Factored Eikonal Equation (FEE for short) is preferable, as pointed out in <ref type="bibr" target="#b11">[12]</ref>. In this case, the Eikonal equation solution can be factored as ϕ(x s , x r ) = τ (x s , x r )ξ(x s , x r ), where x s is the source position, x r is the receiver position, which can be any point that lies in the domain Ω. For the particular scenario where a point source is applied as boundary condition, ξ(x s , x r ) = ∥x s -x r ∥ 2 represents the solution of equation ( <ref type="formula" target="#formula_8">9</ref>) for the case where a point source is applied in a domain such as v(x) = v(x r ) = 1. Substituting the factorization of ϕ in (9), the resulting equation,</p><formula xml:id="formula_10">τ 2 ∥∇ xr ξ∥ 2 2 + 2τ ξ∇ xr τ • ∇ xr ξ + ξ 2 ∥∇ xr τ ∥ 2 2 = 1 v(x r ) 2<label>(10)</label></formula><p>is the FEE. Here, ∇ xr is the gradient operator with respect to the receiver position. This formulation is advantageous in situations where ϕ(x) has point source singularities, which can be well captured by ξ(x s , x r ), while τ (x s , x r ) acts as a smooth correction at the neighborhood of the point source singularities <ref type="bibr" target="#b11">[12]</ref>. Equation ( <ref type="formula" target="#formula_10">10</ref>) is only solved for τ which has as boundary condition τ (x s , x r = x s ) = 1 v(xs) . Given the source position x s and the properties of the domain v(x) it is possible to solve Eq. ( <ref type="formula" target="#formula_10">10</ref>) for the whole domain. This is called the forward problem. A way more challenging task is to find the properties of the domain v(x), given a set of scattered measurements of the travel times ϕ(x s , x r ), which is the equivalent of solving the inverse problem. Figure <ref type="figure">1</ref> shows a simple scheme for using PINNs to solve an inverse problem governed by the FEE.</p></div>
<div><head n="3">Provenance for PINNs</head><p>According to the W3C PROV recommendation <ref type="bibr" target="#b12">[13]</ref>, "provenance is information about entities, activities, and people involved in producing a piece of data or thing, which can be used to form assessments about its quality, reliability, or trustworthiness". Provenance represents the data derivation path, associating the data (entities) with the algorithms/programs (activities) that transform these data, in addition to registering the agents (humans, teams) associated with the entities and activities. As in any NN, the PINN data derivation path includes the activities: (i) Training and (ii) Adaptation. Provenance data is automatically captured during the PINN execution and stored in</p><formula xml:id="formula_11">x s x r σ σ σ σ σ σ σ σ τ φ = τ ξ ξ = ∥x r -x s ∥ 2 R(xs, xr) = τ 2 ∥∇x r ξ∥ 2 2 + 2τ ξ∇x r τ • ∇x r ξ + ξ 2 ∥∇x r τ ∥ 2 2 -1 ṽ2 BC(x s , x s ) = τ (x s , x s ) -1 ṽ(xs) D(x s , x r ) = φ(x s , x r ) -ϕ data (x s , x r ) x r σ σ σ σ σ σ ṽ Figure 1</formula><p>. PINN scheme for solving the inverse FEE. The ϕ(x s , x r ) and v(x r ) are approximated by two different neural networks and their approximations are denoted by φ(x s , x r ) and ṽ(x r ). Those approximations are then fed to the loss components related to the data assimilation, boundary and initial conditions, and the PDE residual.</p><p>a database that associates input data to activities, their parameters, and outputs. The input of the Training activity is the training dataset and the hyperparameters are, for example: the name of the optimizer, the learning rate, number of epochs, batch size and activation functions, and the output is a set of metrics that helps in the evaluation of results obtained during training, e.g., the loss function and its components, the elapsed time, and the date and time of the end of the execution of each epoch. The inputs of Adaptation are the dataset generated by Training and parameters for the adaptation. After an adaptation activity, its output can be a new learning rate and epoch values, which are also in the database with the date and time when the adaptation occurred, and the adaptation identification. Automatic provenance capture during the training of a PINN improves the quality and reliability of the model. As the training takes a long time, it is worth having access to the provenance of what has already been executed to decide on adapting hyperparameters. The evaluation of the several hyperparameters requires us to be aware of the relationship between metadata, e.g., the chosen hyperparameter values, performance data, environment configuration, model metrics, etc. Unlike in other NN provenance approaches, in the case of PINNs, specific loss function component values need to be tracked. This evaluation, through provenance data queries during the training activity, can support fine-tuning decisions, complementing auto-tuning solutions <ref type="bibr" target="#b3">[4]</ref>.</p><p><software ContextAttributes="used">DNNProv</software> is a W3C PROV compliant software library that allows for online hyperparameters' analysis through provenance data <ref type="bibr" target="#b6">[7]</ref>. <software ContextAttributes="used">DNNProv</software> captures typical ML hyperparameters with the flexibility to include other data. In this paper, <software ContextAttributes="used">DNNProv</software> captures parameters relevant to fine-tune PINNs, like those in the loss function. The <software ContextAttributes="used">DNNProv</software> services are used in ML-based workflows with different DL frameworks (e.g., Tensorflow, Theano) while being able to share and analyze captured provenance data using the same W3C PROV representation.</p></div>
<div><head n="4">Experiments</head><p>This section presents how provenance data captured with <software ContextAttributes="used">DNNProv</software> help improving the training of PINNs to solve an inverse FEE problem, the crosshole traveltime tomography. Fig. <ref type="figure" target="#fig_0">2</ref> shows that the PINN solution, using the FEE as the forward solver, is closer to the ground truth than the <software ContextAttributes="used">PyGIMli</software> <ref type="bibr" target="#b13">[14]</ref> solution. However, when querying the provenance database during the training, we notice room to improve the PINNs' performance in several different training scenarios. In all experiments, the FEE PINN runs on GPUs, while <software ContextAttributes="used">DNNProv</software> captures provenance data from the same GPU and sends them asynchronously to a database running on a CPU managed by the columnar relational database system <software ContextAttributes="used">MonetDB</software><ref type="foot" target="#foot_0">1</ref> . All the experiments use a CPU-GPU hybrid computing mode in the Inria's <software ContextAttributes="used">Grid5000</software> computer system, a large-scale and flexible testbed for experiment-driven research, with a focus on parallel and distributed computing, including Cloud, HPC, and Big Data and AI <ref type="bibr" target="#b14">[15]</ref>.</p><p>Several training runs were performed with the FEE PINN, with variations in the activation function (ReLU, Tanh, Sine, Sigmoid) and optimizer (Adam, RMSProp). We use provenance to monitor metrics by epoch during the PINN training and steer the training at runtime, inspecting how the evaluation metrics are evolving, including the loss, L, and its components, L BC , L R , and L D . Thus, we can change parameters and start training again if, for instance, the loss value is not meeting a chosen criteria. In this case, these adaptations are also registered with provenance data (time and date when it happened, and parameters used, such as decay rate and decay steps) since they are essential for a posteriori data analysis. We may also want to discover if the training with adaptation at runtime is effective and learn for the next PINN configurations.</p><p>During the online data analysis for current training (optimizer Adam and activation function Tanh), we submit a query such as "What are the elapsed time and loss for training each epoch?", which the result is presented in Table <ref type="table" target="#tab_0">1</ref>. With this query, we can investigate, for example, if any epoch is taking longer than usual. In addition, with the loss value attribute selected along with the epoch identification, we can verify whether this value is improving over the epochs. Moreover, this query helps decision-making, such as tuning the optimizer or the learning rate.</p><p>Table <ref type="table" target="#tab_0">1</ref> shows the current training epochs, time, and parameters. Based on Table <ref type="table" target="#tab_0">1</ref>, we decide on a further evaluation, considering training with two hyperparameter combinations for the PINN, executing in parallel, one with the optimizer Adam and the other with the optimizer RMSProp, both with the activation function Sigmoid. Then, we submit another query to compare the two optimizers: "What is the elapsed time and training loss in the latest epoch for each combination?", with the results shown in Table <ref type="table" target="#tab_1">2</ref>. From this query, we observe that Adam's optimizer loss presents the best results. Therefore, we decide to stop the training with RMSProp. Consequently, the relationship between the provenance data and the hyperparameter's configuration can help us fine-tune the hyperparameters' values during the search for the most satisfactory configuration.</p><p>Additionally, when we train a model, it is often helpful to lower the learning rate as the training progresses. Although decreasing the learning rate is a known technique, the registered values help trace the consequences of changes when analyzing different models. These data are also important when evaluating the impact of different parameters for decreasing the learning rate. We chose to use an exponential decay function and only one value for the decay rate and the decay steps. However, different values for these parameters can be tested. As these adaptations are saved, we can submit a query like "Retrieve the hyperparameters configuration and the lowest learning rate that was reached during the training.". The results of this query are shown in Table <ref type="table" target="#tab_2">3</ref>. In addition to tuning, the provenance database contributes to the reproducibility and explainability of the model <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref>.</p><p><software ContextAttributes="used">DNNProv</software> can also be connected to data visualization tools, such as <software ContextAttributes="used">Kibana</software><ref type="foot" target="#foot_1">2</ref> to create dashboards and other resources to improve the runtime data analysis. Figure <ref type="figure" target="#fig_1">3</ref> shows the training loss of different executions, presenting an overall perspective of each run. From Figure <ref type="figure" target="#fig_1">3</ref>, we observe that the configuration with RMSProp and Sigmoid shows the worst results. Continuing the training until the loss decreases to 10 -3 , we see that it is a good decision to stop earlier the training for this configuration.  Provenance capturing introduces overhead on the PINN execution. We measure this overhead for all the runs shown in Figure <ref type="figure" target="#fig_1">3</ref>. The overhead corresponds to an increase of 4% over the total time, which is considered negligible, given the hyperparameter fine-tuning benefits. Such low overhead is due to CPU-GPU hybrid-computing model, where the provenance management engine runs on the CPU and the PINN on the GPU.</p></div>
<div><head n="5">Conclusions</head><p>In this paper, we propose using provenance capture to improve online data analysis while training PINNs. By automatically capturing provenance data with <software>DNNProv</software>, it is possible to analyze the chosen hyperparameter values related to the training stages and adjust them during the execution to achieve results with more quality. A case study has been conducted with an actual PINN application on <software ContextAttributes="used">Grid5000</software>. Our experiments show how using the <software ContextAttributes="used">DNNProv</software> approach for online provenance query data analysis, and monitoring can support decision-making with very low overhead. As future work, we plan to capture provenance with different PINN architectures and provide more data to assist the training better. Also, we intend to explore GPU-based databases to store the captured data.</p></div><figure xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Crosshole traveltime tomography problem setup. (left): The ground truth velocity model corresponds to a background velocity model with v true (x) = 2.0[km/s] with two inclusions with v true (x) = 4.0[km/s] (top-left) and v true (x) = 1.5[km/s] (bottom-right). (middle): PyGIMLi[14] solution, R 2 = 0.32. (right): PINN solution (present work), R 2 = 0.52.</figDesc><graphic coords="6,269.71,86.65,68.47,94.26" type="bitmap" /></figure>
<figure xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Graphical view of the training loss</figDesc><graphic coords="7,104.40,249.96,384.00,216.00" type="bitmap" /></figure>
<figure type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Query: "What are the elapsed time and loss for each training epoch?"</figDesc><table><row><cell cols="2">Epoch Time (s)</cell><cell>L</cell><cell>L BC</cell><cell>L R</cell><cell>L D</cell></row><row><cell>0</cell><cell>3.157</cell><cell cols="4">1191.801 0.0122093 0.5859 47.64812</cell></row><row><cell>10000</cell><cell>2.194</cell><cell>0.016</cell><cell cols="3">0.0000047 0.0015 0.00058</cell></row><row><cell>20000</cell><cell>2.125</cell><cell>0.011</cell><cell cols="3">0.0000013 0.0004 0.00043</cell></row><row><cell>30000</cell><cell>2.347</cell><cell>0.025</cell><cell cols="3">0.0000018 0.0011 0.00097</cell></row></table></figure>
<figure type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Query: "What is the elapsed time and training loss in the latest epoch for each combination?"</figDesc><table><row><cell>Time (s)</cell><cell>L</cell><cell>L BC</cell><cell>L R</cell><cell>L D</cell><cell cols="2">Optimizer Act Func</cell></row><row><cell>2.193</cell><cell cols="6">0.438 0.0000392 0.0009 0.017 RMSProp Sigmoid</cell></row><row><cell>2.169</cell><cell cols="4">0.013 0.0000004 0.0008 0.0005</cell><cell>Adam</cell><cell>Sigmoid</cell></row></table></figure>
<figure type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Query: "Retrieve the hyperparameters configuration and the lowest learning rate that was reached during the training." LRate stands for the learning rate. Function Decay Rate Decay Steps Initial LRate Lowest LRate</figDesc><table><row><cell>Exponential decay</cell><cell>0.99</cell><cell>1000</cell><cell>0.001</cell><cell>0.00067</cell></row></table></figure>
			<note place="foot" n="1" xml:id="foot_0"><p>https://www.monetdb.org/</p></note>
			<note place="foot" n="2" xml:id="foot_1"><p>https://www.elastic.co/kibana</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements. This work is funded by <rs type="funder">CNPq</rs>, <rs type="funder">FAPERJ (Center of Excellence in Digital Transformation and Artificial Intelligence of Rio de Janeiro State</rs>: <rs type="projectName">Thematic Network in Renewable Energy and Climate Change</rs>), and <rs type="funder">Inria</rs> (HPDaSc associated team). We are indebted to Inria by providing us access to <software ContextAttributes="used">Grid5000</software>. <rs type="person">D. Pina</rs>, <rs type="person">L. Kunstmann</rs>, and <rs type="person">R. M. Silva</rs> are supported by the <rs type="funder">Coordenac ¸ão de Aperfeic ¸oamento de Pessoal de Nível Superior -Brasil (CAPES) -Finance</rs> <rs type="grantNumber">Code 001</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_GFhfyb8">
					<orgName type="project" subtype="full">Thematic Network in Renewable Energy and Climate Change</orgName>
				</org>
				<org type="funding" xml:id="_rerXnSE">
					<idno type="grant-number">Code 001</idno>
				</org>
			</listOrg>
			<div type="annex">
<div><p>Authorship statement. This section is mandatory and should be positioned immediately before the References section. The text should be exactly as follows: The authors hereby confirm that they are the sole liable persons responsible for the authorship of this work, and that all material that has been herein included as part of the present paper is either the property (and authorship) of the authors, or has the permission of the owners to be included here.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Machine learning and big scientific data</title>
		<author>
			<persName><forename type="first">T</forename><surname>Hey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Butler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jackson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Thiyagalingam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Philosophical Transactions of the Royal Society A</title>
		<imprint>
			<biblScope unit="volume">378</biblScope>
			<biblScope unit="page">20190054</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Neural networks as surrogates of nonlinear high-dimensional parameter-to-prediction maps</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Jakeman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Perego</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Severa</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<pubPlace>Albuquerque, NM (United States)</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Sandia National Lab.(SNL-NM)</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep uq: Learning deep neural network surrogate models for high dimensional uncertainty quantification</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">K</forename><surname>Tripathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Bilionis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of computational physics</title>
		<imprint>
			<biblScope unit="volume">375</biblScope>
			<biblScope unit="page" from="565" to="588" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Human-ai collaboration in data science: Exploring data scientists' perceptions of automated ai</title>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Weisz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Geyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Dugan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tausczik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Samulowitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM on Human-Computer Interaction</title>
		<meeting>the ACM on Human-Computer Interaction</meeting>
		<imprint>
			<publisher>CSCW</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Towards unified data and lifecycle management for deep learning</title>
		<author>
			<persName><forename type="first">H</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Deshpande</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 33rd International Conference on Data Engineering (ICDE)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="page" from="571" to="582" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Modelkb: towards automated management of the modeling lifecycle in deep learning</title>
		<author>
			<persName><forename type="first">G</forename><surname>Gharibi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Walunj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th International Workshop on Realizing Artificial Intelligence Synergies in Software Engineering</title>
		<meeting>the 7th International Workshop on Realizing Artificial Intelligence Synergies in Software Engineering</meeting>
		<imprint>
			<publisher>IEEE Press</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="28" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Provenance supporting hyperparameter analysis in deep neural networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Pina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kunstmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Oliveira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Valduriez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mattoso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Provenance and Annotation of Data and Processes</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="20" to="38" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">DeepXDE: A deep learning library for solving differential equations</title>
		<author>
			<persName><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Karniadakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Review</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="208" to="228" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">First-Order Nonlinear Equations and Their Applications</title>
		<author>
			<persName><forename type="first">L</forename><surname>Debnath</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>Birkhäuser</publisher>
			<biblScope unit="page" from="227" to="256" />
			<pubPlace>Boston, Boston</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Level Set Methods and Dynamic Implicit Surfaces</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">F A</forename><surname>Stanley Osher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Applied Mathematical Sciences</title>
		<imprint>
			<biblScope unit="volume">153</biblScope>
			<date type="published" when="2003">2003</date>
			<publisher>Springer-Verlag</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
	<note>1 edition</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A fast sweeping method for eikonal equations</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematics of Computation</title>
		<imprint>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="issue">250</biblScope>
			<biblScope unit="page" from="603" to="628" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Fast sweeping method for the factored eikonal equation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Fomel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computational Physics</title>
		<imprint>
			<biblScope unit="volume">228</biblScope>
			<biblScope unit="issue">17</biblScope>
			<biblScope unit="page" from="6440" to="6455" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Provenance: an introduction to prov</title>
		<author>
			<persName><forename type="first">L</forename><surname>Moreau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Groth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Synthesis Lectures on the Semantic Web: Theory and Technology</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="129" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">pyGIMLi: An open-source library for modelling and inversion in geophysics</title>
		<author>
			<persName><forename type="first">C</forename><surname>Rücker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Günther</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">M</forename><surname>Wagner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers and Geosciences</title>
		<imprint>
			<biblScope unit="volume">109</biblScope>
			<biblScope unit="page" from="106" to="123" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Adding virtualization capabilities to the Grid'5000 testbed</title>
		<author>
			<persName><forename type="first">D</forename><surname>Balouek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Carpen Amarie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Charrier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Desprez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Jeannot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Jeanvoine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lèbre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Margery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Niclausse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Nussbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Pérez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Quesnel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rohr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sarzyniec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Communications in Computer and Information Science</title>
		<editor>
			<persName><forename type="first">I</forename><forename type="middle">I</forename><surname>Ivanov</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Van</forename><forename type="middle">M</forename><surname>Sinderen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Leymann</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Shan</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">367</biblScope>
			<biblScope unit="page" from="3" to="20" />
			<date type="published" when="2013">2013</date>
			<publisher>Springer International Publishing</publisher>
		</imprint>
	</monogr>
	<note>Cloud Computing and Services Science</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>