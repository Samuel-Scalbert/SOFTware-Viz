<TEI xmlns="http://www.tei-c.org/ns/1.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xml:space="preserve" xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd">
	<teiHeader xml:lang="fr">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Localized Random Shapelets</title>
			</titleStmt>
			<publicationStmt>
				<publisher />
				<availability status="unknown"><licence /></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Maël</forename><surname>Guilleme</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Simon</forename><surname>Malinowski</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Univ Rennes</orgName>
								<orgName type="institution" key="instit2">Inria</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<orgName type="institution" key="instit4">IRISA</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Romain</forename><surname>Tavenard</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Univ Rennes</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<orgName type="institution" key="instit3">LETG</orgName>
								<orgName type="institution" key="instit4">IRISA</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xavier</forename><surname>Renard</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">AXA</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mael</forename><surname>Guillemé</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Univ Rennes</orgName>
								<orgName type="institution" key="instit2">Inria</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<orgName type="institution" key="instit4">IRISA</orgName>
							</affiliation>
						</author>
						<author>
							<persName><surname>Energiency</surname></persName>
						</author>
						<title level="a" type="main">Localized Random Shapelets</title>
					</analytic>
					<monogr>
						<imprint>
							<date />
						</imprint>
					</monogr>
					<idno type="MD5">7607A4DB6D73B3EC63C726F6E8311DB3</idno>
					<idno type="DOI">10.1007/978-3-030-39098-3_7</idno>
					<note type="submission">Submitted on 18 May 2020</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-04-12T14:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid" />
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>time series</term>
					<term>machine learning</term>
					<term>shapelets</term>
				</keywords>
			</textClass>
			<abstract>
<div><p>à la diffusion de documents scientifiques de niveau recherche, publiés ou non, émanant des établissements d'enseignement et de recherche français ou étrangers, des laboratoires publics ou privés.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="fr">
		<body>
<div><head n="1">Introduction</head><p>Time series classification has recently gained an increasing attention from researchers, due in particular to its possible application in various domains such as economics, agriculture, and health for instance. There are two main families of methods for that task: some deal with raw time series and use or design dedicated (dis-)similarity measures, while others rely on feature extraction to embed time series in metric spaces in which standard machine learning tools can be considered. The work presented in this paper is part of this latter category. Feature-based methods include works relying on hand-crafted features <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b11">12]</ref> as well as learning-based approaches, among which the shapelet model plays an important role. Shapelets have first been introduced in <ref type="bibr" target="#b18">[19]</ref>. They correspond to subsequences that are able to discriminate classes. The concept of shapelet Fig. <ref type="figure">1</ref>: Comparison between shapelets extracted by the Learning Time-Series Shapelets (LS) algorithm and our Localized Random Shapelets (LRS) approach. This Figure has been generated using <software ContextAttributes="used">tslearn</software> implementation of LS <ref type="bibr" target="#b13">[14]</ref>. transform has then been proposed in <ref type="bibr" target="#b6">[7]</ref>. It consists in transforming time series into a vector whose components represent the similarity between the time series and shapelets that have been selected beforehand (or learned, as in <ref type="bibr" target="#b4">[5]</ref>). After this transformation, time series are embedded in a Euclidean space where classifiers like Multi-Layer Perceptron or Support Vector Machines can be used. Techniques based on the shapelet transform are amongst the most accurate ones for time series classification (an interesting survey and comparison of time series classification methods can be found in <ref type="bibr" target="#b0">[1]</ref>). However, they have three main drawbacks. First, the step of generating (or learning) shapelets that lead to accurate classification is computationally demanding (especially when dealing with large time series datasets). Some works have been proposed in order to fasten this step, by relying on time series approximation <ref type="bibr" target="#b7">[8]</ref> or by drawing random shapelets <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b9">10]</ref>. Second, no information about the localization of the shapelets in the time series is available after the transformation. Classical shapelet transform only makes use of the similarity between a shapelet and a time series. Information about when the shapelet occurs in the time series might be of importance to discriminate classes. This can be related to previous works that have shown that localization of extracted features in time series improve classification accuracy <ref type="bibr" target="#b14">[15]</ref>. To better understand why, let us consider, for example, a sign language sentence recognition task. In this use case, being able to recognize salient patterns (gesture atoms) is key, but localizing them in the sentence is also important in order to understand the meaning of the sentence. Third, most shapelet-based models suffer from a lack of interpretability in the sense that (i) it is difficult to understand the impact of each shapelet on the final classification decision and/or (ii) extracted shapelets might not always be related to original time series. Figure <ref type="figure">1</ref> compares shapelets learned by the Learning Shapelet (LS) algorithm <ref type="bibr" target="#b4">[5]</ref> on the EarthQuakes dataset and those extracted (at random) by our algorithm. Time series are rescaled to lie in the [0, 1] range before learning the shapelets. However, we can see that LS-extracted shapelets have very little in common with the time series and do not even fit in the [0, 1] range. In other words, shapelets cannot be seen as realistic time series patterns. For methods based on the shapelet transform <ref type="bibr" target="#b6">[7]</ref>, shapelets are extracted from original time series. But they then feed an ensemble of classifiers, which makes it difficult to understand relationship between shapelets and class belongings.</p><p>In this paper, we propose a novel shapelet model that tackles these drawbacks. First, in order to reduce the computing cost of selecting the most discriminant shapelets, our model selects shapelets randomly from training data. Hence, extracted shapelets are, by definition, closely related to the time series. Moreover, this random shapelet framework allows us to easily take the shapelet localization information into account. Second, we propose a dedicated feature selection method called Semi-Sparse Group Lasso (SSGL) capable of either ignoring a shapelet, keeping only its distance information or using both the distance and localization information. Third, we show that obtained shapelets are meaningful and the resulting model can be easily analyzed to get insights about which are the important features (for the classification task) in the dataset and for each of these features, what kind of information (presence only or presence and localization) contributed to the decision. Overall, we show that we are able to reach competitive performance w.r.t. Learning Shapelets <ref type="bibr" target="#b4">[5]</ref> (even outperforming this baseline when larger datasets are considered) with a more interpretable model. The rest of this paper is organized as follows. Our Localized Random Shapelet model is detailed in Section 2 and its interpretability is discussed in Section 3. Section 4 evaluates the benefit of the proposed model on time series classification.</p></div>
<div><head n="2">Localized Random Shapelet Model</head></div>
<div><head n="2.1">Background on shapelets and shapelet transform</head><p>A shapelet S = s 1 , . . . , s l is a temporal sequence (that can be extracted from existing time series or not). Given a time series T = t 1 , . . . , t L , the distance between s and T is defined as :</p><formula xml:id="formula_0">d(T, S) = min 1≤j≤L-l+1 l i=1 (s i -t i+j-1 ) 2 .<label>(1)</label></formula><p>In other words, euclidean distances between s and every subsequence of T (of length l) are computed and only the best match (minimum distance) is kept.</p><p>Given a set S = {S 1 , . . . , S K } of K shapelets, the shapelet transform of T is defined as the vector v 1 , . . . , v K such that v k = d(T, S k ) for all k. The original way to select a set of shapelets for a classification task is to evaluate the discriminatory power of a shapelet candidate by using for instance the information gain <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b6">7]</ref>, and to keep the ones with the higher gain. A strategy based on learning the shapelets that minimize an objective function was proposed in <ref type="bibr" target="#b4">[5]</ref>. Other works consider the use of random shapelets and then rely on classical feature selection algorithms together with the classification step. In <ref type="bibr" target="#b9">[10]</ref>, it has been shown that with this idea, a few thousands subsequences are enough to reach state-of-the-art classification performance on a standard benchmark <ref type="bibr" target="#b1">[2]</ref>.</p></div>
<div><head n="2.2">Localized Random Shapelet model</head><p>In this framework, no information about the localization of the shapelets in the time series is used, while it has been shown that this kind of information helps improving classification performance <ref type="bibr" target="#b14">[15]</ref>. In this section, we explain how we can integrate such information in the shapelet transform framework and derive a feature selection algorithm that keeps localization information only when needed.</p><p>In our Localized Random Shapelet (LRS) model, each shapelet S is drawn uniformly at random from the set of all training time series snippets. Each shapelet leads to two features for each time series T . The first feature is the same as in the classical shapelet transform, i.e. the shapelet distance<ref type="foot" target="#foot_0">1</ref> d(T, S) between T and s as defined in Equation ( <ref type="formula" target="#formula_0">1</ref>). The second feature corresponds to the first time instant at which this distance is reached. It is computed as</p><formula xml:id="formula_1">l(T, S) = argmin 1≤j≤L-l+1 l i=1 (s i -t i+j-1 ) 2 .</formula><p>(</p><formula xml:id="formula_2">)<label>2</label></formula><p>Let T = T 1 , . . . , T N be a dataset of N time series. This set is transformed by the localized random shapelet model into a feature matrix X, such that:</p><formula xml:id="formula_3">X =    d(T 1 , S 1 ) l(T 1 , S 1 ) • • • d(T 1 , S K ) l(T 1 , S K ) . . . . . . . . . . . . d(T N , S 1 ) l(T N , S 1 ) • • • d(T N , S K ) l(T N , S K )    .<label>(3)</label></formula><p>Once this feature matrix computed, we feed it to a standard multi-layer perceptron classifier as shown in Figure <ref type="figure">2</ref>. This model takes as input a 2×K-dimensional vector and outputs probabilities for the different classes at stake. The number of hidden layers can be adapted to application needs. We denote the whole set of parameters of this model by θ, and inside this set of parameters, we isolate the parameters of the first layer in the model and denote them β. </p><formula xml:id="formula_4">β (k) β (k)</formula><p>Fig. <ref type="figure">2</ref>: Overview of our localized random shapelet model. Blue circles indicate distance features while orange ones correspond to location features. For each shapelet, a group is formed whose weights are denoted β (k) (where k is the shapelet index). Note that the number of hidden layers may vary from one application to the other.</p></div>
<div><head n="2.3">Structured feature selection</head><p>As explained above, the proposed shapelet model relies on random shapelets (taken from the original time series) in order to fasten the shapelet generation step. When dealing with such shapelets, a feature selection strategy should be applied before (or jointly with) the classification step, in order to simplify the resulting representation, which tends to improve overall accuracy <ref type="bibr" target="#b8">[9]</ref>. It can be seen in Equation <ref type="formula">(</ref>3) that the extracted features are structured. Distance features tell how well a shapelet matches a time series, while localization features inform about the location of the match. Classical feature selection strategies are hence not adapted to this kind of feature matrix. Indeed, it does not seem reasonable to exclude a distance feature related to a shapelet while keeping the associated localization feature. However, removing a localization feature and keeping the associated distance feature might be meaningful in cases where the localization of a shapelet does not impact the class belonging of the times series. In the following, we design a feature selection strategy adapted to the kind of features extracted from the localized random shapelet model. This strategy, based on regularization, is described below.</p><p>Regularization strategy In the following, we assume that we have a prediction problem with a loss function L to be minimized. This loss function is computed over a dataset X of N observations associated with a target vector y. A standard approach to bias the learning process towards sparse solutions is to derive a regularized loss function, as done for Lasso regression <ref type="bibr" target="#b15">[16]</ref>:</p><formula xml:id="formula_5">L Lasso (X, y, θ) = L(X, y, θ) + λ β 1<label>(4)</label></formula><p>Simon et al. <ref type="bibr" target="#b12">[13]</ref> introduced Sparse-Group Lasso (SGL), a more structured regularization scheme that could take feature group information into account. The resulting regularized loss function for a K-group problem is:</p><formula xml:id="formula_6">L SGL (X, y, θ) = L(X, y, θ) + αλ β 1 + (1 -α)λ K k=1 √ p k β (k) 2 (5)</formula><p>where p k is the number of features in group k and β (k) is the sub-vector of β made of features from group k. Here, the β 1 term enforces per-feature sparsity while β (k)  2 pushes towards sparsity at the group level. The α parameter hence acts as a trade-off between these two regularization terms.</p><p>We consider a slightly different setting in which, inside a group, only part of the features can be dropped. In our case, each group corresponds to a shapelet. For each shapelet, two features are available: one for the distance and one for localization of the match. We aim at designing a strategy that can, for each shapelet (or group), either:</p><p>drop all information related to that shapelet (if the shapelet is useless for prediction), -keep only the distance information (if the location of the shapelet does not help for prediction), -keep both the distance and localization information.</p><p>In order to meet these constraints, we introduce the Semi-Sparse-Group-Lasso (SSGL) framework, which consists in minimizing the following loss function:</p><formula xml:id="formula_7">L SSGL (X, y, θ) = L(X, y, θ) + αλ M ind θ 1 + (1 -α)λ K k=1 √ p k β (k) 2 (6)</formula><p>where M ind is an indicator diagonal matrix made of ones and zeros, the latter corresponding to variables that should be kept as soon as the group is not zeroedout (i.e. variables that will not be considered for 1 regularization). In the case of our localized shapelet model, M ind has a diagonal that alternates between zeros for dimensions corresponding to distances and ones for those related to the localization information. Optimization in practice In practice, minimization of such a regularized loss function can be tackled by several means. First, building on <ref type="bibr" target="#b12">[13]</ref>, one can derive a block-wise procedure that considers feature groups one at a time and starts by deciding on whether it should be zeroed-out or not. In the case of an ordinary least square regression setting, it gives the following sufficient condition for zeroing-out group k (i.e. dropping all the information from the corresponding shapelet):</p><formula xml:id="formula_8">β (1) 1 β (1) 2 β (2) 1 β (2) 2 β (3) 1 β (3) 2 10 -4</formula><formula xml:id="formula_9">M ind S X (k) r -k N , αλ 2 ≤ √ p k (1 -α)λ,<label>(7)</label></formula><p>where X (k) is the submatrix of X in which only variables from group k are kept, r -k is the partial residual of y, substituting all group fits other than group k and S(•, •) is the coordinate-wise soft thresholding operator:</p><formula xml:id="formula_10">(S(z, αλ)) j = sign(z j )(|z j | -αλ) + .<label>(8)</label></formula><p>If this condition is satisfied, all β (k) coefficients are set to zero and the process goes on to the next group. Otherwise, optimization of the coefficients inside group k should be performed, either using subgradient equations in a coordinate-wise algorithm or by performing gradient descent steps inside group k.</p><p>In all the experiments presented in this paper, we rather use full gradient descent on the regularized loss, in order not to face known limitations of blockwise gradient descent algorithms such as slow convergence and low parallelism capabilities. This strategy was already successfully applied in <ref type="bibr" target="#b10">[11]</ref>.</p><p>Toy example: structured linear regression Figure <ref type="figure" target="#fig_1">3</ref> presents a comparison of the three regularization schemes described in this section. For this comparison, we used the following model to draw samples: where X and ε are drawn from centered normal distributions of standard deviation 1 and 0.01 respectively and β = β</p><formula xml:id="formula_11">y = X • β + ε,<label>(9)</label></formula><formula xml:id="formula_12">(1) 1 , β<label>(1) 2 , β (2) 1 , β (2) 2 , β (3) 1 , β (3) 2</label></formula><p>has only three non-zero components:</p><formula xml:id="formula_13">β (2) 1 , β<label>(3) 1 and β (3) 2</label></formula><p>(cf. Figure <ref type="figure" target="#fig_1">3</ref>). We assume to have a problem similar to the shapelet setting, i.e. we have groups of two variables among which only β (k) 2 is concerned by 1 -norm regularization (for group k). This structural information is used for SGL and SSGL variants in the experiments, while Lasso is blind to such structure.</p><p>For this example, we use a simple model without any hidden layer. First, SSGL outperforms both SGL and Lasso in terms of mean squared error (MSE), showing the benefit of taking variable structure into account in the model. Second, from a more qualitative perspective, the structure of the ground truth coefficients is better preserved with SSGL (all three null coefficients have low value estimators and β (1) 2 is better estimated thanks to the intra-group specific regularization scheme).</p></div>
<div><head n="3">Model interpretability</head><p>In this section, we illustrate the interpretability of our method through a simple use-case. We consider the <software ContextAttributes="used">TwoPatterns</software> dataset from the UCR &amp; UEA time series classification repository. <software ContextAttributes="used">TwoPatterns</software> is a synthetic dataset in which each time series is made of two pattern occurrences surrounded by noise. There are two different patterns (named A and B in the following) in the dataset and the classification problem is hence made of four classes, corresponding to all possible permutations of patterns A and B, as shown in Fig. <ref type="figure" target="#fig_2">4</ref>. For this illustration, we rely on a simple variant of our model in which we have no hidden layer and draw K = 2, 000 shapelets to build our feature space. We show that, once trained, our model can be easily analyzed by scrutinizing its weights. More precisely, in the following, we aim at answering the following questions:</p><p>-Which shapelets are the most important for classification? -For each of these shapelets, is the localization information important for the decision ?</p><p>Note that, in this simple setting where no hidden layer is used, this information comes in a straight-forward manner by analyzing weights, but the same analysis could be performed in the multi-layer case through gradient descent in the shapelet transform space.</p><p>First, we can extract most important shapelets (in terms of classification power) by ranking them with respect to the 2 -norm of their associated weight matrix β (k) . Top-4 shapelets for dataset <software ContextAttributes="used">TwoPatterns</software> are presented in Fig. <ref type="figure" target="#fig_3">5</ref>. They fully match expectations since they focus on discriminative parts of the time series and cover both patterns A and B, as well as successions of these.</p><p>Then, for a shapelet that is considered discriminant, we can wonder whether its localization and distance are both used by the model or not. To assess the importance of the distance feature for a shapelet, we compute the 2 -norm of the coefficients of β (k) that are associated with its distance feature. We call this value distance coefficient. Similarly, to assess the importance of the localization feature for a shapelet, we compute the 2 -norm of the coefficients of β (k) that are associated with its localization feature. We call this value localization coefficient.</p><p>The first shapelet of Figure <ref type="figure" target="#fig_3">5</ref> has the 4 th highest localization coefficient amongst the 2,000 shapelets, together with the 14 th highest distance coefficient. This means that for this shapelet both the distance and the localization are important. It is coherent as this shapelet corresponds to a B pattern. The distance feature enables to discriminate class 1 from the others, while the localization features enables to discriminate class 2 and class 3. This is confirmed by the histograms of Figure <ref type="figure" target="#fig_4">6</ref>. The second shapelet of Figure <ref type="figure" target="#fig_3">5</ref> has the 2 nd highest distance coefficient amongst the 2,000 shapelets. However, its localization coefficient is not as important as its distance coefficient (217 th out of 2,000). This means that for this shapelet the distance feature is important but not the localization. This is coherent as this shapelet corresponds to the end of a pattern A followed by the beginning of a pattern B, which only occurs in class 3, hence distance feature is enough to discriminate class 3 from the others, as confirmed by Figure <ref type="figure" target="#fig_4">6</ref>. Similar conclusions can be drawn for shapelets 3 and 4.</p><p>We have seen in this section that our model allows easy extraction of the shapelets that have more contributed to discriminate classes. One can also easily analyze whether the localization of these shapelets was used by the model. This kind of information is very important as it brings a richer interpretability in the decision process. Conversely, state-of-the-art shapelet-based algorithms tend to suffer from lack of interpretability. Shapelet Transform is a weighted ensemble of several standard classifiers, which makes it difficult to assess the importance of shapelets in the decison process. In the Learning Shapelet (LS) framework, obtained shapelets are not constrained to be similar to training time series (cf. Figure <ref type="figure">1</ref>), which limits the interpretability of such framework.</p></div>
<div><head n="4">Experiments</head><p>In this section, we present experimental results to assess the performance of the proposed Localized Random Shapelet (LRS) model and compare it to state-ofthe-art shapelet-based methods.</p></div>
<div><head n="4.1">Experimental setup</head><p>The proposed LRS is compared to the following state-of-the-art baselines: Shapelet Transform (ST) <ref type="bibr" target="#b6">[7]</ref>, the Learning Shapelet (LS) model <ref type="bibr" target="#b4">[5]</ref> and the Fast Shapelet (FS) one <ref type="bibr" target="#b7">[8]</ref>. Experiments are based on the datasets from the UCR &amp; UEA time series classification repository <ref type="bibr" target="#b1">[2]</ref>, a classical benchmark in the time series classification field. It is composed of 85 datasets with diverse time series classification problems. A description of the datasets can be found in <ref type="bibr" target="#b0">[1]</ref>. Classification performance for baseline methods is retrieved from the website associated to this dataset.</p><p>For each dataset, we draw K = 2, 000 shapelets uniformly at random from the time series training set. For the length of the shapelets, we followed the procedure described in <ref type="bibr" target="#b4">[5]</ref>. All time series are represented by a feature vector which size is twice the number of shapelets (cf. Eq. ( <ref type="formula" target="#formula_3">3</ref>)). This feature vector embeds information about shapelet distance and localization.</p><p>We use a 3-hidden-layer model with 256, 128 and 64 units in the hidden layers and each internal layer is followed by a batch normalization layer as suggested in <ref type="bibr" target="#b5">[6]</ref>. We add some dropout in the second and third layer. We use ReLU activations <ref type="bibr" target="#b3">[4]</ref> for all internal layers and softmax activation for the final layer. The regularization in LRS requires two more hyper-parameters: λ that controls the regularization strength, and α that controls the trade-off between lasso and group-lasso terms. We cross-validated the dropout from the set {0.0, 0.3, 0.5, 0.7}, λ from the set {10 -1 , 10 -<ref type="foot" target="#foot_1">2</ref> , . . . 10 -7 } and α from the set {0.1, 0.3, 0.5, 0.7, 0.9}. Models are learned using RMSProp optimizer <ref type="bibr" target="#b16">[17]</ref> with a learning rate of 0.001.</p></div>
<div><head n="4.2">Runtime and memory cost</head><p>Experiments are run on a Laptop with a Fedora 24 system, 16 GB DDR4 RAM and dual core CPU (i7-6600U 2.6 GHz). Python <software>code</software> used for these experiments is publicly available 2 . As an indication of the low computing and memory cost of our method, it requires 66 seconds and 1.5 GB of memory to learn a model (for a given set of hyper-parameters) on the Electric Devices dataset that is the largest in the UCR &amp; UEA archive.</p></div>
<div><head n="4.3">Evaluation of the impact of the SSGL regularization</head><p>Figure <ref type="figure" target="#fig_5">7</ref> shows the error rates on 85 datasets for two different regularization strategies: Lasso <ref type="bibr" target="#b15">[16]</ref> and SSGL. It can be seen that the SSGL regularization strategy (that allows selection for each shapelet of either distance feature, or distance and localization features, or none of these) slightly improves classification performance over Lasso (that makes the selection for each features independently).</p></div>
<div><head n="4.4">Performance comparison against baselines</head><p>Critical diagrams show the average ranks of the classifiers in order, and cliques, represented by a solid bar. A clique represents a group of classifiers within which there is no significant pairwise difference. Figure <ref type="figure" target="#fig_6">8</ref> presents critical diagrams of the performance against the baselines when considering all datasets, and when considering only datasets with more than 300 training instances.  Overall, in terms of classification performance, only Shapelet Transform outperforms our approach. As already stated, ST is a weighted ensemble of several standard classifiers, hence suffering from lack of interpretability about the decision process. The performance of LRS is slightly better than LS (but not When we only consider large datasets (the 42 datasets with more than 300 training instances, which are better suited to the characteristics of our method), performance of LRS is better than LS and FS and gets closer to that of ST. This seems to indicate that taking the localization information into account enables to improve classification performance provided that sufficient training data is available to correctly fit the model.</p></div>
<div><head n="5">Conclusion</head><p>In this paper, we have proposed a novel shapelet model that has low computing cost, competitive performance and better interpretability compared to tradi-tional shapelet-based methods. This model uses shapelet localization in addition to the traditional shapelet transform representation in a random shapelet framework. As such a framework needs many shapelets as input, we have designed a dedicated hierarchical regularization framework (SSGL) to fit our application needs. Experiments show that our model produces a ranking of realistic shapelets, and is able to explain their importance both in terms of distance and localization.This enables an easy and rich interpretability of the classification process. Moreover our classification performance is competitive with state-ofthe-art shapelet-based classifiers.</p></div><figure xml:id="fig_1"><head>Fig. 3 :</head><label>3</label><figDesc>Fig. 3: Coefficients learned using different regularization schemes for a linear regression problem. Ground-truth coefficients are reported in blue.</figDesc></figure>
<figure xml:id="fig_2"><head>Fig. 4 :</head><label>4</label><figDesc>Fig. 4: An example of each class of TwoPatterns dataset.</figDesc></figure>
<figure xml:id="fig_3"><head>Fig. 5 :</head><label>5</label><figDesc>Fig.5: Four most important shapelets (in red) extracted by our method from the TwoPatterns dataset.</figDesc></figure>
<figure xml:id="fig_4"><head>Fig. 6 :</head><label>6</label><figDesc>Fig. 6: The distribution of the localization (left) and distance (right) for the most important (first row) and second most important (second row) shapelet in TwoPatterns dataset.</figDesc></figure>
<figure xml:id="fig_5"><head>Fig. 7 :</head><label>7</label><figDesc>Fig.7: Error rates comparison on 85 UCR datasets between LRS with ssgl regularization against LRS with lasso regularization.</figDesc></figure>
<figure xml:id="fig_6"><head>Fig. 8 :</head><label>8</label><figDesc>Fig. 8: Critical diagrams of the performance against the baselines.</figDesc></figure>
			<note place="foot" n="1" xml:id="foot_0"><p>Note that the term distance is an abuse of notation since d(T, S) is not a distance, mathematically speaking.</p></note>
			<note place="foot" n="2" xml:id="foot_1"><p>https://github.com/rtavenar/localized_random_shapelets</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The great time series classification bake off: a review and experimental evaluation of recent algorithmic advances</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bagnall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lines</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bostrom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Large</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Keogh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Mining and Knowledge Discovery</title>
		<imprint>
			<biblScope unit="page" from="1" to="55" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Bagnall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lines</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Vickers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Keogh</surname></persName>
		</author>
		<ptr target="www.timeseriesclassification" />
		<title level="m">The uea &amp; ucr time series classification repository</title>
		<imprint />
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Dense Bag-of-Temporal-SIFT-Words for Time Series Classification</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bailly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tavenard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chapel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Guyet</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-44412-3_2</idno>
		<ptr target="https://hal.archives-ouvertes.fr/hal-01252726" />
	</analytic>
	<monogr>
		<title level="m">Advanced Analysis and Learning on Temporal Data</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep sparse rectifier neural networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the Fourteenth International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="315" to="323" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning timeseries shapelets</title>
		<author>
			<persName><forename type="first">J</forename><surname>Grabocka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Schilling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wistuba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Schmidt-Thieme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data mining</title>
		<meeting>the ACM SIGKDD International Conference on Knowledge Discovery and Data mining</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="392" to="401" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A shapelet transform for time series classification</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lines</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hills</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bagnall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data mining</title>
		<meeting>the ACM SIGKDD International Conference on Knowledge Discovery and Data mining</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="289" to="297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Fast shapelets: A scalable algorithm for discovering time series shapelets</title>
		<author>
			<persName><forename type="first">T</forename><surname>Rakthanmanon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Keogh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013-05">05 2013</date>
			<biblScope unit="page" from="668" to="676" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Random-shapelet : an algorithm for fast shapelet discovery</title>
		<author>
			<persName><forename type="first">X</forename><surname>Renard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rifqi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Erray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Detyniecki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Data Science and Advanced Analytics</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">EAST Representation: Fast Discriminant Temporal Patterns Discovery From Time Series</title>
		<author>
			<persName><forename type="first">X</forename><surname>Renard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rifqi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Fricout</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Detyniecki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECML/PKDD Workshop on Advanced Analytics and Learning on Temporal Data</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Group sparse regularization for deep neural networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Scardapane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Comminiello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hussain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Uncini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">241</biblScope>
			<biblScope unit="page" from="81" to="89" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The BOSS is concerned with time series classification in the presence of noise</title>
		<author>
			<persName><forename type="first">P</forename><surname>Schäfer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Mining and Knowledge Discovery</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1505" to="1530" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A sparse-group lasso</title>
		<author>
			<persName><forename type="first">N</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computational and Graphical Statistics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="231" to="245" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Tavenard</surname></persName>
		</author>
		<ptr target="https://github.com/rtavenar/tslearn" />
		<title level="m">tslearn: A machine learning toolkit dedicated to time-series data</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Efficient Temporal Kernels between Feature Sets for Time Series Classification</title>
		<author>
			<persName><forename type="first">R</forename><surname>Tavenard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chapel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bailly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sanchez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Bustos</surname></persName>
		</author>
		<ptr target="https://halshs.archives-ouvertes.fr/halshs-01561461" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Machine Learning and Principles and Practice of Knowledge Discovery</title>
		<meeting>the European Conference on Machine Learning and Principles and Practice of Knowledge Discovery</meeting>
		<imprint>
			<date type="published" when="2017-09">Sep 2017</date>
			<biblScope unit="page" from="528" to="543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Regression shrinkage and selection via the lasso</title>
		<author>
			<persName><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society. Series B (Methodological</title>
		<imprint>
			<biblScope unit="page" from="267" to="288" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude</title>
		<author>
			<persName><forename type="first">T</forename><surname>Tieleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">COURSERA: Neural networks for machine learning</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="26" to="31" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Ultra-fast shapelets for time series classification</title>
		<author>
			<persName><forename type="first">M</forename><surname>Wistuba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Grabocka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Schmidt-Thieme</surname></persName>
		</author>
		<idno>CoRR abs/1503.05018</idno>
		<ptr target="http://arxiv.org/abs/1503.05018" />
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Time series shapelets: a new primitive for data mining</title>
		<author>
			<persName><forename type="first">L</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Keogh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data mining</title>
		<meeting>the ACM SIGKDD International Conference on Knowledge Discovery and Data mining</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="947" to="956" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>