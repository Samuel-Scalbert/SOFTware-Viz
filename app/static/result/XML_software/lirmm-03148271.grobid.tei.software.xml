<TEI xmlns="http://www.tei-c.org/ns/1.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xml:space="preserve" xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Parallel Query Processing in a Polystore</title>
				<funder ref="#_PZvRB69">
					<orgName type="full">MINECO/FEDER, UE)</orgName>
				</funder>
				<funder ref="#_zAG7KD8">
					<orgName type="full">European Union</orgName>
				</funder>
				<funder ref="#_nq8DQ8G #_yPkQM2w #_csH59vW">
					<orgName type="full">unknown</orgName>
				</funder>
				<funder ref="#_aqU9Wvx">
					<orgName type="full">Madrid Regional Council, FSE and FEDER</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher />
				<availability status="unknown"><licence /></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Pavlos</forename><surname>Kranas</surname></persName>
							<email>pavlos@leanxcale.com</email>
							<affiliation key="aff0">
								<address>
									<settlement>LeanXcale, Madrid</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Distributed Systems Lab</orgName>
								<orgName type="institution">Universidad Politécnica de Madrid</orgName>
								<address>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Boyan</forename><surname>Kolev</surname></persName>
							<email>bkolev@leanxcale.com</email>
							<affiliation key="aff0">
								<address>
									<settlement>LeanXcale, Madrid</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">LIRMM</orgName>
								<orgName type="institution" key="instit1">Inria</orgName>
								<orgName type="institution" key="instit2">University of Montpellier</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Oleksandra</forename><surname>Levchenko</surname></persName>
							<affiliation key="aff2">
								<orgName type="laboratory">LIRMM</orgName>
								<orgName type="institution" key="instit1">Inria</orgName>
								<orgName type="institution" key="instit2">University of Montpellier</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Esther</forename><surname>Pacitti</surname></persName>
							<affiliation key="aff2">
								<orgName type="laboratory">LIRMM</orgName>
								<orgName type="institution" key="instit1">Inria</orgName>
								<orgName type="institution" key="instit2">University of Montpellier</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">•</forename><surname>Patrick Valduriez</surname></persName>
							<affiliation key="aff2">
								<orgName type="laboratory">LIRMM</orgName>
								<orgName type="institution" key="instit1">Inria</orgName>
								<orgName type="institution" key="instit2">University of Montpellier</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ricardo</forename><surname>Jiménez-Peris</surname></persName>
							<affiliation key="aff0">
								<address>
									<settlement>LeanXcale, Madrid</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Marta</forename><surname>Patiño-Martinez</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Distributed Systems Lab</orgName>
								<orgName type="institution">Universidad Politécnica de Madrid</orgName>
								<address>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Parallel Query Processing in a Polystore</title>
					</analytic>
					<monogr>
						<imprint>
							<date />
						</imprint>
					</monogr>
					<idno type="MD5">5557F8E3BB1D23B5E673E4D177D834E3</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-04-12T14:50+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid" />
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Database integration</term>
					<term>Heterogeneous databases</term>
					<term>Distributed and parallel databases</term>
					<term>Polystores</term>
					<term>Query languages</term>
					<term>Query processing</term>
				</keywords>
			</textClass>
			<abstract>
<div><p>The blooming of different data stores has made polystores a major topic in the cloud and big data landscape. As the amount of data grows rapidly, it becomes critical to exploit the inherent parallel processing capabilities of underlying data stores and data processing platforms. To fully achieve this, a polystore should: (i) preserve the expressivity of each data store's native query or scripting language and (ii) leverage a distributed architecture to enable parallel data integration, i.e. joins, on top of parallel retrieval of underlying partitioned datasets.</p><p>In this paper, we address these points by: (i) using the polyglot approach of the <software>CloudMdsQL</software> query language that allows native queries to be expressed as inline <software ContextAttributes="used">scripts</software> and combined with <software ContextAttributes="used">SQL</software> statements for ad-hoc integration and (ii) incorporating the approach within the <software ContextAttributes="used">LeanXcale</software> distributed query engine, thus allowing for native <software ContextAttributes="used">scripts</software> to be processed in parallel at data store shards. In addition, (iii) efficient optimization techniques, such as bind join, can take place to improve the performance of selective joins. We evaluate the performance benefits of exploiting parallelism in combination with high expressivity and optimization through our experimental validation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div><head n="1">Introduction</head><p>A major trend in cloud computing and data management is the understanding that there is no "one size fits all" solution <ref type="bibr" target="#b33">[34]</ref>. Thus, there has been a blooming of different NoSQL cloud data management infrastructures, distributed file systems (e.g. <software ContextAttributes="used">Hadoop</software> HDFS), and big data processing frameworks (e.g. <software ContextAttributes="used">Hadoop MapReduce</software>, <software ContextAttributes="used">Apache Spark</software>, or <software ContextAttributes="used">Apache Flink</software>), specialized for different kinds of data and tasks and able to scale and perform orders of magnitude better than traditional relational DBMS. This has resulted in a rich offering of services that can be used to build cloud data-intensive applications that can scale and exhibit high performance. However, this has also led to a wide diversification of DBMS interfaces and the loss of a common programming paradigm, which makes it very hard for a user to efficiently integrate and analyze her data sitting in different data stores. For example, let us consider a banking institution that keeps its operational data in a <software ContextAttributes="used">SQL</software> database, but stores data about bank transactions in a document database, because each record typically contains data in just a few fields, so this makes use of the semistructured nature of documents. And because of the big volumes of data, both databases are partitioned into multiple nodes in a cluster. On the other hand, a web application appends data to a big log file, stored in HDFS. In this context, an analytical query that involves datasets from both databases and the HDFS file would face three major challenges. First, in order to execute efficiently, the query needs to be processed in parallel, taking advantage of parallel join algorithms. Second, in order to do this, the query engine must be able to retrieve in parallel the partitions from the underlying data stores and data processing frameworks (such as <software ContextAttributes="used">Spark</software>). And third, the query needs to be expressive enough, so as to combine an <software ContextAttributes="used">SQL</software> subquery (to the relational database) with an arbitrary <software ContextAttributes="used">code</software> in a scripting language (e.g. JavaScript), that requests a dataset from the document database, and another <software ContextAttributes="used">script</software> (e.g. in <software ContextAttributes="used">Python</software> or <software ContextAttributes="used">Scala</software> for <software ContextAttributes="used">Spark</software>), that requests a chain of transformations on the unstructured data from the HDFS log before involving it into relational joins. Existing polystore solutions provide <software ContextAttributes="used">SQL</software> mappings to underlying data objects (document collections, raw files, etc.). However, this leads to limitations of important querying capabilities, as the underlying schema may be very far from relational and data transformations need to take place before being involved in relational operations. Therefore, we address the problem of leveraging the underlying data stores' scripting (querying) mechanisms in combination with parallel data retrieval and joins, as well as optimizability through the use of bind joins.</p><p>A number of polystores that have been recently proposed partially address our problem. In general, they provide integrated access to multiple, heterogeneous data stores through a single query engine. Loosely-coupled polystores <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b32">33]</ref> typically respect the autonomy of the underlying data stores and rely on a mediator/wrapper approach to provide mappings between a common data model with a query language and each particular data store's data model. <software ContextAttributes="used">CloudMdsQL</software> <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b25">26]</ref> with its MFR (map/filter/reduce) extensions <ref type="bibr" target="#b8">[9]</ref> even allows data store native queries to be expressed as inline <software ContextAttributes="used">scripts</software> and combined with regular <software ContextAttributes="used">SQL</software> statements in ad-hoc integration queries. However, even when they access parallel data stores, loosely-coupled polystores typically do centralized access, and thus cannot exploit parallelism for performance.</p><p>Another family of polystore systems <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b38">39]</ref> uses a tightly-coupled approach in order to trade data store autonomy and query expressivity for performance. In particular, much attention is being paid on the integration of unstructured big data (e.g. produced by web applications), typically stored in HDFS, with relational data, e.g., in a (parallel) data warehouse. Thus, tightly-coupled systems take advantage of massive parallelism by bringing in parallel shards from HDFS tables to the <software ContextAttributes="used">SQL</software> database nodes and doing parallel joins. But they are limited to accessing only specific data stores, usually with <software ContextAttributes="used">SQL</software> mappings of the data stores' query interfaces. However, according to a recent benchmarking <ref type="bibr" target="#b24">[25]</ref>, using native queries directly at the data store yields a significant performance improvement compared to mapping native datasets and functions to relational tables and operators. Therefore, what we want to provide is a hybrid system that combines high expressivity (through the use of native queries) with massive parallelism and optimizability.</p><p>In this paper, we present a query engine that addresses the aforementioned challenges of parallel multistore query processing. To preserve the expressivity of the underlying data stores' query/scripting languages, we use the polyglot approach provided by the <software ContextAttributes="used">CloudMdsQL</software> query language, which also enables the use of bind joins <ref type="bibr" target="#b18">[19]</ref> to optimize the execution of selective queries. And to enable the parallel query processing, we incorporated the polyglot approach within the LeanXcale<software>LeanXcale<ref type="foot" target="#foot_0">1</ref> Distributed Query Engine (DQE)</software>, which provides a query engine with intra-query and intra-operator parallelism that operates over a standard <software ContextAttributes="used">SQL</software> interface. We validate the concept with various join queries on four diverse data stores and scripting engines. This paper is a major extension of <ref type="bibr" target="#b23">[24]</ref>. The new material addresses the support of distributed processing platforms such as <software ContextAttributes="used">Apache Spark</software> by enabling the ad-hoc usage of user defined map/filter/reduce (MFR) operators as subqueries, yet allowing for pushing down predicates (e.g. for bind join conditions) and parallel retrieval of intermediate results. We present the major challenges we face in supporting this (Section 3) and introduce extended motivating examples (Section 5.1). We also apply our approach for parallel integration with <software ContextAttributes="used">Spark</software>, together with its architectural and implementation details (Section 5.5). The experimental evaluation has been also extended accordingly, to address an example use case scenario, where unstructured data, stored in HDFS, must go through transformations that require the use of programming techniques like chaining map/reduce operations, which should take place before the data are involved in relational operators. Related work has been extended as well.</p><p>The rest of this paper is organized as follows. Section 2 discusses related work. Section 3 presents the motivation and challenges and states the problem. Section 4 gives an overview of the query language with its polyglot capabilities and discusses the distributed architecture of the <software>LeanXcale</software> query engine. Our major contribution is presented in Section 5, where we describe the architectural extensions that turn the DQE into a parallel polystore system. Section 6 presents the experimental evaluation of various parallel join queries across data stores using combined <software ContextAttributes="used">SQL</software>, MFR, and native queries. Section 7 concludes.</p></div>
<div><head n="2">Related Work</head><p>The problem of accessing heterogeneous data sources has long been studied in the context of multidatabase and data integration systems <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b35">36]</ref>. The typical solution is to provide a common data model and query language to transparently access data sources through a mediator, thus hiding data source heterogeneity and distribution. More recently, with the advent of cloud databases and big data processing frameworks, multidatabase solutions have evolved towards polystore systems that provide integrated access to a number of RDBMS, NoSQL, NewSQL, and HDFS data stores through a common query engine. Polystore systems can be divided between loosely-coupled, tightlycoupled, and hybrid <ref type="bibr" target="#b9">[10]</ref>, which we discuss briefly later in this section. Since looselyand tightly-coupled systems address only partially our problem, we will focus in more detail on hybrid systems as the state-of-the-art category where our work fits in. We also add a fourth category of the recent parallel workflow management systems, which provide polystore support. With respect to combining <software ContextAttributes="used">SQL</software> and map/reduce operators, a number of <software ContextAttributes="used">SQL</software>-like query languages have been introduced. <software ContextAttributes="used">HiveQL</software> is the query language of the data warehousing solution <software ContextAttributes="used">Hive</software>, built on top of <software ContextAttributes="used">Hadoop MapReduce</software> <ref type="bibr" target="#b34">[35]</ref>. <software ContextAttributes="used">Hive</software> gives a relational view of HDFS stored unstructured data. <software ContextAttributes="used">HiveQL</software> queries are decomposed to relational operators, which are then compiled to <software ContextAttributes="used">MapReduce</software> jobs to be executed on <software ContextAttributes="used">Hadoop</software>. In addition, <software ContextAttributes="used">HiveQL</software> allows custom <software ContextAttributes="used">scripts</software>, defining <software ContextAttributes="used">MapReduce</software> jobs, to be referred in queries and used in combination with relational operators. SCOPE <ref type="bibr" target="#b11">[12]</ref> is a declarative language from Microsoft designed to specify the processing of large sequential files stored in <software ContextAttributes="used">Cosmos</software>, a distributed computing platform. SCOPE provides selection, join and aggregation operators and allows the users to implement their own operators and user-defined functions. SCOPE expressions and predicates are translated into C#. In addition, it allows implementing custom extractors, processors and reducers and combining operators for manipulating rowsets. SCOPE has been extended to combine <software ContextAttributes="used">SQL</software> and <software ContextAttributes="used">MapReduce</software> operators in a single language <ref type="bibr" target="#b39">[40]</ref>. These systems are used over a single distributed storage system and therefore do not address the problem of integrating a number of diverse data stores.</p><p>Loosely-coupled polystores are reminiscent of multidatabase systems in that they can deal with autonomous data stores, which can then be accessed through the polystore common interface as well as separately through their local <software ContextAttributes="used">API</software>. Most loosely-coupled systems support only read-only queries. Loosely-coupled polystores follow the mediator/wrapper architecture with several data stores (e.g. NoSQL and RDBMS). <software ContextAttributes="used">BigIntegrator</software> <ref type="bibr" target="#b28">[29]</ref> integrates data from cloud-based NoSQL big data stores, such as Google's <software ContextAttributes="used">Bigtable</software>, and relational databases. The system relies on mapping a limited set of relational operators to native queries expressed in GQL (Google <software ContextAttributes="used">Bigtable</software> query language). With GQL, the task is achievable because it represents a subset of <software ContextAttributes="used">SQL</software>. However, it only works for <software ContextAttributes="used">Bigtable</software>-like systems and cannot integrate data from HDFS. <software ContextAttributes="used">QoX</software> <ref type="bibr" target="#b32">[33]</ref> integrates data from RDBMS and HDFS data stores through an XML common data model. It produces <software ContextAttributes="used">SQL</software> statements for relational data stores, and <software ContextAttributes="used">Pig/Hive code</software> for interfacing <software ContextAttributes="used">Hadoop</software> to access HDFS data. The <software ContextAttributes="used">QoX</software> optimizer uses a dataflow approach for optimizing queries over data stores, with a black box approach for cost modeling. <software ContextAttributes="used">SQL++</software> <ref type="bibr" target="#b29">[30]</ref> mediates <software ContextAttributes="used">SQL</software> and NoSQL data sources through a semi-structured common data model. The data model supports relational operators and to handle efficiently nested data, it also provides a flatten operator. The common query engine translates subqueries to native queries to be executed against data stores with or without schema. All these approaches mediate heterogeneous data stores through a single common data model. The polystore <software ContextAttributes="used">BigDAWG</software> <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17]</ref> goes one step further by defining "islands of information", where each island corresponds to a specific data model and its language and provides transparent access to a subset of the underlying data stores through the island's data model. The system enables cross-island queries (across different data models) by moving intermediate datasets between islands in an optimized way. In addition to typical loosely-coupled systems, some polystore solutions <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b21">22]</ref> consider the problem of optimal data placement and/or selection of data source, mostly driven by application requirements. Estocada <ref type="bibr" target="#b10">[11]</ref> is a self-tuning polystore platform for providing access to datasets in native format while automatically placing fragments of the datasets across heterogeneous stores. For query optimization, Estocada combines both cost-based and rule-based approaches. Estocada has been recently extended with a novel approach for efficient cross-model query processing, using queries as materialized views and performing view-based query rewriting <ref type="bibr" target="#b2">[3]</ref>.</p><p>Tightly-coupled polystores have been introduced with the goal of integrating <software ContextAttributes="used">Hadoop</software> or <software ContextAttributes="used">Spark</software> for big data analysis with traditional (parallel) RDBMSs. Tightly-coupled polystores trade autonomy for performance, typically operating in a shared-nothing cluster, taking advantage of massive parallelism. Odyssey <ref type="bibr" target="#b19">[20]</ref> enables storing and querying data within HDFS and RDBMS, using opportunistic materialized views. MISO <ref type="bibr" target="#b27">[28]</ref> is a method for tuning the physical design of a multistore system (<software ContextAttributes="used">Hive</software>/HDFS and RDBMS), i.e. deciding in which data store the data should reside, in order to improve the performance of big data query processing. The intermediate results of query execution are treated as opportunistic materialized views, which can then be placed in the underlying stores to optimize the evaluation of subsequent queries. JEN <ref type="bibr" target="#b38">[39]</ref> allows joining data from two data stores, HDFS and RDBMS, with parallel join algorithms, in particular, an efficient zigzag join algorithm, and techniques to minimize data movement. As the data size grows, executing the join on the HDFS side appears to be more efficient. Polybase <ref type="bibr" target="#b14">[15]</ref> is a feature of Microsoft <software ContextAttributes="used">SQL Server Parallel</software> Data <software ContextAttributes="used">Warehouse</software> to access HDFS data using <software ContextAttributes="used">SQL</software>. It allows HDFS data to be referenced through external PDW tables and joined with native PDW tables using <software ContextAttributes="used">SQL</software> queries. <software ContextAttributes="used">HadoopDB</software> <ref type="bibr" target="#b0">[1]</ref> provides <software ContextAttributes="used">Hadoop MapReduce</software>/HDFS access to multiple single-node RDBMS servers (e.g. <software ContextAttributes="used">PostgreSQL</software> or <software ContextAttributes="used">MySQL</software>) deployed across a cluster, as in a sharednothing parallel DBMS. It interfaces <software ContextAttributes="used">MapReduce</software> with RDBMS through database connectors that execute <software ContextAttributes="used">SQL</software> queries to return key-value pairs.</p><p>Hybrid polystore systems support data source autonomy as in loosely-coupled systems, and preserve parallelism by exploiting the local data source interface as in tightlycoupled systems. They usually serve as parallel query engines with parallel connectors to external sources. As our work fits in this category, we will briefly discuss some of the existing solutions, focusing on their capabilities to integrate with <software>MongoDB</software> as a representative example of a non-relational data store.</p><p><software ContextAttributes="used">Spark SQL</software> <ref type="bibr" target="#b5">[6]</ref> is a parallel <software ContextAttributes="used">SQL</software> engine built on top of <software ContextAttributes="used">Apache Spark</software> and designed to provide tight integration between relational and procedural processing through a declarative <software ContextAttributes="used">API</software> that integrates relational operators with procedural <software ContextAttributes="used">Spark</software> <software ContextAttributes="used">code</software>, taking advantage of massive parallelism. <software ContextAttributes="used">Spark SQL</software> provides a DataFrame <software ContextAttributes="used">API</software> that can map to relations arbitrary object collections and thus enables relational operations across <software ContextAttributes="used">Spark</software>'s RDDs and external data sources. <software ContextAttributes="used">Spark SQL</software> can access a <software ContextAttributes="used">MongoDB</software> cluster through its <software ContextAttributes="used">MongoDB</software> connector that maps a sharded document collection to a Data-Frame, partitioned as per the collection's sharding setup. Schema can be either inferred by document samples, or explicitly declared.</p><p>Presto <ref type="bibr" target="#b31">[32]</ref> is a distributed <software ContextAttributes="used">SQL</software> query engine, running on a shared-nothing cluster of machines, and designed to process interactive analytic queries against data sources of any size. Presto follows the classical distributed DBMS architecture, which, similarly to <software ContextAttributes="used">LeanXcale</software>, consists of a coordinator, multiple workers and connectors (storage plugins that interface external data stores and provide metadata to the coordinator and data to workers). To access a <software ContextAttributes="used">MongoDB</software> cluster, Presto uses a connector that allows the parallel retrieval of sharded collections, which is typically configured with a list of <software ContextAttributes="used">MongoDB</software> servers. Document collections are exposed as tables to Presto, keeping schema mappings in a special <software ContextAttributes="used">MongoDB</software> collection.</p><p><software ContextAttributes="used">Apache Drill</software> <ref type="bibr" target="#b3">[4]</ref> is a distributed query engine for large-scale datasets, designed to scale to thousands of nodes and query at low latency petabytes of data from various data sources through storage plugins. The architecture runs a so called "drillbit" service at each node. The drillbit that receives the query from a client or application becomes the foreman for the query and compiles the query into an optimized execution plan, further parallelized in a way that maximizes data locality. The <software ContextAttributes="used">MongoDB</software> storage allows running Drill and <software ContextAttributes="used">MongoDB</software> together in distributed mode, by assigning shards to different drillbits to exploit parallelism. Since <software ContextAttributes="used">MongoDB</software> collections are used directly in the FROM clause as tables, the storage plugin translates relational operators to native <software ContextAttributes="used">MongoDB</software> queries.</p><p>Myria <ref type="bibr" target="#b37">[38]</ref> is another recent polystore, built on a shared-nothing parallel architecture, that efficiently federates data across diverse data models and query languages. Its extended relational model and the imperative-declarative hybrid language <software ContextAttributes="used">MyriaL</software> span well all the underlying data models, where rewrite rules apply to transform expressions into specific <software ContextAttributes="used">API</software> calls, queries, etc. for each of the data stores. Non-relational systems, such as <software ContextAttributes="used">MongoDB</software>, are supported by defining relational semantics for their operations and adding rules to translate them properly into the relational algebra, used by Myria's relational algebra compiler RACO.</p><p>Impala <ref type="bibr" target="#b4">[5]</ref> is an open-source distributed <software ContextAttributes="used">SQL</software> engine operating over <software ContextAttributes="used">Hadoop</software> data processing environment. As opposed to typical batch processing frameworks for <software ContextAttributes="used">Hadoop</software>, Impala provides low latency and high concurrency for analytical queries. Impala can access <software ContextAttributes="used">MongoDB</software> collections through a <software ContextAttributes="used">MongoDB</software> connector for <software ContextAttributes="used">Hadoop</software>, designed to provide the ability to read <software ContextAttributes="used">MongoDB</software> data into <software ContextAttributes="used">Hadoop MapReduce</software> jobs.</p><p>Parallel workflow management systems is a recent category of solutions for big data processing that aims to decouple applications from underlying data processing platforms. They typically facilitate applications by choosing the best platform to execute a particular workflow or task from a set of available platforms (e.g. <software ContextAttributes="used">Hadoop</software>, <software ContextAttributes="used">Spark</software>, <software ContextAttributes="used">Giraph</software>, etc.). <software ContextAttributes="used">Musketeer</software> <ref type="bibr" target="#b17">[18]</ref> achieves this through a model that breaks the execution of a data processing workflow in three layers: first, the workflow is specified using a front-end framework of user's choice, e.g. <software ContextAttributes="used">SQL</software>-like query or vertex-centric graphic abstraction; then, the specification is transformed into an internal representation in the form of a data-flow DAG; and finally, <software ContextAttributes="used">code</software> is generated to execute the workflow against the target platform. This saves the tedious work of manually porting workflows across platforms in case some platform is found to be better suited for a particular workflow. More recently, RHEEM <ref type="bibr" target="#b1">[2]</ref> enhanced the concept by allowing a particular subtask of the workflow to be assigned to a specific platform, in order to minimize the overall cost. It also introduces a novel cost-based cross-platform optimizer <ref type="bibr" target="#b26">[27]</ref> that finds the most efficient platform for a task and an executor that orchestrates tasks over different platforms with intermediate data movement. Thus, RHEEM can integrate data from different data stores (hence act as a polystore) by assigning different operators from the query plan to different engines, e.g., perform selections on base tables and associated joins at the RDBMS to exploit indexes, then ship intermediate data and perform other joins at <software ContextAttributes="used">Spark</software> to exploit parallelism. <software ContextAttributes="used">Teradata</software> IntelliSphere <ref type="bibr" target="#b6">[7]</ref> addresses the problem of accessing multiple data stores (called "remote systems") that may be heterogeneous, but must have an <software ContextAttributes="used">SQL</software>-like interface. Each remote system, however, can be a polystore by itself. <software ContextAttributes="used">Teradata</software> is responsible for building an <software ContextAttributes="used">SQL</software> query plan and deciding where each <software ContextAttributes="used">SQL</software> operator (e.g. join or aggregation) will execute on one of the IntelliSphere's systems (either <software ContextAttributes="used">Teradata</software> or a remote system). An important problem the system focuses on is the cost estimation of <software ContextAttributes="used">SQL</software> operators over remote systems. Machine learning techniques are leveraged to train neural networks to approximate the execution time of an operator based on characteristics of its input relation(s).</p><p>Comparative analysis. Table <ref type="table" target="#tab_0">1</ref> summarizes the functionality of polystore systems that enable parallel processing across diverse DBMS clusters (we will use for brevity the term "parallel polystores"). We compare the systems with respect to the features that we hereby address, mainly the parallel support of <software ContextAttributes="used">MongoDB</software>, data processing platforms, and optimizability of selective joins through semi joins across data stores. We exclude loosely-coupled systems, as they do not provide parallel data integration. Workflow managers dispatch the execution of a query/workflow plan to underlying data processing platforms, hence can access <software ContextAttributes="used">MongoDB</software> through the platforms, e.g., <software ContextAttributes="used">Spark</software> using the <software ContextAttributes="used">Spark</software> <software ContextAttributes="used">MongoDB</software> connector. Tightly-coupled systems can perform parallel joins, but since they are focused only on the tight integration between RDBMS and <software ContextAttributes="used">Hadoop</software> stores, cannot be extended to support NoSQL stores. Hybrid systems (besides <software ContextAttributes="used">LeanXcale</software>) usually access document data stores through extended relational mappings, with the added support of flattening operators (UNNEST) to express queries over nested documents. With respect to semi joins across data stores, only a few of the systems are capable. JEN and <software ContextAttributes="used">LeanXcale</software> are applying semi-joins as an optimization technique -JEN with its efficient zigzag join that exchanges bloom filters between the HDFS and RDBMS datasets and <software ContextAttributes="used">LeanXcale</software> through bind joins. With other systems, semi-joins may be supported, but must be explicitly programmed. Concluding remarks. Although these systems enable parallel integration with data clusters (like <software ContextAttributes="used">MongoDB</software>), none of them supports the combination of massive parallelism with native queries and the optimization of bind joins, which is addressed by the <software ContextAttributes="used">LeanXcale</software> distributed query engine. In particular, the parallel query processing with bind joins through <software ContextAttributes="used">SQL</software> queries is not supported by any of the hybrid systems. For example, with <software ContextAttributes="used">Spark</software> <software ContextAttributes="used">SQL</software> it is possible to do a bind join, but this must be defined programmatically by a developer. This, however, limits the use cases, since a data analyst cannot easily take advantage of this feature through an <software ContextAttributes="used">SQL</software> query. With <software ContextAttributes="used">LeanXcale</software>, once the named tables (subqueries to data stores) are defined by the system developer or administrator, they can be easily used and involved in joins (including bind joins) through the <software ContextAttributes="used">SQL</software> interface. Moreover, enabling native queries and <software ContextAttributes="used">scripts</software> allows to fully exploit the power of the underlying data stores, as opposed to using static mappings to a common data model.</p></div>
<div><head n="3">Motivation and Problem Statement</head><p>Existing parallel polystore query engines <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b37">38]</ref> address the problem of accessing in parallel partitions from tables, document collections, or arbitrary distributed datasets, exploiting the parallel capabilities of a diverse set of underlying distributed data stores and performing parallel joins on top. This is typically done by making a number of query engine workers connect independently to data store shards. As an example, elaborated in our previous work <ref type="bibr" target="#b23">[24]</ref>, we addressed the scenario of joining a sharded document collection, residing in a <software ContextAttributes="used">MongoDB</software> cluster, with a partitioned table from a distributed relational database or a distributed HDFS dataset. This setup works well when the underlying datasets can be logically mapped to relations, so that joins can efficiently take place at the parallel polystore query engine. Even if the <software ContextAttributes="used">MongoDB</software> data has to undergo transformations, expressed through user-defined JavaScript functions, this can still be handled in parallel by making each worker initiate the execution of the custom JavaScript <software ContextAttributes="used">code</software> against the <software ContextAttributes="used">MongoDB</software> shard assigned to it and collect its partition of the intermediate data.</p><p>In other cases, complex transformations may need to be applied to a distributed dataset (e.g. through specific map-reduce blocks that have no analogue in relational algebra terms), before the data can be processed by means of relational operators. This problem was addressed in <ref type="bibr" target="#b8">[9]</ref>, by allowing distributed data processing frameworks (in particular <software ContextAttributes="used">Apache Spark</software>) to be accessed as data stores and queried through the semideclarative MFR notation. To achieve this, the query engine creates a session at the <software ContextAttributes="used">Spark</software> driver, then translates the MFR subquery to <software ContextAttributes="used">code</software> (in <software ContextAttributes="used">Scala</software> or <software ContextAttributes="used">Python</software> for <software ContextAttributes="used">Spark</software>), delegates this <software ContextAttributes="used">code</software> to <software ContextAttributes="used">Spark</software> for execution, and collects the intermediate data through the same <software ContextAttributes="used">Spark</software> driver session.</p><p>The limitation. However, the collection of this intermediate result set is centralized, since the <software ContextAttributes="used">Spark</software> driver simply merges the data from all the partitions of the final RDD into a single non-partitioned result set. Thus, even a distributed query engine cannot exploit parallelism in retrieving a <software ContextAttributes="used">Spark</software> RDD, since only one worker will collect the entire RDD through the <software ContextAttributes="used">Spark</software> driver, which is the limitation we want to overcome in this paper as an extended version of <ref type="bibr" target="#b23">[24]</ref>.</p><p>The challenge. This limitation comes from the fact that the query engine, like in most, if not all, parallel polystores, is designed so that each of the involved parallel workers initiates connection to a node of the underlying data management system and pulls its partition of a dataset. However, in the case of <software>Spark</software>, there is no way to directly access the data of an RDD partition. Therefore, the query engine would be forced to use a single worker to retrieve the entire RDD through the <software ContextAttributes="used">Spark</software> driver, serially. We address this problem by introducing an architecture, where each RDD partition (more precisely, the <software ContextAttributes="used">Spark</software> worker that processes the partition) is instructed through generated <software ContextAttributes="used">code</software> to find and connect to a query engine worker and to push the partition data. Doing this in parallel and uniformly across <software ContextAttributes="used">Spark</software> and query engine workers is the major challenge of the current extension of our work.</p><p>Objectives. We can now summarize the objectives of our work as the following requirements to our distributed query engine (DQE):</p><p>• Parallel data processing: DQE parallelizes the execution of every relational operator. • Parallel retrieval from data stores: DQE workers access independently data store shards to retrieve partitioned data in parallel. • Autonomy of data stores: DQE does not rely on full control over the data stores;</p><p>they can be used independently by other applications. • Highly expressive queries: adopt he polyglot approach of the <software>CloudMdsQL</software> query language to allow data store native queries or <software ContextAttributes="used">scripts</software> to be expressed as inline subqueries. • Optimizability: incorporate optimization techniques, such as bind join and MFR rewrite rules (see Section 4.2) to boost the performance of selective queries. • Extensibility: allow for other parallel data stores to be added by implementing adapters through a flexible programming interface (<software ContextAttributes="used">DataLake API</software>, see Section 5.2). • Parallel data push model: data store shards can connect to DQE workers independently to push partitioned data in parallel. This allows for distributed data processing frameworks, such as <software ContextAttributes="used">Spark</software>, to be supported by the DQE.</p><p>While most of these requirements have already been addressed by other systems in the literature, we emphasize on the combination of all of them and in this extended version particularly pay attention to the last one. Comparisons. To choose competitor systems to experimentally compare our solution with, we followed the criteria determined by our evaluation queries, i.e. parallel joins between a sharded document collection and the partitioned result of a workflow query to a distributed data processing platform. In particular, we want to stress on the full parallelism to access the underlying datasets, in our case, a <software ContextAttributes="used">MongoDB</software> collection and a <software ContextAttributes="used">Spark</software> RDD, in the context of expressive subqueries. Considering our comparative analysis (Table <ref type="table" target="#tab_0">1</ref>), we exclude tightly-coupled systems since they do not support document stores. Among hybrid systems, although Presto and Drill support well parallel query processing across <software ContextAttributes="used">SQL</software> and NoSQL stores, the only one that provides parallel support of distributed data platforms is <software ContextAttributes="used">Spark SQL</software>, as it uses <software ContextAttributes="used">Spark</software> natively. As for workflow managers, although they can orchestrate efficiently relational operators across platforms, they do not provide query execution themselves; for example, a parallel join between <software ContextAttributes="used">MongoDB</software> and <software ContextAttributes="used">Spark</software> would be dispatched for execution at <software ContextAttributes="used">Spark</software> by both <software ContextAttributes="used">Musketeer</software> and RHEEM, so we would consider this comparison as equivalent to comparing with <software ContextAttributes="used">Spark</software>. Therefore, we target <software ContextAttributes="used">Spark SQL</software> as the only relevant system to evaluate our contributions against.</p></div>
<div><head n="4">Background</head><p>This section presents the concepts we consider towards the design our solution. We start with an overview of the <software>CloudMdsQL</software> query language focusing on the polyglot capabilities and optimization techniques. Then we discuss the distributed architecture of the <software ContextAttributes="used">LeanXcale</software> query engine, which we use to enable the parallel capabilities of our polystore.</p></div>
<div><head n="4.1">CloudMdsQL Query Language</head><p>The <software ContextAttributes="used">CloudMdsQL</software> language <ref type="bibr" target="#b25">[26]</ref> is <software ContextAttributes="used">SQL</software>-based with the extended capabilities for embedding subqueries expressed in terms of each data store's native query interface. The common data model respectively is table-based, with support of rich datatypes that can capture a wide range of the underlying data stores' datatypes, such as arrays and JSON objects, in order to handle non-flat and nested data, with basic operators over such composite datatypes.</p><p>The design of the query language is based on the assumption that the programmer has deep expertise and knowledge about the specifics of the underlying data stores, as well as awareness about how data are organized across them. Queries that integrate data from several data stores usually consist of native subqueries and an integration SELECT statement. A subquery is defined as a named table expression, i.e., an expression that returns a table and has a name and signature. The signature defines the names and types of the columns of the returned relation. A named table expression can be defined by means of either an <software>SQL</software> SELECT statement (that the query compiler is able to analyze and possibly rewrite) or a native expression (that the query engine considers as a black box and delegates its processing directly to the data store). For example, the following simple <software ContextAttributes="used">CloudMdsQL</software> query contains two subqueries, defined by the named table expressions T1 and T2, and addressed respectively against the data stores rdb (an <software ContextAttributes="used">SQL</software> database) and mongo (a <software ContextAttributes="used">MongoDB</software> database):</p><p>The two subqueries are sent independently for execution against their data stores in order the retrieved relations to be joined at query engine level. The <software>SQL</software> table expression T1 is defined by an <software ContextAttributes="used">SQL</software> subquery, while T2 is a native expression (identified by the special bracket symbols {* *}) expressed as a native <software ContextAttributes="used">MongoDB</software> <software ContextAttributes="used">API</software> call or JavaScript <software ContextAttributes="used">code</software>. The subquery of expression T1 is subject to rewriting by pushing into it the filter condition y &lt;= 3, to increase efficiency.</p><p><software>CloudMdsQL</software> also provides a CREATE NAMED EXPRESSION command that allows an expression to be defined and stored in a global catalog in order to be referenced in several queries, similarly to <software ContextAttributes="used">SQL</software> views and stored procedures/functions. This can facilitate the work of data analysts who just need to run <software ContextAttributes="used">SQL</software> queries on predefined views over the underlying data stores, without the need to deeply understand the specifics of the data technologies and data organization.</p></div>
<div><head>MFR extensions.</head><p>To address distributed processing frameworks (such as <software ContextAttributes="used">Apache Spark</software>) as data stores, <software ContextAttributes="used">CloudMdsQL</software> introduces a formal notation that enables the adhoc usage of user-defined map/filter/reduce (MFR) operators as subqueries to request data processing in an underlying big data processing framework <ref type="bibr" target="#b8">[9]</ref>. An MFR statement represents a sequence of MFR operations on datasets. In terms of <software ContextAttributes="used">Apache Spark</software>, a dataset corresponds to an RDD (Resilient Distributed Dataset -the basic programming unit of <software ContextAttributes="used">Spark</software>). Each of the three major MFR operations (MAP, FILTER and <software ContextAttributes="used">REDUCE</software>) takes as input a dataset and produces another dataset by performing the corresponding transformation. Therefore, for each operation there should be specified the transformation that needs to be applied on tuples from the input dataset to produce the output tuples. Normally, a transformation is expressed with an <software ContextAttributes="used">SQL</software>-like expression that involves special variables; however, more specific transformations may be defined through the use of lambda functions. Let us consider the following simple example inspired by the popular <software ContextAttributes="used">MapReduce</software> tutorial application "word count". We assume that the input dataset for the MFR statement is a text file containing a list of words. To count the words that contain the string 'cloud', we write the following composition of MFR operations:</p><p>T4(word string, count int)@spark = {* SCAN(TEXT, 'words.txt') .MAP(KEY, 1) .<software>REDUCE</software>(SUM) .FILTER( KEY LIKE '%cloud%' ) *}</p><p>For defining map and filter expressions, the special variable TUPLE can be used, which refers to the entire tuple. The variables KEY and VALUE are thus simply aliases to TUPLE[0] and TUPLE <ref type="bibr" target="#b0">[1]</ref> respectively.</p><p>To optimize this MFR subquery, the sequence is a subject to rewriting according to rules based on the algebraic properties of the MFR operators <ref type="bibr" target="#b8">[9]</ref>. In the example above, since the FILTER predicate involves only the KEY, it can be swapped with the <software ContextAttributes="used">REDUCE</software>, thus allowing the filter to be applied earlier in order to avoid unnecessary and expensive computation. The same rules apply for any pushed down predicates, including bind join conditions.</p></div>
<div><head n="4.2">Optimizations</head><p>To provide an optimal execution of selective queries, we consider two optimization opportunities: bind join and MFR rewrite rules. Bind join is a join method, in which the intermediate results of the outer relation (more precisely, the values of the join key) are passed to the subquery of the inner side, which uses these results to filter the data it returns. If the intermediate results are small and index is available on the join key at the inner side, bindings can significantly reduce the work done by the data store. <ref type="bibr" target="#b18">[19]</ref> To provide bind join as an efficient method for performing semi-joins across heterogeneous data stores, <software ContextAttributes="used">CloudMdsQL</software> uses subquery rewriting to push the join conditions. For example, the list of distinct values of the join attribute(s), retrieved from the lefthand side subquery (outer relation), is passed as a filter to the right-hand side (inner) subquery. To illustrate it, let us consider the following <software ContextAttributes="used">CloudMdsQL</software> query:</p><formula xml:id="formula_0">A(id int, x int)@DB1 = (SELECT a.id, a.x FROM a) B(id int, y int)@DB2 = (SELECT b.id, b.y FROM b) SELECT a.x, b.y FROM b JOIN a ON b.id = a.id</formula><p>First, the relation B is retrieved from the corresponding data store using its query mechanism. Then, the distinct values of B.id are used as a filter condition in the query that retrieves the relation A from its data store. Assuming that the distinct values of B.id are b1 … bn, the query to retrieve the right-hand side relation of the bind join uses the following <software>SQL</software> approach (or its equivalent according to the data store's query language), thus retrieving from A only the rows that match the join criteria:</p><formula xml:id="formula_1">SELECT a.id, a.x FROM a WHERE a.id IN (b1, …, bn)</formula><p>The way to do the bind join counterpart for native queries is through the use of a JOINED ON clause in the named table signature, like in the named table A below, defined as a <software>MongoDB</software> <software ContextAttributes="used">script</software>.</p><p>A(id int, x int JOINED ON id REFERENCING OUTER AS b_keys)@mongo = {* return db.A.find( {id: {$in: b_keys}} ); *} Thus, when A.id participates in an equi-join, the values b1,…,bn are provided to the <software>script</software> <software ContextAttributes="used">code</software> through the iterator/list object b_keys (in this context, we refer to the table B as the "outer" table, and b_keys as the outer keys).</p><p>Using bind join can be subject to planning decision. To estimate the expected performance gain of a bind join, the query optimizer takes into account the overhead a bind join may produce. First, when using bind join, the query engine must wait for the lefthand side B to be fully retrieved before initiating the execution of the right-hand side A. Second, if the number of distinct values of the join attribute is large, using a bind join may slower the performance as it requires data to be pushed into the subquery A. To take this decision, the optimizer needs at least to estimate the cardinality of the join keys of B, which can be easily solved if the data store exposes a cost model. However, if B is a native query or no cost information is available, the decision can still be taken, but at runtime: bind join is attempted and the retrieval of B initiated; then, if at some point the number of join keys exceeds a threshold, the execution falls back to an ordinary hash join. Nevertheless, the usage of bind join can be also explicitly requested by the user through the keyword BIND (e.g. FROM b BIND JOIN a).</p><p>MFR rewrite rules are used to optimize an MFR subquery after a selection pushdown takes place. The goal in general is to make filters take place as early as possible in the MFR sequence.</p><p>Rule #1 (name substitution): upon selection pushdown, an MFR FILTER is appended to the MFR sequence and the filter predicate expression is rewritten by substituting column names with references to dataset fields as per the mapping defined through the MFR expression. After this initial inclusion, other rules apply to determine whether it can be moved even farther. Example:</p><formula xml:id="formula_2">T1(a int, b int)@db1 ={* … *} SELECT a, b FROM T1 WHERE a &gt; b is rewritten to: T1(a int, b int)@db1 ={* … .FILTER(KEY &gt; VALUE)*} SELECT a, b FROM T1</formula><p>Rule #2: <software>REDUCE</software>(&lt;transformation&gt;).FILTER(&lt;predicate&gt;) is equivalent to FILTER(&lt;predicate&gt;).<software ContextAttributes="used">REDUCE</software>(&lt;transformation&gt;), if predicate condition is a function only of the KEY, because thus, applying the FILTER before the <software ContextAttributes="used">REDUCE</software> will preserve the values associated to those keys that satisfy the filter condition as they would be if the FILTER was applied after the <software ContextAttributes="used">REDUCE</software>.</p></div>
<div><head>Rule #3: MAP(&lt;expr_list&gt;).FILTER(&lt;predicate1&gt;)</head><p>is equivalent to FILTER(&lt;predicate2&gt;).MAP(&lt;expr_list&gt;), where predicate1 is rewritten to predicate2 by substituting KEY and VALUE as per the mapping defined in expr_list. Example:</p><formula xml:id="formula_3">MAP(VALUE[0], KEY).FILTER(KEY &gt; VALUE) à FILTER(VALUE[0] &gt; KEY).MAP(VALUE[0], KEY)</formula><p>Since planning a filter as early as possible always increases the efficiency, the planner always takes advantage of moving a filter by applying rules #2 and #3 whenever they are applicable. The greatest advantage of these rules can be observed when Rule #2 is applicable, as it enables early filtering of the input to expensive <software>REDUCE</software> operators. MFR rewrites can be combined with bind join in the sense that when a bind join condition is pushed down the MFR subquery, it will be applied as early as possible, in many cases reducing significantly the work done by <software ContextAttributes="used">REDUCE</software> operators on the way.</p></div>
<div><head n="4.3">LeanXcale Architecture Overview</head><p><software ContextAttributes="used">LeanXcale</software> is a scalable distributed SQL database management system with OLTP and OLAP support and full ACID capabilities. It has three main subsystems: the query engine, the transactional engine, and the storage engine, all three distributed and highly scalable (i.e., to hundreds of nodes). The system applies the principles of Hybrid Transactional and Analytical Processing (HTAP) and addresses the hard problem of scaling out transactions in mixed operational and analytical workloads over big data, possibly coming from different data stores (HDFS, <software ContextAttributes="used">SQL</software>, NoSQL, etc.). <software ContextAttributes="used">LeanXcale</software> solves this problem through its patented technology for scalable transaction processing <ref type="bibr" target="#b20">[21]</ref>. The transactional engine provides snapshot isolation by scaling out its components, the ensemble of which guarantees all ACID properties: local transaction managers (atomicity), conflict managers (isolation of writes), snapshot servers (isolation of reads), and transaction loggers (durability).</p><p>The <software ContextAttributes="used">LeanXcale</software> database has derived its OLAP query engine from <software ContextAttributes="used">Apache Calcite</software> <ref type="bibr" target="#b7">[8]</ref>, a Java-based open-source framework for <software ContextAttributes="used">SQL</software> query processing and optimization. <software ContextAttributes="used">LeanXcale</software>'s distributed query engine (DQE) is designed to process OLAP workloads over the operational data, so that analytical queries are answered over real-time data. This enables to avoid ETL processes to migrate data from operational databases to data warehouses by providing both functionalities in a single database manager. The parallel implementation of the query engine for OLAP queries follows the single-program multiple data (SPMD) approach <ref type="bibr" target="#b12">[13]</ref>, where multiple symmetric workers (threads) on different query instances execute the same query/operator, but each of them deals with different portions of the data. In this section we provide a brief overview of the query engine distributed architecture. Fig. <ref type="figure" target="#fig_0">1</ref> illustrates the architecture of <software ContextAttributes="used">LeanXcale</software>'s Distributed Query Engine (DQE). Applications connect to one of the multiple DQE instances running, which exposes a typical JDBC interface to the applications, with support for <software ContextAttributes="used">SQL</software> and transactions. The DQE executes the applications' requests, handling transaction control, and updating data, if necessary. The data itself are stored on a proprietary relational key-value store, KiVi, which allows for efficient horizontal partitioning of <software ContextAttributes="used">LeanXcale</software> tables and indexes, based on the primary key or index key. Each table partition corresponds to a range of the primary/index key and it is the unit of distribution. Each table is stored as a KiVi table, where the key corresponds to the primary key of the <software ContextAttributes="used">LeanXcale</software> table and all the columns are stored as they are into KiVi columns. Indexes are also stored as KiVi tables, where the index keys are mapped to the corresponding primary keys. This model enables high scalability of the storage layer by partitioning tables and indexes across KiVi Data Servers (KVDS). KiVi is relational in the sense that it has a relational schema and implements all relational operators but join, so any relational operator below a join can be pushed down to KiVi.</p><p>This architecture scales by allowing analytical queries to execute in parallel, based on the master-worker model using intra-query and intra-operator parallelism. For parallel query execution, the initial connection (which creates the master worker) will start additional connections (workers), all of which will cooperate on the execution of the queries received by the master.</p><p>When a parallel connection is started, the master worker starts by determining the available DQE instances, and it decides how many workers will be created on each instance. For each additional worker needed, the master then creates a thread, which initiates a TCP connection to the worker. Each TCP connection is initialized as a worker, creating a communication endpoint for an overlay network to be used for intraquery synchronization and data exchange. After the initialization of all workers the overlay network is connected. After this point, the master is ready to accept queries to process.</p><p>The master includes a state-of-the-art <ref type="bibr" target="#b30">[31]</ref> query optimizer that transforms a query into a parallel execution plan. The transformation made by the optimizer involves replacing table scans with parallel table scans, and adding shuffle operators to make sure that, in stateful operators (such as Group By, or Join), related rows are handled by the same worker. Parallel table scans will divide the rows from the base tables among all workers, i.e., each worker will retrieve a disjoint subset of the rows during table scans. This is done by scheduling the obtained subsets to the different DQE instances. This scheduling is handled by a component in the master worker, named DQE scheduler.</p><p>The generated parallel execution plan is broadcast to be processed by all workers. Each worker then processes the rows obtained from subsets scheduled to its DQE instance, exchanging rows with other workers as determined by the shuffle operators added to the query plan.</p><p>Let us consider the query Q1 below, which we will use as a running example throughout the paper to illustrate the different query processing modes. The query assumes a TPC-H <ref type="bibr" target="#b36">[37]</ref> schema. This query is transformed into a query execution plan, where leaf nodes correspond to tables or index scans. The master worker then broadcasts to all workers the generated query plan, with the additional shuffle operators (Fig. <ref type="figure">2a</ref>). Then, the DQE scheduler assigns evenly all database shards across all workers. To handle the leaf nodes of the query plan, each worker will do table/index scans only at the assigned shards. Let us assume for simplicity that the DQE launches the same number of workers as KVDS servers, so each worker connects to exactly one KVDS server and reads the partition of each table that is located in that KVDS server. Then, workers execute in parallel the same copy of the query plan, exchanging rows across each other at the shuffle operators (marked with an S box).</p><p>To process joins, the query engine may use different strategies. First, to exchange data across workers, shuffle or broadcast methods can be used. The shuffle method is efficient when both sides of a join are quite big; however, if one of the sides is relatively small, the optimizer may decide to use the broadcast approach, so that each worker has a full copy of the small table, which is to be joined with the local partition of the other table, thus avoiding the shuffling of rows from the large table (Fig. <ref type="figure">2b</ref>). Apart from the data exchange operators, the DQE supports various join methods (hash, nested loop, etc.), performed locally at each worker after the data exchange takes place.</p></div>
<div><head n="5">Parallel Polyglot Query Processing across Data Stores</head><p><software>LeanXcale DQE</software> is designed to integrate with arbitrary data management clusters, where data resides in its natural format and can be retrieved (in parallel) by running specific <software ContextAttributes="used">scripts</software> or declarative queries. These data management clusters can range from distributed raw data files, through parallel <software ContextAttributes="used">SQL</software> databases, to sharded NoSQL databases (such as <software ContextAttributes="used">MongoDB</software>, where queries can be expressed as JavaScript <software ContextAttributes="used">programs</software>) and parallel data processing frameworks (such as <software ContextAttributes="used">Apache Spark</software>, where data retrieval and/or transformation can be requested by means of <software ContextAttributes="used">Python</software> or <software ContextAttributes="used">Scala</software> scripting). This turns <software ContextAttributes="used">LeanXcale</software> DQE into a powerful "big data lake" polyglot query engine that can process data from its original format, taking full advantage of both expressive scripting and massive parallelism. Moreover, joins across any native datasets, including <software ContextAttributes="used">LeanXcale</software> tables, can be applied, exploiting efficient parallel join algorithms. Here, we specifically focus on parallel joins across a relational table, the result of a JavaScript subquery to <software ContextAttributes="used">MongoDB</software>, and the result of an MFR/<software ContextAttributes="used">Scala</software> subquery to <software ContextAttributes="used">Apache Spark</software>, but the concept relies on an <software ContextAttributes="used">API</software> that allows its generalization to other <software ContextAttributes="used">script engines</software> and data stores as well. To enable ad-hoc querying of an arbitrary data set, using its scripting mechanism, and then joining the retrieved result set at DQE level, DQE processes queries in the <software ContextAttributes="used">CloudMdsQL</software> query language, where <software ContextAttributes="used">scripts</software> are wrapped as native subqueries.</p></div>
<div><head n="5.1">High Expressivity: Motivation</head><p>To better illustrate the necessity of enabling user-defined <software>scripts</software> to <software ContextAttributes="used">MongoDB</software> as subqueries, rather than defining <software ContextAttributes="used">SQL</software> mappings to document collections, let us consider the following <software ContextAttributes="used">MongoDB</software> collection orders that has a highly non-relational structure:</p><p>{order_id: 1, customer: "ACME", status: "O", items: [ {type: "book", title: "Book1", author: "A.Z.", keywords: ["data", "query", "cloud"]}, {type: "phone", brand: "Samsung", os: "Android"} ] }, ...</p></div>
<div><head>Each record contains an array of item objects whose properties differ depending on the item type. A query that needs to return a table listing the title and author of all books</head><p>ordered by a given customer, would be defined by means of a flatMap operator in Ja-vaScript, following a <software>MongoDB</software> find() operator. Furthermore, we aim at processing this join in the most efficient way, i.e. in parallel, by allowing parallel handling of the <software ContextAttributes="used">MongoDB</software> subquery and parallel retrieval of its result set.</p><p>In another example, a more sophisticated data transformation logic (such as a chain of user-defined transformations over <software ContextAttributes="used">Apache</software> <software ContextAttributes="used">Spark</software> RDDs) needs to be applied to unstructured data before processing by means of relational operators <ref type="bibr" target="#b8">[9]</ref>. Let us consider the task of analyzing the logs of a scientific forum in order to identify the top experts for particular keywords, assuming that the most influencing user for a given keyword is the one who mentions the keyword most frequently in their posts. We assume that the forum application keeps log data about its posts in the non-tabular structure below, namely in text files where a single record corresponds to one post and contains a fixed number of fields about the post itself (timestamp, link to the post, and username in the example) followed by a variable number of fields storing the keywords mentioned in the post. The unstructured log data needs to be transformed into the tabular dataset below, containing for each keyword the expert who mentioned it most frequently. Such transformation requires the use of programming techniques like chaining map/reduce operations that should take place before the data is involved in relational operators. This can be expressed with the following MFR subquery, with embedded <software ContextAttributes="used">Scala</software> lambda functions to define custom transformation logic: In this sequence of operations, the first MAP takes a tuple (corresponding to a row from the input file) as an array of string values (tup) and maps the username (tup(2)) to the keywords subarray (tup.slice(…)). Then, the FLAT_MAP emits (keyword, user) pairs for each keyword. The following MAP and <software ContextAttributes="used">REDUCE</software> count the frequencies of each such pair. Then, after grouping by keyword, the last <software ContextAttributes="used">REDUCE</software> selects, for each keyword, the (user, frequency) pair that has the greatest value of frequency. The final MAP selects the keyword and username for the final projection of the returned relation. Optimization. For optimal execution of this query, both bind join and MFR rewrites play their roles. The bind join condition (which involves only the kw column) can be pushed down the MFR sequence as a FILTER operator, in this case FILTER(KEY IN (&lt;set_of_B_keywords&gt;)). As per the MFR rewrite rules, this would take place immediately after the FLAT_MAP operator, significantly reducing at early stage the amount of data to be processed by the expensive <software ContextAttributes="used">REDUCE</software> operators that follow. To build the bind join condition, the query engine flattens B.keywords and identifies the list of distinct values.</p></div>
<div><head>KW</head><formula xml:id="formula_4">Experts(kw</formula></div>
<div><head n="5.2">DataLake API</head><p>By processing such queries, DQE takes advantage of the expressivity of each local scripting mechanism, yet allowing for results of subqueries to be handled in parallel by DQE and involved in operators that utilize the intra-query parallelism. The query engine architecture is therefore extended to access in parallel shards of the external data store through the use of DataLake distributed wrappers that hide the complexity of the underlying data stores' query/scripting languages and encapsulate their interfaces under a common <software>DataLake API</software> to be interfaced by the query engine. Towards the design of a distributed wrapper architecture and its programming interface, we consider the outlined in Section 3 requirements for our polystore. In particular, we pay attention to the following desired capabilities:</p><p>• A DataLake wrapper must have multiple instances, each linked to a DQE worker. • A wrapper instance must be able to execute a native subquery or <software>script</software> against a particular shard of the underlying data store cluster. • The DQE scheduler must be able to retrieve (through one of the wrapper instances) a list of "shard entries", i.e. specifications of the available shards for an underlying dataset. These specifications must be opaque, as the DQE scheduler is agnostic to the specifics of the data store cluster. • The scheduler must be able to assign shards to DQE workers and hence to the corresponding DataLake wrapper instances.</p><p>These requirements make the concept generic in the sense that our polystore can be easily extended to support any data management cluster as long as it provides means to connect directly to database shards and retrieve in parallel dataset partitions. In the subsequent subsections, we give details on how the process of parallel retrieval from Mon-goDB and HDFS datasets is mapped to the methods of the generic <software>DataLake API</software>. We also show that the same methods abstract well enough even the more sophisticated parallel data push model, necessary to support the parallel integration with <software ContextAttributes="used">Apache Spark</software>, as introduced in Section 3.</p><p>For a particular data store, each DQE worker creates an instance of the DataLake wrapper that is generally used for querying and retrieval of shards of data. Each wrapper typically uses the client <software ContextAttributes="used">API</software> of the corresponding data management cluster and implements the following <software ContextAttributes="used">DataLake API</software> methods to be invoked by the query engine in order to provide parallel retrieval of shards (Fig. <ref type="figure">3</ref>).</p></div>
<div><head>Fig. 3. Generic architecture extension for accessing external data stores</head><p>The method init(ScriptContext) requests the execution of a <software>script</software> to retrieve data from the data store. It provides connection details to address the data store and the <software ContextAttributes="used">script</software> as text. It may also provide parameter values, if the corresponding named table is parameterized. Normally, the wrapper does not initiate the execution of the <software ContextAttributes="used">script</software> before a shard is assigned by the setShard method (see below).</p><p>After the initialization, the DQE selects one of the wrapper instances (the one created by the master worker) as a master wrapper instance. The method Object[] listShards() is invoked by the DQE only to the master wrapper to provide a list of shards where the result set should be retrieved from. Each of the returned objects encapsulates information about a single shard, which is implementation-specific, therefore opaque for the query engine. Such an entry may contain, for example, the network address of the database shard, and possibly a range of values of the partition key handled by this shard. Since the query engine is unaware of the structure of these objects, the wrapper provides additional methods for serializing and deserializing shard entries, so that DQE can exchange them across workers.</p><p>Having obtained all the available shards, the DQE schedules the shard assignment across workers and invokes the method setShard(Object shard) to assign a shard to a particular wrapper instance. Normally, this is the point where the connection to the data store shard takes place and the <software>script</software> execution is initiated. This method might be invoked multiple times to a single wrapper instance, in case there are more shards than workers.</p><p>Using the method boolean next(Object[] row), the query engine iterates through a partition of the result set, which is retrieved from the assigned shard. When this iteration is over, the DQE may assign another shard to the wrapper instance.</p><p>By interfacing wrappers through the <software>DataLake API</software>, the DQE has the possibility to retrieve in parallel disjoint subsets of the result set, much like it does with <software ContextAttributes="used">LeanXcale</software> tables. A typical wrapper implementation should use a scripting engine and/or a client library to execute <software ContextAttributes="used">scripts</software> (client-or server-side) against the data store.</p></div>
<div><head n="5.3">Implementation for MongoDB</head><p>In this section, we introduce the design of the distributed <software>MongoDB</software> wrapper. The concept of parallel querying against a <software ContextAttributes="used">MongoDB</software> cluster is built on the assumption that each DQE worker can access directly a <software ContextAttributes="used">MongoDB</software> shard, bypassing the <software ContextAttributes="used">MongoDB</software> router in order to sustain parallelism. This, however, forces the DQE to define certain constraints for parallel processing of document collection subqueries, in order to guarantee consistent results, which is normally guaranteed by the <software ContextAttributes="used">MongoDB</software> router. The full scripting functionality of <software ContextAttributes="used">MongoDB</software> JavaScript library is still provided, but in case parallel execution constraints fail, the execution falls back to a sequential one. First, the wrapper verifies that the <software ContextAttributes="used">MongoDB</software> balancer is not running in background, because otherwise it may be moving chunks of data across <software ContextAttributes="used">MongoDB</software> shards at the same time the query is being executed, which may result in inconsistent reads. Second, the subquery should use only stateless operators (Op) on document collections, as they are distributive over the union operator. In other words, for any disjoint subsets (shards) S1 and S2 of a document collection C, Op(S1)ÈOp(S2) = Op(S1ÈS2) must hold, so that the operator execution can be parallelized over the shards of a document collection while preserving the consistency of the resulting dataset. In our current work, we specifically focus on enabling the parallel execution of filtering, projection (map), and flattening operators, by means of user-defined as JavaScript functions transformations. The distributed wrapper for <software ContextAttributes="used">MongoDB</software> comprises a number of instances of a Java class that implements the DataLake <software ContextAttributes="used">API</software>, each of which embeds a JavaScript <software ContextAttributes="used">scripting</software> engine that uses <software ContextAttributes="used">MongoDB</software>'s JavaScript client library. To support parallel data retrieval, we further enhance the client library with JavaScript primitives that wrap standard <software ContextAttributes="used">MongoCursor</software> objects (usually returned by a <software ContextAttributes="used">MongoDB</software> JavaScript query) in Shard-edCursor objects, which are aware of the sharding of the underlying dataset. In fact, <software ContextAttributes="used">ShardedCursor</software> implements all <software ContextAttributes="used">DataLake API</software> methods and hence serves as a proxy of the <software ContextAttributes="used">API</software> into the JavaScript <software ContextAttributes="used">MongoDB</software> client library. The client library is therefore extended with the following document collection methods that return <software ContextAttributes="used">ShardedCursor</software> and provide the targeted operators (find, map, and flat map) in user <software ContextAttributes="used">scripts</software>.</p><p>The <software>findSharded</software>() method accepts the same arguments as the native <software ContextAttributes="used">MongoDB</software> find() operator, in order to provide the native flexible querying functionality, complemented with the ability to handle parallel iteration on the sharded result set. Note that, as opposed to the behavior of the original find() method, a call to find-Sharded() does not immediately initiate the <software ContextAttributes="used">MongoDB</software> subquery execution, but only memorizes the filter condition (the method argument), if any, in the returned Shard-edCursor object. This delayed iteration approach allows the DQE to internally manipulate the cursor object before the actual iteration takes place, e.g., to redirect the subquery execution to a specific <software ContextAttributes="used">MongoDB</software> shard. And since an instance of Shard-edCursor is created at every worker, this allows for the parallel assignment of different shards.</p><p>In order to make a document result set fit the relational schema required by a Cloud-MdsQL query, the user <software ContextAttributes="used">script</software> can further take advantage of the map() and flatMap() operators. Each of them accepts as argument a JavaScript mapper function that performs a transformation on each document of the result set and returns another document (map) or a list of documents (flatMap). Thus, a composition of <software ContextAttributes="used">findSharded</software> and map/flatMap (such as in the BookOrders example above) makes a user <software ContextAttributes="used">script</software> expressive enough, so as to request a specific <software ContextAttributes="used">MongoDB</software> dataset, retrieve the result set in parallel, and transform it in order to fit the named table signature and further be consumed by relational operators at the DQE level. Let us consider the following modification Q1 ML of query Q1, which assumes that the LINEITEM table resides as a sharded document collection in a <software ContextAttributes="used">MongoDB</software> cluster and the selection on it is expressed by means of the <software ContextAttributes="used">findSharded</software>() JavaScript method, while ORDERS is still a <software ContextAttributes="used">LeanXcale</software> table, the partitions of which are stored in the KV storage layer. Let us assume for simplicity a cluster of equal numbers of: DQE workers, KVDS servers, and <software ContextAttributes="used">MongoDB</software> shards. Thus, each DQE worker gets exactly one partition of both tables by connecting to one <software ContextAttributes="used">MongoDB</software> shard (through a wrapper instance) and one KVDS (Fig. <ref type="figure" target="#fig_4">4</ref>).</p><p>The DQE initiates the subquery request by passing the <software>script</software> <software ContextAttributes="used">code</software> to each wrapper instance through a call to its init() method. At this point, the <software ContextAttributes="used">ShardedCursor</software> object does not yet initiate the query execution, but only memorizes the query filter object. Assuming that W1 is the master worker, it calls the listShards() method of its wrapper instance WR1 to query the <software ContextAttributes="used">MongoDB</software> router for a list of <software ContextAttributes="used">MongoDB</software> shards (database instances identified by host address and port), where partitions of the lineitem collection are stored. The list of shards is then reported to the DQE scheduler, which assigns one <software ContextAttributes="used">MongoDB</software> shard to each of the workers by calling the setShard() method. Each worker then connects to the assigned shard and invokes the find() method to a partition of the lineitem collection using the memorized query condition, thus retrieving a partition of the resulting dataset (if a flatMap or map follows, it is processed for each document of that partition locally at the wrapper). The dataset partition is then converted to a partition of an intermediate relation, according to the signature of the LINEITEM named table expression. At this point, the DQE is ready to involve the partitioned intermediate relation LINEITEM in the execution of a parallel join with the native <software ContextAttributes="used">LeanXcale</software> partitioned table ORDERS. </p></div>
<div><head n="5.4">Implementation for HDFS Files</head><p>The distributed HDFS wrapper is designed to access in parallel tables stored as HDFS files, thus providing the typical functionality of a tightly-coupled polystore, but through the use of the <software>DataLake API</software>. We assume that each accessed HDFS file is registered as table in a <software ContextAttributes="used">Hive</software> metastore. Therefore, a wrapper instance can use the <software ContextAttributes="used">Hive</software> metastore <software ContextAttributes="used">API</software> to get schema and partitioning information for the subqueried HDFS table and hence to enable iteration on a particular split (shard) of the table. Note that <software ContextAttributes="used">Hive</software> is interfaced only for getting metadata, while the data rows are read directly from HDFS.</p><p>To better illustrate the flow, let us consider another modification Q1 HL of query Q1, which assumes that the LINEITEM table is stored as file in a <software>Hadoop</software> cluster.</p><formula xml:id="formula_5">Q1 HL : SELECT count(*) FROM LINEITEM@hdfs L, ORDERS O WHERE L_ORDERKEY = O_ORDERKEY</formula><p>To schedule parallel retrieval of the LINEITEM table, the DQE redirects the subquery to the HDFS wrapper, preliminarily configured to associate the @hdfs alias with the URI of the <software ContextAttributes="used">Hive</software> metastore, which specifies how the file is parsed and split. This information is used by the master wrapper, which reports the list of file splits (instances of <software ContextAttributes="used">Hive API</software>'s <software ContextAttributes="used">InputSplit</software> class) to the DQE scheduler upon a call to the listShards() method. Then, the scheduler assigns a split to each of the workers, which creates a record reader on it in order to iterate through the split's rows (Fig. <ref type="figure" target="#fig_5">5</ref>). </p></div>
<div><head n="5.5">Implementation for Apache Spark</head><p>As stated in Section 3, the major challenge of supporting <software>Apache</software> <software ContextAttributes="used">Spark</software> as an underlying data management cluster is to enable parallel data movement from <software ContextAttributes="used">Spark</software> workers to DataLake wrappers. Since this necessitates that <software ContextAttributes="used">Spark</software> workers find and connect to DataLake wrapper instances, it results in a different, more complex architecture of the distributed <software ContextAttributes="used">Spark</software> wrapper. A discovery service is introduced through the special component <software ContextAttributes="used">Spark</software> Agent Registry that keeps information about available <software ContextAttributes="used">Spark</software> wrapper instances and dispatches them to the requesting <software ContextAttributes="used">Spark</software> workers so that parallelism is fully exploited in moving data from a <software ContextAttributes="used">Spark</software> RDD to the DQE. In order to make the wrapper instances collect in parallel partitions of the resulting RDD, the master wrapper ships an additional <software ContextAttributes="used">code</software> together with the user defined <software ContextAttributes="used">script</software>, that makes each RDD partition push its data directly to the assigned by the registry wrapper instance. This approach, explained in detail hereafter, differs from the typical retrieval of sharded data, where partitions of the underlying dataset can be directly accessed and pulled by wrapper instances.</p><p>As the wrapper processes MFR expressions wrapped in native subqueries, it implements a subquery processor. It parses and interprets a subquery written in MFR notation; then, uses an MFR planner to find optimization opportunities; and finally translates the resulting sequence of MFR operations to a sequence of <software>Spark</software> methods to be executed, expressed as <software ContextAttributes="used">Scala</software> (in our focus for this paper) or <software ContextAttributes="used">Python</software> <software ContextAttributes="used">script</software>. The MFR planner decides where to position the pushed down filter operations to apply them as early as possible, using rules for reordering MFR operators that take into account their algebraic properties (see Section 4.2). This preprocessing takes place at the master wrapper instance.</p><p>To enable remote submission of the generated <software ContextAttributes="used">Scala</software> <software ContextAttributes="used">script</software> for <software ContextAttributes="used">Spark</software> by the master wrapper, our setup relies on <software ContextAttributes="used">Apache Livy</software><ref type="foot" target="#foot_2">2</ref> , which provides a REST service for easy submission of <software ContextAttributes="used">Spark</software> jobs and <software ContextAttributes="used">Spark</software> context management. Fig. <ref type="figure" target="#fig_6">6</ref> gives a high-level illustration of the processing of the query Q1 SL , assuming a simple MFR subquery that reads the LINEITEM table as a text file from the <software ContextAttributes="used">Hadoop</software> cluster, but this time through <software ContextAttributes="used">Spark</software>. Fig. <ref type="figure" target="#fig_7">7</ref> shows in detail the flow of operations for processing an MFR subquery by the distributed <software ContextAttributes="used">Spark</software> wrapper. Each wrapper instance is composed of PrepareScript and <software ContextAttributes="used">SparkAgent</software> components. PrepareScript is responsible for preparing the <software ContextAttributes="used">Scala</software> <software ContextAttributes="used">script</software> to be submitted as a <software ContextAttributes="used">Spark</software> job and is active only at the master wrapper for a particular query. <software ContextAttributes="used">SparkAgent</software> is the component, which accepts TCP connections from <software ContextAttributes="used">Spark</software> executors to push RDD partition data. To initiate the subquery processing, the DQE sends the user MFR expression to the master wrapper through a call to the init() method. Then, the PrepareScript component of the master wrapper generates the <software ContextAttributes="used">Scala</software> <software ContextAttributes="used">code</software> that corresponds to the MFR query, to initialize a variable named rdd: val rdd = sc.textFile( "lineitem.tbl" )</p><p>.map( _.split(",") )</p><p>Next, the DQE calls the listShards() method of the master wrapper, which returns the number of expected partitions of the result RDD. To figure out this number, PrepareScript opens a Livy session, initializes the rdd variable using the above <software>Scala</software> statement, and then calls rdd.getNumPartitions().</p><p>At this moment, the execution of the prepared <software>Spark</software> job gets initiated by calling through the same Livy session the following foreachPartition action function that makes each partition connect to an available wrapper instance and send its data: In this <software ContextAttributes="used">code</software>, <software ContextAttributes="used">connectSparkAgent</software> is a function that the master wrapper preliminarily generates and defines in the Livy session. It requests from a common component, named <software ContextAttributes="used">AgentRegistry</software>, the address of an available <software ContextAttributes="used">SparkAgent</software> (waiting for such availability, if necessary) and makes a socket connection to it. serialize is another function that serializes each entry of the RDD partition to a byte array in a format that <software ContextAttributes="used">SparkAgent</software> can interpret, which is then sent to the <software ContextAttributes="used">SparkAgent</software> through the socket. This function is also generated by the master wrapper, once the type of the RDD entries is reported back through Livy after initialization of the rdd variable.</p><p>Upon a subsequent call of setShard() to a wrapper instance, the corresponding <software>SparkAgent</software> reports to the <software ContextAttributes="used">AgentRegistry</software> its availability to receive partition data for this particular query. On the other hand, as described above, when processing a partition, each <software ContextAttributes="used">Spark</software> executor finds and sends to an available <software ContextAttributes="used">Spark</software> agent all tuples of the partition. When tuples are received and deserialized, <software ContextAttributes="used">SparkAgent</software> buffers them to a queue, from where they are pulled by the query engine through calls of the next() method of the wrapper instance.</p></div>
<div><head n="6">Experimental Evaluation</head><p>The goal of our experimental validation is to assess the scalability of the query engine when processing integration (join) queries across diverse data sources, as our major objective is to be able to fully exploit both the massive parallelism and high expressivity, provided by the underlying data management technologies and their scripting frameworks. We evaluate the scalability of processing a particular query by varying the volume of queried data and the level of parallelism and analyzing the corresponding execution times. In particular, we strive to retain similar execution times of a particular query when keeping the level of parallelism (in number of data shards and workers) proportional to the scale of data.</p><p>The experimental evaluation was performed on a cluster of the GRID5000 platform <ref type="foot" target="#foot_3">3</ref> . Each node in the cluster runs on two Xeon E5-2630 v3 CPUs at 2.4GHz, 8 physical cores per CPU (i.e., 16 per node), 128 GB main memory, and the network bandwidth is 10Gbps. The highest level of parallelism is determined by the total number of cores in the cluster. We performed the experiments varying the number of nodes from 2 to 32 and the number of workers from 32 to 512 (several workers per node). All the three data stores and the query engine are evenly distributed across all the nodes, i.e. shards of each data store and <software ContextAttributes="used">Spark</software> workers are collocated at each node. The coordinating components for the <software ContextAttributes="used">Spark</software> subsystem, Livy and <software ContextAttributes="used">AgentRegistry</software>, are running on one of the nodes. For each experiment, the level of parallelism determines the number of data shards, as well as the highest number of workers, in accordance with the total number of cores in the cluster.</p><p>We performed our experiments in three general groups of test cases, each having a distinct objective. The first group evaluates the scalability of the system in the context of straightforward <software>SQL</software> mappings with the underlying data stores. The second group adds higher expressivity to the subqueries, which cannot be easily achieved through trivial <software ContextAttributes="used">SQL</software> mappings, while still assessing the scalability. The third group evaluates the benefit of performing bind join in the context of large-scale data and the same highly expressive subqueries. All the queries were run on a cluster of <software ContextAttributes="used">LeanXcale</software> DQE instances, running the distributed wrappers for <software ContextAttributes="used">MongoDB</software>, <software ContextAttributes="used">Hive</software>, and <software ContextAttributes="used">Spark</software>.</p><p>For comparison with the state of the art, the large-scale test case queries were also performed on a <software>Spark</software> <software ContextAttributes="used">SQL</software> cluster, where we used the <software ContextAttributes="used">MongoDB</software> <software ContextAttributes="used">Spark</software> connector to access <software ContextAttributes="used">MongoDB</software> shards in parallel. The choice of <software ContextAttributes="used">Spark</software> <software ContextAttributes="used">SQL</software> for a state-of-the-art representative to compare our work with is justified by the fact that it supports most of the features our approach targets and hereby evaluates, namely: (a) parallel <software ContextAttributes="used">MongoDB</software> subqueries through the use of the <software ContextAttributes="used">MongoDB</software> connector that also supports native Mon-goDB operators (e.g. aggregation pipelines), beyond the trivial <software ContextAttributes="used">SQL</software> mappings; (b) parallel map/filter/reduce subqueries, done natively through <software ContextAttributes="used">Spark</software> RDD transformations; (c) parallel joins and scalability. What <software ContextAttributes="used">Spark</software> <software ContextAttributes="used">SQL</software> is not capable of is bind join through <software ContextAttributes="used">SQL</software> queries; to perform a bind join, one has to write a <software ContextAttributes="used">Spark</software> program, which limits the use cases. Therefore, we stress on our advantage of supporting this powerful optimization technique.</p></div>
<div><head n="6.1">General Scalability</head><p>The first group of test cases aims at generic evaluation of the performance and scalability of joins across any pair of the four involved data stores. The data used was based on the TPC-H benchmark schema <ref type="bibr" target="#b36">[37]</ref>, particularly for the tables LINEITEM, ORDERS, and <software ContextAttributes="used">CUSTOMER</software>. All the generated datasets were: loaded in <software ContextAttributes="used">LeanXcale</software> as relational tables; loaded in <software ContextAttributes="used">MongoDB</software> as document collections; copied to the HDFS cluster as raw CSV files, to be accessed through <software ContextAttributes="used">Hive</software> as tables and through <software ContextAttributes="used">Spark</software> by means of scans expressed as simple MFR/<software ContextAttributes="used">Scala</software> statements. To perform the tests on different volumes of data, the datasets were generated with three different scale factors -60GB, 120GB, and 240GB. Note that here we focus just on the evaluation of joins; therefore, our queries involve only joins over full scans of the datasets, without any filters.</p><p>The six queries used for this evaluation are variants of the following:</p><formula xml:id="formula_6">Q1: SELECT count(*) FROM LINEITEM L, ORDERS O WHERE L_ORDERKEY = O_ORDERKEY</formula><p>We will refer to them with the notation Q1 XY , where X is the first letter of the data store, from which LINEITEM is retrieved, while Y refers to the location of ORDERS. For example, Q1 ML joins LINEITEM from <software ContextAttributes="used">MongoDB</software> with ORDERS from <software ContextAttributes="used">LeanXcale</software>. Subqueries to <software ContextAttributes="used">MongoDB</software> are expressed natively in JavaScript. MFR subqueries to <software ContextAttributes="used">Spark</software> are defined as single SCAN operators, translated to <software ContextAttributes="used">Scala</software> commands. Intermediate result sets from <software ContextAttributes="used">MongoDB</software>, HDFS, and <software ContextAttributes="used">Spark</software> are retrieved in parallel, as described in Section 5. Fig. <ref type="figure" target="#fig_8">8</ref> shows the performance measurements on queries of the first test case, executing joins between LINEITEM and ORDERS tables in any configuration of pairs between the three data stores.</p><p>In general, the execution speed is determined by the performance of processing the LINEITEM side of the join, as this table is much larger than ORDERS. When LINEITEM resides at <software>LeanXcale</software>, the performance is highest, as the query engine processes it natively. For HDFS tables, some overhead is added, due to data conversions, communication with the <software ContextAttributes="used">Hive metastore</software>, and possibly accessing HDFS splits through the network. For <software ContextAttributes="used">Spark</software> result sets, this overhead is a bit higher, because of the additional serialization/deserialization that takes place between <software ContextAttributes="used">Spark</software> executors and <software ContextAttributes="used">SparkAgent</software> instances. <software ContextAttributes="used">MongoDB</software> subqueries show lowest performance as data retrieval passes through the embedded JavaScript interpreter at each worker. All the graphs show reasonable speedup with increase of the parallelism level. Moreover, the correspondence between scale of data and parallelism level is quite stable. For example, quite similar execution times are observed for 60GB with 64 workers, 120GB with 128 workers, and 240GB with 256 workers. This means that, as the volume of data grows, performance can be maintained by simply adding a proportional number of workers and data shards.</p></div>
<div><head n="6.2">High Expressivity and Scalability</head><p>The second group of test cases aims at the evaluation of highly expressive JavaScript and MFR subqueries, such as the BookOrders and Experts examples from Section 5.1. The goal is to show that even with more sophisticated subqueries, scalability is not compromised.</p><p>To evaluate BookOrders, we created a <software ContextAttributes="used">MongoDB</software> nested document collection named Orders_Items, where we combined the ORDERS and LINEITEM datasets as follows. For each ORDERS row we created a document that contains an additional array field items, where the corresponding LINEITEM rows were added as subdocuments. Each of the item subdocuments was assigned a type field, the value of which was randomly chosen between "book" and "phone". Then, "title" and "author" fields were added for the "book" items and "brand" and "os"for the "phone" items, all filled with randomly generated string values. Thus, the following BookOrders named table was used in the test queries: We ran two queries under the same variety of conditionsthree different scale factors for the volume of data and varying the level of parallelism from 32 to 512. Query Q2 M evaluates just the parallel execution of the BookOrders <software ContextAttributes="used">script</software> in <software ContextAttributes="used">MongoDB</software>, while Q2 ML involves a join between <software ContextAttributes="used">MongoDB</software> and the <software ContextAttributes="used">CUSTOMER</software> table from the <software ContextAttributes="used">LeanXcale</software> data store: Fig. <ref type="figure" target="#fig_11">9</ref> shows the performance measurements of Q2 queries that stress on the evaluation of the parallel processing of highly expressive JavaScript queries, with and without join with a <software ContextAttributes="used">LeanXcale</software> table. Similar conclusions on performance and scalability can be done, like for the Q1 queries. To evaluate the Experts subquery, we generated a log file for posts with the structure suggested in Section 5.1. The first and the second fields of each tuple are a timestamp and a URL; they do not have impact on the experimental results. The third field contains the author of the post as a string value. The remainder of the tuple line contains 1 to 10 keyword string values, randomly chosen out of a set of 15,000 distinct keywords. Data have been generated in three scale factors: 60GB (400 million tuples), 120GB (800 million tuples), and 240GB (1.6 billion tuples). The intermediate datasets at the first shuffle, where (keyword, user) pairs are emitted, are about the same size respectively.</p><p>Similarly to the previous test case, we ran two queries varying the level of parallelism from 32 to 512. Query Q2 S evaluates just the parallel execution of the Experts MFR query on <software ContextAttributes="used">Spark</software>, while Q2 SL involves a join with the <software ContextAttributes="used">CUSTOMER</software> table from the <software ContextAttributes="used">LeanXcale</software> data store: Fig. <ref type="figure" target="#fig_0">10</ref> shows the performance measurements of Q2 queries that stress on the evaluation of the parallel processing of MFR/<software ContextAttributes="used">Scala</software> queries against <software ContextAttributes="used">Spark</software>, with and without join with a <software ContextAttributes="used">LeanXcale</software> table. In general, the execution of these queries is much slower, as, at the <software ContextAttributes="used">Spark</software> level, it involves shuffles of significant amounts of intermediate data.</p><p>The results show good scalability and speedup with increase of the parallelism level, like for the Q1 queries. Fig. <ref type="figure" target="#fig_0">10</ref>. Execution times (in seconds) of Q2 queries on complex MFR/<software ContextAttributes="used">Scala</software> queries to <software ContextAttributes="used">Spark</software> with scales of data from 60 to 240 GB and levels of parallelism from 32 to 512.</p></div>
<div><head n="6.3">Large Scale and Bind Joins</head><p>The third group of test cases evaluates the parallel polyglot query processing in the context of much larger data. Q3 performs a join between a 600GB version of the Mon-goDB collection Orders_Items (containing ~770 million documents and ~3 billion order items) and a generated table CLICKS of size 1TB, containing ~6 billion click log records. To make the CLICKS dataset accessible by both <software>Spark</software> and <software ContextAttributes="used">LeanXcale</software>, it is generated as an HDFS file. The query assumes a use case that aims to find orders of books made on the same day the customers visited the website. The predicate C.IPADDR BETWEEN a AND b filters a range of source IP addresses for the web clicks, which results in selecting click data for a particular subset of user IDs. This selectivity makes significant the impact of using bind join within the native table BookOrders. The definition of the named table is hence slightly modified, to allow for the bind join to apply early filtering to reduce significantly the amount of data processed by the <software ContextAttributes="used">MongoDB</software> JavaScript subquery: The query executes by first applying the filter and retrieving intermediate data from the CLICKS table, where a full scan takes place. The intermediate data are then cached at the workers and a list of distinct values for the UID column is pushed to the <software ContextAttributes="used">MongoDB</software> wrapper instances, to form the bind join condition. We use the parameters a and b to control the selectivity on the large table, hence also the selectivity of the bind join. We ran experiments varying the selectivity factor SF between 0.02%, 0.2%, and 2%. Comparison with <software ContextAttributes="used">Spark SQL</software>. To run an analogue of the BookOrders subquery through the <software ContextAttributes="used">MongoDB</software> connector for <software ContextAttributes="used">Spark SQL</software>, we used the <software ContextAttributes="used">MongoDB</software> aggregation framework against the same sharded collection in our <software ContextAttributes="used">MongoDB</software> cluster as follows:</p><p>db.orders_items.aggregate([{$unwind: "$items"}, {$match: {"items.type": "BOOK"}}, ...])</p><p>Fig. <ref type="figure" target="#fig_17">11</ref> shows the times for processing Q3 queries: with <software ContextAttributes="used">Spark</software> <software ContextAttributes="used">SQL</software>, with <software ContextAttributes="used">LeanXcale</software> without using bind join, and with <software ContextAttributes="used">LeanXcale</software> using bind join. The level of parallelism for both storing and querying data is 512. Without bind join, <software ContextAttributes="used">Spark</software> <software ContextAttributes="used">SQL</software> shows a slight advantage compared to <software ContextAttributes="used">LeanXcale</software> DQE, which is explainable by the overhead of the JavaScript interpreting that takes place at DQE wrappers for <software ContextAttributes="used">MongoDB</software>. Predicate selectivity does not affect significantly the query execution time, as full scans take place on both datasets anyway. Performance benefits are noticeable when using <software ContextAttributes="used">LeanXcale</software> with bind join, where smaller values of the selectivity factor SF result in shorter lists of outer keys for the bind join condition and hence faster execution of the BookOrders subquery.</p><p>The last test case extends the Q3 query by adding another join with the result of the Experts MFR subquery to <software>Spark</software> against the 240GB version of the generated posts log file.</p><p>Thus, Q4 is defined as follows: Using bind join, the query executes as follows: first, the join between CLICKS at HDFS and BookOrders at <software ContextAttributes="used">MongoDB</software> takes place, as in Q3; then, after flattening O.keywords and identifying the list of distinct keywords, another bind join condition is pushed to the Experts subquery to <software ContextAttributes="used">Spark</software>, as described in Section 5.1, to reduce the amount of data processed by <software ContextAttributes="used">Spark</software> transformations. To use the same mechanism for controlling the selectivity of the second join, the keywords for each book item in the Orders_Items <software ContextAttributes="used">MongoDB</software> collection are generated in a way that a selectivity factor SF on the first join results in about the same SF on the second join. Fig. <ref type="figure" target="#fig_0">12</ref> shows the times for processing Q4 queries, involving 2 joins: with <software ContextAttributes="used">Spark</software> <software ContextAttributes="used">SQL</software>, with <software ContextAttributes="used">LeanXcale</software> without using bind join, and with <software ContextAttributes="used">LeanXcale</software> using bind join. Similarly to the previous test case, the performance evaluation shows that the ability for applying bind join that cannot be handled with <software ContextAttributes="used">Spark</software> <software ContextAttributes="used">SQL</software> gives our approach a significant advantage for selective queries. This is very useful in a wide range of industrial scenarios. Fig. <ref type="figure" target="#fig_0">12</ref>. Execution times (in seconds) of Q4 queries joining the result of Q3 queries (1TB <software ContextAttributes="used">LeanXcale</software> table joining 600GB <software ContextAttributes="used">MongoDB</software> collection) with an MFR/<software ContextAttributes="used">Scala</software>/<software ContextAttributes="used">Spark</software> subquery against 240GB HDFS file. The level of parallelism was set to 512, i.e. 512 <software ContextAttributes="used">MongoDB</software> shards, 512 <software ContextAttributes="used">LeanXcale</software> DQE instances, and 512 <software ContextAttributes="used">Spark</software> executors. To assess bind join, SF varied between 0.02%, 0.2%, and 2%; this is applied at both joins.</p></div>
<div><head n="7">Conclusion</head><p>In this paper, we introduced a parallel polystore system that builds on top of <software>LeanXcale</software>'s distributed query engine and processes queries in the <software ContextAttributes="created">CloudMdsQL</software> query language. This allows data store native subqueries to be expressed as inline <software ContextAttributes="created">scripts</software> and combined with regular <software ContextAttributes="created">SQL</software> statements in ad-hoc integration statements. We contribute by adding polyglot capabilities to the distributed data integration engine that takes advantage of the parallel processing capabilities of underlying data stores. We introduced architectural extensions that enable specific native <software ContextAttributes="created">scripts</software> to be handled in parallel at data store shards, so that efficient and scalable parallel joins take place at query engine level. The concept relies on an <software ContextAttributes="created">API</software> that allows its generalization to multiple <software ContextAttributes="created">script engines</software> and data stores. In our work, we focused on parallel joins across a partitioned relational table, the result of a parallel JavaScript subquery to Mon-goDB, and the result of a <software ContextAttributes="created">Spark</software>/<software ContextAttributes="created">Scala</software> <software ContextAttributes="created">script</software> against an HDFS file.</p><p>Our experimental validation demonstrates the scalability of the query engine by measuring the performance of various join queries. In particular, even in the context of <software>LeanXcale</software> DQE with 2 bind joins sophisticated subqueries expressed as JavaScript or <software ContextAttributes="used">Scala</software> <software ContextAttributes="used">code</software>, parallel join processing shows good speedup with increase of the parallelism level. This means that, as the volume of data grows, performance can be maintained by simply extending the parallelism to a proportional number of workers and data shards. This evaluation illustrates the benefits of combining the massive parallelism of the underlying data management technologies with the high expressivity of their scripting frameworks and optimizability through the use of bind join, which is the major strength of our work.</p></div><figure xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. DQE distributed architecture</figDesc></figure>
<figure xml:id="fig_2"><head>5 Fig. 2 .</head><label>52</label><figDesc>Fig. 2. Query processing in parallel mode</figDesc><graphic coords="16,176.98,323.15,244.34,135.40" type="bitmap" /></figure>
<figure xml:id="fig_3"><head>Q1</head><label /><figDesc>ML : LINEITEM( L_ORDERKEY int, … )@mongo = {* return db.lineitem.findSharded( {l_quantity: {$lt: 5}} ); *} SELECT count(*) FROM LINEITEM L, ORDERS O WHERE L_ORDERKEY = O_ORDERKEY</figDesc></figure>
<figure xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Parallel join between sharded datasets: LeanXcale table and MongoDB collection.</figDesc><graphic coords="23,203.08,209.02,192.12,139.95" type="bitmap" /></figure>
<figure xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Parallel join between LeanXcale and HDFS tables.</figDesc><graphic coords="24,202.60,148.90,193.10,136.40" type="bitmap" /></figure>
<figure xml:id="fig_6"><head>Q1Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Parallel join between LeanXcale and Spark.</figDesc><graphic coords="25,209.20,230.25,179.65,153.99" type="bitmap" /></figure>
<figure xml:id="fig_7"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Architecture of the distributed Spark wrapper.</figDesc></figure>
<figure xml:id="fig_8"><head>Fig. 8 .</head><label>8</label><figDesc>Fig.8. Execution times (in seconds) of Q1 queries on TPC-H data with different scales of data (60, 120, and 240 GB) and different levels of parallelism<ref type="bibr" target="#b31">(32,</ref> 64, 128, 256, and 512 workers).</figDesc></figure>
<figure xml:id="fig_10"><head /><label /><figDesc>Q2 M : SELECT count(*) FROM BookOrders Q2 ML : SELECT count(*) FROM BookOrders O, CUSTOMER C WHERE O.CUSTKEY = C.C_CUSTKEY</figDesc></figure>
<figure xml:id="fig_11"><head>Fig. 9 .</head><label>9</label><figDesc>Fig.9. Execution times (in seconds) of Q2 queries on more sophisticated JavaScript MongoDB subqueries with scales of data from 60 to 240 GB and levels of parallelism from 32 to 512.</figDesc></figure>
<figure xml:id="fig_12"><head /><label /><figDesc>Q2 S : SELECT count(*) FROM Experts Q2 SL : SELECT count(*) FROM Experts E, CUSTOMER C WHERE E.expert = C.C_NAME</figDesc></figure>
<figure xml:id="fig_14"><head>Q3:</head><label /><figDesc>SELECT O.CUSTKEY, O.TITLE, C.URL, O.ORDERDATE FROM CLICKS C, BookOrders O WHERE C.UID = O.CUSTKEY AND C.CLICKDATE = O.ORDERDATE AND C.IPADDR BETWEEN a AND b</figDesc></figure>
<figure xml:id="fig_15"><head /><label /><figDesc>BookOrders( ... JOINED ON custkey REFERENCING OUTER AS uids )@mongo = {* return db.orders_items.findSharded( {custkey: {$in: uids}} ) .flatMap( function(doc) {...} ); *}</figDesc></figure>
<figure xml:id="fig_17"><head>Q4:Fig. 11 .</head><label>11</label><figDesc>Fig.11. Execution times (in seconds) of Q3 queries joining an expressive JavaScript MongoDB subquery on a 600GB document collection with a 1TB click logs dataset. The level of parallelism was set to 512, i.e. 512 MongoDB shards, 512 LeanXcale DQE instances, and 512 Spark executors. To assess bind join, SF varied between 0.02%, 0.2%, and 2%.</figDesc></figure>
<figure type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Comparison of parallel polystores.</figDesc><table><row><cell>System</cell></row></table></figure>
<figure type="table" xml:id="tab_1"><head>Query interface Supported data stores Supports docu- ment databases (MongoDB) Supports data processing platforms Cross-plat- form semi joins Exten- sible</head><label /><figDesc /><table><row><cell cols="2">Tightly-coupled systems</cell><cell /><cell /><cell /><cell /><cell /></row><row><cell>Polybase</cell><cell>SQL</cell><cell>RDBMS, HDFS</cell><cell>N</cell><cell>N</cell><cell>N</cell><cell>N</cell></row><row><cell>HadoopDB</cell><cell>SQL-like</cell><cell>RDBMS, HDFS</cell><cell>N</cell><cell>N</cell><cell>N</cell><cell>N</cell></row><row><cell /><cell>(Hive QL)</cell><cell /><cell /><cell /><cell /><cell /></row><row><cell>Odyssey/Miso</cell><cell>SQL</cell><cell>RDBMS, HDFS</cell><cell>N</cell><cell>N</cell><cell>N</cell><cell>N</cell></row><row><cell>JEN</cell><cell>SQL</cell><cell>RDBMS, HDFS</cell><cell>N</cell><cell>N</cell><cell>Zig-zag bloom</cell><cell>N</cell></row><row><cell /><cell /><cell /><cell /><cell /><cell>join</cell><cell /></row><row><cell cols="2">Workflow managers</cell><cell /><cell /><cell /><cell /><cell /></row><row><cell>Musketeer</cell><cell>SQL-like +</cell><cell>RDBMS, HDFS,</cell><cell>Through underly-</cell><cell>Multiple, incl.</cell><cell>N</cell><cell>Y</cell></row><row><cell /><cell>Graph queries;</cell><cell>NoSQL</cell><cell>ing platform, e.g.</cell><cell>Spark</cell><cell /><cell /></row><row><cell /><cell>extensible</cell><cell /><cell>Spark</cell><cell /><cell /><cell /></row><row><cell>RHEEM</cell><cell>RheemLatin</cell><cell>RDBMS, HDFS,</cell><cell>Through underly-</cell><cell>Multiple, incl.</cell><cell>Explicitly</cell><cell>Y</cell></row><row><cell /><cell>(imperative)</cell><cell>NoSQL</cell><cell>ing platform, e.g.</cell><cell>Spark</cell><cell>programmed</cell><cell /></row><row><cell /><cell /><cell /><cell>Spark</cell><cell /><cell /><cell /></row><row><cell>Teradata</cell><cell>SQL-like</cell><cell>RDBMS, HDFS,</cell><cell>Through another</cell><cell>Any, with SQL-</cell><cell>N</cell><cell>Y</cell></row><row><cell>IntelliSphere</cell><cell /><cell>NoSQL</cell><cell>SQL-like polystore</cell><cell>like interface</cell><cell /><cell /></row><row><cell>Hybrid polystores</cell><cell /><cell /><cell /><cell /><cell /><cell /></row><row><cell>SparkSQL</cell><cell>SQL-like</cell><cell>RDBMS, HDFS,</cell><cell>Relational</cell><cell>Spark,</cell><cell>Explicitly</cell><cell>Y</cell></row><row><cell /><cell /><cell>NoSQL</cell><cell>mappings</cell><cell>natively</cell><cell>programmed</cell><cell /></row><row><cell>Presto</cell><cell>SQL-like</cell><cell>RDBMS, HDFS,</cell><cell>Relational</cell><cell>N</cell><cell>N</cell><cell>Y</cell></row><row><cell /><cell /><cell>NoSQL</cell><cell>mappings</cell><cell /><cell /><cell /></row><row><cell>Apache Drill</cell><cell>SQL-like</cell><cell>RDBMS, HDFS,</cell><cell>Relational</cell><cell>N</cell><cell>N</cell><cell>Y</cell></row><row><cell /><cell /><cell>NoSQL</cell><cell>mappings</cell><cell /><cell /><cell /></row><row><cell>Myria</cell><cell>MyriaL (rela-</cell><cell>RDBMS, HDFS,</cell><cell>Relational</cell><cell>N</cell><cell>N</cell><cell>Y</cell></row><row><cell /><cell>tional model)</cell><cell>NoSQL</cell><cell>mappings</cell><cell /><cell /><cell /></row><row><cell>Impala</cell><cell>SQL</cell><cell>RDBMS, HDFS,</cell><cell>Relational</cell><cell>N</cell><cell>N</cell><cell>Y</cell></row><row><cell /><cell /><cell>NoSQL</cell><cell>mappings</cell><cell /><cell /><cell /></row><row><cell>LeanXcale [24]</cell><cell>SQL-like +</cell><cell>RDBMS, HDFS,</cell><cell>Native JavaScript</cell><cell>N</cell><cell>Bind join</cell><cell>Y</cell></row><row><cell /><cell>native queries</cell><cell>NoSQL</cell><cell /><cell /><cell>optimization</cell><cell /></row><row><cell>LeanXcale</cell><cell>SQL-like +</cell><cell>RDBMS, HDFS,</cell><cell>Native JavaScript</cell><cell>Spark, through</cell><cell>Bind join</cell><cell>Y</cell></row><row><cell>[this]</cell><cell>native queries +</cell><cell>NoSQL</cell><cell /><cell>native Scala or</cell><cell>optimization</cell><cell /></row><row><cell /><cell>MFR</cell><cell /><cell /><cell>MFR</cell><cell /><cell /></row></table></figure>
<figure type="table" xml:id="tab_2"><head /><label /><figDesc>The example below wraps such a subquery as a CloudMdsQL named table: And if this table has to be joined with a LeanXcale table named authors, this can be expressed directly in the main SELECT statement of the CloudMdsQL query:</figDesc><table><row><cell>if (i.type == "book")</cell></row><row><cell>r.push({title:i.title, author:i.author,</cell></row><row><cell>keywords:i.keywords});</cell></row><row><cell>} );</cell></row><row><cell>return r; });</cell></row><row><cell>*}</cell></row><row><cell>SELECT B.title, B.author, A.nationality</cell></row><row><cell>FROM BookOrders B, Authors A</cell></row><row><cell>WHERE B.author = A.name</cell></row><row><cell>BookOrders(title string, author string,</cell></row><row><cell>keywords string[])@mongo =</cell></row><row><cell>{*</cell></row><row><cell>return db.orders.find({customer: "ACME"})</cell></row><row><cell>.flatMap( function(v) {</cell></row><row><cell>var r = [];</cell></row><row><cell>v.items.forEach( function(i){</cell></row></table></figure>
<figure type="table" xml:id="tab_6"><head /><label /><figDesc>Now, the named table Experts can be joined to BookOrders, for example the following way:</figDesc><table><row><cell>SELECT B.title, B.author, E.kw, E.expert</cell></row><row><cell>FROM BookOrders B, Experts E</cell></row><row><cell>WHERE E.kw IN B.keywords</cell></row></table></figure>
			<note place="foot" n="1" xml:id="foot_0"><p>http://www.leanxcale.com</p></note>
			<note place="foot" xml:id="foot_1"><p>T1(x int, y int)@rdb = (SELECT x, y FROM A) T2(x int, z array)@mongo = {* return db.A.find( {x: {$lt: 10}}, {x:1, z:1} ); *} SELECT T1.x, T2.z FROM T1, T2 WHERE T1.x = T2.x AND T1.y &lt;= 3</p></note>
			<note place="foot" n="2" xml:id="foot_2"><p>https://livy.apache.org</p></note>
			<note place="foot" n="3" xml:id="foot_3"><p>http://www.grid5000.fr</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgment</head><p>This research has been partially funded by the <rs type="funder">European Union</rs>'s <rs type="programName">Horizon 2020 Programme</rs>, project <rs type="projectName">BigDataStack</rs> (grant <rs type="grantNumber">779747</rs>), project <rs type="projectName">INFINITECH</rs> (grant <rs type="grantNumber">856632</rs>), project <rs type="projectName">PolicyCLOUD</rs> (grant <rs type="grantNumber">870675</rs>), by the <rs type="funder">Madrid Regional Council, FSE and FEDER</rs>, project <rs type="projectName">EDGEDATA</rs> (<rs type="grantNumber">P2018/TCS-4499</rs>), <rs type="projectName">CLOUDDB</rs> project <rs type="grantNumber">TIN2016-80350-P</rs> (<rs type="funder">MINECO/FEDER, UE)</rs>, and industrial doctorate grant for <rs type="person">Pavlos Kranas</rs> (<rs type="grantNumber">IND2017/TIC-7829</rs>).</p><p><rs type="person">Prof. Jose Pereira</rs>, <rs type="person">Ricardo Vilaça</rs>, and <rs type="person">Rui Gonçalves</rs> contributed to this work when they were with <software ContextAttributes="used">LeanXcale</software>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_zAG7KD8">
					<idno type="grant-number">779747</idno>
					<orgName type="project" subtype="full">BigDataStack</orgName>
					<orgName type="program" subtype="full">Horizon 2020 Programme</orgName>
				</org>
				<org type="funded-project" xml:id="_nq8DQ8G">
					<idno type="grant-number">856632</idno>
					<orgName type="project" subtype="full">INFINITECH</orgName>
				</org>
				<org type="funded-project" xml:id="_aqU9Wvx">
					<idno type="grant-number">870675</idno>
					<orgName type="project" subtype="full">PolicyCLOUD</orgName>
				</org>
				<org type="funded-project" xml:id="_yPkQM2w">
					<idno type="grant-number">P2018/TCS-4499</idno>
					<orgName type="project" subtype="full">EDGEDATA</orgName>
				</org>
				<org type="funded-project" xml:id="_PZvRB69">
					<idno type="grant-number">TIN2016-80350-P</idno>
					<orgName type="project" subtype="full">CLOUDDB</orgName>
				</org>
				<org type="funding" xml:id="_csH59vW">
					<idno type="grant-number">IND2017/TIC-7829</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">HadoopDB: an architectural hybrid of MapReduce and DBMS technologies for analytical workloads</title>
		<author>
			<persName><forename type="first">A</forename><surname>Abouzeid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Badja-Pawlikowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Silberschatz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rasin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PVLDB</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="922" to="933" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">RHEEM: enabling cross-platform data processing: may the big data be with you!</title>
		<author>
			<persName><forename type="first">D</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chawla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Contreras-Rojas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Elmagarmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Idris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Kaoudi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kruse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Mansour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ouzzani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Papotti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-A</forename><surname>Quiané-Ruiz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Thirumuruganathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Troudi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. VLDB Endow</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1414" to="1427" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Towards Scalable Hybrid Stores: Constraint-Based Rewriting to the Rescue</title>
		<author>
			<persName><forename type="first">R</forename><surname>Alotaibi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bursztyn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Deutsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Manolescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGMOD</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1660" to="1677" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Apache Drill -Schema-free SQL Query Engine for Hadoop, NoSQL and Cloud Storage</title>
		<ptr target="https://drill.apache.org/" />
		<imprint />
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<ptr target="http://impala.apache.org/" />
		<title level="m">Apache Impala</title>
		<imprint />
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Spark SQL: relational data processing in Spark</title>
		<author>
			<persName><forename type="first">M</forename><surname>Armbrust</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Xin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Huai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bradley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kaftan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Franklin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ghodsi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zaharia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGMOD</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1383" to="1394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Cost Estimation Across Heterogeneous SQL-Based Big Data Infrastructures in Teradata IntelliSphere</title>
		<author>
			<persName><forename type="first">K</forename><surname>Awada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Eltabakh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Al-Kateb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Au</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EDBT</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="534" to="545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Apache Calcite: A Foundational Framework for Optimized Query Processing Over Heterogeneous Data Sources</title>
		<author>
			<persName><forename type="first">E</forename><surname>Begoli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Camacho-Rodriguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hyde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lemire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGMOD</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="221" to="230" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Multistore big data integration with CloudMdsQL</title>
		<author>
			<persName><forename type="first">C</forename><surname>Bondiombouy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kolev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Levchenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Valduriez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transactions on Large-Scale Data and Knowledge-Centered Systems (TLDKS)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="48" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Query Processing in Multistore Systems: an overview</title>
		<author>
			<persName><forename type="first">C</forename><surname>Bondiombouy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Valduriez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. Journal of Cloud Computing</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="309" to="346" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Invisible glue: scalable selftuning multi-stores</title>
		<author>
			<persName><forename type="first">F</forename><surname>Bugiotti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bursztyn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Deutsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Ileana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Manolescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Innovative Data Systems Research (CIDR)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">SCOPE: Easy and Efficient Parallel Processing of Massive Data Sets</title>
		<author>
			<persName><forename type="first">R</forename><surname>Chaiken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Jenkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Larson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ramsey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shakib</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Weaver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PVLDB</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1265" to="1276" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The SPMD model: Past, present and future</title>
		<author>
			<persName><forename type="first">F</forename><surname>Darema</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Recent Advances in Parallel Virtual Machine and Message Passing Interface</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="volume">2131</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Analytics-driven data ingestion and derivation in the AWESOME polystore</title>
		<author>
			<persName><forename type="first">S</forename><surname>Dasgupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Coakley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Big Data</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2555" to="2564" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Split query processing in Polybase</title>
		<author>
			<persName><forename type="first">D</forename><surname>Dewitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Halverson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Nehme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shankar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Aguilar-Saborit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Avanes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Flasza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gramling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGMOD</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1255" to="1266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The BigDAWG polystore system</title>
		<author>
			<persName><forename type="first">J</forename><surname>Duggan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Elmore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Stonebraker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Balazinska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Howe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kepner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Madden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Maier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mattson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zdonik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGMOD Record</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="11" to="16" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The BigDawg polystore system and architecture</title>
		<author>
			<persName><forename type="first">V</forename><surname>Gadepally</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Duggan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Elmore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Haynes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kepner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Madden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mattson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Stonebraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE High Performance Extreme Computing Conference (HPEC)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Musketeer: all for one, one for all in data processing systems</title>
		<author>
			<persName><forename type="first">I</forename><surname>Gog</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schwarzkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Crooks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Grosvenor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Clement</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth European Conference on Computer Systems (EuroSys '15)</title>
		<meeting>the Tenth European Conference on Computer Systems (EuroSys '15)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015. 2015</date>
			<biblScope unit="page" from="1" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Optimizing Queries across Diverse Data Sources</title>
		<author>
			<persName><forename type="first">L</forename><surname>Haas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kossmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Wimmers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Very Large Databases (VLDB)</title>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="276" to="285" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Odyssey: a multi-store system for evolutionary analytics</title>
		<author>
			<persName><forename type="first">H</forename><surname>Hacigümüs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sankaranarayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tatemura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lefevre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Polyzotis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PVLDB</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1180" to="1181" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">System and method for highly scalable decentralized and low contention transactional processing</title>
		<author>
			<persName><forename type="first">R</forename><surname>Jiménez-Peris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Patiño-Martinez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">European Patent #EP2780832, US Patent #US9</title>
		<imprint>
			<biblScope unit="volume">760</biblScope>
			<biblScope unit="page">597</biblScope>
			<date type="published" when="2011">2011. 2011</date>
		</imprint>
	</monogr>
	<note>Filed at USPTO</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Querying web polystores</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zimmermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rebholz-Schuhmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sahay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Big Data</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The Cloud-MdsQL multistore system</title>
		<author>
			<persName><forename type="first">B</forename><surname>Kolev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bondiombouy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Valduriez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jimenez-Peris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGMOD</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2113" to="2116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Parallel Polyglot Query Processing on Heterogeneous Cloud Data Stores with LeanXcale</title>
		<author>
			<persName><forename type="first">B</forename><surname>Kolev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Levchenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Paciti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Valduriez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vilaca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Goncalves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jimenez-Peris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kranas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Big Data</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1756" to="1765" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Benchmarking polystores: the CloudMdsQL experience</title>
		<author>
			<persName><forename type="first">B</forename><surname>Kolev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Levchenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Valduriez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jimenez-Peris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Big Data</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2574" to="2579" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">CloudMd-sQL: querying heterogeneous cloud data stores with a common language</title>
		<author>
			<persName><forename type="first">B</forename><surname>Kolev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Valduriez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bondiombouy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jiménez-Peris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Distributed and Parallel Databases</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="463" to="503" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">RHEEMix in the data jungle: a cost-based optimizer for cross-platform systems</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kruse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Kaoudi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Contreras-Rojas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chawla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Naumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-A</forename><surname>Quiané-Ruiz</surname></persName>
		</author>
		<idno type="DOI">10.1007/s00778-020-00612-x</idno>
		<ptr target="https://doi.org/10.1007/s00778-020-00612-x" />
	</analytic>
	<monogr>
		<title level="j">The VLDB Journal</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">MISO: souping up big data query processing with a multistore system</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lefevre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sankaranarayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hacıgümüs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tatemura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Polyzotis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Carey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGMOD</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1591" to="1602" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Querying combined cloud-based and relational databases</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Minpeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Cloud and Service Computing (CSC)</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="330" to="335" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">The SQL++ semi-structured data model and query language: a capabilities survey of SQL-on-Hadoop, NoSQL and NewSQL databases</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">W</forename><surname>Ong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Papakonstantinou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vernoux</surname></persName>
		</author>
		<idno>CoRR, abs/1405.3631</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Principles of Distributed Database Systems</title>
		<author>
			<persName><forename type="first">T</forename><surname>Özsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Valduriez</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>Springer</publisher>
			<biblScope unit="page">700</biblScope>
		</imprint>
	</monogr>
	<note>4 th ed.</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Presto -Distributed Query Engine for Big Data</title>
		<ptr target="https://prestodb.io/" />
		<imprint />
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Optimizing analytic data flows for multiple execution engines</title>
		<author>
			<persName><forename type="first">A</forename><surname>Simitsis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wilkinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Castellanos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Dayal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGMOD</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="829" to="840" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">One size fits all: an idea whose time has come and gone</title>
		<author>
			<persName><forename type="first">M</forename><surname>Stonebraker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Cetintemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDE</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Hive -A Warehousing Solution Over a Map-Reduce Framework</title>
		<author>
			<persName><forename type="first">A</forename><surname>Thusoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Sarma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Chakka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Anthony</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wyckoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Murthy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PVLDB</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1626" to="1629" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Scaling access to heterogeneous data sources with DISCO</title>
		<author>
			<persName><forename type="first">A</forename><surname>Tomasic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Raschid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Valduriez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. On Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="808" to="823" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<ptr target="http://www.tpc.org/tpch/" />
		<title level="m">TPC-H</title>
		<imprint />
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">The Myria big data management and analytics system and cloud service</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Balazinska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Halperin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Haynes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Howe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hutchison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Moritz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Myers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ortiz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Suciu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Whitaker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Innovative Data Systems Research (CIDR)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Joins for hybrid warehouses: exploiting massive parallelism in hadoop and enterprise data warehouses</title>
		<author>
			<persName><forename type="first">T</forename><surname>Yuanyuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Özcan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gonscalves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Pirahesh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EDBT/ICDT Conf</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="373" to="384" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">SCOPE: Parallel Databases Meet MapReduce</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Bruno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Larson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chaiken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shakib</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PVLDB</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="611" to="636" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>