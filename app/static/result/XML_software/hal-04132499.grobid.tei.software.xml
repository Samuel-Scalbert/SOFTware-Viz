<TEI xmlns="http://www.tei-c.org/ns/1.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xml:space="preserve" xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Effects of Locality and Rule Language on Explanations for Knowledge Graph Embeddings</title>
			</titleStmt>
			<publicationStmt>
				<publisher />
				<availability status="unknown"><licence /></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Luis</forename><surname>Galárraga</surname></persName>
							<email>luis.galarraga@inria.fr</email>
							<affiliation key="aff0">
								<address>
									<settlement>Inria</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Effects of Locality and Rule Language on Explanations for Knowledge Graph Embeddings</title>
					</analytic>
					<monogr>
						<imprint>
							<date />
						</imprint>
					</monogr>
					<idno type="MD5">D316FE980D6B68C8B5FE387E5446608C</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-04-12T14:51+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid" />
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>knowledge graph embeddings, explainable AI</keywords>
			</textClass>
			<abstract>
<div><p>Knowledge graphs (KGs) are key tools in many AI-related tasks such as reasoning or question answering. This has, in turn, propelled research in link prediction in KGs, the task of predicting missing relationships from the available knowledge. Solutions based on KG embeddings have shown promising results in this matter. On the downside, these approaches are usually unable to explain their predictions. While some works have proposed to compute post-hoc rule explanations for embedding-based link predictors, these efforts have mostly resorted to rules with unbounded atoms, e.g., bornIn(x, y) ⇒ residence(x, y), learned on a global scope, i.e., the entire KG. None of these works has considered the impact of rules with bounded atoms such as nationality(x, England) ⇒ speaks(x, English), or the impact of learning from regions of the KG, i.e., local scopes. We therefore study the effects of these factors on the quality of rule-based explanations for embedding-based link predictors. Our results suggest that more specific rules and local scopes can improve the accuracy of the explanations. Moreover, these rules can provide further insights about the inner-workings of KG embeddings for link prediction.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div><head n="1">Introduction</head><p>The continuous advances in information extraction on the Web have given rise to large repositories of machine-friendly statements modeled as knowledge graphs (KGs). These are collections of facts of the form p(s, o) that describe real-world entities, e.g., capital(Italy, Rome). In this formalism, the predicate p in a statement p(s, o) can be seen as a directed labeled edge that connects the subject s to the object o. KGs allow computers to "understand" the real world, and find applications in multiple AI-related tasks such as entity-centric IR, reasoning, question answering, smart assistants, etc. Since KGs usually suffer from incompleteness, a central task in KGs is link prediction, where the goal is to infer new facts from the available knowledge. Link prediction constitutes a fundamental step towards proper knowledge graph completion.</p><p>Approaches for link prediction in KGs abound and fall mainly into two paradigms. On the one hand, symbolic methods <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b18">19]</ref> mine explicit patterns on the graph, e.g., the rule capital(x, y) ⇒ inCountry(y, x), and use those patterns to infer new relationships between entities. On the other hand, approaches based on latent factors <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b33">34]</ref> embed predicates p and entities s, o in a latent space driven by a score function that ranks true facts better than false ones. For example, <software ContextAttributes="used">TransE</software> <ref type="bibr" target="#b3">[4]</ref> learns d-dimensional embeddings (in bold) for predicates and entities such that s + p ≈ o, if p(s, o) holds in reality. <software ContextAttributes="used">TransE</software>'s score function for facts is thens + po l (l = {1, 2}).</p><p>Embedding-based methods have exhibited promising performance for link prediction, however their main downside is that they operate as black boxes: one cannot obtain an explanation of the logic behind a predicted fact p(s, o) from the latent representations of s, p, and o. This has therefore motivated some works on mining rule-based explanations for KG embeddings <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27]</ref>. Those explanations can help us, for instance, verify if the embeddings meet expected reasoning guarantees such as transitivity, i.e., p(x, z) ∧ p(z, y) ⇒ p(x, y), or detect biases in the data. It is known that redundancy in the form of inverse predicates, e.g., hyponym(feline, cat), hypernym(cat, feline) in benchmark datasets, led to over-estimated accuracies for state-of-the-art embedding-based link predictors <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b17">18]</ref>. Had a mechanism to understand that the embeddings mainly captured patterns such as hyponym(x, y) ⇒ hypernym(y, x), this issue could have been detected in advance.</p><p>A limitation of existing explanations for KG embeddings is that they only capture global inference patterns. This is tantamount to mining explanations in the language of unbounded atoms, i.e., rules with no constants in the arguments such as bornIn(x, z) ∧ officialLang(z, y) ⇒ speaks(x, y), that hold globally, that is, on the entire KG. However, such rules cannot express specific entity associations such as nationality(x, USA) ⇒ speaks(x, English), presumably captured by link predictors. On those grounds, Section 4 addresses the following research question (RQ1): what is the impact of specific rules in the quality of the explanations for embedding-based predictors?. Moreover, and in line with existing works in interpretable AI <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24]</ref>, we also study a second research question (RQ2): how does learning explanation rules on specific regions of the KG, i.e., local explanations, impact the quality of the resulting rules?. Before answering these questions, we discuss basic concepts and related work in Section 2, and explain how to compute rule-based explanations for link predictors in Section 3.</p></div>
<div><head n="2">Preliminaries</head></div>
<div><head n="2.1">Background Concepts</head><p>Knowledge Graphs. A knowledge graph K = (V, E, l v , l e ) is a directed labeled graph with sets of vertices V and edges E, where the injective functions l v : V → I, l e : E → P assign labels to the vertices and edges. The sets I and P contain entity and predicate labels. An edge labeled capital departing from a vertex labeled France to a vertex labeled Paris denotes the statement or fact capital(France, Paris). Hence, a KG K ⊂ I × P × I is also a set of facts p(s, o) with subject s, predicate p, and object o. Usually, standard KGs store only facts believed to be true.</p><p>We define the potential set Ω(K) of a KG as the universe of facts that could be constructed from the entities and predicates in K. More formally, Ω p (K) therefore defines the set of all possible facts that could be constructed with the known subjects and objects of predicate p.</p><formula xml:id="formula_0">Ω(K) = D v (K) × D e (K) × D v (K) where D v (K) = {l v (v) : v ∈ V}</formula><p>Horn rules. An atom A is a statement with constant predicate such that its subject and object arguments can be variables v ∈ V with V ∩ I = ∅. If A has only variable arguments, we say A is unbounded, otherwise it is bounded.</p><p>A Horn rule R is a statement of the form B ⇒ H where the body B is a conjunction of atoms 1≤i≤n A i , and H is the head atom. For instance, the rule parent(x, z) ∧ nationality(z, y) ⇒ nationality(x, y) states that parents and children have the same nationality. These rules usually come with scores that quantify their precision. It is common to require atoms in rules to have at least one variable, be transitively connected, and form safe rules, that is, ensure that the head variables occur also in the body. This condition guarantees that the head variables are universally quantified, allowing for concrete predictions via substitutions. A substitution σ : V → I is a partial mapping from variables to constants, such that its application to atoms or rules replaces each variable with its corresponding constant in the mapping. For example, applying the substitution σ = {x → Marie Curie, y → France} to the atom A : nationality(x, y), gives a new atom σ(A) : nationality(Marie Curie, France).</p><p>We say a rule R :</p><formula xml:id="formula_1">B ⇒ H predicts a fact A in a KG K, denoted by R ∧ K A , iff ∃ σ : (∀B ∈ B : σ(B) ∈ K) ∧ σ(H) = A .</formula><p>Put differently a rule predicts a fact A if there exist a substitution σ that (i) maps each atom in the rule's body to a known KG fact, and (ii) maps the head atom to A . If R predicts a statement A and A ∈ K, we say that R predicts A correctly, i.e., the prediction is a known fact, and we use the notation R ∧ K A .</p><p>Link Predictors. A link predictor f : Ω(K) → R is a function that scores the facts in the potential set of a KG, usually assigning higher values to true facts. Link predictors are mostly used to answer queries of the forms p(s, ?) or p(?, o), in other words, queries that ask for the most likely subject or object of a statement given the other two components. Embedding-based link predictors operate on latent representations for entities, predicates, and facts in Ω(K).</p><p>Hence, they actually have the form f = f • h, where f : C k → R<ref type="foot" target="#foot_0">1</ref> is a function defined on a k-dimensional representation for facts, and h : Ω(K) → C k maps facts to k-dimensional vectors. If the semantics of the vector components are not understandable to humans, we say that f is a black box. That is the case for pure embedding-based link predictors such as <software ContextAttributes="used">TransE</software> <ref type="bibr" target="#b3">[4]</ref> or ComplEx <ref type="bibr" target="#b30">[31]</ref>.</p><p>Explanations. An explanation E = R, g for a black-box link predictor f : Ω(K) → R consists of a set R of Horn rules and a function g : R → R that attributes higher scores to rules that "agree" with f . A rule R : B ⇒ H agrees with f , if R predicts a fact A ∈ Ω(K) also predicted by f . This definition assumes the existence of a threshold θ such that f (A) ≥ θ is interpreted as the black box also "thinking" that A is true. Explanations can be of different scope, namely global when they are learned on the potential set Ω p (K) of a predicate p, or local when they are learned on smaller regions of Ω p (K) as explained in Section 3.</p></div>
<div><head n="2.2">Related Work</head><p>Link Prediction. This problem has received a lot of attention in the last 10 years with approaches lying on a spectrum from symbolic methods to embeddingbased techniques. We refer the reader to <ref type="bibr" target="#b13">[14]</ref> for a comprehensive survey. Symbolic techniques learn explicit patterns, e.g., arbitrary subgraphs, paths, association rules, Horn rules, etc., from KGs and use those patterns as features to predict missing links between entities <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b18">19]</ref>. In contrast, the common principle of embedding-based methods is to model entities and predicates as elements in a latent space, where predicates characterize interactions between entity embeddings. Those interactions are modeled as geometrical operations, e.g., translation in <software ContextAttributes="used">TransE</software> <ref type="bibr" target="#b3">[4]</ref> where s + p ≈ o for true facts p(s, o) (s, p, o ∈ R d ), or rotation in <software ContextAttributes="used">RotatE</software> <ref type="bibr" target="#b29">[30]</ref>. More recent methods resort to neural architectures <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b27">28]</ref> that exploit the vicinity of entities in the graph to learn proper latent representations for both entities and predicates. In all cases, a scoring function -implemented by minimizing a loss functionguides the training of the embeddings, which are learned to yield high scores for true facts and low scores for false facts. The latter are obtained by corrupting the true facts in the KG -a task of utter importance for the quality of the embeddings <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b35">36]</ref>.</p><p>Other methods combine the strengths of symbolic patterns and embeddings <ref type="bibr" target="#b5">[6]</ref>. In <ref type="bibr" target="#b16">[17]</ref>, the authors improve the accuracy of different state-of-the-art embeddingbased link predictors by removing those predictions that are not backed up by any of the Horn rules learned on the data. This strategy is complemented with a combined ranking that takes into account the individual rankings given by the rules and the embeddings. Some approaches <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b34">35]</ref> propose iterative algorithms that use rules and embeddings to produce better examples for subsequent training. In contrast, other methods <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b24">25]</ref> instruct the embeddings to comply to explicit reasoning patterns, e.g., transitivity, p(x, z) ∧ p(z, y) ⇒ p(x, y).</p><p>Explaining the Black Box. Unlike symbolic approaches, link predictors based on embeddings are black boxes. Hence, there have been some efforts to explain their logic by mining explicit patterns <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27]</ref> with attribution scores. Among these patterns, Horn rules are the most expressive. The rules are extracted using state-of-the-art rule or path mining algorithms <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16]</ref>, whereas the attribution scores are learned via machine learning, e.g., linear or logistic regression in the spirit of standard explanation techniques such as LIME <ref type="bibr" target="#b23">[24]</ref>. Nevertheless, none of these approaches exploits the power of Horn rules at its best. For instance, <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b26">27]</ref> mine rule explanations of up to two atoms, e.g., bornIn(x, y) ⇒ livesIn(x, y), whereas DistMult <ref type="bibr" target="#b33">[34]</ref> can only learn pure paths such as bornIn(x, z) ∧ inCountry(z, y) ⇒ nationality(x, y). Hence, none of these methods can induce explanations in the language of bounded atoms such as nationality(x, UK) ⇒ speaks(x, English). Furthermore, all these endeavors mine global explanations. Embedding-based models can, though, be very complex and therefore hard to approximate in the general sense. Thus we explore the effects of bounded atoms and locality in the quality of the explanations.</p></div>
<div><head n="3">Explaining KG Embeddings for Link Prediction</head><p>Algorithm 1 describes a generic procedure to compute rule-based explanations for a black-box link predictor f trained on a KG K, containing both true (K + ) and corrupted facts (K -), in line with existing approaches <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27]</ref>. The rules are learned on a context C consisting of facts of a given predicate p. We elaborate on the stages of the algorithm and the different ways to define the context C.</p></div>
<div><head>Algorithm 1: Build Explanation</head><formula xml:id="formula_2">Input: link predictor f : Ω(K) → R trained on K = K + ∪ K -, context C ⊂ Ω p (K), C ∩ K = ∅ Output: an explanation E = R, g with set of rules R, g : R → R 1 K := ∅ 2 foreach A := p(s, o) ∈ C do 3 if f (A) ≥ θ then 4 K := K ∪ {p f (s, o)} 5 else 6 K := K ∪ {¬p f (s, o)} 7 R := rule mining on K ∪ K for predicates p f , ¬p f 8 return build -rule-based -surrogate(R, K, K, f )</formula><p>Binarizing the black box. To learn Horn rules that mimic a black box f , we need to convert f 's scores for facts into true or false verdicts. To this end, lines 2-6 label each fact in the context C by computing f 's score and then applying a threshold to decide whether the fact is deemed true or not by f . This set K of annotated facts is represented by the surrogate predicates p f , ¬p f . Rule Mining. Line 7 in Alg. 1 learns Horn rules of the forms B ⇒ p f (s, o) and B ⇒ ¬p f (s, o) with confidence scores from the original KG K and the black-box annotated context K.</p><p>Learning the explanation. Finally, line 8 uses the rules as features to learn a surrogate model f s : R |R| → R that mimics the binarized f and provides importance scores for the rules in R. Given a statement A = pf (s, o) ∈ K with pf ∈ {p f , ¬p f }, we encode A as a vector x A ∈ R |R| such that its i-th entry is set as follows:</p><formula xml:id="formula_3">x A [i] =      sgn(A) × conf (R i ) R i ∧ (K ∪ K) A -sgn(A) × conf (R i ) R i ∧ (K ∪ K) A with A = A 0 otherwise Here sgn(A) = 1 if A = pf (s, o), otherwise sgn(A) = -1. If a rule R i ∈ R pre-</formula><p>dicts correctly a statement A ∈ K, the i-th component of x A holds a value equals the confidence of R i (reported by the rule mining phase) with the same polarity of f 's prediction. In that case, R i agrees with f and is a potential explanation for f 's answer on A. If R i is a potential explanation for some other fact A , we change the sign of confidence value. In any other case, we assign a score of 0 to the entry. We use the x A vectors and the binarized labels -given by sgn(A)to train a surrogate logistic regression classifier f s , whose coefficients define an attribution mapping g : R → R for rules -our explanation. The surrogate f s can provide both binary labels and probability scores for facts, and its coefficients can be used to rank the rules predicting true and false verdicts p f (s, o), ¬p f (s, o).</p><p>Explanation Context. Existing explanation approaches for KG embeddings <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b26">27]</ref> mine global explanations, where the context C given as input to Alg. 1 contains a large sample of true and false statements. The latter are obtained by corrupting the true facts, so that for each fact p(s, o) we also add {p(s , o), p(s, o )} (s = s , o = o ). The resulting surrogate f s approximates f 's general logic when predicting p-labeled links.</p><p>A drawback of explanations based on global surrogates is that they assume that rules have always the same importance for all p-labeled predictions. Such a simplistic assumption can make explanation mining uninformative, if for example, the black box has a fine-grained behavior, i.e., it implements different logics for different regions of the KG. On those grounds, we propose to mine explanations within a local scope obtained by calling Alg. </p></div>
<div><head n="4">Evaluation</head><p>To answer our research questions, we study the impact of bounded atoms (RQ1) and locality (RQ2) on the fidelity of rule explanations for embedding-based link predictors through a quantitative and an anecdotal evaluation.</p></div>
<div><head n="4.1">Experimental Setup</head><p>Datasets and Link Predictors. We resort to the benchmark datasets fb15k-237, wn18rr, and yago3-10, on which we trained the bilinear methods ComplEx <ref type="bibr" target="#b30">[31]</ref> and HolE <ref type="bibr" target="#b20">[21]</ref>, and the translational approach <software ContextAttributes="used">TransE</software> <ref type="bibr" target="#b3">[4]</ref>. We used the implementations and data offered by the <software ContextAttributes="used">Torch-KGE</software> library <ref type="bibr" target="#b4">[5]</ref>.</p><p>Rule Mining. We mine Horn rules with AMIE <ref type="bibr" target="#b14">[15]</ref>, a state-of-the-art rule miner for large KGs. By default, AMIE mines closed Horn rules<ref type="foot" target="#foot_2">3</ref> of up to 3 unbounded atoms, but it can be instructed to allow bounded atoms. AMIE does not support explicit counter-examples to estimate the precision of rules, as required by Alg. 1, hence we extended the system to support explicit false facts in the precision computation. These counter-examples were generated through a variant of Bernoulli sampling that accounts for predicate domains <ref type="bibr" target="#b32">[33]</ref>. We use all rules making at least 2 correct predictions with a precision of at least 10% to learn the surrogate model (see Section 3).</p><p>Explanations. We compute rule-based explanations for the studied link predictors using the test instances of the experimental datasets to construct contexts C of different scopes, i.e., global, local, and per-instance as explained in Sec. 3. For each call to Alg. 1, we split C into train and test sets C train and C test (30%), so that we learn the explanations on C train and evaluate them on C test . Link predictors are mainly used for two tasks: fact classification (true vs. false) and subject/object prediction for queries p(?, o) and p(s, ?) where potential candidates are ranked by their score. We quantify the fidelity of our surrogate models (their ability to approximate the link predictors) for these two tasks via standard metrics, namely the ROC-AUC score and the mean reciprocal rank (MRR). The threshold θ to binarize f 's scores (line 6 in Alg. 1) is chosen via logistic regression.</p></div>
<div><head n="4.2">Results</head><p>Quantitative Evaluation. Tables <ref type="table" target="#tab_0">1</ref> and<ref type="table" target="#tab_1">2</ref> report the average ROC-AUC and MRR for the different explanation setups, namely unbounded vs. bounded rules  Our baseline setting (denoted by B) are global unbounded rules as mined by existing approaches <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27]</ref>. We highlight that we could not mine explanations with such a setting for wn18RR and yago3-10 on any of the studied link predictors -not even for local or per-instance scopes. This happens because unbounded rules can only be extracted when the training KG contains very prevalent and general regularities in the interactions between the predicates. The datasets wn18RR and yago3-10, however, have much fewer predicates than fb15k-237: 11 and 37 for the former versus 237 for the latter. Bounded atoms also increase the coverage explanation for fb15k-237. While the baseline provides explanations for 18 different predicates for ComplEx on fb15k-237, allowing bounded rules increases the coverage to 58 predicates (HolE and <software ContextAttributes="used">TransE</software> exhibit comparable increases). Moreover the results in Table <ref type="table" target="#tab_0">1</ref> suggest that bounded atoms in rules generally increase fidelity.</p><p>It is important to remark that allowing constants in the rule atoms comes at the expense of many more, potentially noisy, rules. On fb15k-237 with global scopes, for example, the number of unique rules mined from <software>TransE</software> increases from 1k to 193k. That said, only 134k of those rules get non-zero coefficients during the attribution phase -implemented via logistic regression.</p><p>We also observe that rule-based surrogates tend to be better at mimicking the link predictors for object prediction. This is explained by the nature of KG predicates, which are usually defined in a subject-oriented manner, e.g., nationality(J. Biden, USA) and not hasCitizen(USA, J. Biden). This makes subject prediction generally harder to mimic, because, e.g., it is easier to predict the nationalities of J. Biden than to predict all USA citizens. Besides, this phenomenon is corroborated by the actual performances of the link predictors. For instance, ComplEx exhibits an average S-MRR of 0.29 on wn18RR, whereas the average O-MRR reaches 0.46. That said, Table <ref type="table" target="#tab_1">2a</ref> suggests that S-MRR fidelity can still be high even in the presence of subject-oriented predicates.</p><p>When we look at the effects of locality on fidelity, we notice mixed effects. On fb15k-237, locality hurts ROC-AUC performance for unbounded rules and brings moderate performance gains for the MRR. The situation is different for bounded rules, for which locality boosts fidelity in most cases. These results suggest that locality and bounded rules are complementary. A similar behavior can also be observed for coverage. For example, local scopes combined with bounded rules allow mining 507k unique rules (with non-zero attribution) for 130 different predicates for ComplEx on fb15k-237 vs. 112k rules/58 predicates and 1505 rules/62 predicates when only one of the features is enabled (the baseline mines 730 predicates covering 18 predicates). For per-instance scopes we can compute rule explanations for up to 2782 individual facts (out of 20k) covering 83 predicates (HolE on fb15k-237) fb15k-237  Anecdotal Evaluation. Table <ref type="table" target="#tab_2">3</ref> shows a few examples of rule-based explanations for our experimental link predictors. These correspond to some of the best ranked rules according to the coefficients of the surrogate classifiers. The rules illustrate regularities preserved by the link predictors, since the body of the rules defines conditions satisfied by the facts of the KG, in contrast to the head that matches statements predicted by our black boxes (see Alg 1). Rules with bounded atoms offer legible insights about the information that the link predictors may be capturing to make predictions.</p><p>A key observation is that the different link predictors do not seem to rely on the same information -as suggested by rules (1) and ( <ref type="formula">2</ref>) for ComplEx and <software ContextAttributes="used">TransE</software> on fb15k-237. This is supported by the fact that among the 47 predicates for which ComplEx finds global explanations with bounded atoms, only 3 have common rules with <software ContextAttributes="used">TransE</software>. We bring our attention to rule (3), which suggests that embeddings do reproduce the biases in the source data <ref type="foot" target="#foot_3">4</ref> . Recall that fb15k-237 was mainly extracted from Wikipedia, known to have gender biases <ref type="bibr" target="#b31">[32]</ref>. Those biases are easier to spot with rules with bounded atoms, which are a complement to more general explanations such as (4) and <ref type="bibr" target="#b4">(5)</ref>. We also highlight that local contexts can illustrate the semantics captured by the embeddings. This is exemplified by rules ( <ref type="formula">6</ref>) and ( <ref type="formula">7</ref>) that were learned on the same predicate but on two fact clusters. As we can see, our mining routine learned semantically equivalent rules, defined on different thematic domains, namely athletes and fwc; the latter refers to the 2010 FIFA World Cup.</p></div>
<div><head n="5">Conclusion</head><p>We have studied the effects of specific rules with bounded atoms and local scopes on the quality of explanations for embedding-based link predictors on knowledge graphs. Our results suggest a rather positive impact on the explanation fidelity and the coverage of the explanations. Moreover, specific rules and local scopes exhibit a symbiotic relationship.</p><p>Even though rule-based explanations reflect regularities preserved by blackbox link predictors, they do not shed light on causality. In this line of though, we envision to compute causal explanations that help us understand the role of the different entities, predicates, and latent components of KG embeddings in the resulting predictions. We have also planned to elaborate more on the relationship between link prediction performance and explanation fidelity, in particular at the level of the individual predicates. The source code and experimental data of our work is available at https://gitlab.inria.fr/glatour/geebis. Acknowledgment. This research was supported by TAILOR, a project funded by EU Horizon 2020 research and innovation programme under GA No. 952215.</p></div><figure xml:id="fig_0"><head /><label /><figDesc>, D e (K) = {l e (e) : e ∈ E} are the entity and predicate domains of K. Furthermore, we define the potential set of a predicate p as Ω(K) ⊇ Ω p (K) = {p(s, o) : (s, o) ∈ D p (K) × Dp (K)} with D p (K) = {s : ∃o : p(s, o) ∈ K}, Dp (K) = {o : ∃s : p(s, o) ∈ K}.</figDesc></figure>
<figure xml:id="fig_1"><head /><label /><figDesc>1 on different subcontexts C ⊆ C with triples that are close to each other in the latent space. These sub-contexts are obtained by applying clustering on s ⊕ o, i.e., on the latent representation of pairs s, o for true facts p(s, o) ∈ C 2 . We can also define per-instance contexts around a target fact A = p(s, o) by calling Alg. 1 on a sub-context C = {A = p(s , o) : A ∈ C} ∪ {A = p(s, o ) : A ∈ C} ∪ {A}, that is, on statements that share at least one argument with A.</figDesc></figure>
<figure xml:id="fig_2"><head>( 1 ) 10 ( 8 )</head><label>1108</label><figDesc>place_of_birth(x, Chicago) ⇒ nationality(x, USA) [TransE] (2) has_lived_in(x, Brooklyn) ⇒ nationality(x, USA) [ComplEx] (3) profession(x, Author) ⇒ gender(x, M) [ComplEx] (4) impersonates(z, x) ∧ gender(z, y) ⇒ gender(x, y) [ComplEx, HolE] (5) country(z, y) ∧ birth_place(x, z) ⇒ nationality(x, y) (6) company(z, x) ∧ athlete:sport(z, y) ⇒ sport(x, y) • [HolE] (7) fwc:club(z, x) ∧ sport(z, y) ⇒ sport(x, y) • [HolE] yago3affiliation(x, Umeå IK) ⇒ gender(x, F) † [TransE, ComplEx] (9) wonPrize(x, O. Orange-Nassau) ⇒ wonPrize(x, D.S. Medal) [ComplEx] wn18rr (10) meronym_mb(Insecta, x) ⇒ hypernym(x, Animal) † [ComplEx, HolE]</figDesc></figure>
<figure type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Fidelity on fb15k-237. Best performances are in bold; best locality results are underlined. The baseline B are global explanations with unbounded atoms. G, PI, and L stand for global, per-instance, and local explanations.</figDesc><table><row><cell /><cell /><cell /><cell cols="2">ROC-AUC</cell><cell /><cell /><cell /><cell cols="2">S-MRR</cell><cell /><cell /><cell>O-MRR</cell></row><row><cell /><cell /><cell cols="2">Unbounded</cell><cell cols="3">Bounded</cell><cell cols="2">Unbounded</cell><cell cols="3">Bounded</cell><cell>Unbounded</cell><cell>Bounded</cell></row><row><cell /><cell /><cell cols="3">B L PI G</cell><cell>L</cell><cell cols="4">PI B L PI G</cell><cell>L</cell><cell cols="2">PI B</cell><cell>L PI G</cell><cell>L PI</cell></row><row><cell cols="13">complex 0.71 0.68 0.64 0.93 0.93 0.95 0.13 0.16 0.19 0.31 0.35 0.44 0.97 1.00 0.97 0.97 0.98 0.93</cell></row><row><cell cols="13">transe 0.72 0.70 0.64 0.95 0.92 0.95 0.12 0.20 0.19 0.22 0.47 0.45 0.97 0.99 0.98 0.98 0.93 0.91</cell></row><row><cell>hole</cell><cell /><cell cols="11">0.66 0.63 0.60 0.98 0.99 0.99 0.08 0.16 0.22 0.27 0.36 0.50 0.98 0.98 0.97 0.98 1.00 0.97</cell></row><row><cell /><cell cols="3">ROC-AUC</cell><cell>S-MRR</cell><cell /><cell /><cell cols="2">O-MRR</cell><cell cols="2">ROC-AUC</cell><cell /><cell>S-MRR</cell><cell>O-MRR</cell></row><row><cell /><cell>G</cell><cell>L</cell><cell>PI G</cell><cell>L</cell><cell cols="2">PI G</cell><cell>L</cell><cell>PI</cell><cell>G</cell><cell cols="2">L PI G</cell><cell>L PI G</cell><cell>L</cell><cell>PI</cell></row><row><cell cols="9">complex 0.55 0.64 0.68 0.93 0.60 0.38 0.92 0.93 1.00</cell><cell cols="4">0.55 0.75 0.00 0.94 0.39 0.17 0.87 1.00 1.00</cell></row><row><cell cols="9">transe 0.51 0.55 0.69 0.71 0.87 0.32 0.93 0.92 1.00</cell><cell cols="4">0.66 0.63 0.63 0.73 0.50 0.50 0.94 0.97 1.00</cell></row><row><cell>hole</cell><cell cols="8">-0.65 0.73 -0.42 0.38 -1.00 1.00</cell><cell cols="4">0.71 0.81 0.65 0.90 0.38 0.26 0.93 1.00 0.99</cell></row><row><cell /><cell /><cell /><cell cols="2">(a) wn18RR</cell><cell /><cell /><cell /><cell /><cell /><cell /><cell /><cell>(b) yago3-10</cell></row></table></figure>
<figure type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Fidelity of rule-based explanations with bounded atoms.</figDesc><table /><note><p>learned on global (G), local (L), and per-instance (PI) scopes. The scores are computed by averaging the fidelity obtained for each call to Alg. 1 weighted by the size of the corresponding test set, i.e., |C test |. We disaggregate the MRR into S-MRR and O-MRR -when the task is to predict the subject or object given the other two components.</p></note></figure>
<figure type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Some rule explanations. • , † denote local and per-instance explanations.</figDesc><table /></figure>
			<note place="foot" n="1" xml:id="foot_0"><p>Most methods embed the entities in real spaces, i.e., in R k , but a few, e.g.,<ref type="bibr" target="#b30">[31]</ref> resort to vectors of complex numbers in C k .</p></note>
			<note place="foot" n="2" xml:id="foot_1"><p>⊕ denotes concatenation; sub-contexts are corrupted to obtain counter-examples.</p></note>
			<note place="foot" n="3" xml:id="foot_2"><p>These are safe rules where each variable occurs in at least 2 atoms</p></note>
			<note place="foot" n="4" xml:id="foot_3"><p>Rule (8), on the other hand, refers to a women's football team.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">UniKER: A Unified Framework for Combining Embedding and Horn Rules for Knowledge Graph Inference</title>
	</analytic>
	<monogr>
		<title level="m">ICML Workshop on Graph Representation Learning and Beyond (GRL+)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Mining Expressive Rules in Knowledge Graphs</title>
		<author>
			<persName><forename type="first">Naser</forename><surname>Ahmadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Viet-Phi</forename><surname>Huynh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vamsi</forename><surname>Meduri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Ortona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paolo</forename><surname>Papotti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Data and Information Quality</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Realistic Re-Evaluation of Knowledge Graph Completion Methods: An Experimental Study</title>
		<author>
			<persName><forename type="first">Farahnaz</forename><surname>Akrami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammed</forename><surname>Samiul Saeef</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengkai</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGMOD Conference</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Translating Embeddings for Modeling Multi-relational Data</title>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alberto</forename><surname>Garcia-Duran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oksana</forename><surname>Yakhnenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">26</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">TorchKGE: Knowledge Graph Embedding in Python and Py-Torch</title>
		<author>
			<persName><forename type="first">Armand</forename><surname>Boschin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Knowledge Graphs</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Combining Embeddings and Rules for Fact Prediction</title>
		<author>
			<persName><forename type="first">Armand</forename><surname>Boschin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nitisha</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gurami</forename><surname>Keretchashvili</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabian</forename><surname>Suchanek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Research School in AI in Bergen</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
		<respStmt>
			<orgName>Schloss Dagstuhl-Leibniz-Zentrum für Informatik</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Extracting Interpretable Models from Matrix Factorization Models</title>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Sanchez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carmona</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Cognitive Computation</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">ScaLeKB: Scalable Learning and Inference over Large Knowledge Bases</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daisy</forename><forename type="middle">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">VLDB Journal</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">ExCut: Explainable Embedding-Based Clustering over Knowledge Graphs</title>
		<author>
			<persName><forename type="first">Mohamed</forename><forename type="middle">H</forename><surname>Gad-Elrab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daria</forename><surname>Stepanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trung-Kien</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heike</forename><surname>Adel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gerhard</forename><surname>Weikum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Semantic Web Conference</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Knowledge Graph Embedding Preserving Soft Logical Regularity</title>
		<author>
			<persName><forename type="first">Shu</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingshuai</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bingnan</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lihong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haibin</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Knowledge Management</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Rule-Aware Reinforcement Learning for Knowledge Graph Reasoning</title>
		<author>
			<persName><forename type="first">Zhongni</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaolong</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zixuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Long</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL/IJCNLP (Findings)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Rule-Aware Reinforcement Learning for Knowledge Graph Reasoning</title>
		<author>
			<persName><forename type="first">Zhongni</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaolong</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zixuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Long</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL/IJCNLP (Findings)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Improving Knowledge Graph Embeddings with Ontological Reasoning</title>
		<author>
			<persName><forename type="first">Nitisha</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trung-Kien</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohamed</forename><forename type="middle">H</forename><surname>Gad-Elrab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daria</forename><surname>Stepanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Semantic Web Conference</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A Survey on Knowledge Graphs: Representation, Acquisition, and Applications</title>
		<author>
			<persName><forename type="first">Shaoxiong</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shirui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pekka</forename><surname>Marttinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Fast and Exact Rule Mining with AMIE 3</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Lajus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luis</forename><surname>Galárraga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabian</forename><surname>Suchanek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Extended Semantic Web Conference</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Random Walk Inference and Learning in A Large Scale Knowledge Base</title>
		<author>
			<persName><forename type="first">Ni</forename><surname>Lao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Why a Naive Way to Combine Symbolic and Latent Knowledge Base Completion Works Surprisingly Well</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Meilicke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Betz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heiner</forename><surname>Stuckenschmidt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd Conference on Automated Knowledge Base Construction</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Fine-Grained Evaluation of Rule and Embedding-Based Systems for Knowledge Graph Completion</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Meilicke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manuel</forename><surname>Fink</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Ruffinelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rainer</forename><surname>Gemulla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heiner</forename><surname>Stuckenschmidt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Semantic Web Conference</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Discovering Meta-Paths in Large Heterogeneous Information Networks</title>
		<author>
			<persName><forename type="first">Changping</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reynold</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Silviu</forename><surname>Maniu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Senellart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wangda</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Web Conference</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A Novel Embedding Model for Knowledge Base Completion Based on Convolutional Neural Network</title>
		<author>
			<persName><forename type="first">Tu</forename><surname>Dai Quoc Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dat</forename><surname>Dinh Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dinh</forename><surname>Quoc Nguyen</surname></persName>
		</author>
		<author>
			<persName><surname>Phung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Holographic Embeddings of Knowledge Graphs</title>
		<author>
			<persName><forename type="first">Maximilian</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lorenzo</forename><surname>Rosasco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomaso</forename><surname>Poggio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Tensor Factorization for Multi-relational Learning</title>
		<author>
			<persName><forename type="first">Maximilian</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName><surname>Volker Tresp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning and Knowledge Discovery in Databases</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Explanation Mining: Post Hoc Interpretability of Latent Factor Models for Recommendation Systems</title>
		<author>
			<persName><forename type="first">Georgina</forename><surname>Peake</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGKDD Int. Conference on Knowledge Discovery and Data Mining</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Why should I trust you?: Explaining the Predictions of any Classifier</title>
		<author>
			<persName><forename type="first">Marco</forename><surname>Tulio Ribeiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGKDD Int. Conference on Knowledge Discovery and Data Mining</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">End-to-end Differentiable Proving</title>
		<author>
			<persName><forename type="first">Tim</forename><surname>Rocktäschel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Explaining Completions Produced by Embeddings of Knowledge Graphs</title>
		<author>
			<persName><forename type="first">Andrey</forename><surname>Ruschel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Colombini Gusmão</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gustavo</forename><forename type="middle">Padilha</forename><surname>Polleti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fábio</forename><surname>Gagliardi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cozman</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conf. on Symbolic and Quantitative Approaches with Uncertainty</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Towards Extracting Faithful and Descriptive Representations of Latent Variable Models</title>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Sanchez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Rocktaschel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Spring Symposium on Knowledge Representation and Reasoning</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">End-to-End Structure-Aware Convolutional Networks for Knowledge Base Completion</title>
		<author>
			<persName><forename type="first">Chao</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinbo</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Reasoning with Neural Tensor Networks for Knowledge Base Completion</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">RotatE: Knowledge Graph Embedding by Relational Rotation in Complex Space</title>
		<author>
			<persName><forename type="first">Zhiqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhi-Hong</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian-Yun</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Complex Embeddings for Simple Link Prediction</title>
		<author>
			<persName><forename type="first">Théo</forename><surname>Trouillon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Éric</forename><surname>Gaussier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Bouchard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Women through the Glass Ceiling: Gender Asymmetries in Wikipedia</title>
		<author>
			<persName><forename type="first">Claudia</forename><surname>Wagner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eduardo</forename><surname>Graells-Garrido</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Filippo</forename><surname>Menczer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EPJ Data Science</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="1" to="24" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Knowledge Graph Embedding by Translating on Hyperplanes</title>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianwen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianlin</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">28</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Embedding Entities and Relations for Learning and Inference in Knowledge Bases</title>
		<author>
			<persName><forename type="first">Bishan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Wen-Tau Yih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Iteratively Learning Embeddings and Rules for Knowledge Graph Reasoning</title>
		<author>
			<persName><forename type="first">Wen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bibek</forename><surname>Paudel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaoyan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hai</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abraham</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huajun</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Web Conference</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Efficient, Simple and Automated Negative Sampling for Knowledge Graph Embedding</title>
		<author>
			<persName><forename type="first">Yongqi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quanming</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>