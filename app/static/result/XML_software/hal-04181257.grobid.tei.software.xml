<TEI xmlns="http://www.tei-c.org/ns/1.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xml:space="preserve" xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd">
	<teiHeader xml:lang="fr">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Visualiser des explications contrefactuelles pour des données tabulaires</title>
			</titleStmt>
			<publicationStmt>
				<publisher />
				<availability status="unknown"><licence /></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Victor</forename><surname>Guyomard</surname></persName>
							<affiliation key="aff0">
								<address>
									<settlement>Orange, Lannion</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Univ Rennes</orgName>
								<orgName type="institution" key="instit2">Inria</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<orgName type="institution" key="instit4">IRISA</orgName>
								<address>
									<settlement>Rennes</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Inria -Lyon</orgName>
								<address>
									<settlement>Villeurbanne</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">ENSAI -Rennes</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">François</forename><surname>Wallyn</surname></persName>
							<affiliation key="aff0">
								<address>
									<settlement>Orange, Lannion</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Univ Rennes</orgName>
								<orgName type="institution" key="instit2">Inria</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<orgName type="institution" key="instit4">IRISA</orgName>
								<address>
									<settlement>Rennes</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Inria -Lyon</orgName>
								<address>
									<settlement>Villeurbanne</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">ENSAI -Rennes</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Françoise</forename><surname>Fessant</surname></persName>
							<affiliation key="aff0">
								<address>
									<settlement>Orange, Lannion</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Univ Rennes</orgName>
								<orgName type="institution" key="instit2">Inria</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<orgName type="institution" key="instit4">IRISA</orgName>
								<address>
									<settlement>Rennes</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Inria -Lyon</orgName>
								<address>
									<settlement>Villeurbanne</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">ENSAI -Rennes</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Thomas</forename><surname>Guyet</surname></persName>
							<affiliation key="aff0">
								<address>
									<settlement>Orange, Lannion</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Univ Rennes</orgName>
								<orgName type="institution" key="instit2">Inria</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<orgName type="institution" key="instit4">IRISA</orgName>
								<address>
									<settlement>Rennes</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Inria -Lyon</orgName>
								<address>
									<settlement>Villeurbanne</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">ENSAI -Rennes</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tassadit</forename><surname>Bouadi</surname></persName>
							<affiliation key="aff0">
								<address>
									<settlement>Orange, Lannion</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Univ Rennes</orgName>
								<orgName type="institution" key="instit2">Inria</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<orgName type="institution" key="instit4">IRISA</orgName>
								<address>
									<settlement>Rennes</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Inria -Lyon</orgName>
								<address>
									<settlement>Villeurbanne</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">ENSAI -Rennes</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Alexandre</forename><surname>Termier</surname></persName>
							<affiliation key="aff0">
								<address>
									<settlement>Orange, Lannion</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Univ Rennes</orgName>
								<orgName type="institution" key="instit2">Inria</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<orgName type="institution" key="instit4">IRISA</orgName>
								<address>
									<settlement>Rennes</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Inria -Lyon</orgName>
								<address>
									<settlement>Villeurbanne</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">ENSAI -Rennes</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Visualiser des explications contrefactuelles pour des données tabulaires</title>
					</analytic>
					<monogr>
						<imprint>
							<date />
						</imprint>
					</monogr>
					<idno type="MD5">D1C021D7DEC0A6E7C5AA32441DA9C269</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-04-12T14:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid" />
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div><p>Dans cet article, nous présentons un outil de visualisation interactif destiné à la visualisation d'explications contrefactuelles. Une explication contrefactuelle se présente sous la forme d'une version modifiée de l'exemple à expliquer qui répond à la question : que faudrait-il changer pour obtenir une prédiction différente ? Ces explications visent à fournir aux utilisateurs des informations personnalisées et exploitables qui leur permettent de comprendre, et éventuellement contester ou améliorer les décisions automatisées. Les résultats sont affichés dans une interface où les explications contrefactuelles sont mises en évidence. Des méthodes interactives sont également fournies pour que les utilisateurs puissent explorer différentes solutions. Le fonctionnement de l'outil est illustré sur un cas d'usage de rétention client. L'outil est compatible avec n'importe quel générateur d'explications contrefactuelles et modèle de décision.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="fr">
		<body>
<div><head n="1">Introduction</head><p>L'apprentissage automatique est désormais massivement utilisé pour automatiser la prise de décision dans de nombreux domaines, et en particulier dans des domaines qui impactent notre vie quotidienne tels que la santé, le crédit ou encore la justice. Les modèles utilisés sont généralement complexes et opaques. C'est le phénomène de la « boite noire ». L'IA explicable (ou XAI) vise à limiter ce problème en fournissant un ensemble de méthodes pour qu'un utilisateur humain comprenne les facteurs qui ont motivé la décision d'un modèle. L'enjeu de l'explicabilité devient crucial que ce soit pour l'acceptation de l'IA ou le respect des réglementations existantes 1 et à venir 2 . Par exemple, si une personne se voit refuser son crédit, à la suite d'une décision algorithmique, la banque doit être en mesure de lui expliquer les raisons de cette décision. Dans un tel contexte, il pourrait être intéressant de fournir une explication sur ce que cette personne devrait changer pour influencer la décision du modèle.</p><p>Les explications contrefactuelles sont un type d'explication permettant d'expliquer la décision du modèle de prédiction à l'aide d'un exemple, proche de l'exemple à expliquer, qui montre comment celui-ci devrait changer pour que sa prédiction change. L'explication fournit ainsi un retour utile à l'utilisateur qui va pouvoir identifier les différences entre son dossier et un autre, et donc également les actions à mener pour espérer faire changer la décision à l'avenir.</p><p>Un enjeu important réside dans la présentation d'un exemple contrefactuel à un utilisateur. Au delà de la génération de cet exemple contrefactuel, il est nécessaire que sa présentation soit effectivement comprise pour que l'utilisateur sache exploiter cette information.</p><p>Nous proposons dans cette démonstration un outil de visualisation d'explications contrefactuelles pour faciliter le dialogue avec un utilisateur. L'outil est destiné à des utilisateurs non spécialistes des algorithmes d'apprentissage machine. Ce peut être un expert métier ou un utilisateur final impacté par les décisions du modèle de prédiction. Par le biais de l'outil, l'utilisateur accède aux explications et peut interagir avec le système de décision. L'outil est également indépendant de l'algorithme utilisé pour générer les explications contrefactuelles. Cependant, pour illustrer ses différentes fonctionnalités, nous nous sommes appuyés sur VC-Net <ref type="bibr" target="#b4">(Guyomard et al., 2022)</ref> un modèle adapté au traitement de données tabulaires mixtes, capable de fournir simultanément prédiction et explication contrefactuelle.</p></div>
<div><head n="2">Contexte et travaux connexes</head><p>De nombreux travaux récents traitent de l'explicabilité des modèles de décision basés sur l'apprentissage automatique. On renvoie à Molnar (2022) pour une revue des méthodes, des enjeux et des défis du domaine. On peut chercher à expliquer globalement le modèle de décision ou s'intéresser plus spécifiquement à expliquer une décision prise pour un individu en particulier.</p><p>La plupart des travaux existants sur les outils de visualisation pour l'explicabilité s'intéressent à la première catégorie, c'est-à-dire l'explication globale de modèles. Ainsi, What-If <ref type="bibr" target="#b11">(Wexler et al., 2020)</ref> est une interface interactive permettant de visualiser les données, les décisions du modèle et d'explorer différents scénarios en modifiant les caractéristiques des variables. <software ContextAttributes="used">RuleMatrix</software> <ref type="bibr" target="#b8">(Ming et al., 2019)</ref> propose la visualisation interactive d'explications à base de règles. ExplainExplore (Collaris et van Wijk, 2020) quant à lui combine exploration globale et locale avec des approches basées sur l'importance des variables.</p><p>Notre focus est sur l'explication de décisions individuelles (locales) à l'aide d'exemples contrefactuels pour des données tabulaires. <ref type="bibr" target="#b7">Miller (2019)</ref> pense qu'un tel mode d'explication est facilement appréhendé par des utilisateurs non-experts. L'explication contrefactuelle consiste à proposer un changement minimal des valeurs des caractéristiques qui permet à la prédiction de l'instance de changer pour un résultat différent. Cela peut se formaliser comme trouver une perturbation de l'exemple de sorte à changer la décision. Par exemple, trouver la plus petite perturbation des caractéristiques qui changerait la prédiction d'une demande de prêt de rejetée à approuvée. Ce nouvel exemple est appelé exemple contrefactuel ou bien contrefactuel, et le changement associé explication contrefactuelle.</p><p>Il y a encore peu de travaux dédiés à la visualisation des explications individuelles de type contrefactuelles. <ref type="bibr" target="#b1">Gomez et al. (2020)</ref>   <ref type="formula">2022</ref>) ont réalisé une étude utilisateur pour identifier les informations visuelles que ceux-ci estimaient être les plus intéressantes à recevoir dans un contexte d'explications de décisions automatisées. Leur étude portait sur les explications individuelles par importance de variables (obtenues à l'aide de SHAP <ref type="bibr" target="#b5">(Lundberg et Lee, 2017)</ref>). Le cas d'usage évalué concernait la prédiction du prix d'une prime d'assurance par apprentissage supervisé. Les auteurs ont montré que les utilisateurs de l'étude accordaient une forte importance à la mise en contexte et à l'interactivité de l'outil de visualisation. La mise en contexte correspond principalement à une description des variables qui sont utilisées pour la prédiction tandis que l'interaction laisse de la liberté à l'utilisateur pour explorer plus en détail chaque explication.</p><p>Nous nous sommes appuyés sur ces différents travaux pour spécifier les différentes fonctionnalités de notre outil interactif de visualisation d'explications contrefactuelles.</p><p>3 Description de l'outil L'objectif principal de l'outil proposé est de fournir une représentation visuelle intuitive des explications contrefactuelles fournies par un algorithme d'explicabilité (ici l'algorithme VCNet). Plus précisément notre objectif est de montrer, pour une instance donnée, 1) quelles caractéristiques doivent être modifiées pour que la décision du modèle change, 2) quelle doit être l'amplitude du changement et 3) de permettre l'exploration de solutions alternatives.</p></div>
<div><head n="3.1">Génération des explications contrefactuelles</head><p>La plupart des méthodes d'explications contrefactuelles sont basées sur la perturbation de l'instance originale grâce à l'optimisation d'une fonction de coût <ref type="bibr" target="#b10">Wachter et al. (2018)</ref>. Selon les propriétés souhaitées pour l'explication on rajoute des contraintes dans le processus d'optimisation sous la forme de termes supplémentaires dans la fonction de coût. Par exemple, on peut souhaiter un contrefactuel le plus proche possible de l'exemple à expliquer, avec le moins de variables perturbées possible, actionnable (où seules certaines variables peuvent être perturbées) ou encore réaliste. Pour une revue récente de ces approches, on peut se reporter à <ref type="bibr" target="#b3">Guidotti (2022)</ref>.</p><p>L'algorithme de génération d'explications contrefactuelles que nous avons utilisé dans le cadre de cet article est décrit dans <ref type="bibr" target="#b4">Guyomard et al. (2022)</ref>. L'originalité du modèle (VCNet) est qu'il apprend simultanément à prédire et à générer une explication associée à la prédiction. Un des intérêts de l'approche est qu'elle assure un meilleur alignement entre la prédiction et l'explication, et ainsi la génération de contrefactuels valides (au sens qu'ils ont bien une classe différente de la classe de l'exemple). Un autre intérêt réside dans le temps de génération des contrefactuels. Contrairement aux approches post-hoc l'explication est ici générée de façon immédiate une fois le modèle entraîné. De plus, VCNet est un modèle à base de réseaux de neurones de type autoencodeur conditionnel variationnel permettant la génération de contrefactuels réalistes.</p></div>
<div><head n="3.2">Description de l'interface</head><p>La figure 1 illustre la présentation d'une explication, pour une instance donnée, pour un cas d'usage de désabonnement client (appelé chrun). Le problème de décision auquel on s'intéresse est un problème de classification à deux classes (churn/non churn). Une instance est décrite par 20 variables. On trouve différentes informations sur la partie haute de l'interface concernant l'exemple et sa prédiction. La partie centrale de l'interface est dédiée aux informations relatives aux valeurs des variables : la valeur actuelle pour l'exemple et la valeur proposée pour le contrefactuel. La partie basse de l'interface est dédiée à la traduction sous forme textuelle de l'explication. Un code couleur permet l'identification de chacune des classes (ici orange pour un churner, et vert pour un non churner). Plus précisément :</p><p>-En 1 ⃝ on trouve les informations concernant la classe prédite par le modèle de décision pour l'individu à expliquer (ici le client est étiqueté comme churner) ainsi que la probabilité avec laquelle le client a été prédit dans la classe (69%).</p><p>-En 2 ⃝ on trouve les informations concernant la classe prédite pour le contrefactuel correspondant et la probabilité associée (prédiction de churn à 21% i.e. non churn à 79%). On observe que la classe du contrefactuel est, comme attendu, bien différente de celle de l'individu à expliquer. ⃝ ou la nouvelle modalité dans le cas d'une variable catégorielle 6 ⃝. Le code couleur associé aux variables correspond à celui de la classe du contrefactuel (vert pour un non churner ici).</p><p>-Les différents changements de variables sont résumés sous forme textuelle 8</p><p>⃝. -Une information supplémentaire concernant l'erreur de classification de l'exemple par le modèle de décision est fournie (si elle est disponible) sous la forme d'un code graphique particulier 9 ⃝. On raye le rond 1 ⃝ correspondant à l'exemple quand il a été mal classé par le modèle de décision. L'outil dispose de plusieurs autre fonctionnalités accessibles par navigation à partir de la page principale de l'interface :</p><p>-L'utilisateur peut interagir avec l'interface et demander à sélectionner un autre contrefactuel 7 ⃝. Il est alors redirigé vers une page illustrée dans la figure 2. Plusieurs contrefactuels lui sont proposés et il peut choisir un contrefactuel selon les critères qu'il souhaite privilégier : parcimonie (le moins de changement de variables possibles) ou performance de prédiction (le score le plus faible prédit pour la classe de l'exemple). Le contrefactuel proposé par défaut est le contrefactuel qui nécessite le moins de changements. On rappelle en partie haute de l'interface, les informations liées à l'exemple en cours d'analyse. -Une page d'accueil qui rappelle les caractéristiques principales de l'algorithme de génération de contrefactuels, et donne une description des données analysées (caractéristiques et sémantique des variables).</p></div>
<div><head n="3.3">Implémentation</head><p>Pour la réalisation de notre outil de visualisation, nous avons utilisé une application <software>Flask</software>, qui est un micro-framework de développement web en Python permettant de présenter les données et d'afficher les pages web. Les visualisations et interactions sont créées grâce à Ja-vaScript et d3js. Nous utilisons également HTML et CSS pour créer les pages web. L'interface est compatible avec n'importe quel modèle de prédiction, ainsi qu'avec n'importe quel générateur d'explication contrefactuelle. Les données nécessaires à la visualisation sont fournies via un fichier JSON. Ce fichier doit contenir :</p><p>-les noms des variables, -une matrice variables/instances, contenant les instances à expliquer et une autre contenant les contrefactuels, -les probabilités de prédiction du modèle ainsi que les classes prédites, pour les instances à expliquer ainsi que pour les contrefactuels. lité de churn de 69% à 49%, un second qui propose la modification de 4 variables ramène la probabilité de churn à 36% (suppression de plusieurs services et modification de la méthode de paiement), etc. L'expert métier a ainsi la possibilité de choisir le critère qui lui parait le meilleur entre parcimonie et score de classification. Une autre utilisation possible de notre outil, dans un contexte métier, consisterait à observer de manière préventive les clients non churners et leurs contrefactuels. L'objectif étant de détecter une évolution dans les variables qui indiqueraient que le client est sur le point de churner.</p></div>
<div><head n="5">Conclusion et évolutions futures</head><p>Cet article a présenté un outil de visualisation d'explications contrefactuelles. Pour chaque instance à analyser, on présente, via une interface graphique les variables descriptives qui ont été modifiées (ainsi que comment elles on été modifiées) pour obtenir le contrefactuel et faire changer la décision du modèle. L'utilisateur a la possibilité d'interagir avec l'interface pour explorer des contrefactuels alternatifs. Cet outil est dédié pour l'instant à des utilisateurs de type expert métier ou utilisateur destinataire de la décision. Il est compatible avec tout type de modèle de décision et générateur d'explications contrefactuelles. L'outil a été illustré sur un cas d'usage de rétention client.</p><p>Le travail présenté ici est une première étape dans l'objectif de doter les utilisateurs d'outils de visualisations simples et intuitifs pour l'explicabilité des modèles d'intelligence artificielle. L'outil pourrait à terme s'enrichir de différentes fonctionnalités. Par exemple, pour l'instant les interactions avec l'utilisateur sont limitées au choix d'un contrefactuel dans un ensemble possible selon des critères de parcimonie ou de performance de classification. L'utilisateur pourrait également être intéressé par une sélection des variables qui composent le contrefactuel. Un autre axe d'amélioration concerne la formalisation textuelle de l'explication qui est pour l'instant très limitée. Un travail sur l'ergonomie de l'interface serait également d'intérêt, ainsi qu'une étude utilisateur. </p></div>
<div><head>Références</head></div><figure xml:id="fig_0"><head /><label /><figDesc>ont proposé ViCE, un outil qui permet de générer les explications contrefactuelles et de les visualiser dans le cadre de la classification d'octroi de crédit. ViCE ne traite que les variables numériques. Une extension dédiée à l'explication globale a été proposée récemment Gomez et al. (2021). Avec DECE, Cheng et al. (2021) auto-risent l'analyse exploratoire des décisions du modèle au niveau des instances mais également au niveau d'un groupe d'instances. Enfin, récemment, Garcia-Zanabria et al. (2022) ont proposé SDA-Vis un outil de visualisation d'explications contrefactuelles dans un contexte d'aide à l'analyse du décrochage scolaire. Bove et al. (</figDesc></figure>
<figure xml:id="fig_1"><head /><label /><figDesc>-Le camembert en 3 ⃝ présente un résumé des changements entre l'exemple et son contrefactuel. Il indique la proportion de variables modifiées. Le camembert est interactif. En cliquant sur celui ci on peut naviguer entre les variables modifiées par le contrefactuel et celles qui sont restées inchangées. -Les exemples à analyser sont sélectionnés individuellement grâce au menu déroulant 4 ⃝ à l'aide de leur identifiant. -La partie centrale du graphique s'intéresse aux variables de l'exemple qui ont fait l'objet d'une modification afin d'obtenir le contrefactuel. Dans l'exemple présenté, 7 variables ont été modifiées, chacune étant identifiée par son label 5 ⃝, 6 ⃝. Une flèche associée à chaque variable indique le sens et l'amplitude du changement dans le cas d'une variable numérique 5</figDesc></figure>
<figure xml:id="fig_2"><head>FIG. 1 -</head><label>1</label><figDesc>FIG. 1 -Interface de présentation d'un exemple à expliquer et d'un contrefactuel associé.</figDesc><graphic coords="5,112.42,150.81,370.43,264.98" type="bitmap" /></figure>
<figure xml:id="fig_3"><head /><label /><figDesc>FIG. 2 -Interface de sélection d'un contrefactuel alternatif selon les axes à privilégier (parcimonie/score de classification).</figDesc><graphic coords="6,147.97,150.81,299.34,228.95" type="bitmap" /></figure>
<figure xml:id="fig_4"><head /><label /><figDesc>Bove, C., J. Aigrain, M.-J.Lesot, C. Tijus, et M. Detyniecki (2022). Contextualization and exploration of local feature importance explanations to improve understanding and satisfaction of non-expert users. In Proceedings of the 27th International Conference on Intelligent User Interfaces (IUI), pp. 807-819. Association for Computing Machinery. Cheng, F., Y. Ming, et H. Qu (2021). DECE : Decision explorer with counterfactual explanations for machine learning models. Transactions on Visualization and Computer Graphics 27(2), 1438-1447. Collaris, D. et J. J. van Wijk (2020). ExplainExplore : Visual exploration of machine learning explanations. In Proceedings of the Pacific Visualization Symposium (PacificVis), pp. 26-35. IEEE.</figDesc></figure>
			<note place="foot" n="4" xml:id="foot_0"><p>Étude d'un cas d'usageNous illustrons l'outil sur un exemple de prédiction de la résiliation de clients d'un opérateur télécom. Le jeu de données utilisé, Telco Customer Churn 3 , comprend 7 043 clients décrits par vingt variables (informations personnelles, services souscrits, type de contrat), dont la résiliation (oui/non). Il s'agit donc d'un problème de classification binaire sur des données tabulaires. Les données ont été découpé en 60% des exemples pour l'apprentissage du modèle, 20% pour la validation, et 20% pour le test.On discute ici l'analyse de l'exemple présenté Figure1. L'exemple correspond à un individu (Id 1682) qui a été étiqueté par le modèle de décision comme churner avec une probabilité de 69%. Le contrefactuel proposé pour l'exemple fait changer la classe de l'exemple de churner à non churner avec une probabilité de non churn de 79% (probabilité de churn à 21%). 7 variables de l'exemple initial ont été modifiées pour obtenir le contrefactuel (37% des variables). En termes d'analyse métier, les modification consistent en un changement de mode d'accès internet (passer de la fibre à l'ADSL), la suppression de certains services (streamingTV), la diminution de la facture mensuelle de 89.5$ à 62.4$, la souscription à un service de protection et de support technique. La figure2indique que 4 contrefactuels alternatifs sont disponibles. Un premier contrefactuel, qui propose la modification de 2 variables (diminution de la facture mensuelle de 89.5$ à 77.25$ et modification de la méthode de paiement), ramène la probabi-3. https://www.kaggle.com/datasets/blastchar/telco-customer-churn</p></note>
		</body>
		<back>
			<div type="annex">
<div><head>Summary</head><p>In this paper we present an interactive visual analytics tool that exibits counterfactual explanations to evaluate model decisions. Each sample is assessed to identify the set of changes needed to flip the model's output. These explanations aim to provide end-users with personalized actionable insights with which to understand automated decisions. The functionality of the tool is demonstrated by its application to a customer retention dataset.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">SDA-Vis : A visualization system for student dropout analysis based on counterfactual exploration</title>
		<author>
			<persName><forename type="first">G</forename><surname>Garcia-Zanabria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Gutierrez-Pachas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Camara-Chavez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Poco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Gomez-Nieto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Sciences</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page">5785</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">ViCE : Visual counterfactual explanations for machine learning models</title>
		<author>
			<persName><forename type="first">O</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Holter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Bertini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Conference on Intelligent User Interfaces (IUI)</title>
		<meeting>the 25th International Conference on Intelligent User Interfaces (IUI)</meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="531" to="535" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">AdViCE : Aggregated visual counterfactual explanations for machine learning model validation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Holter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Bertini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Visualization Conference (VIS)</title>
		<meeting>the Visualization Conference (VIS)</meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="31" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Counterfactual explanations and how to find them : literature review and benchmarking</title>
		<author>
			<persName><forename type="first">R</forename><surname>Guidotti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Mining and Knowledge Discovery</title>
		<imprint>
			<biblScope unit="page" from="1" to="55" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">VCNet : A self-explaining model for realistic counterfactual generation</title>
		<author>
			<persName><forename type="first">V</forename><surname>Guyomard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Fessant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Guyet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (ECML/PKDD)</title>
		<meeting>the European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (ECML/PKDD)</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A unified approach to interpreting model predictions</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Lundberg</surname></persName>
		</author>
		<author>
			<persName><surname>Et S.-I</surname></persName>
		</author>
		<author>
			<persName><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">I</forename><surname>Guyon</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">U</forename><forename type="middle">V</forename><surname>Luxburg</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title />
		<author>
			<persName><forename type="first">Curran</forename><surname>Associates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Inc</forename></persName>
		</author>
		<imprint />
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Explanation in artificial intelligence : Insights from the social sciences</title>
		<author>
			<persName><forename type="first">T</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">267</biblScope>
			<biblScope unit="page" from="1" to="38" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">RuleMatrix : Visualizing and understanding classifiers with rules</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ming</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Bertini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="342" to="352" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Interpretable Machine Learning</title>
		<author>
			<persName><forename type="first">C</forename><surname>Molnar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note>2 ed</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Counterfactual explanations without opening the black box : Automated decisions and the GDPR</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wachter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mittelstadt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Russell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Harvard journal of law &amp; technology</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="841" to="887" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The What-If Tool : Interactive probing of machine learning models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wexler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pushkarna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Bolukbasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wattenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Viégas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wilson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="56" to="65" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>