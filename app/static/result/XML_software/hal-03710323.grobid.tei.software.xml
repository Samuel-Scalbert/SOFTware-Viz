<TEI xmlns="http://www.tei-c.org/ns/1.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xml:space="preserve" xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">PEM360: A dataset of 360°videos with continuous Physiological measurements, subjective Emotional ratings and Motion traces</title>
				<funder ref="#_cSAsdgs #_uuVR3Mv">
					<orgName type="full">Agence Nationale de la Recherche</orgName>
					<orgName type="abbreviated">ANR</orgName>
				</funder>
				<funder>
					<orgName type="full">French government</orgName>
				</funder>
				<funder ref="#_m8Bnggk">
					<orgName type="full">EU Horizon</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher />
				<availability status="unknown"><licence /></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Quentin</forename><surname>Guimard</surname></persName>
							<email>quentin.guimard@univ-cotedazur.fr</email>
						</author>
						<author>
							<persName><forename type="first">Florent</forename><surname>Robert</surname></persName>
							<email>florent.robert@inria.fr</email>
						</author>
						<author>
							<persName><forename type="first">Camille</forename><surname>Bauce</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Aldric</forename><surname>Ducreux</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Hui-Yin</forename><surname>Wu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Marco</forename><surname>Winckler</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Auriane</forename><surname>Gros</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Robert</forename><surname>Guimard</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Université Côte d'Azur</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<orgName type="institution" key="instit3">I3S</orgName>
								<address>
									<settlement>Sophia-Antipolis</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Université Côte d'Azur</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<orgName type="institution" key="instit3">Inria</orgName>
								<address>
									<settlement>I3S Sophia-Antipolis</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution" key="instit1">Université Côte d'Azur</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<orgName type="institution" key="instit3">I3S</orgName>
								<address>
									<settlement>Sophia Antipolis</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution" key="instit1">Université Côte d'Azur</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<orgName type="institution" key="instit3">I3S</orgName>
								<address>
									<settlement>Sophia Antipolis</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution" key="instit1">Lucile Sassatelli Université Côte d'Azur</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<orgName type="institution" key="instit3">I3S</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="institution">Institut Universitaire de France Sophia-Antipolis</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff6">
								<orgName type="institution" key="instit1">Université Côte d'Azur</orgName>
								<orgName type="institution" key="instit2">Inria Sophia-Antipolis</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff7">
								<orgName type="institution" key="instit1">Université Côte d'Azur</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<orgName type="institution" key="instit3">Inria</orgName>
								<address>
									<settlement>I3S Sophia-Antipolis</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff8">
								<orgName type="institution" key="instit1">Université Côte d'Azur</orgName>
								<orgName type="institution" key="instit2">CHU de Nice</orgName>
								<address>
									<settlement>CoBTeK Nice</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">PEM360: A dataset of 360°videos with continuous Physiological measurements, subjective Emotional ratings and Motion traces</title>
					</analytic>
					<monogr>
						<imprint>
							<date />
						</imprint>
					</monogr>
					<idno type="MD5">4AAB9A7C98483A9CB1F7C6C563DEE425</idno>
					<idno type="DOI">10.1145/3524273.3532895</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-04-12T14:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid" />
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Human-centered computing → Virtual reality; User studies 360°videos</term>
					<term>user experiment</term>
					<term>emotions</term>
					<term>physiological data</term>
				</keywords>
			</textClass>
			<abstract>
<div><p>From a user perspective, immersive content can elicit more intense emotions than flat-screen presentations. From a system perspective, efficient storage and distribution remain challenging, and must consider user attention. Understanding the connection between user attention, user emotions and immersive content is therefore key. In this article, we present a new dataset, PEM360 of user head movements and gaze recordings in 360°videos, along with self-reported emotional ratings of valence and arousal, and continuous physiological measurement of electrodermal activity and heart rate. The stimuli are selected to enable the spatiotemporal analysis of the connection between content, user motion and emotion. We describe and provide a set of software tools to process the various data modalities, and introduce a joint instantaneous visualization of user attention and emotion we name Emotional maps. We exemplify new types of analyses the PEM360 dataset can enable. The entire data and code are made available in a reproducible framework.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div><head n="1">INTRODUCTION</head><p>Immersive media is rising as new types of multimedia experiences are becoming more accessible over a wide spectrum of applications, e.g., virtual reality (VR) for games, training and rehabilitation, immersive 360°videos for journalism and new forms of storytelling, and advanced interaction in the Metaverse 1 . Increased accessibility is driven by affordability of VR equipment, and intense effort to design efficient distribution methods in constrained network conditions. Both the design and the efficient distribution of immersive experiences remain challenges, however, for two main reasons. First, it has been shown that immersive content can elicit more intense emotions than flat-screen presentation <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b18">19]</ref>. Second, compression and streaming decisions must be driven by quality of experience (QoE) models <ref type="bibr" target="#b6">[7]</ref>. These models are dependent on user attention prediction. For example, Xu et al. <ref type="bibr" target="#b24">[25]</ref> considered content saliency and field of view (FoV) preferences to extend the PSNR and SSIM metrics to 360°content. It is therefore key to understand the attentional and emotional processes of users in an immersive environment, and how these processes are associated with the content.</p><p>To enable the study of this connection between user attention, user emotions and immersive visual content, this article introduces:</p><p>• PEM360, a new dataset of user head movements and gaze recordings in 360°videos, along with self-reported emotional ratings of valence and arousal, and continuous physiological measurement of electrodermal activity and heart rate. The stimuli are selected based on high-level and low-level content saliency to enable the spatiotemporal analysis of the connection between content, user motion and emotion. • a set of software tools to pre-process the data of gaze, electrodermal activity and content, and to visualize jointly instantaneous heat maps of gaze and arousal level superimposed on the frame, which we name Emotional maps. • a preliminary analysis validating the data and verifying known results, and examples of new connections that can be investigated. The entire collection of artifacts is presented as Python <software ContextAttributes="used">tools</software> and notebooks to enable reproducibility of the data processing. The dataset and tools are now available in a public <software ContextAttributes="used">GitLab</software> repository<ref type="foot" target="#foot_0">2</ref> .</p></div>
<div><head n="2">EXISTING DATASETS</head><p>While a number of works in the domain of human-computer interaction and cognitive sciences have studied emotions in immersive environments <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b23">24]</ref>, it has been only more recently that the multimedia community in particular has considered sensing and recording emotions along with head and gaze motion <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27]</ref>. Human emotions are commonly decomposed along two main dimensions: valence, representing the negative or positive nature of an emotion (unpleasant-pleasant), and arousal, representing the intensity of the perceived emotion (calm-excited) <ref type="bibr" target="#b2">[3]</ref>.</p><p>Li et al. <ref type="bibr" target="#b15">[16]</ref> introduced the first reference database obtained from 95 users freely watching 73 videos in 360°who provided their valence and arousal ratings after every clip using the self-assessment manikin (SAM) tool <ref type="bibr" target="#b4">[5]</ref>. Their head positions were recorded. The material publicly available however only consists of the videos and the average valence and arousal value pair for each video (averaged over all users). Subsequently, Tang et al. <ref type="bibr" target="#b19">[20]</ref> presented an experiment where 19 users watched 36 images in 360°while their self-reported emotions and eye motion were collected.</p><p>More recently, it has been shown that the capability of a single rating issued after experiencing the 360°content is limited and unable to fully represent the variations of user state <ref type="bibr" target="#b23">[24]</ref>. This is why new tools have been introduced to enable the continuous collection of self-reports inside the immersive environment <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b26">27]</ref>. The data collected by Toet et al. <ref type="bibr" target="#b21">[22]</ref> and Xue et al. <ref type="bibr" target="#b26">[27]</ref> also comprise physiological measurements of heart rate and electrodermal activity (EDA, as skin conductance), which has been shown to reliably represent user instantaneous arousal <ref type="bibr" target="#b3">[4]</ref>. Toet et al. <ref type="bibr" target="#b21">[22]</ref> presented a new emotions rating tool, named <software ContextAttributes="used">EmojiGrid</software>, tested on 40 users viewing 62 videos from the reference database of Li et al. <ref type="bibr" target="#b15">[16]</ref>. While they provide the per-user per-video valence and arousal ratings, only time averages are made available for EDA, and no gaze or head motion traces. Xue et al. <ref type="bibr" target="#b26">[27]</ref> introduced a continuous grading tool of valence and arousal. They provide a dataset of 11 immersive videos from the same database <ref type="bibr" target="#b15">[16]</ref> experienced by 32 users. Subjective emotional ratings, physiological measurements (including EDA) and head and gaze movements are continuously collected and made available. This latter work <ref type="bibr" target="#b26">[27]</ref> is closer to ours and has been made partly concurrently. Our dataset is however complementary and enables other types of studies. We provide EDA streams at a higher rate, acquired at 16Hz, compared to 4Hz in <ref type="bibr" target="#b26">[27]</ref>. In the aforementioned objective of understanding the connection between attended regions and instantaneous emotions, like arousal, it is important to enable the detection of several peaks of the phasic component of EDA per second, requiring hence a higher acquisition rate. Also, we sample seven video stimuli from the same reference database <ref type="bibr" target="#b15">[16]</ref> for our experiments, so that specific criteria on saliency are met, as detailed in Sec. 3.1. Out of the seven videos, six differ from the videos selected by Xue et al. <ref type="bibr" target="#b26">[27]</ref>. Our dataset therefore enriches the existing datasets and enables extensive analysis to gain new insights on the connection between attention, emotion and content.</p></div>
<div><head n="3">USER EXPERIMENT AND DATASET DESCRIPTION</head><p>We conducted a controlled, indoor laboratory experiment where users watched 360°videos in a VR headset. We collected eye movement (EM), head movement (HM), heart rate (HR) and skin conductance (EDA) data as well as emotion annotations of valence and arousal. The user experiment has been approved by the university ethics committee.</p></div>
<div><head n="3.1">Stimuli</head><p>The videos are selected to enable several levels of content analysis and description, to correlate with user motion and emotion. User attention in relation with the visual content is described with saliency maps, obtained either from gaze locations, or estimated from the content. Here we consider two levels of content description as two types of saliency maps, and select the videos so that for each, the overlap between both saliency maps is limited. Specifically, we consider low-level (LL) and high-level (HL) saliency. Low-level saliency maps are made up of a combination of colors, intensity and orientations as defined by Itti et al. <ref type="bibr" target="#b10">[11]</ref>. Since we are dealing with videos and not images, we combine this definition with the one of optical flow <ref type="bibr" target="#b9">[10]</ref>, because we also consider motion in the video to be part of the low-level saliency. High-level saliency maps are composed of high-level semantic features, such as faces, cars, or animals. Inspiring from Chopra et al. <ref type="bibr" target="#b7">[8]</ref>, high-level saliency is obtained from YOLOv4 object detector, with object bounding boxes being used as binary saliency maps. We selected 7 videos from the reference database of Li et al. <ref type="bibr" target="#b15">[16]</ref>. The selected videos should have a range of valence and arousal as wide as possible of level of valence and arousal, and the LL saliency should be evenly distributed both within and outside object bounding boxes characterizing HL saliency. To select these videos we compare (i) the number of pixels inside and outside objects, and (ii) the perpixel LL saliency (ranging between 0 and 255), computed as the total LL saliency inside and outside objects normalized with the corresponding number of pixels. Fig. <ref type="figure" target="#fig_1">1</ref> demonstrates this in videos 13 and 73. The number of pixels with such minimum LL saliency inside and outside objects is equivalent over time, as is the per-pixel LL saliency in both areas. Fig. <ref type="figure" target="#fig_2">2</ref> shows a frame where regions with high LL saliency can be seen outside of the detected objects.    </p></div>
<div><head n="3.2">Equipment</head><p>Recordings of head and eye movements have been made with a FOVE headset, equipped with an eye-tracker with a 120Hz acquisition rate, and tethered to a desktop computer. A Unity3D scene was used with a 360°sphere object to display the videos. We use the FOVE Unity plugin to record head and gaze positions.</p><p>Recordings of EDA and optical pulse have been made with a Shimmer3 GSR+ sensor with a frequency range of 15.9Hz and 51.2Hz, respectively. All of the measurements were resampled to 100Hz for analysis. The apparatus is depicted in Fig. <ref type="figure" target="#fig_3">3</ref>. </p></div>
<div><head n="3.3">Participants</head><p>The experiment was carried out with a total of 34 users, in which 31 had complete data (10 women, 20 men, 1 non-binary; 18-29 years old, M=24, SD=3.26). 19 of them had a normal vision, 9 had corrected to normal vision and 3 did not have a normal vision. Most of them played games but rarely or never in VR, and the majority have seen only one or two 360°videos before the experiment. Participants received monetary compensation for their time. The seven videos were experienced by all 31 users for their entire duration (60 to 135 seconds, see Table <ref type="table" target="#tab_0">1</ref>).</p></div>
<div><head n="3.4">Procedure</head><p>The lab experiment started with a pre-questionnaire assessing the user's background with VR and checking for visual deficiencies. Eye tracking calibration was done using the FOVE software for each user before beginning the experiment to make sure the eye tracking data is properly recorded. The VR experiment systematically started with a low-arousal (relaxing) video (ID 32) to bring EDA and HR levels to a user-relative baseline. The remaining six VR videos were then experienced in a random order by every user. Users were in standing position during the experience and could freely explore in 360°while holding the back of a chair to maintain balance and orientation. The videos were played without audio. After each viewing, the headset was removed and the SAM scale presented for arousal and valence rating. At least a 1-min break outside of the headset was observed between videos.</p></div>
<div><head n="3.5">Dataset structure</head><p>The resulting dataset PEM360 is provided with the structure shown in Fig. <ref type="figure" target="#fig_4">4</ref>. The raw_data folder contains 34 folders, one for each user. User folders contain a Shimmer CSV file containing the EDA and optical pulse data recorded over all the 360°videos experienced by the user, and seven CSV files, one per video, containing the gaze and head motion data recorded during the corresponding video.</p><p>Entries in the CSV files include system timestamps to synchronize the data modalities for analysis.</p><p>Valence and arousal ratings of each user for each video are stored in the root folder under graded_valence_arousal.csv. Finally, the root folder PEM360 also contains the Python Jupyter notebook providing the software tools described in Sec. 4, and the entire data processing workflow to reproduce the analysis presented in Sec. 5. </p></div>
<div><head n="4">PRE-PROCESSING SOFTWARE</head><p>Along with the data, we provide a Jupyter notebook to reproduce the entire processing of head and gaze data, EDA, ratings of valence and arousal, and the code to produce saliency maps from the content.</p></div>
<div><head n="4.1">Processing gaze data</head><p>For both HM and EM, 3D positions are logged in Cartesian coordinates (x, y, z) ∈ R 3 . We provide functions:</p><p>• to convert the positions from Cartesian to Eulerian (ϕ, θ,ψ ) denoting respectively yaw, pitch and roll,</p><p>• to obtain speed and acceleration over yaw and pitch,</p><p>• to obtain global speed and acceleration by computing the derivatives of the orthodromic distance,</p><p>• to represent rotational motion with quaternions (hence enabling to compute non-linear motion on the sphere as changes in quaternion rotational axis).</p></div>
<div><head n="4.2">Processing EDA data</head><p>The EDA signal is the raw measurement of skin electrical conductance in micro-Siemens (µS). Two main components can be distinguished in an EDA signal <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b5">6]</ref>: the tonic level, also called skin conductance level (SCL), varies slowly and represents slow autonomic changes that may not be associated with stimulus presentation; and the phasic level, which represents faster changes in EDA, and can better reflect the impact of successive stimuli. Raw EDA, phasic and tonic components are shown in Fig. <ref type="figure" target="#fig_5">5</ref>-top and 5-center. We use the Python toolbox Neurokit <ref type="bibr" target="#b16">[17]</ref> to process EDA data, which uses the cvxEDA method to extract the phasic component. Finally, the physiological arousal to be analyzed in connection with experimental stimuli can be assessed from several metrics on the phasic level, such as peak frequency, duration and amplitude. This is called the skin conductance response (SCR), and can be defined in several ways. In our code, we choose to compute instantaneous SCR as the absolute value of the first-order time derivative of the phasic component, shown in 5-bottom. Note however that the code can easily be modified to implement other definitions of SCR from the phasic component. The obtained SCR is therefore a time series for every user-video pair. This enables analysis with SCR averaged over time for each such pair (as often done), or on a time-dependent basis. </p></div>
<div><head n="4.3">Processing video content</head><p>As introduced in Sec. 3.1, we use LL and HL saliency models designed for regular flat images. We therefore apply them on FoV projections of the entire frame. We first uniformly sample 100 points on the unit sphere and project them on the equirectangular frame using the <software ContextAttributes="used">equirectangular-toolbox</software> <ref type="bibr" target="#b17">[18]</ref>. Each "patch" is made of a projection centered on one of these points, it is a 512x512 image corresponding to a 108°x108°FoV. These patches can overlap each other and are separately given to the appropriate models for both LL and HL saliency. For LL saliency, we use a Python implementation of Itti's saliency map <ref type="bibr" target="#b12">[13]</ref>, which also allows the combination of Itti saliency with the optical flow between consecutive frames, which we do by using separate extractors for each patch. For HL saliency, we use the <software ContextAttributes="used">TensorFlow</software> 2 implementation of YOLOv4 <ref type="bibr" target="#b20">[21]</ref>. For each patch given to the YOLO model, we create a binary saliency map equal to 1 inside the bounding boxes of the detected objects. For both LL and HL saliency, the overlapping patches are back-projected by addition onto the equirectangular frame to obtain a single (LL or HL) saliency map per frame. For LL saliency, the back-projection is normalized by dividing the value of each pixel by the number of patches it belongs to. The final value of a given pixel is the average over all existing projections for this pixel. For HL saliency, the back-projection is normalized by clipping the value of each pixel between 0 and 1. The final value of a given pixel is the maximum over all existing projections for this pixel. Finally, the saliency maps are downscaled by a factor of 5 both horizontally and vertically (from 1920x1080 to 384x216) for storage space reasons. The LL saliency is downscaled using average pooling over blocks of 5x5 pixels, whereas HL saliency is downscaled using max pooling over blocks of the same size. The files are stored in HDF5 format and can be accessed from a link given in the article's repository mentioned in Sec. 1, but can also be re-computed from the provided code. As previously discussed, the stimuli choice and experimental procedure are designed to collect data enabling a time-dependent analysis of the connection between attention, emotion and content. That is why we provide a tool for the experimenter to play the 360°v ideo and visualize the instantaneous gaze locations and arousal (SCR) of a given user from the recorded data. This tool implements a new way of visualizing arousal in connection with gaze, which we name emotional maps. An emotional map is a 4D-array represented as a frame where:</p></div>
<div><head n="4.4">Instantaneous visualization of gaze and emotions: Emotional maps</head><p>• pixel luminance reflects the time the user spent attending the area over a past window of T seconds. A Gaussian kernel of parameter σ is convolved with every gaze location, and accumulated over the sliding window of T seconds. A bright (resp. dark) area can therefore reflect a fixation (resp. a saccade). • pixel color represents the user's SCR, from blue (low arousal) to red (high arousal). Emotional maps generated from a record with our tool are accumulated into videos. Each point persists on the video for P seconds, creating a trail to easily visualize the gaze path and arousal changes. An example of such a video frame is shown in Fig. <ref type="figure" target="#fig_6">6</ref> 3 . The <software ContextAttributes="used">script</software> compute_emotional_map<software ContextAttributes="used">.py</software> creates the emotional maps and blends them with the frames to produce the resulting video visualization from records of gaze and EDA data. We believe this 3 Demonstration of a resulting video is accessible at https://tinyurl.com/25vjwk2s. tool can lead to important qualitative insights for diverse disciplines (including neuroscience) on the connection between visual attention and emotion.</p></div>
<div><head n="5">PRELIMINARY ANALYSIS OF THE DATA</head><p>In this section we first verify the validity of our data and correspondence with the original dataset and between arousal and EDA. We then exemplify possible analyses of correlation between motion and emotion, and between attention, content saliency and emotion.</p></div>
<div><head n="5.1">Data validation</head><p>Reliability of the collected ratings. We verify the reliability of the collected arousal and valence by assessing the similarity of the user ratings for each video. This is achieved with the intra-class correlation coefficient (ICC), with classes corresponding to the 360°v ideos. ICC estimates based on mean ratings with a two-way mixed effects model are 0.96 (95% CI 0.87-0.99) for arousal and 0.88 (95% CI 0.72-0.98) for valence. According to Koo and Li's guidelines <ref type="bibr" target="#b13">[14]</ref>, this is excellent and good inter-rater agreement, respectively.</p><p>Agreement between collected ratings and original dataset. Fig. <ref type="figure" target="#fig_8">7</ref> shows the valence and arousal ratings of our users as a boxplot for each video, along with a red dot representing the corresponding average values available in the original dataset <ref type="bibr" target="#b15">[16]</ref>. We observe the good agreement between both sets, as the latter are all the times but one in the inter-quartile value range of our data. We also compute the median of the root square difference of averages of our valence and arousal ratings with the corresponding averages from <ref type="bibr" target="#b15">[16]</ref>. This median is 1.17 (within a range of 1 to 9), showing the agreement between both.  </p></div>
<div><head n="5.2">Connecting EDA with graded arousal</head><p>We investigate the correspondence between SCR and arousal ratings. We gather the average SCR values SCR u,v for every pair (u, v) of user u and video v, and corresponding graded arousal GA u,v . First, we average both variables over all users for every video, and obtain seven sample pairs (GA v , SCR v ), shown in Fig. <ref type="figure" target="#fig_10">8</ref>-left. We verify as did Toet et al. <ref type="bibr" target="#b21">[22]</ref> that the video ranking according to mean graded arousal is similar to the video ranking according to mean SCR. We also compute the Spearman correlation coefficient (CC) between GA v and SCR v for all seven videos. The Spearman CC between mean graded arousal and mean SCR is (0.92, p = 0.003). According to <ref type="bibr">[12, appx. 6C, p.</ref> 79], such level of correlation is significant (α = 0.05, β = 0.2) from 7 samples (see <ref type="bibr" target="#b22">[23]</ref>).</p><p>We then consider the 217 sample pairs (GA u,v , SCR u,v ). It is interesting to observe that the Pearson or Spearman CCs do not show any correlation between these pairs. Looking more closely at the data, we identify that the mean level of SCR per user, SCR u = E v [SCR u,v ] (averaged over all videos), varies significantly over the users (M = 6.0e -4, S = 6.2e -4). With the rationale that the excitability of a user is person-dependent and impacts the absolute SCR values, we verify whether the SCR variations relative to this individual's mean are better associated with graded arousal. To do so, we define centered SCR as cSCR u,v = SCR u,v -SCR u , and do the same with graded arousal cGA u,v = GA u,v -GA u . Fig. <ref type="figure" target="#fig_10">8</ref>-right represents the scatter plot of cSCR u,v against cGA u,v . The Spearman CC between both is (0.25, p &lt; 0.001).  Left: Scatter plot of SCR v against GA v . The shaded area represents the 95% CI of the linear regressor (solid blue line). Right: Scatter plot of cSCR u,v against cGA u,v .</p></div>
<div><head n="5.3">Analysis of correlations between attention, emotion and content</head><p>This section exemplifies exploratory analysis of correlations between attention (with head or gaze movements), emotion (with valence, arousal or SCR) and content (described with HL or LL saliency). We first show examples of correlation between head motion and arousal, as already partly observed in other works <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b25">26]</ref>. With the same centering process as above, we center the absolute pitch value. A mean absolute pitch at 0 means the user constantly keeps their head in the equatorial position. The centered absolute pitch value for pair (u, v) therefore indicates how much user u deviates in video v from their average slant. We can show that the Pearson CC between centered absolute head pitch and centered graded arousal is (0.42, p &lt; 0.001). Also, the Pearson CC between centered head speed (in rad/s) and centered graded arousal is (0.26, p &lt; 0.001). These results are examples of associations that can then be investigated more ahead (looking at the confounding, mediating or interacting factors for example).</p><p>Second, we exemplify how content description with HL and LL saliency can be leveraged to investigate the association between visual attention, content and emotion. We compare how well both types of saliency maps match the users' fixations over every frame of the 360°video. To do so, we compute the normalized scanpath saliency (NSS), that measures the amount of saliency around fixations <ref type="bibr" target="#b14">[15]</ref>. We consider segments of 5 sec. to average the saliency maps of all frames in this interval, and aggregate the user's fixations in this interval, hence obtaining an NSS value for both saliency types NSS H L u,v ,i and NSS LL u,v ,i for every user u, video v, interval i. The averages over intervals are denoted NSS u,v .</p><p>To study the relationship between both types of saliency and the user's arousal, we consider in Fig. <ref type="figure" target="#fig_12">9</ref> the difference NSS Dif f u,v = NSS H L u,v -NSS LL u,v plotted against mean-centered skin conductance response cSCR u,v (left) and graded arousal GA u,v (right) for all u, v, the points being colored per video. The major finding is the increasing trend of NSS Dif f with EDA and graded arousal. Specifically, the Pearson CC between NSS Dif f and EDA cSCR is (0.25, p &lt; 0.001), and the Pearson CC between NSS Dif f and graded arousal GA u,v is (0.41, p &lt; 0.001). There is therefore a moderate significant correlation between NSS Dif f u,v and cSCR u,v <ref type="bibr" target="#b0">[1]</ref>, meaning that HL saliency is more predictive of the attention in higher arousal states.  </p></div>
<div><head n="6">CONCLUSION</head><p>In this article, we have presented the new PEM360 dataset of 360°v ideos with continuous physiological measurements, subjective emotional ratings and user motion traces. The stimuli are selected to enable investigating the spatiotemporal connection between user attention, user emotions and visual content. We have described the data collection process, the pre-processing workflow of the different data modalities, and exemplified some possible novel types of analyses to demonstrate the potential insights that can be drawn from PEM360. The artifacts are made available in a reproducible framework based on notebooks.</p></div><figure xml:id="fig_0"><head /><label /><figDesc>sec.) Inside object(s) Outside object(s)</figDesc></figure>
<figure xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: HL and LL saliency characterization of video 13 (left) and video 73 (right). Top: number of pixels inside and outside objects. Bottom: average LL saliency per pixel inside and outside objects.</figDesc></figure>
<figure xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: HL and LL saliency visualization for frame 2145 of video 13 (top) and frame 3630 of video 73 (bottom). Left: the frame. Center: HL saliency (detected objects, human on top, animals at bottom). Right: LL saliency.</figDesc></figure>
<figure xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Shimmer3 GSR+ used to record EDA and optical pulse. Gray wires connect the EDA sensor, white wire connects the pulse sensor.</figDesc><graphic coords="4,353.04,165.83,170.08,90.14" type="bitmap" /></figure>
<figure xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Folder structure of the dataset with main files.</figDesc><graphic coords="5,53.80,172.43,240.24,146.32" type="bitmap" /></figure>
<figure xml:id="fig_5"><head>SCRFigure 5 :</head><label>5</label><figDesc>Figure 5: EDA signal recorded for user 03 while watching video 73. The three graphs from the top show the raw EDA data and the tonic component, the phasic component and the SCR (absolute value of phasic first derivative).</figDesc></figure>
<figure xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Emotional map visualizing instantaneous gaze locations (luminance) and user arousal (from blue to red for low to high SCR). Example with high arousal in a roller-coaster video.</figDesc><graphic coords="6,65.81,287.37,216.20,93.52" type="bitmap" /></figure>
<figure xml:id="fig_8"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Arousal and valence ratings by users for each videos. The green dotted line corresponds to the mean and the orange solid line to the median.</figDesc></figure>
<figure xml:id="fig_10"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Dots colors code for video ID (legend on the right). Left: Scatter plot of SCR v against GA v . The shaded area represents the 95% CI of the linear regressor (solid blue line). Right: Scatter plot of cSCR u,v against cGA u,v .</figDesc></figure>
<figure xml:id="fig_12"><head>Figure 9</head><label>9</label><figDesc>Figure 9: NSS Dif f u,v against cSCR u,v and GA u,v for all users u and videos , v. The black line shows a linear regression model fitted on the data.</figDesc></figure>
<figure type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc /><table><row><cell>12 7</cell><cell>4.6</cell><cell>5</cell><cell>103</cell><cell>98</cell><cell>T-aOVE22lEw</cell></row><row><cell>13 4.92</cell><cell>4.08</cell><cell>4</cell><cell>131</cell><cell>127</cell><cell>GJGfxfGEa9Y</cell></row><row><cell>17 5.22</cell><cell>5</cell><cell>5</cell><cell>69</cell><cell>64</cell><cell>g7btxyIbQQ0</cell></row><row><cell>23 7.2</cell><cell>3.2</cell><cell>8</cell><cell>143</cell><cell>135</cell><cell>CDfsFuDuHds</cell></row><row><cell>27 6</cell><cell>1.6</cell><cell>60</cell><cell>180</cell><cell>120</cell><cell>QxxXu_B-ZA</cell></row><row><cell>73 6.27</cell><cell>6.18</cell><cell>9</cell><cell>70</cell><cell>61</cell><cell>bUiP-iGN6oI</cell></row><row><cell>32 6.57</cell><cell>1.57</cell><cell>40</cell><cell>130</cell><cell>90</cell><cell>-bIrUYM-GjU</cell></row></table><note><p><p>lists the video details.</p>ID Valence Arousal Start (s) End (s) Duration (s) YouTubeID</p></note></figure>
<figure type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc /><table /><note><p>Details of selected videos. Videos are accessible at youtube.com/watch?v=[YouTubeID].</p></note></figure>
			<note place="foot" n="2" xml:id="foot_0"><p>https://gitlab.com/PEM360/PEM360/</p></note>
			<note place="foot" xml:id="foot_1"><p>MMSys '22, June 14-17, 2022, Athlone, Ireland Guimard, Robert et al.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGMENTS</head><p>This work has been partly supported by the <rs type="funder">French government</rs>, through the <rs type="funder">UCA</rs> <rs type="projectName">JEDI</rs> and <rs type="programName">EUR DS4H Investments in the Future</rs> projects <rs type="grantNumber">ANR-15-IDEX-0001</rs> and <rs type="grantNumber">ANR-17-EURE-0004</rs>. This work was partly supported by <rs type="funder">EU Horizon</rs> <rs type="programName">2020</rs> project <rs type="projectName">AI4Media</rs>, under contract no. <rs type="grantNumber">951911</rs> (https://ai4media.eu/).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_cSAsdgs">
					<idno type="grant-number">ANR-15-IDEX-0001</idno>
					<orgName type="project" subtype="full">JEDI</orgName>
					<orgName type="program" subtype="full">EUR DS4H Investments in the Future</orgName>
				</org>
				<org type="funding" xml:id="_uuVR3Mv">
					<idno type="grant-number">ANR-17-EURE-0004</idno>
				</org>
				<org type="funded-project" xml:id="_m8Bnggk">
					<idno type="grant-number">951911</idno>
					<orgName type="project" subtype="full">AI4Media</orgName>
					<orgName type="program" subtype="full">2020</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">User's guide to correlation coefficients</title>
		<author>
			<persName><forename type="first">Haldun</forename><surname>Akoglu</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.tjem.2018.08.001</idno>
		<ptr target="https://doi.org/10.1016/j.tjem.2018.08.001" />
	</analytic>
	<monogr>
		<title level="j">Turkish Journal of Emergency Medicine</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="91" to="93" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Presence and Emotions in Virtual Environments: The Influence of Stereoscopy</title>
		<author>
			<persName><forename type="first">Rosa</forename><surname>María Baños</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cristina</forename><surname>Botella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Isabel</forename><surname>Rubió</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soledad</forename><surname>Quero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Azucena</forename><surname>García-Palacios</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mariano</forename><surname>Luis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alcañiz</forename><surname>Raya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">multimedia and virtual reality on behavior and society</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="8" />
			<date type="published" when="2008">2008. 2008</date>
		</imprint>
	</monogr>
	<note>Cyberpsychology &amp; behavior : the impact of the Internet</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Discrete emotions or dimensions? The role of valence focus and arousal focus</title>
		<author>
			<persName><forename type="first">Lisa</forename><surname>Feldman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barrett</forename></persName>
		</author>
		<idno type="DOI">10.1080/026999398379574</idno>
		<ptr target="https://doi.org/10.1080/026999398379574Place:UnitedKingdomPublisher" />
	</analytic>
	<monogr>
		<title level="j">Cognition and Emotion</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="579" to="599" />
			<date type="published" when="1998">1998. 1998</date>
			<publisher>Taylor &amp; Francis</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">Wolfram</forename><surname>Boucsein</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-1-4614-1126-0</idno>
		<ptr target="https://doi.org/10.1007/978-1-4614-1126-0Pages:xviii,618" />
		<title level="m">Electrodermal activity</title>
		<meeting><address><addrLine>New York, NY, US</addrLine></address></meeting>
		<imprint>
			<publisher>Springer Science + Business Media</publisher>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note>2nd ed</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Measuring emotion: The selfassessment manikin and the semantic differential</title>
		<author>
			<persName><forename type="first">Margaret</forename><forename type="middle">M</forename><surname>Bradley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Lang</surname></persName>
		</author>
		<idno type="DOI">10.1016/0005-7916(94)90063-9</idno>
		<ptr target="https://doi.org/10.1016/0005-7916(94)90063-9" />
	</analytic>
	<monogr>
		<title level="j">Journal of Behavior Therapy and Experimental Psychiatry</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="49" to="59" />
			<date type="published" when="1994">1994. 1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Guide for Analysing Electrodermal Activity &amp; Skin Conductance Responses for Psychological Experiments</title>
		<author>
			<persName><forename type="first">Jason</forename><forename type="middle">J</forename><surname>Braithwaite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diana</forename><surname>Patrícia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zethelius</forename><surname>Watson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roland</forename><forename type="middle">S G</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">A</forename><surname>Rowe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
	<note type="report_type">CTIT technical reports series</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">Kjell</forename><surname>Brunnström</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergio</forename><forename type="middle">Ariel</forename><surname>Beker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katrien</forename><surname>De Moor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ann</forename><surname>Dooms</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Egger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marie-Neige</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tobias</forename><surname>Hossfeld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Satu</forename><surname>Jumisko-Pyykkö</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Keimel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohamed-Chaker</forename><surname>Larabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bob</forename><surname>Lawlor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><forename type="middle">Le</forename><surname>Callet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Möller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manuela</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Perkis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jesenka</forename><surname>Pibernik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Raake</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Reichl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ulrich</forename><surname>Reiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raimund</forename><surname>Schatz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Schelkens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lea</forename><surname>Skorin-Kapov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominik</forename><surname>Strohmeier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Timmerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Varela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ina</forename><surname>Wechsung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junyong</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrej</forename><surname>Zgank</surname></persName>
		</author>
		<ptr target="https://hal.archives-ouvertes.fr/hal-00977812" />
		<title level="m">Qualinet White Paper on Definitions of Quality of Experience</title>
		<meeting><address><addrLine>Novi Sad</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-03-12">2013. March 12, 2013</date>
		</imprint>
	</monogr>
	<note>the fifth Qualinet meeting</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">PARIMA: Viewport Adaptive 360-Degree Video Streaming</title>
		<author>
			<persName><forename type="first">Lovish</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarthak</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhijit</forename><surname>Mondal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandip</forename><surname>Chakraborty</surname></persName>
		</author>
		<idno type="DOI">10.1145/3442381.3450070</idno>
		<ptr target="https://doi.org/10.1145/3442381.3450070" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Web Conference 2021</title>
		<meeting>the Web Conference 2021</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2379" to="2391" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Is Virtual Reality Emotionally Arousing? Investigating Five Emotion Inducing Virtual Park Scenarios</title>
		<author>
			<persName><forename type="first">Anna</forename><surname>Felnhofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oswald</forename><forename type="middle">D</forename><surname>Kothgassner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mareike</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna-Katharina</forename><surname>Heinzle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leon</forename><surname>Beutl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Helmut</forename><surname>Hlavacs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilse</forename><surname>Kryspin-Exner</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.ijhcs.2015.05.004</idno>
		<ptr target="https://doi.org/10.1016/j.ijhcs.2015.05.004" />
	</analytic>
	<monogr>
		<title level="j">Int. J. Hum.-Comput. Stud</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="page" from="48" to="56" />
			<date type="published" when="2015-10">2015. oct 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Determining optical flow</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">P</forename><surname>Berthold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><forename type="middle">G</forename><surname>Horn</surname></persName>
		</author>
		<author>
			<persName><surname>Schunck</surname></persName>
		</author>
		<idno type="DOI">10.1016/0004-3702(81)90024-2</idno>
		<ptr target="https://doi.org/10.1016/0004-3702(81)90024-2" />
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="185" to="203" />
			<date type="published" when="1981">1981. 1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A model of saliency-based visual attention for rapid scene analysis</title>
		<author>
			<persName><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Niebur</surname></persName>
		</author>
		<idno type="DOI">10.1109/34.730558</idno>
		<ptr target="https://doi.org/10.1109/34.730558" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1254" to="1259" />
			<date type="published" when="1998">1998. 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Designing Clinical Research: an Epidemiologic Approach, 2nd Ed</title>
		<author>
			<persName><forename type="first">Jeffrey</forename><forename type="middle">J</forename><surname>Walline</surname></persName>
		</author>
		<ptr target="https://journals.lww.com/optvissci/Fulltext/2001/08000/Designing_Clinical_Research__an_Epidemiologic.5.aspx" />
	</analytic>
	<monogr>
		<title level="j">Optometry and Vision Science</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="page">8</biblScope>
			<date type="published" when="2001">2001. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">Akisato</forename><surname>Kimura</surname></persName>
		</author>
		<ptr target="https://github.com/akisatok/pySaliencyMap" />
		<title level="m">pySaliencyMap</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A Guideline of Selecting and Reporting Intraclass Correlation Coefficients for Reliability Research</title>
		<author>
			<persName><forename type="first">K</forename><surname>Terry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mae</forename><forename type="middle">Y</forename><surname>Koo</surname></persName>
		</author>
		<author>
			<persName><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jcm.2016.02.012</idno>
		<ptr target="https://doi.org/10.1016/j.jcm.2016.02.012Edition:2016/03/31Publisher" />
	</analytic>
	<monogr>
		<title level="j">Journal of chiropractic medicine</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="155" to="163" />
			<date type="published" when="2016-06">2016. June 2016</date>
			<publisher>Elsevier</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Methods for comparing scanpaths and saliency maps: strengths and weaknesses</title>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meur</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Thierry</forename><surname>Baccino</surname></persName>
		</author>
		<idno type="DOI">10.3758/s13428-012-0226-9</idno>
		<ptr target="https://doi.org/10.3758/s13428-012-0226-9" />
	</analytic>
	<monogr>
		<title level="j">Behavior Research Methods</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="251" to="266" />
			<date type="published" when="2013-03">2013. March 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A Public Database of Immersive VR Videos with Corresponding Ratings of Arousal, Valence, and Correlations between Head Movements and Self Report Measures</title>
		<author>
			<persName><forename type="first">Benjamin</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeremy</forename><forename type="middle">N</forename><surname>Bailenson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Pines</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Walter</forename><forename type="middle">J</forename><surname>Greenleaf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leanne</forename><forename type="middle">M</forename><surname>Williams</surname></persName>
		</author>
		<idno type="DOI">10.3389/fpsyg.2017.02116</idno>
		<ptr target="https://doi.org/10.3389/fpsyg.2017.02116" />
	</analytic>
	<monogr>
		<title level="j">Frontiers in Psychology</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<date type="published" when="2017-12">2017. Dec. 2017. 2116</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">NeuroKit2: A Python toolbox for neurophysiological signal processing</title>
		<author>
			<persName><forename type="first">Dominique</forename><surname>Makowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tam</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><forename type="middle">C</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">François</forename><surname>Brammer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hung</forename><surname>Lespinasse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Schölzel</surname></persName>
		</author>
		<author>
			<persName><surname>Annabel Chen</surname></persName>
		</author>
		<idno type="DOI">10.3758/s13428-020-01516-y</idno>
		<ptr target="https://doi.org/10.3758/s13428-020-01516-y" />
	</analytic>
	<monogr>
		<title level="j">Behavior Research Methods</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1689" to="1696" />
			<date type="published" when="2021-02">2021. feb 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">Nitish</forename><surname>Mutha</surname></persName>
		</author>
		<ptr target="https://github.com/NitishMutha/equirectangular-toolbox" />
		<title level="m">Equirectangular-toolbox</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Gaming in Virtual Reality: What Changes in Terms of Usability, Emotional Response and Sense of Presence Compared to Non-Immersive Video Games?</title>
		<author>
			<persName><forename type="first">Federica</forename><surname>Pallavicini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Pepe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maria</forename><forename type="middle">Eleonora</forename><surname>Minissi</surname></persName>
		</author>
		<idno type="DOI">10.1177/1046878119831420</idno>
		<ptr target="https://doi.org/10.1177/1046878119831420" />
	</analytic>
	<monogr>
		<title level="j">Simulation &amp; Gaming</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="136" to="159" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Influence of Emotions on Eye Behavior in Omnidirectional Content</title>
		<author>
			<persName><forename type="first">Wei</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiyi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Toinon</forename><surname>Vigier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthieu Perreira Da</forename><surname>Silva</surname></persName>
		</author>
		<idno type="DOI">10.1109/QoMEX48832.2020.9123126</idno>
		<ptr target="https://doi.org/10.1109/QoMEX48832.2020.9123126" />
	</analytic>
	<monogr>
		<title level="m">Twelfth International Conference on Quality of Multimedia Experience (QoMEX)</title>
		<meeting><address><addrLine>Athlone, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020-01">2020. 2020. 1-6</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><surname>Tensorfflow</surname></persName>
		</author>
		<ptr target="https://wiki.loliot.net/docs/lang/python/libraries/yolov4/python-yolov4-about/" />
		<title level="m">TensorFlow 2 YOLOv4</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">An Immersive Self-Report Tool for the Affective Appraisal of 360°VR Videos</title>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Toet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabienne</forename><surname>Heijn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anne-Marie</forename><surname>Brouwer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tina</forename><surname>Mioch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><forename type="middle">B F</forename><surname>Van Erp</surname></persName>
		</author>
		<idno type="DOI">10.3389/frvir.2020.552587</idno>
		<ptr target="https://doi.org/10.3389/frvir.2020.552587" />
	</analytic>
	<monogr>
		<title level="j">Frontiers in Virtual Reality</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">552587</biblScope>
			<date type="published" when="2020-09">2020. Sept. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<ptr target="https://sample-size.net/correlation-sample-size/" />
		<title level="m">Sample Size Calculators for designing clinical research</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
		<respStmt>
			<orgName>UCSF</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Comparing Emotional States Induced by 360°Videos Via Head-Mounted Display and Computer Screen</title>
		<author>
			<persName><forename type="first">Jan-Niklas</forename><surname>Voigt-Antons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eero</forename><surname>Lehtonen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andres</forename><surname>Pinilla Palacios</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danish</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tanja</forename><surname>Kojic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Möller</surname></persName>
		</author>
		<idno type="DOI">10.1109/QoMEX48832.2020.9123125</idno>
		<ptr target="https://doi.org/10.1109/QoMEX48832.2020.9123125" />
	</analytic>
	<monogr>
		<title level="m">Twelfth International Conference on Quality of Multimedia Experience (QoMEX)</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Assessing Visual Quality of Omnidirectional Videos</title>
		<author>
			<persName><forename type="first">Mai</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenzhong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zulin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenyu</forename><surname>Guan</surname></persName>
		</author>
		<idno type="DOI">10.1109/TCSVT.2018.2886277</idno>
		<ptr target="https://doi.org/10.1109/TCSVT.2018.2886277" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="3516" to="3530" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Investigating the Relationship between Momentary Emotion Self-reports and Head and Eye Movements in HMD-based 360°VR Video Watching</title>
		<author>
			<persName><forename type="first">Abdallah</forename><forename type="middle">El</forename><surname>Tong Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gangyi</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pablo</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><surname>Cesar</surname></persName>
		</author>
		<idno type="DOI">10.1145/3411763.3451627</idno>
		<ptr target="https://doi.org/10.1145/3411763.3451627" />
	</analytic>
	<monogr>
		<title level="m">Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems</title>
		<meeting><address><addrLine>Yokohama Japan</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">CEAP-360VR: A Continuous Physiological and Behavioral Emotion Annotation Dataset for 360 VR Videos</title>
		<author>
			<persName><forename type="first">Abdallah</forename><forename type="middle">El</forename><surname>Tong Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyi</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gangyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pablo</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><surname>Cesar</surname></persName>
		</author>
		<idno type="DOI">10.1109/TMM.2021.3124080</idno>
		<ptr target="https://doi.org/10.1109/TMM.2021.3124080" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>