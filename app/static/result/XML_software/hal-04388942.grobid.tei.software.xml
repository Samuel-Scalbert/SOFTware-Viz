<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Adaptation of AI Explanations to Users&apos; Roles</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Julien</forename><surname>Delaunay</surname></persName>
							<email>julien.delaunay@inria.fr</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Inria</orgName>
								<orgName type="institution" key="instit2">IRISA Rennes</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Christine</forename><surname>Largouet</surname></persName>
							<email>christine.largouet@irisa.fr</email>
							<affiliation key="aff1">
								<orgName type="department">Institut Agro</orgName>
								<orgName type="institution">IRISA Rennes</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Luis</forename><surname>Galarraga</surname></persName>
							<email>luis.galarraga@inria.fr</email>
							<affiliation key="aff2">
								<orgName type="institution" key="instit1">Inria</orgName>
								<orgName type="institution" key="instit2">IRISA Rennes</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Niels</forename><surname>Van Berkel</surname></persName>
							<email>nielsvanberkel@cs.aau.dk</email>
							<affiliation key="aff3">
								<orgName type="institution">Aalborg University Aalborg</orgName>
								<address>
									<country key="DK">Danemark</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Adaptation of AI Explanations to Users&apos; Roles</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">AD51CE844681639C5C0D7475DCBC7117</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-04-12T14:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Explainability</term>
					<term>Interpretability</term>
					<term>User Study CCS Concepts</term>
					<term>Human-centered computing → Human computer interaction (HCI)</term>
					<term>User studies</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Surrogate explanations approximate a complex model by training a simpler model over an interpretable space. Among these simpler models, we identify three kinds of surrogate methods: (a) feature-attribution, (b) example-based, and (c) rule-based explanations. Each surrogate approximates the complex model differently, and we hypothesise that this can impact how users interpret the explanation. Despite the numerous calls for introducing explanations for all, no prior work has compared the impact of these surrogates on specific user roles (e.g., domain expert, developer). In this article, we outline a study design to assess the impact of these three surrogate techniques across different user roles.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Machine Learning (ML) models are increasingly used, spanning from recommendation systems for entertainment applications to decision support for critical tasks such as law <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b25">26]</ref> and medicine <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b16">17]</ref>. These algorithms' efficiency has increased at the cost of opaqueness and bias <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b28">29]</ref>. An increasing focus is placed on transparency and explanations to uncover and mitigate the biases and errors introduced by ML algorithms. Among these explanation methods, surrogate-based-model-explanations (for now on surrogate explanations) are the most frequently used <ref type="bibr" target="#b15">[16]</ref>. The surrogate methods train a proxy to imitate the classifier's outcomes. This proxy is selected for its simple design, highly transparent, and ease of understanding. In their survey, Bodria et al. <ref type="bibr" target="#b5">[6]</ref> grouped the surrogate explanations into three categories: (a) feature-attribution, (b) rules, and (c) example-based explanations. Each of these has a different aim, presented first in this paper, ultimately impacting how the explanation is generated and presented to the user. While many researchers have pointed out the need for user studies to evaluate novel XAI methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b9">10]</ref>, relatively few studies have been conducted. Adadi et al. <ref type="bibr" target="#b1">[2]</ref> highlighted that in 2019, from a total of 381 XAI papers, only 5% emphasised users in evaluating XAI methods. Furthermore, although various user roles are involved in the application of ML models (e.g., developers, end-users), evaluations are primarily focused on developers as explanation methods are currently mostly used by developers <ref type="bibr" target="#b4">[5]</ref>. Researchers have proposed to create explanations adapted to users' roles, suggesting a total of three different roles: (a) developers, (b) domain experts, and (c) lay users <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b24">25]</ref>. However, users are more complex entities, and additional criteria may impact their experience with AI systems (e.g. level of trust in AI). We thus propose to conserve the original three roles as one of the multiple dimensions of user roles and present additional criteria. Finally, we introduce a methodology to conduct user studies comparing the impact of the surrogates depending on the context, task, and user role. These studies aim to help select the explanation methods adapted to user roles.</p><formula xml:id="formula_0">Instance x f o = 0, a = 18, mc = 1, bm = 'Low ′ , hc = 'N o ′ , (o = 30) Explanations Feature (f o = 0) → -6, attribution (bm ≥ 'Low ′ ) → -5 Rule If a ≤ 20 ∧ mc = 1 ⇒ non-obese Example bm = 'Sometimes ′ , hc = 'Y es ′ , (o = 70)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Surrogate Explanations</head><p>Each of the three surrogate explanations methods differs in the way they approximate a black-box classifier <ref type="bibr" target="#b5">[6]</ref>. Therefore, before elaborating on adapting the surrogate to the user, we first clarify how these methods work and differ. We represent mathematical and graphical explanations for these three surrogates in Table <ref type="table" target="#tab_0">1</ref> and Figure <ref type="figure" target="#fig_1">1</ref>. Each of these explanations shows the main reason for the prediction made by a random forest classifier.</p><p>Feature attribution methods associate a weight to the input features to indicate a positive or negative impact on the final prediction. Therefore, Figure <ref type="figure" target="#fig_1">1a</ref> shows the explanations in a similar way to what is shown in LIME <ref type="bibr" target="#b22">[23]</ref> and SHAP <ref type="bibr" target="#b17">[18]</ref>, the methods the most commonly used to generate an explanation <ref type="bibr" target="#b15">[16]</ref>. Red and blue horizontal bars indicate respectively positive and negative impact. The final score and the vertical bar correspond to the final prediction. Explanations from Figure <ref type="figure" target="#fig_1">1a</ref> and Table <ref type="table" target="#tab_0">1</ref> indicate that the user is less prone to develop obesity due to the absence of obesity antecedents in their family and low consumption of food between meals.</p><p>Rule-based surrogates provide the minimum requirements for a given outcome <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b23">24]</ref>. These requirements take the form of 'if-then' rules that represent the conditions for a classifier to make a given prediction. These methods are commonly represented as in Table <ref type="table" target="#tab_0">1</ref>, however, Figure <ref type="figure" target="#fig_1">1b</ref> depicts similarly to <ref type="bibr" target="#b19">[20]</ref>, the increasing classifier's confidence in predicting non-obese as the conditions of the rule (i.e., age and monitoring calorie consumption) are met.</p><p>Example-based explanations present instances similar to the target with a comparable (prototype <ref type="bibr" target="#b12">[13]</ref>) or different  (counterfactual <ref type="bibr" target="#b8">[9]</ref>) classifier's reaction. To the best of our knowledge, no graphic illustration exists for counterfactuals, leading us to develop our own interpretation as shown in Figure <ref type="figure" target="#fig_1">1c</ref>. Thus, Figure <ref type="figure" target="#fig_1">1c</ref> shows the change in the AI's outcome when modifying a feature value. The counterfactual from Table <ref type="table" target="#tab_0">1</ref> and Figure <ref type="figure" target="#fig_1">1c</ref> shows that increasing both the consumption of high-caloric food and intake of food between meals would have changed the prediction.</p><p>Due to a lack of surrogate explanations comparison, XAI users are presently unable to indicate why they might use one type of proxy rather than another. However, the choice of the surrogate and its representation may impact the users (e.g., trust, understanding) <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28]</ref>. We hence argue that preferring one type of proxy over another should be driven by criteria and situations rather than for functional reasons. We next present different user roles to guide researchers and actors in investigating the impact of selecting a surrogate and representation depending on user roles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>HOW TO MEASURE THE IMPACT OF EXPLANATIONS ON USERS?</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Intentional measurements</head><p>Trust. Cahour and Forzy's scale <ref type="bibr" target="#b6">[7]</ref>. <ref type="bibr" target="#b14">[15]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Satisfaction. Hoffman et al.'s questionnaire</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Understanding. Madsen and</head><p>Gregor's questionnaire <ref type="bibr" target="#b18">[19]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Behavioural measurements</head><p>Trust. The users have the option to modify their prediction after seeing the AI's prediction.</p><p>Satisfaction. The time mandatory to solve the simple task or predict. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Understanding. The users have to indicate which factors impact the most toward the prediction</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>User Roles</head><p>Most existing research has focused on three types of roles <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b24">25]</ref>: (a) developers that create or assess AI systems; (b) domain experts, persons with knowledge or authority in a particular area; and (c) lay users, individuals to whom the AI decision is applied (e.g., bank client). Yet, we argue that users and usage scenarios are more complex than those three well-defined categories. Instead, users of AI systems are multi-dimensional (e.g., roles, goals, trust in AI), and various scenarios affect the suitability of different explanation methods (e.g., data types, explanation representation). We thus propose four additional aspects to consider when selecting explanations adapted to users:</p><p>• The motivation to compute the explanation (e.g., increasing performance or trust in the system) is a key criterion for determining the appropriate model.</p><p>• The trust in AI systems may differ among the users as not all programmers have blind faith in the systems they code while lay people may place excessive trust in it.</p><p>• The challenges of representing data types such as sound or time series is one of the reasons why few explanation methods exist for these data types <ref type="bibr" target="#b5">[6]</ref>. As such, the data type influences the choice of the surrogate.</p><p>• Selecting one explanation representation over another (e.g., Figure <ref type="figure" target="#fig_1">1</ref> rather than Table <ref type="table" target="#tab_0">1</ref>) is crucial since it has been widely accepted that representations impact how users perceived AI systems <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b21">22]</ref>.</p><p>Evaluating how each dimension of the user roles and usage scenarios impacts the perception of surrogate explanation would allow associating surrogate methods adapted to users. We thus elaborate on our evaluation proposal to compare the impact of the three explanations surrogates on various users' roles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methodology</head><p>Based on various recommendations <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b27">28]</ref>, we outline in this section (i) diverse metrics to measure both user behavioural and perceived impact of an explanation surrogate, and (ii) a roadmap for conducting generic and replicable user studies for a given surrogate and representation. Perception and Behavioural. Van der Waa et al. highlighted the importance of employing mixed metrics to conduct XAI user studies <ref type="bibr" target="#b27">[28]</ref>. We also emphasise differentiating between perceived and behavioural measurements, as the user's perception may differ from their actual behaviour or decision-making. Therefore, we propose combining questionnaires measuring self-reported perception and simple tasks to gauge performance. Table <ref type="table" target="#tab_1">2</ref> summarises possible metrics and questionnaires to measure both the perceived and behavioural users' (a) understanding, (b) trust, and (c) satisfaction. From these multi-axes measurements, users can envision using one explanation method more than another.</p><p>Roadmap. Figure <ref type="figure" target="#fig_2">2</ref> illustrates an experimental protocol to conduct user studies evaluating the impact of a chosen explanation method and representation for one user role.</p><p>In the initial steps, we advise introducing the domain and the objective of the experiments. Then, to reduce the possibility of biases led by a short or long training round <ref type="bibr" target="#b27">[28]</ref>, an example round defines the user's task and the details of the explanation. Following, participants complete the actual study tasks. This can be repeated multiple times to obtain more reliable results. Participants are asked to predict using the same information as the system. Afterwards, participants have access to the AI prediction and its associated explanation. This approach allows for assessing the behavioural understanding, trust, and satisfaction as defined in Table <ref type="table" target="#tab_1">2</ref>. Finally, in the final round, we measure the perceived impact of the explanation through several questionnaires as described in Table <ref type="table" target="#tab_1">2</ref>.</p><p>By running this experiment for distinct (a) user profiles, (b) explanation surrogates, and (c) representations, researchers and actors may gain insight into which explanation and representation are the more suitable for a specific user based on various criteria.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Future Work</head><p>In this paper, we proposed considering users as more complex than the three original roles but as a multi-axes complexity scale. Comparing the impact of the three surrogate categories over the different aspects of users would benefit the ML sub-community of XAI by allowing them to manage and carefully select the appropriate proxy. Conversely, the HCI sub-community would profit from the roadmap we introduced due to the possibility of conducting generic and replicable user studies. Currently, we launched our experiments on tabular data, with 250 crowdworkers, three surrogates and two representations. Finally, we seek to conduct our investigation with computer scientists from different research laboratories, specialists either in HCI or ML, and domain experts in a relevant domain (e.g., healthcare).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>(a) Feature-attribution explanation. (b) Rule-based explanation.(c) Counterfactual explanation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Graphical representation of three different explanation proxies for a given instance predicted as non-obese by a classifier.</figDesc><graphic coords="4,522.46,113.46,176.42,94.64" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Plan for user study evaluating the impact of a given explanation on users. The tasks are repeated n times, where n is the number of instances predicted by the user. Elements in green are behavioural measurements while in blue are self-reported.</figDesc><graphic coords="5,35.46,129.53,129.59,292.87" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Explanations for a</figDesc><table><row><cell>classifier C computing the risk of</cell></row><row><cell>obesity o ∈ [0, 100] with the</cell></row><row><cell>outcome of 'non-obese' if o ≤ 50.</cell></row><row><cell>The attributes consist of the</cell></row><row><cell>patient's family's obesity</cell></row><row><cell>antecedents (fo), age (a),</cell></row><row><cell>monitoring calorie consumption</cell></row><row><cell>(mc), consumption of food between</cell></row><row><cell>meals (bm), and high-caloric food</cell></row><row><cell>(hc).</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Metrics to measure user intent and behaviour.</figDesc><table /></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Persistent Anti-Muslim Bias in Large Language Models</title>
		<author>
			<persName><forename type="first">Abubakar</forename><surname>Abid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maheen</forename><surname>Farooqi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Zou</surname></persName>
		</author>
		<idno type="DOI">10.1145/3461702.3462624</idno>
		<ptr target="https://doi.org/10.1145/3461702.3462624" />
	</analytic>
	<monogr>
		<title level="m">Proc. AIES. ACM</title>
		<meeting>AIES. ACM</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Peeking Inside the Black-Box: A Survey on Explainable Artificial Intelligence (XAI)</title>
		<author>
			<persName><forename type="first">Amina</forename><surname>Adadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammed</forename><surname>Berrada</surname></persName>
		</author>
		<idno type="DOI">10.1109/ACCESS.2018.2870052</idno>
		<ptr target="http://dx.doi.org/10.1109/ACCESS.2018.2870052" />
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="52138" to="52160" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Explainable Agents and Robots: Results from a Systematic Literature Review</title>
		<author>
			<persName><forename type="first">Sule</forename><surname>Anjomshoae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amro</forename><surname>Najjar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Davide</forename><surname>Calvaresi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kary</forename><surname>Främling</surname></persName>
		</author>
		<ptr target="http://dl.acm.org/citation.cfm?id=3331806" />
	</analytic>
	<monogr>
		<title level="m">Proc. AAMAS. International Foundation for Autonomous Agents and Multiagent Systems</title>
		<meeting>AAMAS. International Foundation for Autonomous Agents and Multiagent Systems</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Thirty years of Artificial Intelligence and Law: overviews</title>
		<author>
			<persName><forename type="first">Michał</forename><surname>Araszkiewicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Bench-Capon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Enrico</forename><surname>Francesconi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Lauritsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonino</forename><surname>Rotolo</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10506-022-09324-9</idno>
		<ptr target="https://doi.org/10.1007/s10506-022-09324-9" />
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence and Law</title>
		<imprint>
			<date type="published" when="2022-08-06">2022. 06 Aug 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Explainable machine learning in deployment</title>
		<author>
			<persName><forename type="first">Umang</forename><surname>Bhatt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alice</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shubham</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrian</forename><surname>Weller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankur</forename><surname>Taly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunhan</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joydeep</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruchir</forename><surname>Puri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>José</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Moura</surname></persName>
		</author>
		<author>
			<persName><surname>Eckersley</surname></persName>
		</author>
		<idno type="DOI">10.1145/3351095.3375624</idno>
		<ptr target="http://dx.doi.org/10.1145/3351095.3375624" />
	</analytic>
	<monogr>
		<title level="m">Proc. on Fairness, Accountability, and Transparency (FAT)</title>
		<meeting>on Fairness, Accountability, and Transparency (FAT)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Benchmarking and Survey of Explanation Methods for Black Box Models</title>
		<author>
			<persName><forename type="first">Francesco</forename><surname>Bodria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fosca</forename><surname>Giannotti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Riccardo</forename><surname>Guidotti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francesca</forename><surname>Naretto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dino</forename><surname>Pedreschi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Salvatore</forename><surname>Rinzivillo</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2102.13076" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Does projection into use improve trust and exploration? An example with a cruise control system</title>
		<author>
			<persName><forename type="first">Béatrice</forename><surname>Cahour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean-François</forename><surname>Forzy</surname></persName>
		</author>
		<ptr target="https://www.sciencedirect.com/science/article/pii/S0925753509000587" />
	</analytic>
	<monogr>
		<title level="j">Safety Science</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1260" to="1270" />
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Explaining Decision-Making Algorithms through UI: Strategies to Help Non-Expert Stakeholders</title>
		<author>
			<persName><forename type="first">Ruotong</forename><surname>Hao Fei Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fiona O'</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Terrance</forename><surname>Connell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">Maxwell</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haiyi</forename><surname>Harper</surname></persName>
		</author>
		<author>
			<persName><surname>Zhu</surname></persName>
		</author>
		<idno type="DOI">10.1145/3290605.3300789</idno>
		<ptr target="https://doi.org/10.1145/3290605.3300789" />
	</analytic>
	<monogr>
		<title level="m">Proc. CHI. ACM</title>
		<meeting>CHI. ACM</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">When Should We Use Linear Explanations?</title>
		<author>
			<persName><forename type="first">Julien</forename><surname>Delaunay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luis</forename><surname>Galárraga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christine</forename><surname>Largouët</surname></persName>
		</author>
		<idno type="DOI">10.1145/3511808.3557489</idno>
		<ptr target="https://doi.org/10.1145/3511808.3557489" />
	</analytic>
	<monogr>
		<title level="m">Proc. CIKM. ACM</title>
		<meeting>CIKM. ACM</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Towards A Rigorous Science of Interpretable Machine Learning</title>
		<author>
			<persName><forename type="first">Finale</forename><surname>Doshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">-</forename><surname>Velez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Been</forename><surname>Kim</surname></persName>
		</author>
		<idno>arXiv:</idno>
		<ptr target="https://arxiv.org/abs/1702.08608" />
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Dermatologist-level classification of skin cancer with deep neural networks</title>
		<author>
			<persName><forename type="first">Andre</forename><surname>Esteva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brett</forename><surname>Kuprel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roberto</forename><forename type="middle">A</forename><surname>Novoa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Justin</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Susan</forename><forename type="middle">M</forename><surname>Swetter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Helen</forename><forename type="middle">M</forename><surname>Blau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Thrun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">nature</title>
		<imprint>
			<biblScope unit="volume">542</biblScope>
			<biblScope unit="page" from="115" to="118" />
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Local Rule-Based Explanations of Black Box Decision Systems</title>
		<author>
			<persName><forename type="first">Riccardo</forename><surname>Guidotti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Monreale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Salvatore</forename><surname>Ruggieri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dino</forename><surname>Pedreschi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Franco</forename><surname>Turini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fosca</forename><surname>Giannotti</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1805.10820" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">VCNet: A self-explaining model for realistic counterfactual generation</title>
		<author>
			<persName><forename type="first">Victor</forename><surname>Guyomard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Françoise</forename><surname>Fessant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Guyet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tassadit</forename><surname>Bouadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Termier</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2212.10847</idno>
		<idno>CoRR abs/2212.10847</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2212.10847" />
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Who wants what and how: a Mapping Function for Explainable Artificial Intelligence</title>
		<author>
			<persName><forename type="first">Maryam</forename><surname>Hashemi</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2302.03180</idno>
		<idno>CoRR abs/2302.03180</idno>
		<ptr target="http://dx.doi.org/10.48550/arXiv.2302.03180" />
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Metrics for Explainable AI: Challenges and Prospects</title>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">R</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shane</forename><forename type="middle">T</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gary</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jordan</forename><surname>Litman</surname></persName>
		</author>
		<idno>CoRR abs/1812.04608</idno>
		<ptr target="http://arxiv.org/abs/1812.04608" />
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Trends in Explainable AI (XAI) Literature</title>
		<author>
			<persName><forename type="first">Alon</forename><surname>Jacovi</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2301.05433</idno>
		<idno>CoRR abs/2301.05433</idno>
		<ptr target="http://dx.doi.org/10.48550/arXiv.2301.05433" />
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Interpretability methods of machine learning algorithms with applications in breast cancer diagnosis</title>
		<author>
			<persName><forename type="first">P</forename><surname>Karatza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Dalakleidi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Athanasiou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">S</forename><surname>Nikita</surname></persName>
		</author>
		<idno type="DOI">10.1109/EMBC46164.2021.9630556</idno>
		<ptr target="http://dx.doi.org/10.1109/EMBC46164.2021.9630556" />
	</analytic>
	<monogr>
		<title level="m">Proc. Engineering in Medicine &amp; Biology Society (EMBC)</title>
		<meeting>Engineering in Medicine &amp; Biology Society (EMBC)</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A Unified Approach to Interpreting Model Predictions</title>
		<author>
			<persName><forename type="first">M</forename><surname>Scott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Su-In</forename><surname>Lundberg</surname></persName>
		</author>
		<author>
			<persName><surname>Lee</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2017/hash/8a20a8621978632d76c43dfd28b67767-Abstract.html" />
	</analytic>
	<monogr>
		<title level="m">Proc. NeuIPS</title>
		<meeting>NeuIPS</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Measuring Human-Computer Trust</title>
		<author>
			<persName><forename type="first">Maria</forename><surname>Madsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shirley</forename><forename type="middle">D</forename><surname>Gregor</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Interprtable machine learning: A guide for making black box models explainable</title>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Molnar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Weapons of Math Destruction: How Big Data Increases Inequality and Threatens Democracy</title>
		<author>
			<persName><forename type="first">O'</forename><surname>Cathy</surname></persName>
		</author>
		<author>
			<persName><surname>Neil</surname></persName>
		</author>
		<idno type="DOI">10.5860/crl.78.3.403</idno>
		<ptr target="https://doi.org/10.5860/crl.78.3.403" />
	</analytic>
	<monogr>
		<title level="j">Coll. Res. Libr</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="403" to="404" />
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Manipulating and Measuring Model Interpretability</title>
		<author>
			<persName><forename type="first">Forough</forename><surname>Poursabzi-Sangdeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">G</forename><surname>Goldstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jake</forename><forename type="middle">M</forename><surname>Hofman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennifer</forename><forename type="middle">Wortman</forename><surname>Vaughan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanna</forename><forename type="middle">M</forename><surname>Wallach</surname></persName>
		</author>
		<idno type="DOI">10.1145/3411764.3445315</idno>
		<ptr target="https://doi.org/10.1145/3411764.3445315" />
	</analytic>
	<monogr>
		<title level="m">Proc. CHI. ACM</title>
		<meeting>CHI. ACM</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Explaining the Predictions of Any Classifier</title>
		<author>
			<persName><forename type="first">Marco</forename><surname>Túlio Ribeiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Guestrin</surname></persName>
		</author>
		<idno type="DOI">10.1145/2939672.2939778</idno>
		<ptr target="https://doi.org/10.1145/2939672.2939778" />
	</analytic>
	<monogr>
		<title level="m">Proc. SIGKDD. ACM</title>
		<meeting>SIGKDD. ACM</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note>Why Should I Trust You?</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Anchors: High-Precision Model-Agnostic Explanations</title>
		<author>
			<persName><forename type="first">Marco</forename><surname>Túlio Ribeiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Guestrin</surname></persName>
		</author>
		<ptr target="https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16982" />
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI. AAAI Press</title>
		<meeting>AAAI. AAAI Press</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Can we do better explanations? A proposal of user-centered explainable AI</title>
		<author>
			<persName><forename type="first">Mireia</forename><surname>Ribera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Àgata</forename><surname>Lapedriza</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-2327/IUI19WS-ExSS2019-12.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proc. IUI Workshops</title>
		<title level="s">CEUR Workshop Proceedings). CEUR</title>
		<meeting>IUI Workshops</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Unsupervised law article mining based on deep pre-trained language representation models with application to the Italian civil code</title>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Tagarelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Simeri</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10506-021-09301-8</idno>
		<ptr target="https://doi.org/10.1007/s10506-021-09301-8" />
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence and Law</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="417" to="473" />
			<date type="published" when="2022-09-01">2022. 01 Sep 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Effect of Information Presentation on Fairness Perceptions of Machine Learning Predictors</title>
		<author>
			<persName><forename type="first">Niels</forename><surname>Van Berkel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jorge</forename><surname>Goncalves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Russo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simo</forename><surname>Hosio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikael</forename><forename type="middle">B</forename><surname>Skov</surname></persName>
		</author>
		<idno type="DOI">10.1145/3411764.3445365</idno>
		<ptr target="http://dx.doi.org/10.1145/3411764.3445365" />
	</analytic>
	<monogr>
		<title level="m">Proc. CHI. ACM</title>
		<meeting>CHI. ACM</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Evaluating XAI: A comparison of rule-based and example-based explanations</title>
		<author>
			<persName><forename type="first">Elisabeth</forename><surname>Jasper Van Der Waa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anita</forename><forename type="middle">H M</forename><surname>Nieuwburg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><forename type="middle">A</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName><surname>Neerincx</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.artint.2020.103404</idno>
		<ptr target="https://doi.org/10.1016/j.artint.2020.103404" />
	</analytic>
	<monogr>
		<title level="j">Artif. Intell</title>
		<imprint>
			<biblScope unit="volume">291</biblScope>
			<biblScope unit="page">103404</biblScope>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Failure of chatbot Tay was evil, ugliness and uselessness in its nature or do we judge it through cognitive shortcuts and biases</title>
		<author>
			<persName><forename type="first">Tomás</forename><surname>Zemcík</surname></persName>
		</author>
		<idno type="DOI">10.1007/s00146-020-01053-4</idno>
		<ptr target="https://doi.org/10.1007/s00146-020-01053-4" />
	</analytic>
	<monogr>
		<title level="j">AI Soc</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="361" to="367" />
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
