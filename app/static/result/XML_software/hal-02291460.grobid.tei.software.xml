<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Weighted Linear Bandits for Non-Stationary Environments</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yoan</forename><surname>Russac</surname></persName>
							<email>yoan.russac@ens.fr</email>
						</author>
						<author>
							<persName><forename type="first">Claire</forename><surname>Vernade</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Olivier</forename><surname>Cappé</surname></persName>
							<email>olivier.cappe@cnrs.fr</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="laboratory">ENS</orgName>
								<orgName type="institution" key="instit1">CNRS</orgName>
								<orgName type="institution" key="instit2">Inria</orgName>
								<orgName type="institution" key="instit3">Université PSL</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="laboratory">ENS</orgName>
								<orgName type="institution" key="instit1">CNRS</orgName>
								<orgName type="institution" key="instit2">Inria</orgName>
								<orgName type="institution" key="instit3">Université PSL</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Weighted Linear Bandits for Non-Stationary Environments</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">4ABA745934564615965FF454390CF15D</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-04-12T14:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We consider a stochastic linear bandit model in which the available actions correspond to arbitrary context vectors whose associated rewards follow a non-stationary linear regression model. In this setting, the unknown regression parameter is allowed to vary in time. To address this problem, we propose D-LinUCB, a novel optimistic algorithm based on discounted linear regression, where exponential weights are used to smoothly forget the past. This involves studying the deviations of the sequential weighted least-squares estimator under generic assumptions. As a by-product, we obtain novel deviation results that can be used beyond nonstationary environments. We provide theoretical guarantees on the behavior of D-LinUCB in both slowly-varying and abruptly-changing environments. We obtain an upper bound on the dynamic regret that is of order d 2/3 B 1/3 T T 2/3 , where B T is a measure of non-stationarity (d and T being, respectively, dimension and horizon). This rate is known to be optimal. We also illustrate the empirical performance of D-LinUCB and compare it with recently proposed alternatives in simulated environments.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Multi-armed bandits offer a class of models to address sequential learning tasks that involve exploration-exploitation trade-offs. In this work we are interested in structured bandit models, known as stochastic linear bandits, in which linear regression is used to predict rewards <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b21">22]</ref>.</p><p>A typical application of bandit algorithms based on the linear model is online recommendation where actions are items to be, for instance, efficiently arranged on personalized web pages to maximize some conversion rate. However, it is unlikely that customers' preferences remain stable and the collected data becomes progressively obsolete as the interest for the items evolve. Hence, it is essential to design adaptive bandit agents rather than restarting the learning from scratch on a regular basis. In this work, we consider the use of weighted least-squares as an efficient method to progressively forget past interactions. Thus, we address sequential learning problems in which the parameter of the linear bandit is evolving with time.</p><p>Our first contribution consists in extending existing deviation inequalities to sequential weighted least-squares. Our result applies to a large variety of bandit problems and is of independent interest. In particular, it extends the recent analysis of heteroscedastic environments by <ref type="bibr" target="#b17">[18]</ref>. It can also be useful to deal with class imbalance situations, or, as we focus on here, in non-stationary environments.</p><p>As a second major contribution, we apply our results to propose D-LinUCB, an adaptive linear bandit algorithm based on carefully designed exponential weights. D-LinUCB can be implemented fully recursively -without requiring the storage of past actions-with a numerical complexity that is comparable to that of LinUCB. To characterize the performance of the algorithm, we provide a unified regret analysis for abruptly-changing or slowly-varying environments.</p><p>The setting and notations are presented below and we state our main deviation result in Section 2. Section 3 is dedicated to non-stationary linear bandits: we describe our algorithms and provide regret upper bounds in abruptly-changing and slowly-varying environments. We complete this theoretical study with a set of experiments in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Model and Notations</head><p>The setting we consider in this paper is a non-stationary variant of the stochastic linear bandit problem considered in <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b21">22]</ref>, where, at each round t ≥ 1, the learner • receives a finite set of feasible actions A t ⊂ R d ;</p><p>• chooses an action A t ∈ A t and receives a reward X t such that</p><formula xml:id="formula_0">X t = A t , θ t + η t ,<label>(1)</label></formula><p>where θ t ∈ R d is an unknown parameter and η t is, conditionally on the past, a σ-subgaussian random noise.</p><p>The action set A t may be arbitrary but its components are assumed to be bounded, in the sense that a 2 ≤ L, ∀a ∈ A t . The time-varying parameter is also assumed to be bounded: ∀t, θ t 2 ≤ S. We further assume that | a, θ t | ≤ 1, ∀t, ∀a ∈ A t , (obviously, this could be guaranteed by assuming that L = S = 1, but we indicate the dependence in L and S in order to facilitate the interpretation of some results). For a positive definite matrix M and a vector x, we denote by x M the norm √</p><p>x M x.</p><p>The goal of the learner is to minimize the expected dynamic regret defined as</p><formula xml:id="formula_1">R(T ) = E T t=1 max a∈At a, θ t -X t = T t=1 max a∈At a -A t , θ t .<label>(2)</label></formula><p>Even in the stationary case -i.e., when θ t = θ -, there is, in general, no single fixed best action in this model.</p><p>When making stronger structural assumption on A t , one recovers specific instances that have also been studied in the literature. In particular, the canonical basis of R d , A t = {e 1 , . . . , e d }, yields the familiar -non contextual-multi-armed bandit model <ref type="bibr" target="#b19">[20]</ref>. Another variant, studied by <ref type="bibr" target="#b14">[15]</ref> and others, is obtained when A t = {e 1 ⊗ a t , . . . , e k ⊗ a t }, where ⊗ denotes the Kronecker product and a t is a time-varying context vector shared by the k actions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Related Work</head><p>There is an important literature on online learning in changing environments. For the sake of conciseness, we restrict the discussion to works that consider specifically the stochastic linear bandit model in <ref type="bibr" target="#b0">(1)</ref>, including its restriction to the simpler (non-stationnary) multi-armed bandit model. Note that there is also a rich line of works that consider possibly non-linear contextual models in the case where one can make probabilistic assumptions on the contexts <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b22">23]</ref>.</p><p>Controlling the regret with respect to the non-stationary optimal action defined in (2) depends on the assumptions that are made on the time-variations of θ t . A generic way of quantifying them is through a variation bound B T =</p><p>T -1 s=1 θ s -θ s+1 2 <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b10">11]</ref>, similar to the penalty used in the group fused Lasso <ref type="bibr" target="#b7">[8]</ref>. The main advantage of using the variation budget is that is includes both slowly-varying and abruptly-changing environments. For the K-armed bandits with known B T , <ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref> achieve the tight dynamic regret bound of O(K 1/3 B 1/3 T T 2/3 ). For linear bandits, <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref> propose an algorithm based on the use of a sliding-window and provide a O(d 2/3 B 1/3 T T 2/3 ) dynamic regret bound; since this contribution is close to ours, we discuss it further in Section 3.2.</p><p>A more specific non-stationary setting arises when the number of changes in the parameter is bounded by Γ T , as in traditional change-point models. The problem is usually referred to as switching bandits or abruptly-changing environments. It is, for instance, the setting considered in the work by Garivier and Moulines <ref type="bibr" target="#b13">[14]</ref>, who analyzed the dynamic regret of UCB strategies based on either a slidingwindow or exponential discounting. For both policies, they prove upper bounds on the regret in O( √ Γ T T ) when Γ T is known. They also provide a lower bound in a specific non-stationary setting, showing that R(T ) = Ω( √ T ). The algorithm ideas can be traced back to <ref type="bibr" target="#b18">[19]</ref>. <ref type="bibr" target="#b27">[28]</ref> shows that an horizon-independent version of the sliding window algorithm can also be analyzed in a slowly-varying setting. <ref type="bibr" target="#b16">[17]</ref> analyze windowing and discounting approaches to address dynamic pricing guided by a (time-varying) linear regression model. Discount factors have also been used with Thomson sampling in dynamic environments as in <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b25">26]</ref>.</p><p>In abruptly-changing environments, the alternative approach relies on change-point detection <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30]</ref>. A bound on the regret in O(( <ref type="formula" target="#formula_0">1</ref>2 + 1 ∆ ) log(T )) is proven by <ref type="bibr" target="#b29">[30]</ref>, where is the smallest gap that can be detected by the algorithm, which had to be given as prior knowledge. <ref type="bibr" target="#b8">[9]</ref> proves a minimax bound in O( √ Γ T KT ) if Γ T is known. <ref type="bibr" target="#b6">[7]</ref> achieves a rate of O( √ Γ T KT ) without any prior knowledge of the gaps or Γ T . In the contextual case, <ref type="bibr" target="#b28">[29]</ref> builds on the same idea: they use a pool of LinUCB learners called slave models as experts and they add a new model when no existing slave is able to give good prediction, that is, when a change is detected. A limitation however of such an approach is that it can not adapt to some slowly-varying environments, as will be illustrated in Section 4. From a practical viewpoint, the methods based either on sliding window or change-point detection require the storage of past actions whereas those based on discount factors can be implemented fully recursively.</p><p>Finally, non-stationarity may also arise in more specific scenarios connected, for instance, to the decaying attention of the users, as investigated in <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b26">27]</ref>. In the following, we consider the general case where the parameters satisfy the variation bound, i.e., T -1 t=1 θ t -θ t+1 2 ≤ B T and we propose an algorithm based on discounted linear regression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Confidence Bounds for Weighted Linear Bandits</head><p>In this section, we consider the concentration of the weighted regularized least-squares estimator, when used with general weights and regularization parameters. To the best of our knowledge there is no such results in the literature for sequential learning -i.e., when the current regressor may depend on the random outcomes observed in the past. The particular case considered in Lemma 5 of <ref type="bibr" target="#b17">[18]</ref> (heteroscedastic noise with optimal weights) stays very close to the unweighted case and we show below how to extend this result. We believe that this new bound is of interest beyond the specific model considered in this paper. For the sake of clarity, we first focus on the case of regression models with fixed parameter, where θ t = θ , for all t.</p><p>First consider a deterministic sequence of regularization parameters (λ t ) t≥1 . The reason why these should be non-constant for weighted least-squares will appear clearly in Section 3. Next, define by F t = σ(X 1 , . . . , X t ) the filtration associated with the random observations. We assume that both the actions A t and positive weights w t are predictable, that is, they are F t-1 measurable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Defining by θt = arg min</head><formula xml:id="formula_2">θ∈R d t s=1 w s (X s -A s , θ ) 2 + λ t θ 2 2</formula><p>the regularized weighted least-squares estimator of θ at time t, one has</p><formula xml:id="formula_3">θt = V -1 t t s=1 w s A s X s where V t = t s=1 w s A s A s + λ t I d ,<label>(3)</label></formula><p>and I d denotes the d-dimensional identity matrix. We further consider an arbitrary sequence of positive parameters (µ t ) t≥1 and define the matrix</p><formula xml:id="formula_4">V t = t s=1 w 2 s A s A s + µ t I d .<label>(4)</label></formula><p>V is strongly connected to the variance of the estimator θt , which involves the squares of the weights (w 2 s ) s≥1 . For the time being, µ t is arbitrary and will be set as a function of λ t in order to optimize the deviation inequality.</p><p>We then have the following maximal deviation inequality. Theorem 1. For any F t -predictable sequences of actions (A t ) t≥1 and positive weights (w t ) t≥1 and for all δ &gt; 0,</p><formula xml:id="formula_5">P   ∀t, θt -θ Vt V -1 t Vt ≤ λ t √ µ t S + σ 2 log(1/δ) + d log 1 + L 2 t s=1 w 2 s dµ t   ≥ 1 -δ.</formula><p>The proof of this theorem is deferred to the appendix and combines an argument using the method of mixtures and the use of a proper stopping time. The standard result used for least-squares <ref type="bibr" target="#b19">[20,</ref><ref type="bibr">Chapter 20</ref>] is recovered by taking µ t = λ t and w t = 1 (note that V t is then equal to V t ). When the weights are not equal to 1, the appearance of the matrix V t is a consequence of the fact that the variance terms are proportional to the squared weights w 2 t , while the least-squares estimator itself is defined with the weights w t . In the weighted case, the matrix V t V -1 t V t must be used to define the confidence ellipsoid.</p><p>An important property of the least-squares estimator is to be scale-invariant, in the sense that multiplying all weights (w s ) 1≤s≤t and the regularization parameter λ t by a constant leaves the estimator θt unchanged. In Theorem 1, the only choice of sequence (µ t ) t≥1 that is compatible with this scale-invariance property is to take µ t proportional to λ 2 t : then the matrix V t V -1 t V t becomes scale-invariant (i.e. unchanged by the transformation w s → αw s ) and so does the upper bound of θt -θ Vt V -1 t</p><p>Vt in Theorem 1. In the following, we will stick to this choice, while particularizing the choice of the weights w t to allow for non-stationary models.</p><p>It is possible to extend this result to heteroscedastic noise, when η t is σ t sub-Gaussian and σ t is F t-1 measurable, by defining V t as t s=1 w 2 s σ 2 s A s A s + µ t I d . In the next section, we will also use an extension of Theorem 1 to the non-stationary model presented in <ref type="bibr" target="#b0">(1)</ref> . In this case, Theorem 1 holds with θ replaced by V -1 t t s=1 w s A s A s θ s + λ t θ r , where r is an arbitrary time index (proposition 3 in Appendix). The fact that r can be chosen freely is a consequence of the assumption that the sequence of L2-norms of the parameters (θ t ) t≥1 is bounded by S.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Application to Non-stationary Linear Bandits</head><p>In this section, we consider the non-stationary model defined in <ref type="bibr" target="#b0">(1)</ref> and propose a bandit algorithm in Section 3.1, called Discounted Linear Upper Confidence Bound (D-LinUCB), that relies on weighted least-squares to adapt to changes in the parameters θ t . Analyzing the performance of D-LinUCB in Section 3.2, we show that it achieves reliable performance both for abruptly changing or slowly drifting parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">The D-LinUCB Algorithm</head><p>Being adaptive to parameter changes indeed implies to reduce the influence of observations that are far back in the past, which suggests using weights w t that increase with time. In doing so, there are two important caveats to consider. First, this can only be effective if the sequence of weights is growing sufficiently fast (see the analysis in the next section). We thus consider exponentially increasing weights of the form w t = γ -t , where 0 &lt; γ &lt; 1 is the discount factor.</p><p>Next, due to the absence of assumptions on the action sets A t , the regularization is instrumental in obtaining guarantees of the form given in Theorem 1. In fact, if w t = γ -t while λ t does not increase sufficiently fast, then the term log 1 + (L 2 t s=1 w 2 s )/(dµ t ) will eventually dominate the radius of the confidence region since we choose µ t proportional to λ 2 t . This occurs because there is no guarantee that the algorithm will persistently select actions A t that span the entire space. With this in mind, we consider an increasing regularization factor of the form λ t = γ -t λ, where λ &gt; 0 is a hyperparameter.</p><p>Note that due to the scale-invariance property of the weighted least-square estimator, we can equivalently consider that at time t, we are given time-dependent weights w t,s = γ t-s , for 1 ≤ s ≤ t and that θt is defined as arg min</p><formula xml:id="formula_6">θ∈R d t s=1 γ t-s (X s -A s , θ ) 2 + λ/2 θ 2 2 .</formula><p>For numerical stability reasons, this form is preferable and is used in the statement of Algorithm 1. In the analysis of Section 3.2 however we revert to the standard form of the weights, which is required to apply the concentration result of Section 1. We are now ready to describe D-LinUCB in Algorithm 1.</p><p>Algorithm 1: D-LinUCB Input: Probability δ, subgaussianity constant σ, dimension d, regularization λ, upper bound for actions L, upper bound for parameters S, discount factor γ.</p><formula xml:id="formula_7">Initialization: b = 0 R d , V = λI d , V = λI d , θ = 0 R d for t ≥ 1 do Receive A t , compute β t-1 = √ λS + σ 2 log 1 δ + d log 1 + L 2 (1-γ 2(t-1) ) λd(1-γ 2 ) for a ∈ A t do Compute UCB(a) = a θ + β t-1 a V -1 V V -1 a A t = arg max a (UCB(a))</formula><p>Play action A t and receive reward X t Updating phase:</p><formula xml:id="formula_8">V = γV + A t A t + (1 -γ)λI d , V = γ 2 V + A t A t + (1 -γ 2 )λI d b = γb + X t A t , θ = V -1 b</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Analysis</head><p>As discussed previously, we consider weights of the form w t = γ -t (where 0 &lt; γ &lt; 1) in the D-LinUCB algorithm. In accordance with the discussion at the end of Section 1, Algorithm 1 uses µ t = γ -2t λ as the parameter to define the confidence ellipsoid around θt-1 . The confidence ellipsoid C t is defined as θ :</p><formula xml:id="formula_9">θ -θt-1 Vt-1 V -1 t-1 Vt-1 ≤ β t-1</formula><p>where</p><formula xml:id="formula_10">β t = √ λS + σ 2 log(1/δ) + d log 1 + L 2 (1 -γ 2t ) λd(1 -γ 2 ) .<label>(5)</label></formula><p>Using standard algebraic calculations together with the remark above about scale-invariance it is easily checked that at time t Algorithm 1 selects the action A t that maximizes a, θ for a ∈ A t and θ ∈ C t . The following theorem bounds the regret resulting from Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Theorem 2. Assuming that</head><p>T -1 s=1 θ s -θ s+1 2 ≤ B T , the regret of the D-LinUCB algorithm is bounded for all γ ∈ (0, 1) and integer D ≥ 1, with probability at least 1 -δ, by</p><formula xml:id="formula_11">R T ≤ 2LDB T + 4L 3 S λ γ D 1 -γ T + 2 √ 2β T √ dT T log(1/γ) + log 1 + L 2 dλ(1 -γ) .<label>(6)</label></formula><p>The first two terms of the r.h.s. of ( <ref type="formula" target="#formula_11">6</ref>) are the result of the bias due to the non-stationary environment. The last term is the consequence of the high probability bound established in the previous section and an adaptation of the technique used in <ref type="bibr" target="#b0">[1]</ref>.</p><p>We give the complete proof of this result in appendix. The high-level idea of the proof is to isolate bias and variance terms. However, in contrast with the stationary case, the confidence ellipsoid C t does not necessarily contain (with high probability) the actual parameter value θ t due to the (unknown) bias arising from the time variations of the parameter. We thus define</p><formula xml:id="formula_12">θt = V -1 t-1 t-1 s=1 γ -s A s A s θ s + λγ -(t-1) θ t</formula><p>which is an action-dependent analogue of the parameter value θ in the stationary setting (although this is a random value). As mentioned in section 2, θt does belong to C t with probability at least 1 -δ (see Proposition 3 in Appendix). The regret may then be split as</p><formula xml:id="formula_13">R T ≤ 2L T t=1 θ t -θt 2 + T t=1 A t , θ t -θt (with probability at least 1 -δ),</formula><p>where (A t , θ t ) = arg max (a∈At,θ∈Ct) a, θ . The rightmost term can be handled by proceeding as in the case of stationary linear bandits, thanks to the deviation inequality obtained in Section 2. The first term in the r.h.s. can be bounded deterministically, from the assumption made on</p><formula xml:id="formula_14">T -1 s=1 θ s -θ s+1 2 .</formula><p>In doing so, we introduce the analysis parameter D that, roughly speaking, corresponds to the window length equivalent to a particular choice of discount factor γ: the bias resulting from observations that are less than D time steps apart may be bounded in term of D while the remaining ones are bounded globally by the second term of the r.h.s. of ( <ref type="formula" target="#formula_11">6</ref>). This sketch of proof is substantially different from the arguments used by <ref type="bibr" target="#b10">[11]</ref> to analyze their sliding window algorithm (called SW-LinUCB). We refer to the appendix for a more detailed analysis of these differences. Interestingly, the regret bound of Theorem 2 holds despite the fact that the true parameter θ t may not be contained in the confidence ellipsoid C t-1 , in contrast to the proof of <ref type="bibr" target="#b13">[14]</ref>.</p><p>It can be checked that, as T tends to infinity, the optimal choice of the analysis parameter D is to take D = log(T )/(1 -γ). Further assuming that one may tune γ as a function of the horizon T and the variation upper bound B T yields the following result. Corollary 1. By choosing γ = 1 -(B T /(dT )) 2/3 , the regret of the D-LinUCB algorithm is asymptotically upper bounded with high probability by a term O(</p><formula xml:id="formula_15">d 2/3 B 1/3 T T 2/3 ) when T → ∞.</formula><p>This result is favorable as it corresponds to the same order as the lower bound established by <ref type="bibr" target="#b3">[4]</ref>. More precisely, the case investigated by <ref type="bibr" target="#b3">[4]</ref> corresponds to a non-contextual model with a number of changes that grows with the horizon. On the other hand, the guarantee of Corollary 1 requires horizon-dependent tuning of the discount factor γ, which opens interesting research issues (see also <ref type="bibr" target="#b10">[11]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>This section is devoted to the evaluation of the empirical performance of D-LinUCB. We first consider two simulated low-dimensional environments that illustrate the behavior of the algorithms when confronted to either abrupt changes or slow variations of the parameters. The analysis of the previous section, suggests that D-LinUCB should behave properly in both situations. We then consider a more realistic scenario in Section 4.2, where the contexts are high-dimensional and extracted from a data set of actual user interactions with a web service.</p><p>For benchmarking purposes, we compare D-LinUCB to the Dynamic Linear Upper Confidence Bound (dLinUCB) algorithm proposed by <ref type="bibr" target="#b28">[29]</ref> and with the Sliding Window Linear UCB (SW-LinUCB) of <ref type="bibr" target="#b10">[11]</ref>. The principle of the dLinUCB algorithm is that a master bandit algorithm is in charge of choosing the best LinUCB slave bandit for making the recommendation. Each slave model is built to run in each one of the different environments. The choice of the slave model is based on a lower confidence bound for the so-called badness of the different models. The badness is defined as the number of times the expected reward was found to be far enough from the actual observed reward on the last τ steps, where τ is a parameter of the algorithm. When a slave is chosen, the action proposed to a user is the result of the LinUCB algorithm associated with this slave. When the action is made, all the slave models that were good enough are updated and the models whose badness were too high are deleted from the pool of slaves models. If none of the slaves were found to be sufficiently good, a new slave is added to the pool.</p><p>The other algorithm that we use for comparison is SW-LinUCB, as presented in <ref type="bibr" target="#b10">[11]</ref>. Rather than using exponentially increasing weights, a hard threshold is adopted. Indeed, the actions and rewards included in the l-length sliding window are used to estimate the linear regression coefficients. We expect D-LinUCB and SW-LinUCB to behave similarly as they both may be shown to have the same sort of regret guarantees (see appendix).</p><p>In the case of abrupt changes, we also compare these algorithms to the Oracle Restart LinUCB (LinUCB-OR) strategy that would know the change-points and simply restart, after each change, a new instance of the LinUCB algorithm. The regret of this strategy may be seen as an empirical lower bound on the optimal behavior of an online learning algorithm in abruptly changing environments.</p><p>In the following figures, the vertical red dashed lines correspond to the change-points (in abrupt changes scenarios). They are represented to ease the understanding but except for LinUCB-OR, they are of course unknown to the learning algorithms. When applicable, the blue dashed lines correspond to the average detection time of the breakpoints with the dLinUCB algorithm. For D-LinUCB the discount parameter is chosen as γ = 1 -( B T dT ) 2/3 . For SW-LinUCB the window's length is set to l = ( dT B T ) 2/3 , where d = 2 in the experiment. Those values are theoretically supposed to minimize the asymptotic regret. For the Dynamic Linear UCB algorithm, the badness is estimated from τ = 200 steps, as in the experimental section of <ref type="bibr" target="#b28">[29]</ref>.  <ref type="bibr">[2000,</ref><ref type="bibr">3000]</ref>], θ t = (0, 1); and, finally, for t &gt; 3000, θ t = (0, -1). This corresponds to a hard problem as the sequence of parameters is widely spread in the unit ball. Indeed it forces the algorithm to adapt to big changes, which typically requires a longer adaptation phase. On the other hand, it makes the detection of changes easier, which is an advantage for dLinUCB. In the second half of the experiment (when t ≥ 3000) there is no change, LinUCB struggles to catch up and suffers linear regret for long periods after the last change-point. The results of our simulations are shown in the left column of Figure <ref type="figure" target="#fig_0">1</ref>. On the top row we show a 2-dimensional scatter plot of the estimate of the unknown parameters θt every 1000 steps averaged on 100 independent experiment. The bottom row corresponds to the regret averaged over 100 independent experiments with the upper and the lower 5% quantiles. In this environment, with 1-subgaussian random noise, dLinUCB struggles to detect the change-points. Over the 100 experiments, the first change-point was detected in 95% of the runs, the second was never detected and the third only in 6% of the runs, thus limiting the effectiveness of the dLinUCB approach. When decreasing the variance of the noise, the performance of dLinUCB improves and gets closer to the performance of the oracle restart strategy LinUCB-OR. It is worth noting that for both SW-LinUCB and D-LinUCB, the estimator θt adapts itself to non-stationarity and is able to follow θ t (with some delay), as shown on the scatter plot. Predictably, LinUCB-OR achieves the best performance by restarting exactly whenever a change-point happens.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Synthetic data in abruptly-changing or slowly-varying scenarios</head><p>The second experiment corresponds to a slowly-changing environment. It is easier for LinUCB to keep up with the adaptive policies in this scenario. Here, the parameter θ t starts at (1 and moves continuously counter-clockwise on the unit-circle up to the position [0, 1] in 3000 steps. We then have a steady period of 3000 steps. For this sequence of parameters,</p><formula xml:id="formula_16">B T = T -1 t=1 θ t -θ t+1 2 = 1.57.</formula><p>The results are reported in the right column of Figure <ref type="figure" target="#fig_0">1</ref>. Unsurprisingly, dLinUCB does not detect any change and thus displays the same performance as LinUCB. SW-LinUCB and D-LinUCB behaves similarly and are both robust to such an evolution in the regression parameters. The performance of LinUCB-OR is not reported here, as restarting becomes ineffective when the changes are too frequent (here, during the first 3000 time steps, there is a change at every single step). The scatter plot also gives interesting information: θt tracks θ t quite effectively for both SW-LinUCB and D-LinUCB but the two others algorithms lag behind. LinUCB will eventually catch up if the length of the stationary period becomes larger. For this experiment, a dataset providing a sample of 30 days of Criteo live traffic data <ref type="bibr" target="#b12">[13]</ref> was used. It contains banners that were displayed to different users and contextual variables, including the information of whether the banner was clicked or not. We kept the categorical variables cat1 to cat9 , together with the variable campaign, which is a unique identifier of each campaign. Beforehand, these contexts have been onehot encoded and 50 of the resulting features have been selected using a Singular Value Decomposition. θ is obtained by linear regression. The rewards are then simulated using the regression model with an additional Gaussian noise of variance σ 2 = 0.15. At each time step, the different algorithms have the choice between two 50-dimensional contexts drawn at random from two separate pools of 10000 contexts corresponding, respectively, to clicked or not clicked banners. The non-stationarity is created by switching 60% of θ coordinates to -θ at time 4000, corresponding to a partial class inversion. The cumulative dynamic regret is then averaged over 100 independent replications. The results are shown on Figure <ref type="figure" target="#fig_1">2</ref>. In the first stationary period, LinUCB and dLinUCB perform better than the adaptive policies by using all available data, whereas the adaptive policies only use the most recent events. After the breakpoint, LinUCB suffers a large regret, as the algorithm fails to adapt to the new environment. In this experiment, dLinUCB does not detect the change-point systematically and performs similarly as LinUCB on average, it can still outperform adaptive policies from time to time when the breakpoint is detected as can be seen with the 5% quantile. D-LinUCB and SW-LinUCB adapt more quickly to the change-point and perform significantly better than the non-adaptive policies after the breakpoint. Of course, the oracle policy LinUCB-OR is the best performing policy. The take-away message is that there is no free lunch: in a stationary period by using only the most recent events SW-LinUCB and D-LinUCB do not perform as good as a policy that uses all the available information. Nevertheless, after a breakpoint, the recovery is much faster with the adaptive policies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Simulation based on a real dataset</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A Confidence Bounds for Weighted Linear Bandits</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Preliminary results</head><p>In this section we give the main results for obtaining Theorem 1. For the sake of conciseness all the results will be stated with σ-subgaussian noises but the proofs will be done with the particular value of σ = 1. The model we consider is the one defined by equation ( <ref type="formula" target="#formula_0">1</ref>), where we recall that (η s ) s is, conditionally on the past, a sequence of σ-subgaussian random noises. The results of this section are close to the one proposed in <ref type="bibr" target="#b0">[1]</ref> but our results are valid with a sequence of predictable weights.</p><p>We introduce the quantity</p><formula xml:id="formula_17">S t = t s=1 w s A s η s and V t = t s=1 w 2 s A s A s + µ t I d . When the regularization term is omitted, let V t (0) = t s=1 w 2 s A s A s .</formula><p>The filtration associated with the random observations is denoted F t = σ(X 1 , . . . , X t ) such that A t is F t-1 -measurable and η t is F t -measurable. The weights are also assumed to be predictable. The following lemma is an extension to the weighted case of Lemma 8 of <ref type="bibr" target="#b0">[1]</ref>. Lemma 1. Let (w t ) t≥1 be a sequence of predictable and positive weights. Let x ∈ R d be arbitrary and consider for any t ≥ 1</p><formula xml:id="formula_18">M t (x) = exp 1 σ x S t - 1 2 x V t (0)x .</formula><p>Let τ be a stopping time with respect to the filtration {F t } ∞ t=0 . Then M τ (x) is almost surely well-defined and</p><formula xml:id="formula_19">∀x ∈ R d , E[M τ (x)] ≤ 1. Proof. First, we prove that ∀x ∈ R d , (M t (x)) ∞ t=0 is a super-martingale. Let x ∈ R d , E[M t (x)|F t-1 ] = E exp x S t-1 + x w t A t η t -1/2x ( V t-1 (0) + w 2 t A t A t )x |F t-1 = M t-1 (x)E exp(x w t A t η t - 1 2 w 2 t x A t A t x)|F t-1 = M t-1 (x) exp(- 1 2 w 2 t x A t A t x)E exp(x w t A t η t )|F t-1 ≤ M t-1 (x) exp(- 1 2 w 2 t x A t A t x) exp(1/2w 2 t (x A t ) 2 ) = M t-1 (x).</formula><p>The second equality comes from the fact that S t-1 and V t-1 are F t-1 -measurable. The inequality is the definition of the conditional 1-subgaussianity where we also use the F t-1 -measurability of w t .</p><p>Using this supermartingale property, we have E[M t (x)] ≤ 1. The convergence theorem for non-negative supermartingales ensures that M ∞ (x) = lim t→∞ M t (x) is almost surely well defined. By introducing the stopped supermartingale M t (x) = M min(t,τ ) (x), we have M τ (x) = lim t→∞ M t (x). Knowing that M t (x) is also a supermartingale, we have</p><formula xml:id="formula_20">E[M t (x)] = E[M min(t,τ ) (x)] ≤ E[M min(0,τ ) (x)] = E[M 0 (x)] = 1.</formula><p>By using Fatou's lemma:</p><formula xml:id="formula_21">E[M τ (x)] = E[lim inf t→∞ M t (x)] ≤ lim inf t→∞ E[M t (x)] ≤ 1.</formula><p>In the next lemma, we will integrate M t (x) with respect to a time-dependent probability measure. This is the key for allowing sequential regularizations in the concentration inequality stated in Theorem 1. This lemma is inspired by the method of mixtures first presented in <ref type="bibr" target="#b24">[25]</ref>. The idea of using time-varying probability measures is inspired from the proof of Theorem 11 in <ref type="bibr" target="#b17">[18]</ref>. The two following lemmas are included in the appendix so that the article is self-contained. There are not a mere consequence of the results in <ref type="bibr" target="#b0">[1]</ref> because of the time-dependent regularization parameters. As explained in Section 3, this is unavoidable when using exponential weights to avoid the vanishing effect of the regularization. Lemma 2. Let (h t ) t be a sequence of probability measures on R d . We define</p><formula xml:id="formula_22">M t = R d M t (x)dh t (x). Then, ∀t, E[ M t ] ≤ 1</formula><p>Proof.</p><formula xml:id="formula_23">E[ M t ] = M t dP = R d M t (x)dh t (x) dP = R d M t (x)dP dh t (x) (Fubini's theorem) = R d E[M t (x)]dh t (x) ≤ R d dh t (x) (Lemma 1) ≤ 1. (h t probability measure.)</formula><p>Lemma 2 is a warm-up for the next lemma and is helpful for understanding why Lemma 3 holds. It is valid for any fixed time t. The next step is to give its equivalent in a stopped version in the specific case of gaussian random vectors. Lemma 3. Let (µ t ) t be a deterministic sequence of regularization parameters. Let F ∞ = σ (∪ ∞ t=1 F t ) be the tail σ-algebra of the filtration (F t ) t . Let X = (X t ) t≥1 be an independent sequence of gaussian random vectors such that X t ∼ N (0, 1 µt I d ) = h t with X independent of F ∞ . We define</p><formula xml:id="formula_24">Mt (µ t ) = E[M t (X t )|F ∞ ] = R d M t (x)f µt (x)dx,</formula><p>where f µt is the probability density function associated with h t defined as,</p><formula xml:id="formula_25">f µt (x) = 1 (2π) d det(1/µ t I d ) exp(- µ t x x 2 ).</formula><p>Let τ be a stopping time with respect to the filtration (F t ) t then,</p><formula xml:id="formula_26">E[ Mτ (µ τ )] ≤ 1.</formula><p>Proof. We can use the result of Lemma 1 which gives</p><formula xml:id="formula_27">∀x ∈ R d , E[M τ (x)] ≤ 1.</formula><p>We have,</p><formula xml:id="formula_28">E[ Mτ (µ τ )] = E[E[M τ (X τ )|F ∞ ]] = E[E[E[M τ (X τ )|F ∞ ]|(X t ) t≥1 ]] = E[E[E[M τ (X τ )|(X t ) t≥1 ]|F ∞ ]] ≤ 1.</formula><p>The inequality is a consequence of Lemma 1 as, conditionally to the sequence (X t ) t , M τ (X τ ) is of the form M τ (x) with a fixed x.</p><p>We finally state the main result needed to obtain Theorem 1. Proposition 1. For (w s ) s≥1 a sequence of predictable and positive weights, ∀δ &gt; 0, the following deviation inequality holds</p><formula xml:id="formula_29">P   ∃t ≥ 0, S t V -1 t ≥ σ 2 log 1 δ + log det( V t ) µ d t   ≤ δ.</formula><p>Proof. For a fixed t,</p><formula xml:id="formula_30">Mt (µ t ) = R d M t (x)f µt (x)dx = 1 (2π) d det(1/µ t I d ) R d exp x S t - 1 2 x 2 µtI d - 1 2 x 2 Vt(0) dx = 1 (2π) d det(1/µ t I d ) R d exp x S t - 1 2 x 2 Vt dx = 1 (2π) d det(1/µ t I d ) R d exp 1 2 S t 2 V -1 t - 1 2 x -V -1 t S t 2 Vt dx = exp 1 2 S t 2 V -1 t (2π) d det(1/µ t I d ) R d exp - 1 2 x -V -1 t S t 2 Vt dx = exp 1 2 S t 2 V -1 t (2π) d det(1/µ t I d ) (2π) d det V -1 t = exp 1 2 S t 2 V -1 t det(µ t I d ) det( V t ) .</formula><p>We introduce the particular stopping time,</p><formula xml:id="formula_31">τ = min t ≥ 0, S t V -1 t ≥ 2 log 1 δ + log det( V t ) det(µ t I d ) .</formula><p>Thus,</p><formula xml:id="formula_32">P   ∃t ≥ 0, S t V -1 t ≥ 2 log 1 δ + log det( V t ) det(µ t I d )   = P(τ &lt; ∞) = P   τ &lt; ∞, S τ V -1 τ ≥ 2 log 1 δ + log det( V τ ) det(µ τ I d )   ≤ P   S τ V -1 τ ≥ 2 log 1 δ + log det( V τ ) det(µ τ I d )   = P exp 1 2 S τ 2 V -1 τ det(µ τ I d ) det( V τ ) ≥ 1 δ ≤ δE[ Mτ (µ τ )] (Markov's inequality) ≤ δ (Lemma 3).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Proof of Theorem 1</head><p>We recall that Theorem 1 is established in a stationary environment where ∀t ≥ 1, θ t = θ .</p><p>Proof. First note that,</p><formula xml:id="formula_33">θt = V -1 t t s=1 w s A s X s = V -1 t t s=1 w s A s (A s θ + η s ) (Equation 1) = V -1 t t s=1 w s A s A s θ + λ t θ -λ t θ + V -1 t S t = θ -λ t V -1 t θ + V -1 t S t . Thus, θt -θ = V -1 t S t -λ t V -1 t θ . (7) ∀x ∈ R d , ∀t &gt; 0, we have |x ( θt -θ )| ≤ x V -1 t VtV -1 t V -1 t S t Vt V -1 t Vt + λ t V -1 t θ Vt V -1 t Vt ≤ x V -1 t VtV -1 t S t V -1 t + λ t θ V -1 t .</formula><p>By applying the previous inequality with</p><formula xml:id="formula_34">x = V t V -1 t V t ( θt -θ ), we have ∀t, θt -θ Vt V -1 t Vt ≤ S t V -1 t + λ t θ V -1 t . Knowing that V t ≥ µ t I d and that V t is positive definite, we have θ V -1 t ≤ 1 √ µt θ 2 .</formula><p>Finally,</p><formula xml:id="formula_35">∀t, θt -θ Vt V -1 t Vt ≤ S t V -1 t + λ t √ µ t θ 2 . (<label>8</label></formula><formula xml:id="formula_36">)</formula><p>From Proposition 1, we obtain the following any time high probability upper bound for S t V -1</p><formula xml:id="formula_37">t , P   ∀t ≥ 0, S t V -1 t ≤ σ 2 log 1 δ + log det( V t ) µ d t   ≥ 1 -δ.</formula><p>Therefore by using inequality 8,</p><formula xml:id="formula_38">P   ∀t ≥ 0, θt -θ V -1 t ≤ λ t √ µ t S + σ 2 log 1 δ + log det( V t ) µ d t   ≥ 1 -δ.</formula><p>We obtain the exact formula of Theorem 1 by upper bounding det( V t ) as proposed in Proposition 2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B D-LinUCB Analysis</head><p>In this section, the environment is non-stationary, which means that the unknown parameter θ may evolve over time and is denoted θ t . The reward generation process in the one presented in Equation (1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Preliminary results</head><p>In this section, V t and V t are defined by</p><formula xml:id="formula_39">V t = t s=1 γ -s A s A s + λγ -t I d , V t = t s=1 γ -2s A s A s + λγ -2t I d .</formula><p>We recall the definition of β t :</p><formula xml:id="formula_40">β t = √ λS + σ 2 log(1/δ) + d log 1 + L 2 (1 -γ 2t ) λd(1 -γ 2 ) .</formula><p>With θt defined in equation ( <ref type="formula" target="#formula_3">3</ref>), the confidence ellipsoid we consider is defined by</p><formula xml:id="formula_41">C t = θ ∈ R d : θ -θt-1 Vt-1 V -1 t-1 Vt-1 ≤ β t-1 .<label>(9)</label></formula><p>Theorem 1 can be applied with this choice of weights and regularization. We combine it with an upper bound for det( V t ) given below.</p><p>Proposition 2 (Determinant inequality for the weighted design matrix). Let (λ t ) t be a deterministic sequence of regularization parameters. Let V t = t s=1 w s A s A s + λ t I d be the weighted design matrix. Under the assumption ∀t, A t 2 ≤ L, the following holds</p><formula xml:id="formula_42">det(V t ) ≤ λ t + L 2 t s=1 w s d d .</formula><p>Proof.</p><formula xml:id="formula_43">det(V t ) = d i=1 l i (l i are the eigenvalues) ≤ 1 d d i=1 l i d (AM-GM inequality) ≤ 1 d trace(V t ) d ≤ 1 d t s=1 w s trace(A s A s ) + λ t d ≤ 1 d t s=1 w s A s 2 2 + λ t d ≤ λ t + L 2 d t s=1 w s d .</formula><p>Corollary 2. In the specific case where the weights are given by w t = γ -t with 0 &lt; γ &lt; 1. Proposition 2 can be rewritten</p><formula xml:id="formula_44">det(V t ) ≤ λ t + L 2 (γ -t -1) d(1 -γ) d = λγ -t + L 2 (γ -t -1) d(1 -γ) d .</formula><p>We also have,</p><formula xml:id="formula_45">det( V t ) ≤ µ t + L 2 (γ -2t -1) d(1 -γ 2 ) d = λγ -2t + L 2 (γ -2t -1) d(1 -γ 2 ) d .</formula><p>Proof. Apply Proposition 2 and use</p><formula xml:id="formula_46">t s=1 γ -s = γ -t -1 1-γ and t s=1 γ -2s = γ -2t -1 1-γ 2 .</formula><p>Corollary 2 and Proposition 1 yield the following result. Corollary 3. ∀δ &gt; 0, with the weights w t = γ -t and 0 &lt; γ &lt; 1, we have</p><formula xml:id="formula_47">P ∃t ≥ 0, S t V -1 t ≥ σ 2 log 1 δ + d log 1 + L 2 (1 -γ 2t ) λd(1 -γ 2 ) ≤ δ.</formula><p>Thanks to this corollary we are now ready to show that θt belongs to C t-1 with high probability.</p><formula xml:id="formula_48">Proposition 3. Let C t = θ ∈ R d : θ -θt-1 Vt-1 V -1 t-1 Vt-1 ≤ β t-1 denote the confidence ellipsoid. Let θt = V -1 t-1 t-1 s=1 γ -s A s A s θ s + λγ -(t-1) θ t . Then, ∀δ &gt; 0, P ∀t ≥ 1, θt ∈ C t ≥ 1 -δ. Proof. θt -θt-1 = V -1 t-1 t-1 s=1 γ -s A s A s θ s + λγ -(t-1) θ t - t-1 s=1 γ -s A s X s = V -1 t-1 t-1 s=1 γ -s A s A s θ s + λγ -(t-1) θ t - t-1 s=1 γ -s A s A s θ s - t-1 s=1 γ -s A s η s = -V -1 t-1 S t-1 + λγ -(t-1) V -1 t-1 θ t . Therefore, θt -θt-1 Vt-1 V -1 t-1 Vt-1 ≤ S t-1 V -1 t-1 + λγ -(t-1) θ t V -1 t-1 ≤ S t-1 V -1 t-1 + √ λS ( V -1 t-1 ≤ 1/(γ -2(t-1) λ)I d and θ t 2 ≤ S) ≤ β t-1 (Corollary 3). B.2 Control of the norm of actions Lemma 4. Let V t = t s=1 γ -s A s A s + λγ -t I d and V t = t s=1 γ -2s A s A s + λγ -2t I d and 0 &lt; γ &lt; 1. We have ∀t, V -1 t V t V -1 t ≤ γ -t V -1 t .</formula><p>Proof.</p><formula xml:id="formula_49">V t = t s=1 γ -2s A s A s + λγ -2t I d ≤ γ -t t s=1 γ -s A s A s + λγ -2t I d = γ -t V t . Consequently, V -1 t V t V -1 t ≤ γ -t V -1 t V t V -1 t ≤ γ -t V -1 t .</formula><p>Thanks to Lemma 4 we establish the following proposition, Proposition 4.</p><formula xml:id="formula_50">T t=1 min 1, A t 2 V -1 t-1 Vt-1V -1 t-1 ≤ 2 T t=1 log 1 + γ -t A t 2 V -1 t-1 ≤ 2 log det(V T ) λ d .</formula><p>Proof. We first use the fact that: ∀x ≥ 0, min(1, x) ≤ 2 log(1 + x).</p><formula xml:id="formula_51">min 1, A t 2 V -1 t-1 Vt-1V -1 t-1 ≤ 2 log 1 + A t 2 V -1 t-1 Vt-1V -1 t-1 ≤ 2 log 1 + γ -(t-1) A t 2 V -1 t-1 (Lemma 4) ≤ 2 log 1 + γ -t A t 2 V -1 t-1 (γ ≤ 1).</formula><p>Furthermore,</p><formula xml:id="formula_52">V t ≥ γ -t A t A t + V t-1 ≥ V 1/2 t-1 (I d + γ -t V -1/2 t-1 A t A t V -1/2 t-1 )V 1/2 t-1 .</formula><p>Given that all those matrices are symmetric positive definite, the previous inequality implies that</p><formula xml:id="formula_53">det(V t ) ≥ det(V t-1 ) det(1 + (γ -t/2 V -1/2 t-1 A t )(γ -t/2 V -1/2 t-1 A t ) ) ≥ det(V t-1 ) 1 + γ -t A t 2 V -1 t-1 Using det(I d + xx ) = 1 + x 2 2 .</formula><p>Therefore,</p><formula xml:id="formula_54">det(V T ) det(V 0 ) = T t=1 det(V t ) det(V t-1 ) ≥ T t=1 (1 + γ -t A t 2 V -1 t-1</formula><p>).</p><p>Finally by applying the log function to the previous inequality,</p><formula xml:id="formula_55">T t=1 min 1, A t 2 V -1 t-1 Vt-1V -1 t-1 ≤ 2 T t=1 log 1 + γ -t A t 2 V -1 t-1 ≤ 2 log det(V T ) det(V 0 ) . Corollary 4. T t=1 min 1, A t 2 V -1 t-1 Vt-1V -1 t-1 ≤ √ 2d T log 1 γ + log 1 + L 2 dλ(1 -γ) .</formula><p>Proof. The proof of this corollary is based on the previous lemma and on Corollary 2. We have</p><formula xml:id="formula_56">log det(V T ) det(V 0 ) ≤ log 1 λ d λγ -T + L 2 (γ -T -1) d(1 -γ) d (<label>Corollary 2)</label></formula><formula xml:id="formula_57">≤ dT log 1 γ + d log 1 + L 2 dλ(1 -γ) .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 Proof of Theorem 2</head><p>In this subsection we give the proof of Theorem 2 for the high probability upper-bound of the regret for D-LinUCB.</p><p>Proof.</p><p>First step: Upper bound for the instantaneous regret.</p><p>Let A t = arg max a∈At a, θ t and θ t = arg max θ∈Ct A t , θ . We have,</p><formula xml:id="formula_58">r t = max a∈At a, θ t -A t , θ t = A t -A t , θ t = A t -A t , θt + A t -A t , θ t -θt .</formula><p>Under the event {∀t &gt; 0, θt ∈ C t }, that occurs with probability at least 1 -δ thanks to Proposition 3, we have,</p><formula xml:id="formula_59">A t , θt ≤ arg max θ∈Ct A t , θ = UCB t (A t ) ≤ UCB t (A t ) = arg max θ∈Ct A t , θ = A t , θ t .<label>(10)</label></formula><p>Then, with probability at least 1 -δ, ∀t &gt; 0,</p><formula xml:id="formula_60">r t ≤ A t , θ t -θt + A t -A t , θ t -θt ≤ A t V -1 t-1 Vt-1V -1 t-1 θ t -θt Vt-1 V -1 t-1 Vt-1 + A t -A t 2 θ t -θt 2 (Cauchy-Schwarz) ≤ A t V -1 t-1 Vt-1V -1 t-1 θ t -θt Vt-1 V -1 t-1 Vt-1 + 2L θ t -θt 2 (∀a ∈ A t a 2 ≤ L).</formula><p>As discussed in Section 3.2, the two terms are upper bounded using different techniques. The first term is handled with the equivalent in a non-stationary environment of the deviation inequality of Theorem 1 and the second term is the equivalent of the bias.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Second step: Upper bound for θ</head><formula xml:id="formula_61">t -θt Vt-1 V -1 t-1 Vt-1 . We have, θ t -θt Vt-1 V -1 t-1 Vt-1 ≤ θ t -θt-1 Vt-1 V -1 t-1 Vt-1 + θt -θt-1 Vt-1 V -1 t-1 Vt-1 ≤ 2β t-1</formula><p>, where the last inequality holds because under our assumption θt ∈ C t with high probability and by definition θ t ∈ C t .</p><p>Third step: Upper bound for the bias.</p><formula xml:id="formula_62">Let D &gt; 0, θ t -θt 2 = V -1 t-1 t-1 s=1 γ -s A s A s (θ s -θ t ) 2 ≤ t-1 s=t-D V -1 t-1 γ -s A s A s (θ s -θ t ) 2 + V -1 t-1 t-D-1 s=1 γ -s A s A s (θ s -θ t ) 2 ≤ t-1 s=t-D V -1 t-1 γ -s A s A s t-1 p=s (θ p -θ p+1 ) 2 + t-D-1 s=1 γ -s A s A s (θ s -θ t ) V -2 t-1 ≤ t-1 p=t-D V -1 t-1 γ -s A s A s p s=t-D (θ p -θ p+1 ) 2 + 1 λ t-D-1 s=1 γ t-1-s A s A s (θ s -θ t ) 2 ≤ t-1 p=t-D V -1 t-1 p s=t-D γ -s A s A s (θ p -θ p+1 ) 2 + 2L 2 S λ t-D-1 s=1 γ t-1-s ≤ t-1 p=t-D λ max V -1 t-1 p s=t-D γ -s A s A s θ p -θ p+1 2 + 2L 2 S λ γ D 1 -γ .</formula><p>The first inequality is a consequence of the triangular inequality. The third inequality uses that</p><formula xml:id="formula_63">V -2 t-1 ≤ ( γ t-1 λ ) 2 I d .</formula><p>In the last inequality, we have used the fact that for a symmetric matrix</p><formula xml:id="formula_64">M ∈ M d (R) and a vector x ∈ R d , M x 2 ≤ λ max (M ) x 2 . Furthermore, for x such that x 2 ≤ 1, we have that for t -D ≤ p ≤ t -1, x V -1 t-1 p s=t-D γ -s A s A s x ≤ x V -1 t-1 t-1 s=1 γ -s A s A s x + λγ -(t-1) x V -1 t-1 x ≤ x V -1 t-1 ( t-1 s=1 γ -s A s A s + λγ -(t-1) I d )x = x x ≤ 1.</formula><p>Therefore, for all p such that t -</p><formula xml:id="formula_65">D ≤ p ≤ t -1, λ max V -1 t-1 p s=t-D γ -s A s A s ≤ 1.</formula><p>By combining the second and the third step, with probability at least 1 -δ:</p><formula xml:id="formula_66">r t ≤ 2L t-1 p=t-D θ p -θ p+1 2 + 4L 3 S λ γ D 1 -γ + 2β t-1 A t V -1 t-1 Vt-1V -1 t-1</formula><p>.</p><p>The assumption | A t , θ t | ≤ 1 also implies r t ≤ 2. Hence, with probability at least 1 -δ:</p><formula xml:id="formula_67">r t ≤ 2L t-1 p=t-D θ p -θ p+1 2 + 4L 3 S γ D 1 -γ + 2β t-1 min(1, A t V -1 t-1 Vt-1V -1 t-1</formula><p>).</p><p>To conclude the proof we use the results of Subsection B.2.</p><p>Final step:</p><formula xml:id="formula_69">R T = T t=1 r t ≤ 2L T t=1 t-1 p=t-D θ p -θ p+1 2 + 4L 3 S λ γ D 1 -γ T + 2β T T t=1 min 1, A t V -1 t-1 Vt-1V -1 t-1 ≤ 2L T t=1 t-1 p=t-D θ p -θ p+1 2 + 4L 3 S λ γ D 1 -γ T + 2β T √ T T t=1 min 1, A t 2 V -1 t-1 Vt-1V -1 t-1 ≤ 2LB T D + 4L 3 S λ γ D 1 -γ T + 2 √ 2β T √ dT T log(1/γ) + log 1 + L 2 dλ(1 -γ) .</formula><p>In the first inequality, we use that t → β t is increasing. The second inequality is an application of the Cauchy-Schwarz inequality to the third term and the last inequality is an application of Corollary 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4 Proof of Corollary 1</head><p>Proof. Let γ be defined as γ = 1 -( B T dT ) 2/3 and D = log(T ) (1-γ) . With this choice of γ, D is equivalent to d 2/3 B -2/3 T T 2/3 log(T ). Thus, DB T is equivalent to d 2/3 B 1/3 T T 2/3 log(T ). In addition,</p><formula xml:id="formula_70">γ D = exp(D log(γ)) = exp log(γ) 1 -γ log(T ) ∼ 1/T. Hence, T γ D 1 1-γ behaves as d 2/3 T 2/3 B -2/3 T . Furthermore, log(1/γ) ∼ d -2/3 B 2/3 T T -2/3 , implying that T log(1/γ) ∼ d -2/3 B 2/3 T T 1/3 .</formula><p>As a result, it holds that,</p><formula xml:id="formula_71">β T √ dT T log(1/γ) + log 1 + L 2 dλ(1-γ)</formula><p>is equivalent to</p><formula xml:id="formula_72">dT 1/2 log(T /B T ) d -2/3 B 2/3 T T 1/3 = d 2/3 B 1/3</formula><p>T T 2/3 log(T /B T ). By adding those three terms and neglecting the log factors, we obtain the desired result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C A new analysis of the SW-LinUCB algorithm</head><p>In this section we propose a new analysis of the SW-LinUCB algorithm. This is useful as the proof provided in <ref type="bibr" target="#b10">[11]</ref> has several gaps. First, Lemma 2 of <ref type="bibr" target="#b10">[11]</ref> is presented as a specific case of the analysis of <ref type="bibr" target="#b0">[1]</ref>. It would hold in the case of a growing window, where the argument developed in <ref type="bibr" target="#b0">[1]</ref> could be used, but not with a sliding window, where past actions are removed from the design matrix. Furthermore, Theorem 2 of <ref type="bibr" target="#b10">[11]</ref> that bounds | x, θt-1 -θ t | for any fixed direction x with high probability is used in equation (42) with x replaced by A t , whereas A t is a random variable strongly related to θt-1 .</p><p>We only mention this analysis in the Appendix because the deviation inequalities established for the weighted model can not be used. Nevertheless, we believe that this analysis gives new insights on the problem with a sliding window.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 Deviation inequality</head><p>Let us introduce some notations to clarify the model. We suppose that there is a sliding window of length l, such that the estimate of the unknown parameter at time t is based on the l last observations. The optimization program solved is θt = arg min</p><formula xml:id="formula_73">θ∈R d   t s=max(1,t-l+1) (X s -A s , θ ) 2 + λ/2 θ 2 2 )   .</formula><p>One has</p><formula xml:id="formula_74">θt = V -1 t t s=max(1,t-l+1) A s X s , where V t = t s=max(1,t-l+1) A s A s + λI d .<label>(12)</label></formula><p>The expression linking the matrices V t and V t-1 is the following</p><formula xml:id="formula_75">V t = V t-1 + A t A t -A t-l A t-l .</formula><p>The specificity of the sliding window model is that at time t, to update the design matrix, a new action vector A t is added but the oldest term A t-l is also removed . When considering the equivalent of the quantity M t (x) defined in the Appendix A, the property of supermartingale does not hold anymore because of this loss of information. For this reason, all the reasoning that was done in <ref type="bibr" target="#b0">[1]</ref> can not be applied directly.</p><p>The reward generation process we consider is still the one presented in Equation <ref type="formula" target="#formula_0">1</ref>. As for the D-LinUCB model, the results are stated with σ-subgaussian random noises but the proofs are done with σ = 1. Let S t = t s=max(1,t-l+1) A s η s . We start by giving the proof of the analogue of Lemma 2 presented in <ref type="bibr" target="#b10">[11]</ref>. We give an instantaneous deviation inequality. Proposition 5 (Instantaneous deviation inequality with a sliding window). Let t be a fixed time instant. For all δ &gt; 0,</p><formula xml:id="formula_76">P S t V -1 t ≥ σ 2 log 1 δ + log det(V t ) λ d ≤ δ.</formula><p>Proof. We present an interesting trick in this proof for avoiding the loss of information due to the sliding window that is only usable for instantaneous deviation inequalities.</p><p>Let t be the time instant of interest. We assume that t ≥ l. We know that the estimate θt is only based on observations between time t -l + 1 to t. The trick is to create a fictive regression model starting a time t -l and receiving the exact same information as the true model between the time instants t -l + 1 to t.</p><p>To ease the understanding of the proof, the notations with dotted symbols refer to the fictive model. Let u be a time instant in Using the 1-subgaussianity and following the lines of the proof of Lemma 1,</p><formula xml:id="formula_77">[[t -l, t]]. Let Vu = u s=max(1,t-l+1) A s A s + λI d , Ṡu =</formula><formula xml:id="formula_78">E[ Ṁu (x)|F u-1 ] ≤ Ṁu-1 (x). Therefore, ∀u ∈ [[t -l, t]], E[ Ṁu (x)] ≤ E[ Ṁt-l (x)] = 1. In particular for u = t, ∀x ∈ R d , E[ Ṁt (x)] ≤ 1.</formula><p>By introducing a measure of probability h = N (0, 1 λ I d ), we still have E Ṁt (x)dh(x) ≤ 1 using a similar reasoning than in Lemma 2. We can also give an exact formula for Ṁt (x)dh(x) with the chosen h. Let us remark that Ṡt = S t and Vt = V t .</p><formula xml:id="formula_79">R d Ṁt (x)dh(x) = 1 (2π) d det(1/λI d ) R d exp x S t - 1 2 x 2 λI d - 1 2 x 2 Vt(0) dx = 1 (2π) d det(1/λI d ) R d exp 1/2 S t 2 V -1 t -1/2 x -V -1 t S t 2 Vt dx = exp 1 2 S t 2 V -1 t (2π) d det(1/λI d ) R d exp - 1 2 x -V -1 t S t 2 Vt dx = exp 1 2 S t 2 V -1 t (2π) d det(1/λI d ) (2π) d det V -1 t = exp 1 2 S t 2 V -1 t det(λI d ) det(V t ) .</formula><p>For this reason,</p><formula xml:id="formula_80">P S t V -1 t ≥ 2 log 1 δ + log det(V t ) det(λI d ) = P exp 1 2 S t 2 V -1 t det(λI d ) det(V t ) ≥ 1 δ ≤ δE R d Ṁt (x)dh(x) (Markov's inequality) ≤ δ.</formula><p>The next step is to upper-bound the quantity det(V t ) similarly as in Proposition 2 for the weighted model. Proposition 6 (Determinant inequality for the design matrix with a sliding window). In the specific case where V t is defined as</p><formula xml:id="formula_81">V t = t s=max(1,t-l+1) A s A s + λI d . Under the assumption ∀t, A t 2 ≤ L, the following holds, det(V t ) ≤ λ + L 2 min(t, l) d d .</formula><p>The proof of this proposition is the same as in Proposition 2. By using the previous inequality, we can obtain the following proposition, Proposition 7. When using a sliding window model where the last l terms are considered, for all δ &gt; 0,</p><formula xml:id="formula_82">P ∃t ≤ T, S t V -1 t ≥ σ 2 log T δ + d log 1 + L 2 min(t, l) λd ≤ δ.</formula><p>Proof.</p><formula xml:id="formula_83">P ∃t ≤ T, S t V -1 t ≥ σ 2 log T δ + d log 1 + L 2 min(t, l) λd ≤ T t=1 P S t V -1 t ≥ σ 2 log T δ + d log 1 + L 2 min(t, l) λd ≤ T t=1 P S t V -1 t ≥ σ 2 log T δ + log det(V t ) λ d ≤ T t=1 δ T (Proposition 5) ≤ δ.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Regret analysis</head><p>The regret analysis of the SW-LinUCB algorithm is similar to the one proposed for D-LinUCB. We start by defining the confidence ellipsoid used by the algorithm SW-LinUCB.</p><p>With the SW-LinUCB algorithm, the β t term is defined in the following way,</p><formula xml:id="formula_84">β t = √ λS + σ 2 log T δ + d log 1 + L 2 min(t, l) λd<label>(13)</label></formula><p>Remark: The cost of loosing some information at each step due to the sliding window when t &gt; l is the term log T δ rather than log 1 δ in the definition of β t . Note that due to the use of a union bound technique the confidence radius is larger than the one suggested in <ref type="bibr" target="#b10">[11]</ref>. Nevertheless, this was not taken into account in simulations for SW-LinUCB. </p><formula xml:id="formula_85">8. Let C t = θ ∈ R d : θ -θt-1 V -1 t-1 ≤ β t-1 denote the confidence ellipsoid. Let θt = V -1 t-1 t-1 s=max(1,t-l) A s A s θ s + λθ t . Then, ∀δ &gt; 0, P ∀t ≥ 1, θt ∈ C t ≥ 1 -δ. Proof. θt -θt-1 = V -1 t-1   t-1 s=max(1,t-l) A s A s θ s + λθ t - t-1 s=max(1,t-l) A s A s θ s - t-1 s=max(1,t-l) A s η s   = -V -1 t-1 S t-1 + λV -1 t-1 θ t . Therefore, θt -θt-1 V -1 t-1 ≤ S t-1 V -1 t-1 + λ θ t V -1 t-1 ≤ S t-1 V -1 t-1 + √ λS (V -1 t-1 ≤ 1 λ I d )</formula><p>≤ β t-1 (with probability ≥ 1 -δ thanks to Proposition 7).</p><p>We need to bound the quantity</p><formula xml:id="formula_86">T t=1 min 1, A t 2 V -1 t-1</formula><p>. An analysis of this quantity is already proved in <ref type="bibr" target="#b10">[11]</ref>. Nevertheless, we provide a simpler analysis in the following proposition. Proposition 9. With the sliding window model, the following upper bound holds,</p><formula xml:id="formula_87">T t=1 min 1, A t 2 V -1 t-1 ≤ 2d T /l log 1 + lL 2 λd .</formula><p>Proof. We start by rewriting the sum as follows.</p><formula xml:id="formula_88">T t=1 min 1, A t 2 V -1 t-1 = T /l -1 k=0 (k+1)l t=kl+1 min 1, A t 2 V -1 t-1</formula><p>For the k-th block of length l we define the matrix W</p><formula xml:id="formula_89">(k) t = t s=kl+1 A s A s + λI d . We also have ∀t ∈ [[kl, (k + 1)l]], V t ≥ W (k) t as every term in W (k) t</formula><p>is contained in V t and the extra-terms in V t correspond to positive definite matrices. The matrices are definite positive, thus</p><formula xml:id="formula_90">V -1 t ≤ (W (k) t ) -1 and consequently, T /l -1 k=0 (k+1)l t=kl+1 min 1, A t 2 V -1 t-1 ≤ T /l -1 k=0 (k+1)l t=kl+1 min 1, A t 2 (W (k) t-1 ) -1 Furthermore, ∀t ∈ [[kl, (k + 1)l]] we have, det(W (k) t ) = det(W (k) t-1 ) 1 + A t 2 (W (k) t-1 ) -1 .</formula><p>With positive definitive matrices whose determinants are strictly positive, this implies that With those results we can give a high probability upper bound for the cumulative dynamic regret of the SW-LinUCB algorithm.</p><formula xml:id="formula_91">det(W (k) (k+1)l ) det(W (k) kl ) = (k+1)l t=kl+1 det(W (k) t ) det(W (k) t-1 ) = (k+1)l t=kl+1 1 + A t 2 (W (k) t-1 ) -1 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>By definition we have W</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Theorem 3. Assuming that</head><p>T -1 s=1 θ s -θ s+1 2 ≤ B T , the regret of the SW-LinUCB algorithm may be bounded for all l &gt; 0, with probability at least 1 -δ, by</p><formula xml:id="formula_92">R T ≤ 2LB T l + 2 √ 2β T √ dT T /l log 1 + L 2 l λd ,</formula><p>where β T is defined in Equation ( <ref type="formula" target="#formula_84">13</ref>).</p><p>Proof. </p><formula xml:id="formula_93">1rst</formula><p>Using Inequality <ref type="bibr" target="#b13">(14)</ref>, with probability larger than 1 -δ, ∀t &gt; 0,</p><formula xml:id="formula_95">r t ≤ A t , θ t -θt + A t -A t , θ t -θt ≤ A t V -1 t-1 θ t -θt Vt-1 + A t -A t 2 θ t -θt 2 (Cauchy-Schwarz) ≤ A t V -1 t-1</formula><p>θ t -θt Vt-1 + 2L θ t -θt 2 (Bounded action assumption).</p><p>As for the analysis of the regret for the D-LinUCB algorithm, the two terms are upper bounded using different techniques. The first term is handled with the deviation inequality of Proposition 8.</p><p>2nd step: Upper bound for θ t -θt Vt-1</p><p>We have, θ t -θt Vt-1 ≤ θ t -θt-1 Vt-1 + θtθt-1 Vt-1 ≤ 2β t-1 .</p><p>Where the last inequality holds because under our assumption θt ∈ C t with probability at least 1 -δ and by definition θ t ∈ C t .</p><p>3rd step: Upper bound for the bias.</p><p>This step is similar to the proof proposed in <ref type="bibr" target="#b10">[11]</ref> for Lemma 1. </p><formula xml:id="formula_96">θ t -θt 2 = V -1 t-1   t-1 s=max(</formula><formula xml:id="formula_97">A s A s x ≤ x V -1 t-1 t-1 s=max(1,t-l) A s A s x + λx V -1 t-1 x ≤ x V -1 t-1   t-1 s=max(1,t-l) A s A s + λI d   x = x x ≤ 1.</formula><p>By combining the second and the third step, </p><formula xml:id="formula_98">r</formula><p>To conclude the proof, we use the results of Proposition 9. In the first inequality, we use the fact that t → β t is increasing. The second inequality is an application of the Cauchy-Schwarz inequality to the second term. The last inequality is an application of Proposition 9</p><p>By denoting Õ the function growth when omitting the logarithmic terms, we have the following Corollary.</p><p>Corollary 5 (Asymptotic regret bound for SW-LinUCB). If B T is known, by choosing l = ( dT B T ) 2/3 , the regret of the SW-LinUCB algorithm is asymptotically upper bounded with high probability by a term Õ(d 2/3 B 1/3 T T 2/3 ) when T → ∞. If B T is unknown, by choosing l = d 2/3 T 2/3 , the regret of the SW-LinUCB algorithm is asymptotically upper bounded with high probability by a term Õ(d 2/3 B T T 2/3 ) when T → ∞.</p><p>Proof. With this particular choice of l, we have:</p><formula xml:id="formula_100">lB T ∼ d 2/3 T 2/3 B 1/3 T .</formula><p>β T as defined by equation ( <ref type="formula" target="#formula_84">13</ref>) is equivalent to d log(T ). </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Performances of the algorithms in the abruptly-changing environment (on the left), and, the slowly-varying environment (on the right). The upper plots correspond to the estimated parameter and the lower ones to the accumulated regret, both are averaged on N = 100 independent experimentsIn this first experiment, we observe the empirical performance of all algorithms in an abruptly changing environment of dimension 2 with 3 breakpoints. The number of rounds is set to T = 6000. The light blue triangles correspond to the different positions of the true unknown parameter θ t : before t = 1000,θ t = (1, 0); for t ∈ [[1000, 2000]], θ t = (-1, 0); for t ∈ [[2000, 3000]], θ t = (0, 1); and, finally, for t &gt; 3000, θ t = (0, -1). This corresponds to a hard problem as the sequence of parameters is widely spread in the unit ball. Indeed it forces the algorithm to adapt to big changes, which typically requires a longer adaptation phase. On the other hand, it makes the detection of changes easier, which is an advantage for dLinUCB. In the second half of the experiment (when t ≥ 3000) there is no change, LinUCB struggles to catch up and suffers linear regret for long periods after the last change-point. The results of our simulations are shown in the left column of Figure1. On the top row we show a 2-dimensional scatter plot of the estimate of the unknown parameters θt every 1000 steps averaged on 100 independent experiment. The bottom row corresponds to the regret averaged over 100 independent experiments with the upper and the lower 5% quantiles. In this environment, with 1-subgaussian random noise, dLinUCB struggles to detect the change-points. Over the 100 experiments, the first change-point was detected in 95% of the runs, the second was never detected and the third only in 6% of the runs, thus limiting the effectiveness of the dLinUCB approach. When decreasing the variance of the noise, the performance of dLinUCB improves and gets closer to</figDesc><graphic coords="8,312.97,364.63,144.54,117.24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Behavior of the different algorithms on large-dimensional data</figDesc><graphic coords="9,224.82,303.33,162.36,131.48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>t-l+1) A s η s and Ṁu (x) = exp(x Ṡu -x Vu (0)x/2). Once again, Vu (0) = u s=max(1,t-l+1) A s A s corresponds to the design matrix without the regularization term. By definition, ∀x ∈ R d , Ṁt-l (x) = 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Proposition</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>1 ≤</head><label>1</label><figDesc>(k) kl = λI d and ∀x ≥ 0, min(1, x) ≤ 2 log(1 + x). So, l contains exactly l terms allows us to give the following bound (by following the proof of Proposition 2), 2d T /l log 1 + L 2 l λd .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>3 B 1 / 3 T 3 B 1 / 3 T 3 T</head><label>3133133</label><figDesc>has a similar behavior than d -1/3 T 1-1/, consequently the behavior of β T √ dT T /l log 1 + lL 2 λd is similar to d 2/T 2/3 log(T ) log(T /B T ). By neglecting the logarithmic term, we have with high probability, R T = ÕT →∞ (d 2/3 B 1/T 2/3 ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>step: Upper bound for the instantaneous regret Defining A t = arg max a∈At a, θ t and θ t = arg max θ∈Ct A t , θ . We have,</figDesc><table><row><cell>r t = max</cell></row></table><note><p><p><p>a∈At a, θ t -A t , θ t = A t -A t , θ t = A t -A t , θt + A t -A t , θ t -θt</p>Under the event {∀t &gt; 0, θt ∈ C t }, that occurs with probability at least 1 -δ thanks to Proposition 8,</p>A t , θt ≤ arg max θ∈Ct A t , θ = UCB t (A t ) ≤ UCB t (A t ) = arg max θ∈Ct A t , θ = A t , θ t</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>1,t-l)A s A s (θ s -θ t ) A s A s (θ p -θ p+1 ) A s A s (θ p -θ p+1 ) Furthermore, for x ∈ R d such that x 2 ≤ 1, we have that for max(1, t -l) ≤ p ≤ t -1,</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>2</cell></row><row><cell>t-1</cell><cell></cell><cell>t-1</cell><cell></cell></row><row><cell>≤</cell><cell cols="2">V -1 t-1 A s A s</cell><cell>(θ p -θ p+1 )</cell></row><row><cell cols="2">s=max(1,t-l)</cell><cell>p=s</cell><cell>2</cell></row><row><cell>t-1</cell><cell></cell><cell>p</cell><cell></cell></row><row><cell>≤</cell><cell cols="2">V -1 t-1</cell><cell></cell></row><row><cell cols="2">p=max(1,t-l)</cell><cell cols="2">s=max(1,t-l)</cell><cell>2</cell></row><row><cell>t-1</cell><cell></cell><cell>p</cell><cell></cell></row><row><cell>≤</cell><cell cols="2">V -1 t-1</cell><cell></cell></row><row><cell>p=max(1,t-l)</cell><cell></cell><cell cols="2">s=max(1,t-l)</cell><cell>2</cell></row><row><cell>t-1</cell><cell></cell><cell></cell><cell>p</cell></row><row><cell>≤</cell><cell>λ max</cell><cell> V -1 t-1</cell><cell>A s A s</cell></row><row><cell>p=max(1,t-l)</cell><cell></cell><cell cols="2">s=max(1,t-l)</cell></row><row><cell>p</cell><cell></cell><cell></cell><cell></cell></row><row><cell>x V -1 t-1</cell><cell></cell><cell></cell><cell></cell></row><row><cell>s=max(1,t-l)</cell><cell></cell><cell></cell><cell></cell></row></table><note><p>  θ p -θ p+1 2 .</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Improved algorithms for linear stochastic bandits</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Abbasi-Yadkori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Pál</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szepesvári</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="2312" to="2320" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Using confidence bounds for exploitation-exploration trade-offs</title>
		<author>
			<persName><forename type="first">P</forename><surname>Auer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="397" to="422" />
			<date type="published" when="2002-11">Nov. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Adaptively tracking the best arm with an unknown number of distribution changes</title>
		<author>
			<persName><forename type="first">P</forename><surname>Auer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Gajane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ortner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Workshop on Reinforcement Learning</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Stochastic multi-armed-bandit problem with non-stationary rewards</title>
		<author>
			<persName><forename type="first">O</forename><surname>Besbes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zeevi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="199" to="207" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Non-stationary stochastic optimization</title>
		<author>
			<persName><forename type="first">O</forename><surname>Besbes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zeevi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Operations research</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1227" to="1244" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Optimal exploration-exploitation in a multi-armed-bandit problem with non-stationary rewards</title>
		<author>
			<persName><forename type="first">O</forename><surname>Besbes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zeevi</surname></persName>
		</author>
		<idno>SSRN 2436629</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Besson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Kaufmann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.01575</idno>
		<title level="m">The generalized likelihood ratio test meets klucb: an improved algorithm for piece-wise non-stationary bandits</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>Bleakley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-P</forename><surname>Vert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1106.4199</idno>
		<title level="m">The group fused lasso for multiple change-point detection</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Nearly optimal adaptive procedure for piecewisestationary bandit: a change-point detection approach</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kveton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.03692</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-W</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-Y</forename><surname>Wei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.00980</idno>
		<title level="m">A new algorithm for non-stationary contextual bandits: Efficient, optimal, and parameter-free</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">C</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Simchi-Levi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.03024</idno>
		<title level="m">Learning to optimize under non-stationarity</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">C</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Simchi-Levi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.01461</idno>
		<title level="m">Hedging the drift: Learning to optimize under non-stationarity</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Attribution modeling increases efficiency of bidding in display advertising</title>
		<author>
			<persName><forename type="first">Diemert</forename><surname>Eustache</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Meynet Julien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Galland</surname></persName>
		</author>
		<author>
			<persName><surname>Lefortier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AdKDD and TargetAd Workshop</title>
		<meeting>the AdKDD and TargetAd Workshop<address><addrLine>Halifax, NS, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017-08-14">August, 14, 2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">On upper-confidence bound policies for switching bandit problems</title>
		<author>
			<persName><forename type="first">A</forename><surname>Garivier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Moulines</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Algorithmic Learning Theory</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="174" to="188" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A linear response bandit problem</title>
		<author>
			<persName><forename type="first">A</forename><surname>Goldenshluger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zeevi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Stoch. Syst</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="230" to="261" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Thompson sampling for dynamic multi-armed bandits</title>
		<author>
			<persName><forename type="first">N</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O.-C</forename><surname>Granmo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Agrawala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 10th International Conference on Machine Learning and Applications and Workshops</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Chasing demand: Learning and earning in a changing environment</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">B</forename><surname>Keskin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zeevi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematics of Operations Research</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="277" to="307" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Information directed sampling and bandits with heteroscedastic noise</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kirschner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krause</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.09667</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Discounted ucb</title>
		<author>
			<persName><forename type="first">L</forename><surname>Kocsis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szepesvári</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2nd Pascal Challenge Workshop</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Bandit Algorithms</title>
		<author>
			<persName><forename type="first">T</forename><surname>Lattimore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szepesvári</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Rotting bandits</title>
		<author>
			<persName><forename type="first">N</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mannor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3074" to="3083" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A contextual-bandit approach to personalized news article recommendation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Langford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Schapire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Langford</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.01799</idno>
		<title level="m">Efficient contextual bandits in non-stationary worlds</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Mintz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Aswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kaminsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Flowers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fukuoka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.08423</idno>
		<title level="m">Non-stationary bandits with habituation and recovery dynamics</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Self-normalized processes: Limit theory and Statistical Applications</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">H</forename><surname>Peña</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q.-M</forename><surname>Shao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Taming non-stationary bandits: A bayesian approach</title>
		<author>
			<persName><forename type="first">V</forename><surname>Raj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kalyani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.09727</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Seznec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Locatelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Carpentier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lazaric</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Valko</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.11043</idno>
		<title level="m">Rotting bandits are no harder than stochastic ones</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">On abruptly-changing and slowly-varying multiarmed bandit problems</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Srivatsva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 Annual American Control Conference (ACC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="6291" to="6296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning contextual bandits in a non-stationary environment</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 41st International ACM SIGIR Conference on Research &amp; Development in Information Retrieval, SIGIR &apos;18</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="495" to="504" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Piecewise-stationary bandit problems with side observations</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mannor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th Annual International Conference on Machine Learning</title>
		<meeting>the 26th Annual International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1177" to="1184" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
