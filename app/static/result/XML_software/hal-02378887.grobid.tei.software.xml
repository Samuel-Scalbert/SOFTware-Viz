<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Model-Based Reinforcement Learning Exploiting State-Action Equivalence</title>
				<funder ref="#_UDf4r32">
					<orgName type="full">French Agence Nationale de la Recherche</orgName>
					<orgName type="abbreviated">ANR</orgName>
				</funder>
				<funder ref="#_Ph5HmcU">
					<orgName type="full">French Ministry of Higher Education and Research, Inria</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Mahsa</forename><surname>Asadi</surname></persName>
							<email>mahsa.asadi@inria.fr</email>
							<affiliation key="aff0">
								<orgName type="institution">Inria Lille -Nord</orgName>
								<address>
									<country>Europe</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mohammad</forename><forename type="middle">Sadegh</forename><surname>Talebi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Inria Lille -Nord</orgName>
								<address>
									<country>Europe</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hippolyte</forename><surname>Bourel</surname></persName>
							<email>hippolyte.bourel@ens-rennes.frodalric-ambrym</email>
							<affiliation key="aff0">
								<orgName type="institution">Inria Lille -Nord</orgName>
								<address>
									<country>Europe</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><surname>Maillard</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Inria Lille -Nord</orgName>
								<address>
									<country>Europe</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Wee</forename><forename type="middle">Sun</forename><surname>Lee</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Inria Lille -Nord</orgName>
								<address>
									<country>Europe</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Taiji</forename><surname>Suzuki</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Inria Lille -Nord</orgName>
								<address>
									<country>Europe</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Model-Based Reinforcement Learning Exploiting State-Action Equivalence</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">E8C5FC6F143546BD315EEBC1EBDC639A</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-04-12T14:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Reinforcement Learning</term>
					<term>Regret</term>
					<term>Confidence Bound</term>
					<term>Equivalence</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Leveraging an equivalence property in the state-space of a Markov Decision Process (MDP) has been investigated in several studies. This paper studies equivalence structure in the reinforcement learning (RL) setup, where transition distributions are no longer assumed to be known. We present a notion of similarity between transition probabilities of various state-action pairs of an MDP, which naturally defines an equivalence structure in the state-action space. We present equivalence-aware confidence sets for the case where the learner knows the underlying structure in advance. These sets are provably smaller than their corresponding equivalence-oblivious counterparts. In the more challenging case of an unknown equivalence structure, we present an algorithm called ApproxEquivalence that seeks to find an (approximate) equivalence structure, and define confidence sets using the approximate equivalence. To illustrate the efficacy of the presented confidence sets, we present C-UCRL, as a natural modification of UCRL2 for RL in undiscounted MDPs. In the case of a known equivalence structure, we show that C-UCRL improves over UCRL2 in terms of regret by a factor of SA/C, in any communicating MDP with S states, A actions, and C classes, which corresponds to a massive improvement when C SA. To the best of our knowledge, this is the first work providing regret bounds for RL when an equivalence structure in the MDP is efficiently exploited. In the case of an unknown equivalence structure, we show through numerical experiments that C-UCRL combined with ApproxEquivalence outperforms UCRL2 in ergodic MDPs.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>This paper studies the Reinforcement Learning (RL) problem, where an agent interacts with an unknown environment in a single stream of observations, with the aim of maximizing the cumulative reward gathered over the course of experience. The environment is modeled as a Markov Decision Process (MDP), with finite state and action spaces, as considered in most literature; we refer to <ref type="bibr" target="#b29">(Puterman, 2014;</ref><ref type="bibr" target="#b32">Sutton and Barto, 1998)</ref> for background materials on MDPs, and to Section 2. In order to act optimally or nearly so, the agent needs to learn the parameters of the MDP using the observations from the environment. The agent thus faces a fundamental trade-off between exploitation vs. exploration: Namely, whether to gather more experimental data about the consequences of the actions (exploration) or acting c 2019 M. Asadi, M.S. Talebi, H. Bourel &amp; O.-A. <ref type="bibr">Maillard.</ref> consistently with past observations to maximize the rewards (exploitation); see <ref type="bibr" target="#b32">(Sutton and Barto, 1998)</ref>. Over the past two decades, a plethora of studies have addressed the above RL problem in the undiscounted setting, where the goal is to minimize the regret (e.g., <ref type="bibr" target="#b4">Bartlett and Tewari (2009)</ref>; <ref type="bibr" target="#b17">Jaksch et al. (2010)</ref>; Gheshlaghi <ref type="bibr" target="#b13">Azar et al. (2017)</ref>), or in the discounted setting (as in, e.g., <ref type="bibr" target="#b31">Strehl and Littman (2008)</ref>) with the goal of bounding the sample complexity of exploration as defined in <ref type="bibr" target="#b18">(Kakade, 2003)</ref>. In most practical situations, the state-space of the underlying MDP is too large, but often endowed with some structure. Directly applying the state-of-the-art RL algorithms, for instance from the above works, and ignoring the structure would lead to a prohibitive regret or sample complexity.</p><p>In this paper, we consider RL problems where the state-action space of the underlying MDP exhibits some equivalence structure. This is quite typical in many MDPs in various application domains. For instance, in a grid-world MDP when taking action 'up' from state s or 'right' from state s when both are away from any wall may result in similar transitions (typically, move towards the target state with some probability, and stay still or transit to other neighbors with the remaining probability); see, e.g., Figure <ref type="figure" target="#fig_1">1</ref> in Section 2. We are interested in exploiting such a structure in order to speed up the learning process. Leveraging an equivalence structure is popular in the MDP literature; see <ref type="bibr" target="#b30">(Ravindran and Barto, 2004;</ref><ref type="bibr" target="#b22">Li et al., 2006;</ref><ref type="bibr" target="#b1">Abel et al., 2016)</ref>. However, most notions are unfortunately not well adapted to the RL setup, that is when the underlying MDP is unknown, as opposed to the known MDP setup. In particular, amongst those considering such structures, to our knowledge, none has provided performance guarantees in terms of regret or sample complexity. Our goal is to find a near-optimal policy, with controlled regret or sample complexity. To this end, we follow a model-based approach, which is popular in the RL literature, and aim at providing a generic model-based approach capable of exploiting this structure, to speed up learning. We do so by aggregating the information of state-action pairs in the same equivalence class when estimating the transition probabilities or reward function of the MDP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contributions.</head><p>We make the following contributions. (i) We first introduce a notion of similarity between state-action pairs, which naturally yields a partition of the state-action space S × A, and induces an equivalence structure in the MDP (see Definition 1-2). To our knowledge, while other notions of equivalence have been introduced, our proposed definition appears to be the first, in a discrete RL setup, explicitly using profile (ordering) of distributions. (ii) We present confidence sets that incorporate equivalence structure of transition probabilities and reward function into their definition, when the learner has access to such information. These confidence sets are smaller than those obtained by ignoring equivalence structures. (iii) In the case of an unknown equivalence structure, we present ApproxEquivalence, which uses confidence bounds of various state-action pairs as a proxy to estimate an empirical equivalence structure of the MDP. (iv) Finally, in order to demonstrate the application of the above equivalence-aware confidence sets, we present C-UCRL, which is a natural modification of the UCRL2 algorithm <ref type="bibr" target="#b17">(Jaksch et al., 2010)</ref> employing the presented confidence sets. As shown in Theorem 13, when the learner knows the equivalence structure, C-UCRL achieves a regret which is smaller than that of UCRL2 by a factor of SA/C, where C is the number of classes. This corresponds to a massive improvement when C SA. We also verify, through numerical experiments, the superiority of C-UCRL over UCRL2 in the case of an unknown equivalence structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work.</head><p>There is a rich literature on state-abstraction (or state-aggregation) in MDPs; we refer to <ref type="bibr" target="#b22">(Li et al., 2006)</ref> on earlier methods, and to <ref type="bibr" target="#b1">(Abel et al., 2016)</ref> for a good survey of recent approaches. <ref type="bibr" target="#b30">(Ravindran and Barto, 2004)</ref> introduces aggregation based on homo-morphisms of the model, but with no algorithm nor regret analysis. <ref type="bibr" target="#b8">(Dean et al., 1997;</ref><ref type="bibr" target="#b14">Givan et al., 2003)</ref> consider a partition of state-space of MDPs based on the notion of stochastic bi-simulation, which is a generalization of the notion of bi-simulation from the theory of concurrent processes to stochastic processes. This path is further followed in <ref type="bibr" target="#b10">(Ferns et al., 2004</ref><ref type="bibr" target="#b11">(Ferns et al., , 2011))</ref>, where bi-simulation metrics for capturing similarities are presented. Bi-simulation metrics can be thought of as quantitative analogues of the equivalence relations, and suggest to resort to optimal transport, which is intimately linked with our notions of similarity and equivalence (see Definition 1). However, these powerful metrics have only been studied in the context of a known MDP, and not the RL setup. The approach in <ref type="bibr" target="#b2">(Anand et al., 2015)</ref> is similar to our work in that it considers state-action equivalence. Unlike the present paper, however, it does not consider orderings, transition estimation errors, or regret analysis. Another relevant work to our approach is <ref type="bibr" target="#b26">(Ortner, 2013)</ref> on aggregation of states (but not of pairs, and with no ordering) based on concentration inequalities, a path that we follow. We also mention the works <ref type="bibr" target="#b5">(Brunskill and Li, 2013;</ref><ref type="bibr" target="#b25">Mandel et al., 2016)</ref>, where clustering of the state-space is studied. As other relevant works, we refer to <ref type="bibr" target="#b21">(Leffler et al., 2007)</ref>, where relocatable action model is introduced, and to <ref type="bibr" target="#b9">(Diuk et al., 2009)</ref> that studies RL in the simpler setting of factored MDPs. We also mention interesting works revolving around complementary RL questions including the one on selection amongst different state representations in <ref type="bibr" target="#b27">(Ortner et al., 2014)</ref> and on state-aliasing in <ref type="bibr" target="#b15">(Hallak et al., 2013)</ref>.</p><p>As part of this paper is devoted to presenting an equivalence structure aware variant of UCRL2, we provide here a brief review of the literature related to undiscounted RL. Undiscounted RL dates back at least to <ref type="bibr" target="#b6">(Burnetas and Katehakis, 1997)</ref>, and is thoroughly investigated later on in <ref type="bibr" target="#b17">(Jaksch et al., 2010)</ref>. The latter work presents UCRL2, which is inspired by multi-armed bandit algorithms. Several studies continued this line, including <ref type="bibr" target="#b4">(Bartlett and Tewari, 2009;</ref><ref type="bibr" target="#b24">Maillard et al., 2014;</ref><ref type="bibr" target="#b13">Gheshlaghi Azar et al., 2017;</ref><ref type="bibr" target="#b7">Dann et al., 2017;</ref><ref type="bibr" target="#b33">Talebi and Maillard, 2018;</ref><ref type="bibr" target="#b12">Fruit et al., 2018)</ref>, to name a few. Most of these works present UCRL2-style algorithms, and try to reduce the regret dependency on the number of states, as in, e.g., <ref type="bibr" target="#b13">(Gheshlaghi Azar et al., 2017;</ref><ref type="bibr" target="#b7">Dann et al., 2017)</ref> (restricted to the episodic RL with a fixed and known horizon). Although the concept of equivalence is well-studied in MDPs, no work seems to have investigated the possibility of defining an aggregation that both is based on state-action pairs (instead of states only) for RL problems, and uses optimal transportation maps combined with statistical tests. Especially, the use of profile maps seems novel and we show it is also effective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Model and Equivalence Classes</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">The RL Problem</head><p>In this section, we describe the RL problem, which we study in this paper. Let M = (S, A, p, ν) be an undiscounted MDP 1 , where S denotes the discrete state-space with cardinality S, and A denotes the discrete action-space with cardinality A. Here, p represents the transition kernel such that p(s |s, a) denotes the probability of transiting to state s , starting from state s and executing action a. Finally, ν is a reward distribution function on [0, 1], whose mean is denoted by µ.</p><p>The game proceeds as follows. The learner starts in some state s 1 ∈ S at time t = 1. At each time step t ∈ N, the learner chooses one action a ∈ A in its current state s t based on its past decisions and observations. When executing action a t in state s t , the learner receives a random reward r t := r t (s t , a t ) drawn independently from distribution ν(s t , a t ), and whose mean is µ(s t , a t ). The state then transits to a next state s t+1 ∼ p(•|s t , a t ), and a new decision step begins. We refer to <ref type="bibr" target="#b32">(Sutton and Barto, 1998;</ref><ref type="bibr" target="#b29">Puterman, 2014)</ref> for background material on MDPs and RL. The goal of the learner is to maximize the cumulative reward gathered in the course of interaction with the environment. As p and ν are unknown, the learner has to learn them by trying different actions and recording the realized rewards and state transitions. The performance of the learner can be assessed through the notion of regret<ref type="foot" target="#foot_0">2</ref> with respect to an optimal oracle, being aware of p and ν. More formally, as in <ref type="bibr" target="#b17">(Jaksch et al., 2010)</ref>, under a learning algorithm A, we define the T -step regret as</p><formula xml:id="formula_0">R(A, T ) := T g - T t=1 r t (s t , a t ) ,</formula><p>where g denotes the average reward (or gain<ref type="foot" target="#foot_1">3</ref> ) attained by an optimal policy, and where a t is chosen by A as a function of ((s t , a t ) t &lt;t , s t ). Alternatively, the objective of the learner is to minimize the regret, which calls for balancing between exploration and exploitation. In the present work, we are interested in exploiting equivalence structure in the state-action space in order to speed up exploration, which, in turn, reduces the regret.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Similarity and Equivalence Classes</head><p>We now present a precise definition of the equivalence structure considered in this paper. We first introduce a notion of similarity between state-action pairs of the MDP: Definition 1 (Similar state-action pairs) The pair (s , a ) is said to be ε-similar to the pair (s, a),</p><formula xml:id="formula_1">for ε = (ε p , ε µ ) ∈ R 2 + , if p(σ s,a (•)|s, a) -p(σ s ,a (•)|s , a ) 1 ε p and |µ(s, a) -µ(s , a )| ε µ ,</formula><p>where σ s,a : {1, . . . , S} → S indexes a permutation of states such that p(σ s,a (1)|s, a) p(σ s,a (2)|s, a) . . . p(σ s,a (S)|s, a). We refer to σ s,a as a profile mapping (or for short, profile) for (s, a), and denote by σ = (σ s,a ) s,a the set of profile mappings of all pairs.</p><p>The notion of similarity introduced above naturally yields a partition of the state-action space S × A, as detailed in the following definition: Definition 2 (Equivalence classes) (0, 0)-similarity is an equivalence relation and induces a canonical partition of S × A. We refer to such a canonical partition as equivalence classes or equivalence structure, denote it by C, and let C := |C|.</p><p>In order to help understand Definitions 1 and 2, we present in Figure <ref type="figure" target="#fig_1">1</ref> an MDP with 13 states, where the state-action pairs (6,Up) and (8,Right) are equivalent up to a permutation: Let the permutation σ be such that σ(2) = 9, σ(6) = 8, and σ(i) = i for all i = 2, 6. Now <ref type="table">p(σ(x)|6, Up) = p(x|8, Right) for all x ∈ S,</ref> and<ref type="table">thus, the pairs (8,Right)</ref> and<ref type="table">(6,Up</ref>  Remark 3 Crucially, the equivalence relation is not only stated about states, but about state-action pairs. For instance, pairs (6,Up) and ( <ref type="formula">8</ref>,Right) in this example are in the same class although corresponding to playing different actions in different states.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Remark 4</head><p>The profile mapping σ s,a in Definition 1 may not be unique in general, especially if distributions have sparse supprts. For ease of presentation, in the sequel we assume that the restriction of σ s,a to the support of p(•|s, a) is uniquely defined. We also remark that Definition 1 can be easily generalized by replacing the • 1 norm with other contrasts, such as the KL divergence, squared distance, etc.</p><p>In many environments considered in RL with large state and action spaces, the number C of equivalent classes of state-action pairs using Definitions 1-2 stays small even for large SA, thanks to the profile mappings. This is the case in typical grid-world MDPs as well as in RiverSwim shown in Figure <ref type="figure" target="#fig_2">2</ref>. For example, in Ergodic RiverSwim with L states, we have C = 6. We also refer to Appendix F for additional illustrations of grid-world MDPs. This remarkable feature suggests that leveraging this structure may yield significant speed-up in terms of learning guarantees if well-exploited.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Equivalence-Aware Confidence Sets</head><p>We are now ready to present an approach that defines confidence sets for p and µ taking into account the equivalence structure in the MDP. The use of confidence bounds in a model-based approach is related to strategies implementing the optimism in the face of uncertainty principle, as in stochastic bandit problems <ref type="bibr" target="#b20">(Lai and Robbins, 1985;</ref><ref type="bibr" target="#b3">Auer et al., 2002)</ref>. Such an approach relies on maintaining a set of plausible MDPs (models) that are consistent with the observations gathered, and where the set contains the true MDP with high probability. Exploiting equivalence structure of the MDP, one could obtain a more precise estimation of mean reward µ and transition kernel p of the MDP by aggregating observations from various state-action pairs in the same class. This, in turn, yields smaller (hence, better) sets of models.  For a given confidence parameter δ and time t, we write M t,δ to denote the set of plausible MDPs at time t, which may be generically expressed as</p><formula xml:id="formula_2">M t,δ = {(S, A, p , ν ) : p ∈ CB t,δ and µ ∈ CB t,δ } , (<label>1</label></formula><formula xml:id="formula_3">)</formula><p>where CB t,δ (resp. CB t,δ ) denotes the confidence set for p (resp. µ) centered at p (resp. µ), and where µ is the mean of ν . Note that both CB t,δ and CB t,δ depend on N t (s, a), (s, a) ∈ S × A.</p><p>For ease of presentation, in the sequel we consider the following confidence sets<ref type="foot" target="#foot_2">4</ref> used in several model-based RL algorithms, e.g., <ref type="bibr" target="#b17">(Jaksch et al., 2010;</ref><ref type="bibr" target="#b7">Dann et al., 2017)</ref>:</p><formula xml:id="formula_4">CB t,δ := p : p t (•|s, a) -p (•|s, a) 1 β Nt(s,a) δ SA , ∀s, a , (<label>2</label></formula><formula xml:id="formula_5">)</formula><formula xml:id="formula_6">CB t,δ := µ : | µ t (s, a) -µ (•|s, a)| β Nt(s,a)</formula><p>δ SA , ∀s, a , where</p><formula xml:id="formula_7">β n (δ) = 2(1 + 1 n ) log √ n + 1 2 S -2 δ n and β n (δ) = (1 + 1 n ) log( √ n + 1/δ) 2n , ∀n.<label>(3)</label></formula><p>These confidence sets were derived by combining Hoeffding's <ref type="bibr" target="#b16">(Hoeffding, 1963)</ref> and Weissman's <ref type="bibr" target="#b34">(Weissman et al., 2003)</ref> concentration inequalities with the Laplace method <ref type="bibr" target="#b28">(Peña et al., 2008;</ref><ref type="bibr" target="#b0">Abbasi-Yadkori et al., 2011)</ref>, which enables to handle the random stopping times N t (s, a) in a sharp way; we refer to <ref type="bibr" target="#b23">(Maillard, 2019)</ref> for further discussion. In particular, this ensures that the true transition function p and mean reward function µ are contained in the confidence sets with probability at least 1 -2δ, uniformly over all time t.</p><p>Remark 5 As the bounds for µ and p are similar, to simplify the presentation, from now on we assume the mean reward function µ is known<ref type="foot" target="#foot_3">5</ref> .</p><p>We now provide modifications to CB t,δ in order to exploit the equivalence structure C, when the learner knows C in advance. The case of an unknown C is addressed in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Case 1: Known Classes and Profiles</head><p>Assume that an oracle provides the learner with a perfect knowledge of the equivalence classes C as well as profiles σ = (σ s,a ) s,a . In this ideal situation, the knowledge of C and σ allows to straightforwardly aggregate observations from all state-action pairs in the same class to build more accurate estimates of p and µ. Formally, for a class c ⊂ C, we define</p><formula xml:id="formula_8">p σ t (x|c) = 1 n t (c) (s,a)∈c N t (s, a) p t σ s,a (x)|s, a , ∀x ∈ S,<label>(4)</label></formula><p>where we recall that n t (c) = (s,a)∈c N t (s, a). The superscript σ in (4) signifies that the aggregate empirical distribution p σ t depends on σ. Having defined p σ t , we modify the confidence set (2) by modifying the L 1 bound there as follows:</p><formula xml:id="formula_9">p σ t (σ -1 (•)|c) -p (•|c) 1 β nt(c) δ C , ∀c ∈ C,<label>(5)</label></formula><p>and further define: CB t,δ (C, σ) := p : (5) holds , where (C, σ) stresses that C and σ are provided as input. Then, for all time t and class c ∈ C, by construction, the true transition p belongs to CB t,δ (C, σ), with probability greater than 1 -δ.</p><p>Remark 6 It is crucial to remark that the above confidence set does not use elements of C as "meta states" (i.e., replacing the states with classes), as considered for instance in the literature on state-aggregation. Rather, the classes are only used to group observations from different sources and build more refined estimates for each pair: The plausible MDPs are built using the same state-space S and action-space A, unlike in, e.g., <ref type="bibr" target="#b26">(Ortner, 2013)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Case 2: Known Classes, Unknown Profiles</head><p>Now we consider a more realistic setting when the oracle provides C to the learner, but σ is unknown. In this more challenging situation, we need to estimate profiles as well. Given time t, we find an empirical profile mapping (or for short, empirical profile) σ s,a,t satisfying p t (σ s,a,t (1)|s, a) p t (σ s,a,t (2)|s, a) . . . p t (σ s,a,t (S)|s, a) , and define σ t = (σ s,a,t ) s,a . We then build the modified empirical estimate in a similar fashion to (4): For any c ∈ C,</p><formula xml:id="formula_10">p σt t (x|c) = 1 n t (c) (s,a)∈c N t (s, a) p t (σ s,a,t (x)|s, a) , ∀x ∈ S.</formula><p>Then, we may modify the L 1 inequality in (2) as follows:</p><formula xml:id="formula_11">p σt t (σ -1 t (•)|c) -p (•|c) 1 1 n t (c) (s,a)∈c N t (s, a)β Nt(s,a) δ C , ∀c ∈ C,<label>(6)</label></formula><p>which further yields the following modified confidence set that uses only C as input: CB t,δ (C) := p : (6) holds . The above construction is justified by the following nonexpansive property of the ordering operator, as it ensures that Weissman's concentration inequality also applies to the ordered empirical distribution:</p><p>Lemma 7 (Non-expansive ordering) Let p and q be two discrete distributions, defined on the same alphabet S, with respective profile mappings σ p and σ q . Then,</p><formula xml:id="formula_12">p(σ p (•)) -q(σ q (•)) 1 p -q 1 .</formula><p>The proof of Lemma 7 is provided in Appendix B. An immediate corollary follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Corollary 8</head><p>The confidence set CB t,δ (C) contains the true transition function p with probability at least 1 -δ, uniformly over all time t.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Unknown Classes: The ApproxEquivalence Algorithm</head><p>In this section, we turn to the most challenging situation when both C and σ are unknown to the learner. To this end, we first introduce an algorithm, which we call ApproxEquivalence, that finds an approximate equivalence structure in the MDP by grouping transition probabilities based on statistical tests. ApproxEquivalence is inspired by <ref type="bibr" target="#b19">(Khaleghi et al., 2016</ref>) that provides a method for clustering time series. Interestingly enough, ApproxEquivalence does not require the knowledge of the number of classes in advance. We first introduce some definitions. Given u, v ⊆ S × A, we define the distance between u and v as d(u, v) := p σu (•|u) -p σv (•|v) 1 . ApproxEquivalence relies on finding subsets of S × A that are statistically close in terms of the distance function d(•, •). As d(•, •) is unknown, ApproxEquivalence relies on a lower confidence bound on it: For u, v ⊆ S × A, we define the lower-confidence distance function between u and v as</p><formula xml:id="formula_13">d(u, v) := d t,δ (u, v) := p σu,t t (•|u) -p σv,t t (•|v) 1 -ε u,t -ε v,t ,</formula><p>where for u ∈ S × A and t ∈ N, we define ε u,t := 1 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>d(c, u) .</head><p>ApproxEquivalence proceeds as follows. At time t, it receives as input a parameter α &gt; 1 that controls the level of aggregation, as well as N t (s, a) for all pairs (s, a). Starting from the trivial partition of {1, . . . , SA} into C 0 := {1}, . . . , {SA} , the algorithm builds a coarser partition by iteratively merging elements of C 0 that are statistically close. More precisely, the algorithm sorts elements of C 0 in a non-increasing order of n t (c), c ∈ C 0 so as to promote pairs with the tightest confidence intervals. Then, starting from c with the largest n t (c), it finds the PAC Nearest Neighbor c of c, that is c = Near(c, C 0 ). If</p><formula xml:id="formula_14">1 α nt(c)/L(c) nt(c )/L(c )</formula><p>α, where L(c) = |c|, the algorithm merges c and c , thus leading to a novel partition C 1 , which contains the new cluster c ∪ c , and removes c and c . The algorithm continues this procedure with the next set in C 0 , until exhaustion, thus finishing the creation of the novel partition C 1 of {1, . . . , SA}. ApproxEquivalence continues this process, by ordering the elements of C 1 in a non-increasing order, and carrying out similar steps as before, yielding the new partition C 2 . ApproxEquivalence continues the same procedure until iteration k when C k+1 = C k (convergence). The pseudo-code of ApproxEquivalence is shown in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 ApproxEquivalence</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input: Nt, α</head><p>Initialization:</p><formula xml:id="formula_15">C 0 ← {{1}, {2}, . . . , {SA}}; n ← Nt; L ← 1SA changed ← True; k ← 1; while changed do C k+1 ← C k ; changed ← False; Index ← argsort(n); for all i ∈ Index do if n(i) = 0 then Break; end if if Near(i, C k-1 ) = ∅ then j ← Near(i, C k-1 ); if 1 α n(i)/L(i) n(j)/L(j) α then p σ j,t t (•|j) ← 1 n(j)+n(i) n(j) p σ j,t t (•|j) + n(i) p σ i,t t (•|i) L(i) ← L(j) + L(i); n(i) ← n(j) + n(i); n(j) ← 0, L(j) ← 0; C k+1 ← C k+1 \ ({i}, {j}) ∪ {i, j}; changed ← True; end if end if end for k ← k + 1; end while output C k The purpose of condition 1 α nt(c)/L(c) nt(c )/L(c )</formula><p>α is to ensure the stability of the algorithm. It prevents merging pairs whose numbers of samples differ a lot. We note that a very similar condition (with α = 2) is considered in <ref type="bibr" target="#b26">(Ortner, 2013)</ref> for state-aggregation. Nonetheless, we believe such a condition could be relaxed. Remark 11 Since at each iteration, either two or more subsets are merged, ApproxEquivalence converges after, at most, SA -1 steps.</p><p>We provide a theoretical guarantee for the correctness of ApproxEquivalence for the case when α tends to infinity. The result relies on the following separability assumption: Assumption 1 (Separability) There exists some ∆ &gt; 0 such that</p><formula xml:id="formula_16">∀c = c ∈ C, ∀ ∈ c, ∀ ∈ c , d({ }, { }) ∆ .</formula><p>Proposition 12 Under Assumption 1, provided that min s,a N t (s, a) &gt; f -1 (∆), where f : n → 4β n ( δ SA ), ApproxEquivalence with the choice α → ∞ outputs the correct equivalence structure C of state-action pairs with probability at least 1 -δ.</p><p>The proof of Proposition 12 is provided in Appendix C. We note that Assumption 1 bears some similarity to the separability assumption used in <ref type="bibr" target="#b5">(Brunskill and Li, 2013)</ref>. Note further although the proposition relies on Assumption 1, we believe one may be able to derive a similar result under a weaker assumption as well. We leave this for future work. Now we turn to defining the aggregated confidence sets. Given t, let C t denote the equivalence structure output by the algorithm. We may use the following confidence set:</p><formula xml:id="formula_17">CB t,δ (C t ) := p : p σt t (σ -1 t (•)|c) -p (•|c) 1 (s,a)∈c Nt(s,a) nt(c) β Nt(s,a) δ SA , ∀c ∈ C t . (7)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Application: The C-UCRL Algorithm</head><p>This section is devoted to presenting some applications of equivalence-aware confidence sets introduced in Section 3. We present C-UCRL, a natural modification of UCRL2 <ref type="bibr" target="#b17">(Jaksch et al., 2010)</ref>, which is capable of exploiting the equivalence structure of the MDP. We consider variants of C-UCRL depending on which information is available to the learner in advance. First, we briefly recall UCRL2. At a high level, UCRL2 maintains the set M t,δ of MDPs at time t,<ref type="foot" target="#foot_4">6</ref> which is defined in (1). It then implements the optimistic principle by trying to compute π + t = argmax π:S→A max M ∈M t,δ g M π , where g M π denotes the gain of policy π in MDP M . This is carried out approximately by the Extended Value Iteration (EVI) algorithm that builds a near-optimal policy π + t and MDP M t such that g Mt</p><formula xml:id="formula_18">π + t max π,M ∈M t,δ g M π -1 √ t .</formula><p>Finally, UCRL2 does not recompute π + t at each time step. Instead, it proceeds in internal episodes (indexed by k ∈ N), and computes π + t only at the starting time t k of each episode, defined as t 1 = 1 and for all k &gt; 1, t k = min t &gt; t k-1 : ∃s, a, ν t k-1 :t (s, a) N t k-1 (s, a) + , where ν t 1 :t 2 (s, a) denotes the number of observations of pair (s, a) between time t 1 +1 and t 2 , and where for z ∈ N, z + := max{z, 1}. We provide the pseudo-code of UCRL2 in Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">C-UCRL: Known Equivalence Structure</head><p>Here we assume that the learner knows C and σ in advance, and provide a variant of UCRL2, referred to as C-UCRL(C, σ), capable of exploiting the knowledge on C and σ. Given δ, at time t, C-UCRL(C, σ) uses the following set of models</p><formula xml:id="formula_19">M t,δ (C, σ) = (S, A, p , ν) : p ∈ Pw(C) and p C ∈ CB t,δ (C, σ) ,</formula><p>where Pw(C) denotes the state-transition functions that are piece-wise constant on C, and where p C denotes the function induced by</p><formula xml:id="formula_20">p ∈ Pw(C) over C (that is p (•|s, a) = p C (•|c) for all (s, a) ∈ c). Moreover, C-UCRL(C, σ) defines t k+1 = min t &gt; t k : ∃c ∈ C : (s,a)∈c ν t k :t (s, a) n t k (c) + .</formula><p>We note that forcing the condition p ∈ Pw(C) may be computationally difficult. To ensure efficient implementation, we use the same EVI algorithm of UCRL2, where for (s, a) ∈ c, we replace p t (•|s, a) and β Nt(s,a) ( δ SA ) respectively with p σ t (•|c) and β nt(c) ( δ C ). The precise modified steps of C-UCRL(C, σ) are presented in Appendix A for completeness. An easy modification of the analysis of <ref type="bibr" target="#b17">(Jaksch et al., 2010)</ref>  The proof of Theorem 13 is provided in Appendix D. This theorem shows that efficiently exploiting the knowledge of C and σ yields an improvement over the regret bound of UCRL2 by a factor of SA/C, which could be a huge improvement when C SA. This is the case in, for instance, many grid-world environments thanks to Definitions 1-2; see Appendix F.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">C-UCRL: Unknown Equivalence Structure</head><p>Now we consider the case where C is unknown to the learner. In order to accommodate this situation, we use ApproxEquivalence in order to estimate the equivalence structure.</p><p>We introduce C-UCRL, which proceeds similarly to C-UCRL(C, σ). At each time t, ApproxEquivalence outputs C t as an estimate of the true equivalence structure C. Then, C-UCRL uses the following set of models taking C t as input:</p><formula xml:id="formula_21">M t,δ (C t ) = (S, A, p , ν) : p ∈ Pw(C t ) and p Ct ∈ CB t,δ (C t ) .</formula><p>Further, it sets the starting step of episode k + 1 as:</p><formula xml:id="formula_22">t k+1 = min t &gt; t k : ∃c ∈ C t k ,</formula><p>(s,a)∈c ν t k :t (s, a) n t k (c) + or ∃s, a, ν t k :t (s, a) N t k (s, a) + .</p><p>Similarly to C-UCRL(C, σ), we use a modification of EVI to implement C-UCRL.</p><p>Remark 14 Note that M t,δ (C) = M t,δ (C t ) as we may have C t = C. Nonetheless, the design of ApproxEquivalence, which relies on confidence bounds, ensures that C t is informative enough, in the sense that M t,δ (C t ) could be much smaller (hence, better) than a set of models that one would obtain by ignoring equivalence structure; this is also validated by the numerical experiments in ergodic environments in the next subsection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Numerical Experiments</head><p>We conduct numerical experiments to examine the performance of the proposed variants of C-UCRL, and compare it to that of UCRL2-L 7 . In our experiments, for running ApproxEquivalence, as a sub-routine of C-UCRL, we set ε u,t = β nt(u) δ 3SA for u ∈ S × A (in the definition of both d(•, •) and CB t,δ (C t )). Although this could lead to a biased estimation of p, such a bias is controlled thanks to using α &gt; 1 (in our experiments, we set α = 4).</p><p>7. UCRL2-L is a variant of UCRL2, which uses confidence bounds derived by combining Hoeffding's and Weissman's inequalities with the Laplace method, as in (2). We stress that UCRL2-L attains a smaller regret than the original UCRL2 of <ref type="bibr" target="#b17">(Jaksch et al., 2010)</ref>. In the first set of experiments, we examine the regret of various algorithms in ergodic environments. Specifically, we consider the ergodic RiverSwim MDP, shown in Figure <ref type="figure" target="#fig_2">2</ref>, with 25 and 50 states. In both cases, we have C = 6 classes. In Figure <ref type="figure" target="#fig_5">3</ref>, we plot the regret against time steps under C-UCRL(C, σ), C-UCRL, and UCRL2-L executed in the aforementioned environments. The results are averaged over 100 independent runs, and the 95% confidence intervals are shown. All algorithms use δ = 0.05, and for C-UCRL, we use α = 4. As the curves show, the proposed C-UCRL algorithms significantly outperform UCRL2-L, and C-UCRL(C, σ) attains the smallest regret. In particular, in the 25-state environment and at the final time step, C-UCRL(C, σ) attains a regret smaller than that of UCRL2-L by a factor of approximately SA/C = 50/6 ≈ 2.9, thus verifying Theorem 13. Similarly, we may expect an improvement in regret by a factor of around SA/C = 100/6 ≈ 4.1 in the other environment. We however get a better improvement (by a factor of around 8), which can be attributed to the increase in the regret of UCRL2-L due to a long burn-in phase (i.e., the phase before the algorithm starts learning).</p><p>We now turn our attention to the quality of approximate equivalence structure produced by ApproxEquivalence (Algorithm 1), which is run as a sub-routine of C-UCRL. To this aim, we introduce two performance measures to assess the quality of clustering: The first one is defined as the total number of pairs that are mis-clustered, normalized by the total number SA of pairs. We refer to this measure as the mis-clustering ratio. More precisely, let C t denote the empirical equivalence structure output by ApproxEquivalence at time t. For a given c ∈ C t , we consider the restriction of C to c, denoted by C|c. We find (c) ∈ C|c that has the largest cardinality: (c) ∈ argmax x∈C|c |x|. Now, we define mis-clustering ratio at time</p><formula xml:id="formula_23">t := 1 SA c∈Ct |c \ (c)| .</formula><p>Note that the mis-clustering ratio falls in [0, 1] as c∈Ct |c| = SA for all t. Our second performance measure accounts for the error in the aggregated empirical transition probability In Figure <ref type="figure" target="#fig_6">4</ref>, we plot the "mis-clustering ratio" and "mis-clustering bias" for the empirical equivalence structures produced in the previous experiments. We observes on the figures that the errors in terms of the aforementioned performance measures reduce. These errors do now vanish quickly, thus indicating that the generated empirical equivalence structures do not agree with the true one. Yet, they help reduce uncertainty in the transition probabilities, and, in turn, reduce the regret; we refer to Remark 14 for a related discussion.</p><p>In the second set of experiments, we consider two communicating environments: 4room grid-world (with 49 states) and RiverSwim (with 25 states). These environments are described in Appendix E. In Figure <ref type="figure" target="#fig_7">5</ref>, we plot the regret against time steps under C-UCRL(C, σ), C-UCRL, and UCRL2-L, and similarly to the previous case, we set δ = 0.05 and α = 4. The results are averaged over 100 independent runs, and the 95% confidence intervals are shown. In both environments, C-UCRL(C, σ) significantly outperforms UCRL2-L. However, C-UCRL attains a regret, which is slightly worse than that of UCRL2-L. This can be attributed to the fact that ApproxEquivalence is unable to find an accurate enough equivalence structure in these non-ergodic environments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We introduced a similarity measure of state-action pairs, which induces a notion of equivalence of profile distributions in the state-action space of a Markov Decision Process. In the case of a known equivalence structure, we have presented confidence sets incorporating such knowledge that are provably tighter than their corresponding counterparts ignoring equivalence structure. In the case of an unknown equivalence structure, we presented an algorithm, based on confidence bounds, that seeks to estimate an empirical equivalence structure for the MDP. In order to illustrate the efficacy of our developments, we further preseted C-UCRL, which is a natural modification of UCRL2 using the presented confidence sets. We show that when the equivalence structure is known to the learner, C-UCRL attains a regret smaller than that of UCRL2 by a factor of SA/C in communicating MDPs, where C denotes the number of classes. In the case of an unknown equivalence structure, we show through numerical experiments that in ergodic environments, C-UCRL outperforms UCRL2 significantly. The regret analysis in this case is much more complicated, and we leave it for future work. We believe that the presented confidence sets can be combined with model-based algorithms for the discounted setup, which we expect to yield improved performance in terms of sample complexity both in theory and practice.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>) belong to the same class.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: A grid-world MDP showing similar transitions from state-action pairs (6,Up) and (8,Right).</figDesc><graphic coords="6,90.00,160.34,216.00,143.58" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The L-state Ergodic RiverSwim MDP</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>nt(u) ∈u N t ( )β Nt( ) δ SA . We stress that, unlike d(•, •), d(•, •) is not a distance function. Definition 9 (PAC Neighbor) For a given equivalence structure C, and given c ∈ C, we say that c ∈ C is a PAC Neighbor of c if it satisfies: (i) d(c, c ) 0; (ii) d({j}, {j }) 0, for all j ∈ c and j ∈ c ; and (iii) d({j}, c ∪ c ) 0, for all j ∈ c ∪ c . We further define N (c) := c ∈ C \ {c} : (i)-(iii) hold as the set of all PAC Neighbors of c. Definition 10 (PAC Nearest Neighbor) For a given equivalence structure C and c ∈ C, we define the PAC Nearest Neighbor of c (when it exists) as: Near(c, C) ∈ argmin u∈N (c)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>yields: Theorem 13 (Regret of C-UCRL(C, σ)) With probability higher than 1 -3δ, uniformly over all time horizon T , R(C-UCRL(C, σ), T ) 18 CT S + log(2C √ T + 1/δ) + DC log 2 ( 8T C ) .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Regret of various algorithms in Ergodic RiverSwim environments</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Assessment of quality of approximate equivalence structures for Ergodic RiverSwim with 25 and 50 states</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Regret of various algorithms in communicating environments</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>state Ergodic RiverSwim MDP Notations. We introduce some necessary notations. Under a given algorithm, for a pair (s, a), we denote by N t (s, a) the total number of observations of (s, a) up to time t. Let us define µ t (s, a) as the empirical mean reward built using N t (s, a) i.i.d. samples from ν(s, a), and p t (•|s, a) as the empirical distribution built using N t (s, a) i.i.d. observations from p(•|s, a).</figDesc><table /><note><p>For a set c ⊆ S × A, we denote by n t (c) the total number of observations of pairs in c up to time t, that is n t (c) := (s,a)∈c N t (s, a). For c ⊆ S × A, we further denote by µ t (c) and p t (•|c) the empirical mean reward and transition probability built using n t (c) samples, respectively; we provide precise definitions of µ t (c) and p t (•|c) later on in this section.</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>We note that in the discounted setting, the quality of a learning algorithm is usually assessed through the notion of sample complexity as defined in<ref type="bibr" target="#b18">(Kakade, 2003)</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1"><p>See, e.g.,<ref type="bibr" target="#b29">(Puterman, 2014)</ref> for background material on MDPs.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2"><p>The approach presented in this section can be extended to other concentration inequalities, as well.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3"><p>This is a common assumption in the RL literature; see, e.g.,<ref type="bibr" target="#b4">(Bartlett and Tewari, 2009)</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_4"><p>This set is described by the Weismann confidence bounds combined with the Laplace method. The original UCRL2 algorithm in<ref type="bibr" target="#b17">(Jaksch et al., 2010)</ref> uses looser confidence bounds relying on union bounds instead of the Laplace method.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>This work has been supported by <rs type="programName">CPER Nord-Pas-de-Calais/FEDER DATA Advanced data science</rs> and technologies 2015-2020, the <rs type="funder">French Ministry of Higher Education and Research, Inria</rs>, and the <rs type="funder">French Agence Nationale de la Recherche (ANR)</rs>, under grant <rs type="grantNumber">ANR-16-CE40-0002</rs> (project <rs type="projectName">BADASS</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_Ph5HmcU">
					<orgName type="program" subtype="full">CPER Nord-Pas-de-Calais/FEDER DATA Advanced data science</orgName>
				</org>
				<org type="funded-project" xml:id="_UDf4r32">
					<idno type="grant-number">ANR-16-CE40-0002</idno>
					<orgName type="project" subtype="full">BADASS</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Improved algorithms for linear stochastic bandits</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Abbasi-Yadkori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Pál</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cs</forename><surname>Szepesvári</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NIPS</title>
		<meeting>of NIPS</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="2312" to="2320" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Near optimal behavior via approximate state abstraction</title>
		<author>
			<persName><forename type="first">D</forename><surname>Abel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hershkowitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Littman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICML</title>
		<meeting>of ICML</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2915" to="2923" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">ASAP-UCT: Abstraction of state-action pairs in UCT</title>
		<author>
			<persName><forename type="first">A</forename><surname>Anand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mausam</surname></persName>
		</author>
		<author>
			<persName><surname>Singla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IJCAI</title>
		<meeting>of IJCAI</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1509" to="1515" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Finite time analysis of the multiarmed bandit problem</title>
		<author>
			<persName><forename type="first">P</forename><surname>Auer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Cesa-Bianchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="235" to="256" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">REGAL: A regularization based algorithm for reinforcement learning in weakly communicating MDPs</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">L</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tewari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of UAI</title>
		<meeting>of UAI</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="35" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Sample complexity of multi-task reinforcement learning</title>
		<author>
			<persName><forename type="first">E</forename><surname>Brunskill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of UAI</title>
		<meeting>of UAI</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page">122</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Optimal adaptive policies for Markov decision processes</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Burnetas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">N</forename><surname>Katehakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematics of Operations Research</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="222" to="255" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Unifying PAC and regret: Uniform PAC bounds for episodic reinforcement learning</title>
		<author>
			<persName><forename type="first">C</forename><surname>Dann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lattimore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Brunskill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NIPS</title>
		<meeting>of NIPS</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5711" to="5721" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Model reduction techniques for computing approximately optimal solutions for Markov decision processes</title>
		<author>
			<persName><forename type="first">T</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Givan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Leach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of UCAI</title>
		<meeting>of UCAI</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="124" to="131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The adaptive k-meteorologists problem and its application to structure learning and feature selection in reinforcement learning</title>
		<author>
			<persName><forename type="first">C</forename><surname>Diuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">R</forename><surname>Leffler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICML</title>
		<meeting>of ICML</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Metrics for finite Markov decision processes</title>
		<author>
			<persName><forename type="first">N</forename><surname>Ferns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Panangaden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Precup</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of UAI</title>
		<meeting>of UAI</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="162" to="169" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Bisimulation metrics for continuous Markov decision processes</title>
		<author>
			<persName><forename type="first">N</forename><surname>Ferns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Panangaden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Precup</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Computing</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1662" to="1714" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Efficient bias-span-constrained explorationexploitation in reinforcement learning</title>
		<author>
			<persName><forename type="first">R</forename><surname>Fruit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pirotta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lazaric</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ortner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICML</title>
		<meeting>of ICML</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1573" to="1581" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Minimax regret bounds for reinforcement learning</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Gheshlaghi</forename><surname>Azar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Osband</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Munos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICML</title>
		<meeting>of ICML</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="263" to="272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Equivalence notions and model minimization in Markov decision processes</title>
		<author>
			<persName><forename type="first">R</forename><surname>Givan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Greig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">147</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="163" to="223" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Model selection in Markovian processes</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hallak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Di-Castro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mannor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACM SIGKDD</title>
		<meeting>of ACM SIGKDD</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="374" to="382" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Probability inequalities for sums of bounded random variables</title>
		<author>
			<persName><forename type="first">W</forename><surname>Hoeffding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">301</biblScope>
			<biblScope unit="page" from="13" to="30" />
			<date type="published" when="1963">1963</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Near-optimal regret bounds for reinforcement learning</title>
		<author>
			<persName><forename type="first">T</forename><surname>Jaksch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ortner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Auer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="1563" to="1600" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">On the sample complexity of reinforcement learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kakade</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
			<pubPlace>London London, England</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Consistent algorithms for clustering time series</title>
		<author>
			<persName><forename type="first">A</forename><surname>Khaleghi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ryabko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Preux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="94" to="125" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Asymptotically efficient adaptive allocation rules</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Robbins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Applied Mathematics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="4" to="22" />
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Efficient reinforcement learning with relocatable action models</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">R</forename><surname>Leffler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Littman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Edmunds</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of AAAI</title>
		<meeting>of AAAI</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="572" to="577" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Towards a unified theory of state abstraction for MDPs</title>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Walsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Littman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISAIM</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Mathematics of statistical sequential decision making</title>
		<author>
			<persName><forename type="first">O.-A</forename><surname>Maillard</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>à Diriger des Recherches</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Habilitation</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">How hard is my MDP? &quot;the distribution-norm to the rescue</title>
		<author>
			<persName><forename type="first">O.-A</forename><surname>Maillard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">A</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mannor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NIPS</title>
		<meeting>of NIPS</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1835" to="1843" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Efficient Bayesian clustering for reinforcement learning</title>
		<author>
			<persName><forename type="first">T</forename><surname>Mandel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-E</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Brunskill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Popovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IJCAI</title>
		<meeting>of IJCAI</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1830" to="1838" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Adaptive aggregation for reinforcement learning in average reward Markov decision processes</title>
		<author>
			<persName><forename type="first">R</forename><surname>Ortner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Operations Research</title>
		<imprint>
			<biblScope unit="volume">208</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="321" to="336" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Selecting near-optimal approximate state representations in reinforcement learning</title>
		<author>
			<persName><forename type="first">R</forename><surname>Ortner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O.-A</forename><surname>Maillard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ryabko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ALT</title>
		<meeting>of ALT</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="140" to="154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Self-normalized processes: Limit theory and Statistical Applications</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">H</forename><surname>Peña</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q.-M</forename><surname>Shao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Markov decision processes: discrete stochastic dynamic programming</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Puterman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>John Wiley &amp; Sons</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Approximate homomorphisms: A framework for non-exact minimization in Markov decision processes</title>
		<author>
			<persName><forename type="first">B</forename><surname>Ravindran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Barto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of KBCS</title>
		<meeting>of KBCS</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">An analysis of model-based interval estimation for Markov decision processes</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Strehl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Littman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computer and System Sciences</title>
		<imprint>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1309" to="1331" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Reinforcement learning: An introduction</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Barto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>MIT Press Cambridge</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Variance-aware regret bounds for undiscounted reinforcement learning in MDPs</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Talebi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O.-A</forename><surname>Maillard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ALT</title>
		<meeting>of ALT</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="770" to="805" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Inequalities for the L1 deviation of the empirical distribution</title>
		<author>
			<persName><forename type="first">T</forename><surname>Weissman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ordentlich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Seroussi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Verdu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Weinberger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
		<respStmt>
			<orgName>Hewlett-Packard Labs</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
