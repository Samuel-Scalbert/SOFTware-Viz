<TEI xmlns="http://www.tei-c.org/ns/1.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xml:space="preserve" xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">GraphCite: Citation Intent Classification in Scientific Publications via Graph Embeddings</title>
				<funder ref="#_6f2dtxj">
					<orgName type="full">GENCI-IDRIS</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher />
				<availability status="unknown"><licence /></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Dan</forename><surname>Berrebbi</surname></persName>
							<email>dan.berrebbi@polytechnique.edu</email>
						</author>
						<author>
							<persName><forename type="first">Nicolas</forename><surname>Huynh</surname></persName>
							<email>nicolas.huynh@polytechnique.edu</email>
						</author>
						<author>
							<persName><forename type="first">Oana</forename><surname>Balalau</surname></persName>
							<email>oana.balalau@inria.fr</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Ecole Polytechnique</orgName>
								<orgName type="institution">Institut Polytechnique de Paris Palaiseau</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Inria</orgName>
								<orgName type="institution" key="instit2">Institut Polytechnique de Paris Palaiseau</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">GraphCite: Citation Intent Classification in Scientific Publications via Graph Embeddings</title>
					</analytic>
					<monogr>
						<imprint>
							<date />
						</imprint>
					</monogr>
					<idno type="MD5">F06B5F6CB003B155608636C03E6C1056</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-04-12T14:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid" />
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>‚Ä¢ Computing methodologies ‚Üí Neural networks; Natural language processing citation intent classification, graph neural network</keywords>
			</textClass>
			<abstract>
<div><p>Citations are crucial in scientific works as they help position a new publication. Each citation carries a particular intent, for example, to highlight the importance of a problem or to compare against results provided by another method. The authors' intent when making a new citation has been studied to understand the evolution of a field over time or to make recommendations for further citations. In this work, we address the task of citation intent prediction from a new perspective. In addition to textual clues present in the citation phrase, we also consider the citation graph, leveraging high-level information of citation patterns. In this novel setting, we perform a thorough experimental evaluation of graph-based models for intent prediction. We show that our model, <software>GraphCite</software>, improves significantly upon models that take into consideration only the citation phrase. Our <software ContextAttributes="used">code</software> is available online 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div><head n="1">INTRODUCTION</head><p>Scientific articles are pivotal in sharing knowledge within a scientific community, but also with nonexperts. A 2019 report of the National Science Foundation found that the number of publications in science and engineering had grown constantly over the years, from 1.8 million articles in 2008 to 2.6 million articles in 2018. 2  Given this profusion of information, citations play a crucial role in understanding the lineage and evolution of a field. Authors cite other works with different intentions, such as highlighting the importance of a problem or comparing against results provided by another method. Citation intent has been used to show the evolution of a scientific field <ref type="bibr" target="#b11">[12]</ref>, or to facilitate the recommendation of further citations <ref type="bibr" target="#b12">[13]</ref>. Interestingly, in <ref type="bibr" target="#b11">[12]</ref> the authors show that citations can be correlated to a shift to rapid discovery science <ref type="bibr" target="#b4">[5]</ref>, that is a period in which a field reaches a consensus on the research topics, methods and technologies and strives to improve upon them.</p><p>Different intent taxonomies and several methods for automatic classification of intent have been proposed over the years. In <ref type="bibr" target="#b11">[12]</ref>,</p><p>Figure <ref type="figure">1</ref>: The intent is not clear from the citing phrase, however the majority of papers cite S2AG because they use it as a dataset, a pattern that can be observed in a citation graph.</p><p>the authors propose six intent classes and use a large number of features together with a random forest classifier for prediction. The features include lexical, grammatical, and morphological information, field-related metrics such as the citation count and the Pagerank of the reference paper, patterns in the citation phrase, and topic model features. Cohan et al. <ref type="bibr" target="#b3">[4]</ref> propose three intent classes and a neural network trained on multi-task classification. Citation intent is learned jointly by predicting the section title in which the citation occurs and predicting whether a sentence needs a citation. State of the art results on the datasets proposed in <ref type="bibr" target="#b11">[12]</ref> and <ref type="bibr" target="#b3">[4]</ref> are obtained by <software ContextAttributes="used">SciBert</software> <ref type="bibr" target="#b2">[3]</ref>, a pretrained language model based on Bert <ref type="bibr" target="#b6">[7]</ref> and trained on a large corpus of scientific text.</p><p>In this work, in addition to textual clues present in the citation phrase, we also leverage semantic information present in a citation graph. Differently from <ref type="bibr" target="#b11">[12]</ref>, we are not retrieving the importance of a particular article via its degree or Pagerank, but its representation based on the citation graph. The neighborhood of an article in a citation graph should bring more context to the classification task and help for ambiguous phrases, such as shown in Figure <ref type="figure">1</ref>. For example, when considering other neighboring papers that cited the referenced paper, the most frequent intent used could be a good indication of the intent in a new citation. When publishing in the same area, an author will frequently reuse references with the same citation intent. Moreover, we can incorporate textual information such as titles or abstracts, which can help predict citation intents. For example, in <ref type="bibr" target="#b3">[4]</ref>, the textual information surrounding the citation phrase was used, but the authors claimed that it led to a degradation of the algorithm's performance. We were motivated to look at the macro-level, i.e., using a citation graph. The citation graph has been used with success for a personalized recommendation for researchers <ref type="bibr" target="#b0">[1]</ref> or context-aware citation recommendation <ref type="bibr" target="#b10">[11]</ref>, but not for citation intent prediction, to the best of our knowledge.</p><p>Our salient contributions are: i) we investigate the creation of citation graphs for existing intent datasets; ii) we propose a new intent classification model that takes into account both the citation graph and the citation phrase; iii) in a thorough experimental evaluation, we show that using citation graphs as additional context for citation prediction can significantly improve upon state-of-the-art results.</p></div>
<div><head n="2">PROBLEM STATEMENT</head><p>Given a phrase of the form "S2AG is described in detail in <ref type="bibr" target="#b6">[7]</ref>" contained in the paper Anonymous et al. (2022), we denote as the citing paper Anonymous et al. (2022), cited paper <ref type="bibr" target="#b6">[7]</ref>, and citation phrase S2AG is described in detail in <ref type="bibr" target="#b6">[7]</ref>. A citation intent dataset contains pairs of (citation phrase, citation intent). Citation graph. We define a citation graph as a graph ùê∫ = (ùëâ , ùê∏, ùúÉ ) with auxiliary information on nodes, given by the function ùúÉ : ùëâ ‚Ü¶ ‚Üí ùëÖ ùëë , where ùëë is an input dimension. Nodes can be papers, authors, or conferences, and auxiliary information for nodes can be the paper title or abstract, an author's short bio, and conference tracks. Edges are between papers when there is a citation, between papers and authors for authorship relations, and between papers and conferences for publications. The auxiliary information is transformed from text to a dense vector via an embedding technique. When no auxiliary information is present, the dense vectors are initialized randomly. Intent classification with auxiliary information. Given a dataset of citation intents, we construct a citation graph to facilitate the task of citation intent prediction. The citation graph initially contains only the citing and cited papers. However, one can add additional nodes and edges by considering inbound and outbound citations of the original papers, the authors, and the conferences. Problem statement Given a citation graph ùê∫ = (ùëâ , ùê∏, ùúÉ ) and a citation intent dataset, predict the intent of unseen citation phrases.</p></div>
<div><head n="3">DATASETS</head><p>Existing citation intent datasets <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b11">12]</ref> consist of information regarding the citation phrase, its intent, and metadata about the cited and citing papers. However, often this metadata is incomplete. Below we describe the datasets and how we construct citation graphs. S2AG. Ammar et al. <ref type="bibr" target="#b1">[2]</ref> introduced the Semantic Scholar Academic Graph (S2AG), a dataset accessible via an API 3 , that provides metadata on research papers published in all fields. The metadata information includes the paper title, abstract, authors, citations, publication venues. We used this dataset to construct citation graphs when context information was missing in the citation datasets. ACL-ARC. Jurgens et al. <ref type="bibr" target="#b11">[12]</ref> created a dataset of 2ùêæ citations annotated for their intent for 186 papers in the field of natural language processing. The authors considered 6 intents: background (51% of citations), uses (19%), compare/contrast (18%), motivation (5%), extends (4%), future work (4%). Each entry in the dataset contains several fields, however, we only keep the citation intent, citing phrase, citing and cited paper title and ID. We tried to use the paper's ID to obtain more information on the papers, such as  their authors and venues. However, 65% of citations have at least one paper from outside the ACL Anthology Corpus<ref type="foot" target="#foot_0">4</ref> , indicated by a prefix "External" in the ID, and we cannot match them to any existing source. We retrieved their authors and publication venue from the S2AG dataset for the rest of the papers. <software ContextAttributes="used">SciCite</software>. Cohan et al. <ref type="bibr" target="#b3">[4]</ref> created a dataset of 11ùêæ manually annotated citation intents for papers in computer science and medicine, covering 6.6ùêæ papers. The authors consider 3 citation intents: background information (58% of citations), method (29%), and result comparison (13%). Each entry in the dataset contains the citation intent, citing phrase, name of the section where the citation occurs, citing and cited paper ID corresponding to Semantic Scholar IDs. We use S2AG to retrieve the title, authors and venues. Citation graphs. We created several graphs by leveraging citations in the datasets of citation intents. The citations labeled with intents are used to create an initial graph, with nodes representing citing or cited papers. To capture more context about the papers, we add their authors and the venues of publication as nodes, as shown in Figure <ref type="figure" target="#fig_1">2</ref>. This additional information also creates better-connected graphs, where we should observe citation patterns. We give the size of these graphs in Table <ref type="table" target="#tab_0">1</ref>.</p></div>
<div><head>Graph</head></div>
<div><head n="4">CITATION INTENT PREDICTION 4.1 Graph-based Intent Classifier</head><p>The Graph Attention Network (GAT) <ref type="bibr" target="#b15">[16]</ref> is an inductive method for computing node embeddings in graphs with node attributes. An inductive approach will be able to provide embeddings for dynamic graphs and hence it is appropriate for citations graphs that are updated to reflect new publications. GAT computes node embeddings by aggregating information from their neighbourhood but differs from earlier techniques <ref type="bibr" target="#b9">[10]</ref>, as it uses masked self-attentional layers to specify different weights for nodes in a neighborhood. <software ContextAttributes="used">SciBert</software> <ref type="bibr" target="#b2">[3]</ref> is a pretrained language model based on Bert <ref type="bibr" target="#b6">[7]</ref>, that is trained on a large corpus of scientific text, from a variety of scientific domains. <software ContextAttributes="used">SciBert</software> has shown improved performance compared to Bert on many NLP tasks. <software ContextAttributes="used">SciBert</software> is generally finetuned on the downstream task to achieve optimal performance. In <ref type="bibr" target="#b2">[3]</ref>, the authors show that on the citation intent prediction task the fine-tuned architecture had a better performance than the frozen architecture with an additional prediction layer. The node embeddings are the output of GAT, which aggregates information in the neighborhood of each node. We note that the GAT architecture receives as node features for the nodes of type paper, the <software ContextAttributes="used">SciBert</software> embeddings of the titles of the papers. We do not provide node features for the nodes of type author and venue. We aggregate the two node embeddings using the Hadamard product, which has been shown to produce the best edge embeddings <ref type="bibr" target="#b14">[15]</ref>. We concatenate these two types of embeddings (citation phrase embedding and edge embedding), passing the result to the MLP. The loss is jointly backpropagated through the GAT and the <software ContextAttributes="used">SciBert</software> models. Hence, <software ContextAttributes="used">SciBert</software> is fined tuned on the task while the GAT network is trained. The loss we use is the multiclass classification loss over the classes. Our intuition is that a unique loss backpropagated on the two parts of the model would enable learning semantic features and graph-hereditary features.</p></div>
<div><head n="4.2">Experimental Evaluation</head><p>We compare against several baselines: Random. We generate predictions by respecting the class distributions in the training set.</p><p>Rule-based classifier. We predict the intent based on the section containing the citation. In the <software>SciCite</software> dataset, each citation has one of five sections tags: introduction, methods, results, background, discussion. For citations appearing in introduction and background we predict the intent background, for the section methods we predict the intent method, while for results and discussion we predict the intent result. These rules were chosen as they obtained the best results. In the ACL-ARC dataset, the sections are the following: introduction, related work, method, background, experiments, conclusion. We map the intents as follows: background for sections introduction and related work, uses for section method, compare/contrast for experiments and finally future for section conclusion.</p><p><software ContextAttributes="used">SciBert</software>. We consider the <software ContextAttributes="used">SciBert</software> architecture as described in <ref type="bibr" target="#b2">[3]</ref>. This model receives as input only the citation phrase, and it is fine tuned on citation intent classification.</p><p>Pre-Training Variants. We also report results of <ref type="bibr" target="#b8">[9]</ref> and <ref type="bibr" target="#b5">[6]</ref> on the ACL-ARC classification task. Those two papers study taskaware continued pre-training methods and evaluate their systems on several downstream tasks including citation intent prediction in the ACL-ARC dataset. These recent methods have high F1-scores, <ref type="bibr" target="#b8">[9]</ref> being current state-of-the-art on the ACL-ARC dataset, hence they are relevant comparisons for our proposed approach. We name TARTAN and DAPT respectively the best methods of <ref type="bibr" target="#b5">[6]</ref> and <ref type="bibr" target="#b8">[9]</ref>. We note that we do not have the precision and recall results of these methods, or the results on <software ContextAttributes="used">SciCite</software>.</p><p>GAT. Finally, we consider both the structural information of the citation network and titles in order to compute paper embeddings. We compute the citing and cited paper's node embeddings and pass them through a trainable MLP network to predict a citation intent. This approach does not consider the citation phrase.</p><p>Settings. We train for 10 epochs, with a learning rate of 2ùëí -5 as suggested in the <software ContextAttributes="used">SciBert</software> paper, and a batch of 16. We averaged the macro precision, recall and F1 score over five seeds. We implemented our models using <software ContextAttributes="used">PyTorch</software> <ref type="bibr" target="#b13">[14]</ref> and <software ContextAttributes="used">PyTorch</software> Geometric <ref type="bibr" target="#b7">[8]</ref>.</p><p>The <software ContextAttributes="used">SciBert</software> model has embeddings of size 768, while the GAT model has embeddings of size 128. The code is available online<ref type="foot" target="#foot_1">5</ref> .</p></div>
<div><head n="5">DISCUSSION AND CONCLUSION</head><p>In this work, we investigate if more contextual information, e.g. a citation graph, can help to predict citation intent. Our intuition is Table <ref type="table">3</ref>: Macro results (P,R,F1) on the <software ContextAttributes="used">SciCite</software> dataset.</p><p>that a citation graph might reveal patterns such as a specific paper being cited often with the same intent or an author reusing citations in new papers with the same intent. To investigate this hypothesis, we propose a new architecture that accounts for both the citation phrase and the citation network of the citing and cited papers.</p><p>Because the graph constructed from citations labeled with intent can be very sparse, we investigate different strategies for increasing its density. We note that this might not always be possible, when we lack the required metadata. We present the results in Table <ref type="table">2</ref> and Table <ref type="table">3</ref>. For <software ContextAttributes="used">SciCite</software>, we observe that a simple rule-based method can give very good results. This is not the case for ACL-ARC, partly because we have more citation intents than section types. Our method, <software ContextAttributes="used">GraphCite</software> gives better results that the previous state-of-the-art when considering both the citation graphs containing the papers, and citation graphs that contain both the papers and the authors. For the latter configuration, the results are substantially better than the SOTA, with an increase of 1.96 in macro ùêπ 1 score for ACL-ARC <ref type="foot" target="#foot_2">6</ref> , and an increase of 2.36 for <software ContextAttributes="used">SciCite</software>. We note that the improvement is both in precision and recall. However, adding the venues leads to a drop in performance. This can be explained because venues are not important in predicting the intent, and venues nodes in the citation graph add noise to the task. We also observe that the GAT model is insufficient to have a good performance, and the additional information carried by citation phrase has to be incorporated. Finally, we recall that for the ACL-ARC data we could retrieve the authors' information for only around 35% of the papers, hence it is very likely that we could observe a bigger increase in performance if the complete data was available.</p><p>In conclusion, we have showed in this work that citation intent prediction benefits significantly from macro-level information related to the citation graph of a paper. Citation intent classification can be an important tool in assessing the impact of papers or in recommending papers according to the reader's goal, e.g. finding papers that extend a given work, or finding papers that use resources created in a given article.</p><p>In the future, we will investigate the use of our method on larger and more complete citation graphs. One interesting direction is to include the papers cited by the cited paper, for which we might not know the citation intent, but which could leveraged to obtain a better representation of the cited paper node. The attention mechanism underpinning our method will then learn to select relevant information in the neighborhood of the papers. Adding more textual information, in the form of the paper abstract or authors' research interest is another promising direction.</p></div><figure xml:id="fig_0"><head /><label /><figDesc>3 https://api.semanticscholar.org/graph/v1</figDesc></figure>
<figure xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Creating a citation graph.</figDesc><graphic coords="3,356.57,83.69,163.02,171.60" type="bitmap" /></figure>
<figure xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: GraphCite</figDesc><graphic coords="4,53.80,301.25,252.21,242.07" type="bitmap" /></figure>
<figure type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Citation graphs</figDesc><table><row><cell>Nodes Edges</cell></row></table></figure>
			<note place="foot" n="4" xml:id="foot_0"><p>https://www.aclweb.org/anthology/</p></note>
			<note place="foot" n="5" xml:id="foot_1"><p>https://github.com/nyxpho/graphcite</p></note>
			<note place="foot" n="6" xml:id="foot_2"><p>It is interesting to note that we increased the F1-score by 5.64 from <software>SciBert</software>, so we might have reached an even better score by using DAPT or TARTAN.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGEMENTS</head><p>This work was performed using HPC resources from <rs type="funder">GENCI-IDRIS</rs> (Grant <rs type="grantNumber">2021-AD011011614R1</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_6f2dtxj">
					<idno type="grant-number">2021-AD011011614R1</idno>
				</org>
			</listOrg>
			<div type="annex">
<div><head>Model</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep learning in citation recommendation models survey</title>
		<author>
			<persName><forename type="first">Zafar</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pavlos</forename><surname>Kefalas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Khan</forename><surname>Muhammad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bahadar</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Muhammad</forename><surname>Imran</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.eswa.2020.113790</idno>
		<ptr target="https://doi.org/10.1016/j.eswa.2020.113790" />
	</analytic>
	<monogr>
		<title level="j">Expert Systems with Applications</title>
		<imprint>
			<biblScope unit="volume">162</biblScope>
			<biblScope unit="page">113790</biblScope>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Construction of the Literature Graph in Semantic Scholar</title>
		<author>
			<persName><forename type="first">Waleed</forename><surname>Ammar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dirk</forename><surname>Groeneveld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chandra</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miles</forename><surname>Crawford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Doug</forename><surname>Downey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Dunkelberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmed</forename><surname>Elgohary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Feldman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vu</forename><surname>Ha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT (3)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">SciBERT: A Pretrained Language Model for Scientific Text</title>
		<author>
			<persName><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arman</forename><surname>Cohan</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1371</idno>
		<ptr target="https://doi.org/10.18653/v1/D19-1371" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). Association for Computational Linguistics</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). Association for Computational Linguistics<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3615" to="3620" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Structural Scaffolds for Citation Intent Classification in Scientific Publications</title>
		<author>
			<persName><forename type="first">Arman</forename><surname>Cohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Waleed</forename><surname>Ammar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Madeleine</forename><surname>Van Zuylen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Field</forename><surname>Cady</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1361</idno>
		<ptr target="https://doi.org/10.18653/v1/N19-1361" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3586" to="3596" />
		</imprint>
	</monogr>
	<note>Long and Short Papers</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Why the social sciences won't become high-consensus, rapid-discovery science</title>
		<author>
			<persName><forename type="first">Randall</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sociological forum</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="155" to="177" />
			<date type="published" when="1994">1994</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Should We Be Pre-training? An Argument for End-task Aware Training as an Alternative</title>
		<author>
			<persName><forename type="first">Lucio</forename><forename type="middle">M</forename><surname>Dery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ameet</forename><surname>Talwalkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=2bO2x8NAIMB" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
		<ptr target="https://doi.org/10.18653/v1/N19-1423" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
	<note>Long and Short Papers</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Fast Graph Representation Learning with PyTorch Geometric</title>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><forename type="middle">E</forename><surname>Lenssen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR Workshop on Representation Learning on Graphs and Manifolds</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Don't Stop Pretraining: Adapt Language Models to Domains and Tasks</title>
		<author>
			<persName><forename type="first">Suchin</forename><surname>Gururangan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ana</forename><surname>Marasoviƒá</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Swabha</forename><surname>Swayamdipta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Doug</forename><surname>Downey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.740</idno>
		<ptr target="https://doi.org/10.18653/v1/2020.acl-main.740" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="8342" to="8360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">Rex</forename><surname>William L Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Neural Information Processing Systems</title>
		<meeting>the 31st International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1025" to="1035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A contextaware citation recommendation model with BERT and graph convolutional networks</title>
		<author>
			<persName><forename type="first">Chanwoo</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sion</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eunjeong</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sungchul</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientometrics</title>
		<imprint>
			<biblScope unit="volume">124</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1907" to="1922" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Measuring the Evolution of a Scientific Field through Citation Frames</title>
		<author>
			<persName><forename type="first">David</forename><surname>Jurgens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Srijan</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raine</forename><surname>Hoover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Mcfarland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00028</idno>
		<ptr target="https://doi.org/10.1162/tacl_a_00028" />
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="391" to="406" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Exploiting Citation Knowledge in Personalised Recommendation of Recent Scientific Publications</title>
		<author>
			<persName><forename type="first">Anita</forename><surname>Khadka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Cantador</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miriam</forename><surname>Fernandez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 12th Language Resources and Evaluation Conference</title>
		<meeting>The 12th Language Resources and Evaluation Conference</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2231" to="2240" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">PyTorch: An Imperative Style, High-Performance Deep Learning Library</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alykhan</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sasank</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benoit</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junjie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Verse: Versatile graph embeddings from similarity measures</title>
		<author>
			<persName><forename type="first">Anton</forename><surname>Tsitsulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Davide</forename><surname>Mottin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Panagiotis</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emmanuel</forename><surname>M√ºller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 world wide web conference</title>
		<meeting>the 2018 world wide web conference</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="539" to="548" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Graph attention networks</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Veliƒçkoviƒá</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10903</idno>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>