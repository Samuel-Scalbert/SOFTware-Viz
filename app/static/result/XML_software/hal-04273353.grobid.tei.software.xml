<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Fair Without Leveling Down: A New Intersectional Fairness Definition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Gaurav</forename><surname>Maheshwari</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">UMR 9189</orgName>
								<orgName type="institution" key="instit1">Univ. Lille</orgName>
								<orgName type="institution" key="instit2">Inria</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<orgName type="institution" key="instit4">Centrale Lille</orgName>
								<address>
									<addrLine>-CRIStAL</addrLine>
									<postCode>F-59000</postCode>
									<settlement>Lille</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Aurélien</forename><surname>Bellet</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">UMR 9189</orgName>
								<orgName type="institution" key="instit1">Univ. Lille</orgName>
								<orgName type="institution" key="instit2">Inria</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<orgName type="institution" key="instit4">Centrale Lille</orgName>
								<address>
									<addrLine>-CRIStAL</addrLine>
									<postCode>F-59000</postCode>
									<settlement>Lille</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Pascal</forename><surname>Denis</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">UMR 9189</orgName>
								<orgName type="institution" key="instit1">Univ. Lille</orgName>
								<orgName type="institution" key="instit2">Inria</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<orgName type="institution" key="instit4">Centrale Lille</orgName>
								<address>
									<addrLine>-CRIStAL</addrLine>
									<postCode>F-59000</postCode>
									<settlement>Lille</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mikaela</forename><surname>Keller</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">UMR 9189</orgName>
								<orgName type="institution" key="instit1">Univ. Lille</orgName>
								<orgName type="institution" key="instit2">Inria</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<orgName type="institution" key="instit4">Centrale Lille</orgName>
								<address>
									<addrLine>-CRIStAL</addrLine>
									<postCode>F-59000</postCode>
									<settlement>Lille</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Fair Without Leveling Down: A New Intersectional Fairness Definition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">4B7367DA8134568F20167FA525DF8CB9</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-04-12T14:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this work, we consider the problem of intersectional group fairness in the classification setting, where the objective is to learn discrimination-free models in the presence of several intersecting sensitive groups. First, we illustrate various shortcomings of existing fairness measures commonly used to capture intersectional fairness. Then, we propose a new definition called the α-Intersectional Fairness, which combines the absolute and the relative performance across sensitive groups and can be seen as a generalization of the notion of differential fairness. We highlight several desirable properties of the proposed definition and analyze its relation to other fairness measures. Finally, we benchmark multiple popular in-processing fair machine learning approaches using our new fairness definition and show that they do not achieve any improvement over a simple baseline. Our results reveal that the increase in fairness measured by previous definitions hides a "leveling down" effect, i.e., degrading the best performance over groups rather than improving the worst one.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The aim of fair machine learning is to develop models that are devoid of discriminatory behavior towards sensitive subgroups of the population. A broad range of approaches have been developed (see e.g., <ref type="bibr" target="#b45">Zafar et al., 2017;</ref><ref type="bibr" target="#b8">Denis et al., 2021;</ref><ref type="bibr" target="#b31">Maheshwari and Perrot, 2022;</ref><ref type="bibr">Mehrabi et al., 2022, and references therein)</ref>, with most of them focusing on a single sensitive axis <ref type="bibr" target="#b28">(Lohaus et al., 2020;</ref><ref type="bibr" target="#b1">Agarwal et al., 2018)</ref> such as gender (e.g., Male vs. Female) or race (e.g., African-Americans vs. European-Americans). However, recent studies <ref type="bibr" target="#b42">(Yang et al., 2020;</ref><ref type="bibr" target="#b24">Kirk et al., 2021)</ref> have demonstrated that even when fairness can be ensured at the level of each individual sensitive axis, significant unfairness can still exist at the intersection levels (e.g., Male European-Americans vs.</p><p>Female African-Americans). For example, <ref type="bibr" target="#b2">Buolamwini and Gebru (2018)</ref> showed that commercially available face recognition tools exhibit significantly higher error rates for darker-skinned females than for lighter-skinned males. Similar observations have been made by several studies in NLP including contextual word representation (Tan and Celis, 2019), and generative models <ref type="bibr" target="#b24">(Kirk et al., 2021)</ref>. These findings resonate with the analytical framework of intersectionality <ref type="bibr" target="#b7">(Crenshaw, 1989)</ref>, which argues that systems of inequality based on various attributes (like gender and race) may "intersect" to create unique effects.</p><p>To capture these effects in the context of machine learning, several intersectional fairness measures have been proposed <ref type="bibr" target="#b21">(Kearns et al., 2018;</ref><ref type="bibr" target="#b15">Hébert-Johnson et al., 2018;</ref><ref type="bibr" target="#b10">Foulds et al., 2020)</ref>. Amongst them the most commonly used <ref type="bibr" target="#b25">(Lalor et al., 2022;</ref><ref type="bibr" target="#b47">Zhao et al., 2022;</ref><ref type="bibr" target="#b40">Subramanian et al., 2021)</ref> is Differential Fairness (DF) <ref type="bibr" target="#b10">(Foulds et al., 2020)</ref>, which is the log-ratio of the best-performing group to the worst-performing group for a given performance measure (such as the True Positive Rate). While DF has many desirable properties, in this work we emphasize that DF implements a "strictly egalitarian" view, i.e., it only considers the relative performance between the group and ignores their absolute performance. In particular, a trivial way to improve fairness as measured by DF is by harming the best-off group without improving the worst-off group. This phenomenon, known as leveling down, does not fit the desired fairness requirements in many practical use-cases <ref type="bibr" target="#b34">(Mittelstadt et al., 2023;</ref><ref type="bibr" target="#b48">Zietlow et al., 2022</ref>). Yet, we empirically observe that (i) popular fairness-promoting approaches tend to level down more in intersectional fairness, and (ii) this often goes unnoticed in the overall performance of the model due to the large number of groups induced by intersectional fairness.</p><p>To address these issues and explicitly capture the leveling down phenomena, we propose a gener-alization of DF, called α-Intersectional Fairness (IF α ), which takes into account both the relative performance between the groups and the absolute performance of the groups. More precisely, IF α is a weighted average between the relative and absolute performance of the groups, and allows the exploration of the whole trade-off between these two quantities by changing their relative importance via a weight α ∈ [0, 1]. Our extensive benchmarks across various datasets show that many existing fairness-inducing methods aim for a different point in the aforementioned trade-off and generally show no consistent improvement over a simple unconstrained approach.</p><p>In summary, our primary contributions are as follows:</p><p>• We showcase the shortcomings of the existing intersectional fairness definition and propose a generalization called α-Intersectional Fairness. We analyze the properties and behavior of the proposed fairness measure, and contrast them with DF.</p><p>• We benchmark existing fairness approaches on multiple datasets and evaluate their performance with several fairness measures, including ours. On the one hand, we find that many fairness approaches optimize for existing fairness measures by harming both the worst-off and best-off groups or only the best-off group.</p><p>On the other hand, our measure is more careful in showing improvements over a simple baseline than previous metrics, allowing the emphasis on cases of leveling down.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Setting</head><p>In this section, we begin by introducing our notations and then formally define problem statement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Notations</head><p>In this study, we adopt and extend the notations proposed by <ref type="bibr" target="#b35">Morina et al. (2019)</ref>. Let p denote the number of distinct sensitive axes of interest, which generally correspond to socio-demographic features of a population. We refer to these sensitive axes as A 1 , . . . , A p , each of which is a set of discrete-valued sensitive attributes. For instance, a dataset may be composed of gender, race, and age as the three sensitive axes, and each of these sensitive axes may be encoded by a set of sensitive attributes, such as gender: {male, female, non-binary}, race: {European American, African American}, and age: {under 45, above 45}. We define a sensitive group g as any p-dimensional vector in the Cartesian product set G = A 1 × • • • × A p of these sensitive axes. A sensitive group g ∈ G can then be written as (a 1 , . . . , a p ) with a j ∈ A j .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Problem Statement</head><p>Consider a feature space X , a finite discrete label space Y, and a set G representing all possible intersections of p sensitive axes as defined above. Let D be an unknown distribution over X × Y × G through which we sample i.i.d a finite dataset T = {(x i , y i , g i )} n i=1 consisting of n examples. This sample can be rewritten as T = g∈G T g where T g represents the subset of examples from group g. The goal of fair machine learning is then to learn an accurate model h θ ∈ H, with learnable parameters θ ∈ R D , such that h θ : X → Y is fair with respect to a given group fairness definition like Equal Opportunity <ref type="bibr" target="#b13">(Hardt et al., 2016)</ref>, Equal Odds <ref type="bibr" target="#b13">(Hardt et al., 2016)</ref>, Accuracy Parity <ref type="bibr" target="#b45">(Zafar et al., 2017)</ref>, etc.</p><p>Existing group fairness definitions generally consist of comparing a certain performance measure, such as True Positive Rate (TPR), False Positive Rate (FPR) or accuracy, across groups. In the following, for the sake of generality, we abstract away from the particular measure and denote by m(h θ , T g ) ∈ [0, 1] the group-wise performance for model h θ on the group of examples T g , with the convention that higher values of m correspond to better performance. For instance, in the case of TPR (used to define Equal Opportunity) we define m(h θ , T g ) = TPR(h θ , T g ), while for FPR, we define it as m(h θ , T g ) = 1 -FPR(h θ , T g ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Existing Intersectional Framework</head><p>While the literature on group fairness in machine learning initially considered a single sensitive axis, several works have recently proposed fairness definitions for the intersectional setting <ref type="bibr" target="#b11">(Gohar and Cheng, 2023)</ref>. <ref type="bibr" target="#b21">Kearns et al. (2018)</ref> proposed subgroup-fairness, which is based on the difference in performance of a particular group weighted by the size of the group. Several calibration and metric fairness-based variants were considered by <ref type="bibr" target="#b15">Hébert-Johnson et al. (2018)</ref> and <ref type="bibr" target="#b43">Yona and Rothblum (2018)</ref>. A shortcoming of these notions is that they weight each group by its size, hence small groups may not be protected even though they are often the disadvantaged ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Differential Fairness</head><p>To circumvent the above issue, <ref type="bibr" target="#b10">Foulds et al. (2020)</ref> proposed Differential Fairness (DF), which puts a constraint on the relative performance between all pairs of groups. DF was originally proposed for statistical parity <ref type="bibr" target="#b10">(Foulds et al., 2020)</ref>, and was then extended by <ref type="bibr" target="#b35">(Morina et al., 2019)</ref>  </p><formula xml:id="formula_0">(h θ , m) ≡ max g,g ′ ∈G log m(h θ , T g ) m(h θ , T g ′ ) ≤ ϵ.</formula><p>It is important to note that DF only depends on the relative performance between the bestperforming group and the worst-performing group.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Shortcomings of Differential Fairness</head><p>We now highlight what we believe to be a key shortcoming of DF in the context of intersectional fairness: DF can be improved by leveling down, i.e., harming the best-off and/or worst-off group, without significantly affecting the overall performance of the model. This problem is caused by the combination of two factors.</p><p>First, DF is a strictly egalitarian measure that only considers the relative performance between groups. This can lead to situations where a model that improves the performance across all groups is deemed more unfair by DF. To illustrate this, let the group-wise performance measure m to be the TPR and consider two models h θ and h θ. Let the worst-off and best-off group-wise performance of h θ be 0.50 and 0.60, respectively. For h θ, let it be 0.65 and 0.95. According to DF, h θ is more fair than h θ as the two groups are closer, while h θ has better performance for both groups. In other words, h θ is leveling down compared to h θ, but is deemed more fair. This exhibits the tension between the relative performance between groups, and the absolute performance of the groups.</p><p>The second factor is that in intersectional fairness, leveling down can have a negligible effect on the overall performance of a model on the full dataset. This is because the number of groups in intersectional fairness is typically quite large (exponential in the number of sensitive axes p). Therefore, the bulk of examples generally do not belong to either the worst-off or best-off group, leading to a situation where the performance of other groups accounts for most of the model's overall performance. This issue may be further exacerbated if the class proportions are imbalanced across groups.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">α-Intersectional Fairness</head><p>In order to circumvent the above issue and effectively capture intersectional fairness while taking into account the leveling down phenomena, we propose α-Intersectional Fairness (IF α ). Our definition is essentially a convex combination of two components, namely (i) ∆ rel , which takes into account the relative performance between the two groups, such as the ratio of their performance, and (ii) ∆ abs , which captures the leveling down effect by accounting for the absolute performance of the worst-off group.</p><p>More precisely, given a model h θ and a groupwise performance measure m, let us first define a measure of fairness for a pair of groups g and g ′ :</p><formula xml:id="formula_1">I α (g, g ′ , h θ , m) = α∆ abs + (1 -α)∆ rel , (1)</formula><p>where α ∈ [0, 1] and</p><formula xml:id="formula_2">∆ abs = max 1 -m(h θ , T g ), 1 -m(h θ , T g ′ ) , ∆ rel = 1 -max m(h θ , T g ), m(h θ , T g ′ ) 1 -min m(h θ , T g ), m(h θ , T g ′ ) .</formula><p>Now taking the maximum value of I α over all pairs of groups, we get our proposed notion of α-Intersectional Fairness.</p><formula xml:id="formula_3">Definition 2 (α-Intersectional Fairness). A model h θ is (α, γ)-intersectionally fair (IF α ) with respect to a group-wise performance measure m, if IF α (h θ , m) ≡ max g,g ′ ∈G I α (g, g ′ , h θ , m) ≤ γ.</formula><p>Note that IF α (h θ , m) can be equivalently obtained as the the value of I α over the pair of worst performing and the best performing group, as shown by the following proposition.</p><p>Proposition 1. If a model h θ is (α, γ)intersectionally fair with respect to a group-wise performance measure m, then</p><formula xml:id="formula_4">IF α (h θ , m) = I α (g w , g b , h θ , m) ≤ γ,</formula><p>where g w = arg min g∈G m(h θ , T g ) and g b = arg max g∈G m(h θ , T g ).</p><p>In the following, we compare and contrast our fairness definition with DF when evaluating the fairness of the two models. We then investigate various properties of our proposed definition and discuss the impact of α. In the interest of space, we delegate our discussion around the design choices for ∆ rel and ∆ abs to Appendix A.</p><p>Comparing DF and IF α . The primary difference between DF and IF α when comparing two models arises when one model adversely affects the worst-off group (∆ abs ) more than the other, despite having better relative performance (∆ rel ). In this case, DF would consistently consider one model more fair than the other, whereas IF α enables the exploration of this tension by varying the relative importance of both criteria through α.</p><p>We formally capture this intuition as follows. Consider two models h θ and h θ. Let the value of the worst-off and the best-off group's performance for the model h θ be w and b, respectively. Similarly, for model h θ let the worst and the best group's performance be w and b, respectively. Without the loss of generality, w and b can be written as w = w + x and b = b + y. Note that x and y can be either positive or negative as long as w ≤ b. We visualize this setup in Figure <ref type="figure" target="#fig_0">1</ref>. Based on this setup, we have following cases:</p><p>• x ≥ y ≥ 0: In this case, h θ harms the worstoff group (absolute performance) more, and its relative performance is worse than h θ. In Figure <ref type="figure" target="#fig_0">1</ref>, this corresponds to w ≥ w and the blue region is smaller than the red region.</p><p>Here,</p><formula xml:id="formula_5">IF α (h θ, m) ≤ IF α (h θ , m) ∀ α ∈ [0, 1],</formula><p>and DF(h θ, m) ≤ DF(h θ , m).</p><p>• x ≤ y ≤ 0: This is similar to the case above, but with h θ harming the groups more than h θ .</p><p>In Figure <ref type="figure" target="#fig_0">1</ref>, this corresponds to w ≤ w and the blue region is larger than the red region.</p><p>Here,</p><formula xml:id="formula_6">IF α (h θ, m) ≤ IF α (h θ , m) ∀ α ∈ [0, 1],</formula><p>and DF(h θ, m) ≤ DF(h θ , m).</p><p>• All other cases: In this setting, one of the model has better ∆ abs performance, while</p><p>x y the other model has better ∆ rel performance.</p><formula xml:id="formula_7">0 1 | w + w | b + b</formula><p>The fairness in this setting depends on the relative importance of absolute and relative performance for IF α , while for DF it exclusively depends on absolute performance. In Figure <ref type="figure" target="#fig_0">1</ref>, this corresponds to w ≤ w and the blue region is smaller than the red region or vice-versa. Here, ∃α ∈ [0, 1] for which</p><formula xml:id="formula_8">IF α (h θ, m) ≥ IF α (h θ , m) and vice versa. On the other hand, DF(h θ, m) ≤ DF(h θ , m) if y ×m(h θ , T g w ) ≤ x×m(h θ , T g b ), otherwise DF(h θ, m) &gt; DF(h θ , m).</formula><p>To summarize, in the first two cases, one model harms the worst-off group (absolute performance), and the relative performance of that model is worse than the other. Thus, a good fairness measure should assign a higher unfairness to that model, which both DF and IF α do. In the third case, one model performs better on the worst-off group, while the other model has a closer relative performance. The fairness in this setting depends on the relative importance of absolute and relative performance. Here, DF consistently assigns one model a higher fairness than the other, while IF α enables to explore this tension and tune the relative importance of both criteria through α. For instance, the previous example in Section 2 falls in the third case. On the one hand, DF will assign higher ϵ for h θ 1 in comparison to h θ 2 . On the other hand, IF α will assign higher γ for h θ 1 for α ∈ (0.0, 0.81), while for all other α, the γ would be higher for h θ 2 . We illustrate the effect of α in more details below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Impact of α:</head><p>The parameter α allows to tune the relative importance of ∆ abs and ∆ rel . On the one end of the spectrum, α = 0 corresponds to considering only the relative performance ∆ rel , while α = 1 corresponds to considering only the absolute performance. At α = 0.0 we recover the same relative ranking of unfairness as DF, and thus DF can be seen as a special case of IF α . In other words, for any three models h θ 1 , h θ 2 , and</p><formula xml:id="formula_9">h θ 3 such that DF(h θ 1 , m) ≥ DF(h θ 2 , m) ≥ DF(h θ 3 , m), then IF 0 (h θ 1 , m) ≥ IF 0 (h θ 2 , m) ≥ IF 0 (h θ 3 , m).</formula><p>On the other end, α = 1 only considers the absolute performance ∆ abs ), and α = 0.5 corresponds to giving ∆ abs and ∆ rel an equal importance. In practice, it is useful to visualize the complete trade-off by plotting α → IF α (see Section 5).</p><p>Intersectional Property: We have the following intersectional property.</p><p>Proposition 2. Let the model h θ be (α, γ)intersectionally fair over the set of groups defined by</p><formula xml:id="formula_10">G = A 1 × • • • A p . Let 1 ≤ s 1 ≤ • • • ≤ s k ≤ p, and P = A s 1 × • • • A s k be</formula><p>the Cartesian product of the sensitive axes where s j ∈ N + . Then, h θ is (α, γ)-intersectionally fair over P.</p><p>In other words, the fairness value calculated over the intersectional groups also holds over independent and "gerrymandering" intersectional groups <ref type="bibr" target="#b42">(Yang et al., 2020)</ref>. For instance, if a model is (α, γ)-intersectionally fair in a space defined by gender, race, and age, then it is also (α, γ)intersectionally fair in the space defined by gender and race, or just gender. We delegate the proof to Appendix B.</p><p>Generalization Guarantees: α-Intersectional Fairness enjoys the same generalization guarantees as the ones shown for DF in <ref type="bibr" target="#b10">(Foulds et al., 2020)</ref>. Indeed, the result of <ref type="bibr" target="#b10">Foulds et al. (2020)</ref> relies on a generalization analysis of the group-wise performance measure m, which directly translates into generalization guarantees for IF α .</p><p>Guidelines for setting α: α-Intersectional Fairness enables exploring the tradeoff between worstcase performance and relative performance across groups. Indeed, at alpha=0.0, only relative performance is considered, aligning with strictly egalitarian measures. On the other extreme, at alpha=1.0, solely the worst-off group performance is considered. Based on this, we recommend: Setting α = 0.75 (more focus towards worst case performance) in:</p><p>• Situations where the cost of misclassification is not similar for each group. In these cases, leveling down would disproportionately affect those subgroups for whom the cost is higher. One example can be seen in education system, where the cost of denying financial assistance has higher impact on minority <ref type="bibr" target="#b36">(Nora and Horvath, 1989;</ref><ref type="bibr" target="#b17">Hinojosa, 2023)</ref>.</p><p>• Cases where data for disadvantaged groups is unreliable due to historical underrepresentation and lack of opportunities. For instance, certain facial recognition systems exhibit a higher likelihood of error when analyzing images of dark-skinned female individuals (Buolamwini and Gebru, 2018). Similarly, <ref type="bibr" target="#b38">Sap et al. (2019)</ref> found that the hate speech detection systems are biased against black people.</p><p>In such contexts, emphasizing improvement for these disadvantaged groups is more pivotal than uniform performance over all subgroups, in line with the ideas of affirmative action. These scenarios best align with strategies seeking Demographic Parity or Equalized Odds. Setting α = 0.25 (more focus on relative performance) in:</p><p>• Scenarios where no group is significantly worse off, but to make sure that the algorithm behaves similarly for all the groups involved. This is related to algorithmic bias, as presented by <ref type="bibr" target="#b32">Mehrabi et al. (2021)</ref>. Moreover, the misclassification costs are similar in this setting.</p><p>• Legal or regulatory requirements may mandate similar outcomes across groups, like the 4/5th employment rule<ref type="foot" target="#foot_1">2</ref> . However, one must exercise caution when extrapolating this to other contexts, as it can lead to the "portability trap" as discussed by <ref type="bibr" target="#b39">(Selbst et al., 2019)</ref>.</p><p>In such a context, the emphasis is on equality among the groups. In practice, these scenarios indicate places where the partitioner would advocate for Accuracy Parity.</p><p>Otherwise, we recommend setting α = 0.50 (a neutral default) when no domain or context-specific insights are available. This is what we used in our experiments. Ultimately, the choice of alpha reflects an understanding of the domain, the inherent biases in the data, and the real-world consequences of misclassifications.</p><p>In this section, we present experiments<ref type="foot" target="#foot_2">3</ref> that showcase (i) the model's performance over the worst-off group as the number of sensitive axes increases, and (ii) the "leveling down" phenomenon observed in various fairness-promoting mechanisms, along with the effectiveness of α-Intersectional Fairness in uncovering it. However, before describing these experiments, we begin with an overview of the datasets, baselines, and fairness measures used. Both these datasets consists of free text responses and binarized scores by the medical expert. Moreover, each response is associated with gender, race, age, and income. We use same pre-processing as <ref type="bibr" target="#b25">(Lalor et al., 2022)</ref> and follow the same procedure to split the dataset as described above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Datasets</head><p>For improved readability, we present a subset of experiments in the main article. The remaining experiments are included in the Appendix.</p><p>Methods. We evaluate the fairness performance of the following methods: (i) Unconstrained which is oblivious to any fairness measure and solely optimizes the model's accuracy; (ii) Adversarial implements standard adversarial learning approach <ref type="bibr" target="#b26">(Li et al., 2018)</ref>, where an adversary is added to the Unconstrained with the objective to predict the sensitive attributes; (iii) FairGrad <ref type="bibr" target="#b31">(Maheshwari and Perrot, 2022)</ref>, is an inprocessing approach that iteratively learns groupspecific weights based on the fairness level of the model; (iv) INLP <ref type="bibr" target="#b37">(Ravfogel et al., 2020)</ref>, is a postprocessing approach that iteratively trains a classifier to predict the sensitive attributes and then projects the representation on the classifier's null space. To enforce fairness across multiple sensitive axes in this work, we follow the extension proposed by <ref type="bibr" target="#b40">Subramanian et al. (2021)</ref>; (v) Fair MixUp <ref type="bibr" target="#b5">(Chuang and Mroueh, 2021</ref>) is a data augmentation mechanism that enforces fairness by regularizing the model on the paths of interpolated samples between the sensitive groups.</p><p>In all our experiments, we employ the same model architecture for all the approaches to have a fair comparison. Specifically, we use a threehidden layer fully connected neural network with 128, 64, and 32 corresponding sizes. Furthermore, we use ReLU as the activation with dropout fixed to 0.5. We optimize cross-entropy loss in all cases with Adam (Kingma and Ba, 2015) as the optimizer using default parameters. Moreover, for Twitter Hate Speech and Numeracy datasets, we encode the text using bert-base-uncased <ref type="bibr" target="#b9">(Devlin et al., 2019)</ref> text encoder. For CelebA, an image dataset, we employ ResNet18 <ref type="bibr" target="#b14">(He et al., 2016)</ref> as the encoder. In all cases, we do not fine-tune the pre-trained encoders. Lastly, several previous studies have shown the effectiveness of equal sampling in improving fairness <ref type="bibr" target="#b19">(Kamiran and Calders, 2009;</ref><ref type="bibr" target="#b4">Chawla et al., 2003;</ref><ref type="bibr" target="#b20">Kamiran and Calders, 2010;</ref><ref type="bibr" target="#b12">González-Zelaya et al., 2021)</ref>. That is, to Note that in FPR, lower the value better it is, while for TPR opposite is true.</p><p>counter the imbalance in the training data, the data is resampled so that there is an equal number of examples from each group and class in the final training set. Through preliminary experiments, we determine that equal sampling improves the worstcase performance of several approaches, including Unconstrained in various settings. We thus incorporate it as a hyperparameter indicating a continuous scale between undersampling and oversampling. Note that we also incorporate a setting where no equal sampling is performed, and we take the distribution as it is.</p><p>Fairness performance measure. In this work we focus on True Positive Rate parity and False Positive Rate parity as the fairness measure. The corresponding group wise performance measure m for these fairness measures are TPR and FPR. Formally, m in case of TPR for a group g is:</p><formula xml:id="formula_11">m(h θ , T g ) = P (h θ (x) = 1|y = 1) ∀x, y ∈ T g ,</formula><p>while the FPR for a group g is:</p><formula xml:id="formula_12">m(h θ , T g ) = 1 -P (h θ (x) = 0|y = 1) ∀x, y ∈ T g</formula><p>In order to estimate the empirical probabilities, we employ the bootstrap estimation procedure as proposed by <ref type="bibr" target="#b35">Morina et al. (2019)</ref>. In total, we generate 1000 datasets by sampling from the original dataset with replacement. We then estimate the probabilities on this dataset using smoothed empirical estimation mechanism and then average the results over all the sampled datasets. In order to evaluate the utility of various methods, we employ balanced accuracy. Note that the choice of TPR Parity, and FPR Parity allows the derivation of several other fairness measures including Equal Opportunities, and Equalized Odds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Worst-off performance and number of sensitive axis</head><p>In this experiment, we empirically evaluate the interplay between the number of sensitive groups and the harm towards the worst-off group. To this end, we iteratively increase the number of sensitive axes in the dataset and report the performance of the worst-off group for each approach. For instance, with CelebA we first randomly added gender (randomly chosen) when considering 1 sensitive axis.</p><p>In the next iteration, we added race (randomly chosen) to the set with gender (previously added). Similarly, we then added age, and finally country. Note that for all the datasets, we start with a random choice of sensitive axis hoping to remove any form of selection bias. To select the optimal hyperparameters for this experiment, we follow the same procedure described in <ref type="bibr">(Maheshwari et al., 2022)</ref> with the objective to select the hyperparameters with the best performance over the worst-off group.</p><p>We plot the results of this experiment in Figure <ref type="figure" target="#fig_1">2</ref>. The results over the Anxiety dataset, which follow similar trend, can be found in the Appendix C. Based on these results, we observe that as the number of subgroups increases, the performance of the  . For both fairness definition, lower is better, while for balanced accuracy, higher is better. The Best Off and Worst Off, in both cases lower is better, represents the min FPR and max FPR.</p><p>Results have been averaged over 5 different runs. We deem a method to exhibit leveling down if its performance on either the worst-off or best-off group is inferior to the performance of an unconstrained model which we have highlighted using cyan ( ).</p><p>worst-off group becomes worse for all approaches in all settings. This can be attributed to the fact that the number of training examples available for each group decreases as the number of sensitive axis in the dataset increases. In terms of the performance of other approaches in comparison to Unconstrained, we find that fairness-inducing approaches generally perform better or similar to Unconstrained when 1 or 2 sensitive axes are considered. However, when 3 or more sensitive axis are considered, the performance of all approaches tends to converge to that of Unconstrained. For instance, in CelebA, on the one hand, with 1 sensi-tive axis, all approaches significantly outperform Unconstrained with the difference between the best-performing method and Unconstrained being 0.26. On the other hand, when 4 sensitive axes are considered, the difference between the bestperforming method and Unconstrained is 0.03, with only Adversarial outperforming it.</p><p>In a similar fashion, when considering TPR over Numeracy dataset, Unconstrained performs significantly worse than FairGrad and Adversarial with 1 sensitive axis while outperforming all approaches apart from INLP when 3 sensitive axis are considered. Similar observations can be made for Numeracy and Twitter Hate Speech datasets in the FPR setting, with some minor exceptions. Overall we find that most fairness approaches start harming or do not improve the worst-off group as the number of sensitive axes grows in the dataset. Thus it is pivotal for an intersectional fairness measure to consider the harm induced by an approach while calculating its fairness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Benchmarking Intersectional Fairness</head><p>In this experiment, we showcase the leveling down phenomena shown by various existing approaches. We also compare and contrast IF α and DF. The results of this comparison over FPR parity can be found in Table <ref type="table" target="#tab_2">1a</ref> and 1b for CelebA and Anxiety respectively. The results of remaining two datasets over FPR parity, and all datasets over TPR Parity can be found in Appendix C. In these experiment, we deem a method to exhibit leveling down if its performance on either the worst-off or best-off group is inferior to the performance of an unconstrained model. In the results table, we highlight the methods that show leveling down in cyan ( ).</p><p>We find that most of the methods have similar balanced accuracy across all the datasets, even if the fairness levels are different. This observation aligns with the arguments presented in Section 3 about the relationship between group fairness measure and the overall performance. In terms of fairness, most methods showcase leveling down. For instance, over the CelebA dataset, all methods apart from Adversarial shows leveling down. While in the case of Anxiety, all methods apart from INLP shows leveling down.</p><p>While comparing DF and IF α=0.5 , we find that IF α=0.5 is more conservative in assigning fairness value, with most approaches performing similarly to Unconstrained. Moreover, leveling down cases may go unnoticed in DF. For instance, over the CelebA dataset, even though FairGrad and INLP showcases leveling down, the fairness value assigned by DF is lower for them than the one assigned to Unconstrained. Similar observation can be seen over Numeracy in case of INLP.</p><p>A particular advantage of IF α over DF is that it equips the practitioner with a more nuanced view of the results. In Figure <ref type="figure" target="#fig_2">3</ref>, we plot the complete trade-off between the relative and the absolute performance of groups by varying α. For instance, in CelebA FPR, Fair MixUp shows the lowest level of unfairness at α = 0.0. However, as soon as the worst-off group's performance is considered, i.e., α &gt; 0.0, it rapidly becomes unfair with it being one of the most unfair method at α = 1.0. Interestingly, in Anxiety, INLP starts as one of the worstperforming mechanisms. However, with α &gt; 0.0, it quickly outperforms most approaches.</p><p>These findings shed light on the trade-offs and complexities inherent in optimizing fairness while maintaining worst-off group performance. It highlights the need for comprehensive evaluation metrics and the importance of considering the performance of both advantaged and disadvantaged groups in the fairness analysis. Finally, we emphasize that methods do not always exhibit leveling down. In settings without leveling down, DF adequately captures unfairness, producing values similar to α-Intersectional Fairness. However, every method displays some degree of leveling down for some combinations of datasets and metrics. A robust fairness measure should expose unfairness universally, which our experiments demonstrate IF α achieves.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We propose a new definition for measuring intersectional fairness of statistical models, in the group classification setting. We provide various comparative analyses of our proposed measure, and contrast it with existing ones. Through them, we show that our fairness definition can uncover various notions of harm, including notably, the leveling down phenomenon. We further show that many fairnessinducing methods show no significant improvement over a simple unconstrained approach. Through this work, we provide tools to the community to better uncover latent vectors of harm. Further, our findings chart a path for developing new fairnessinducing approaches which optimizes for fairness without harming the groups involved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Acknowledgement</head><p>The authors would like to thank the Agence Nationale de la Recherche for funding this work under grant number ANR-19-CE23-0022, as well as the reviewers for their feedback and suggestions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Limitations</head><p>While appealing, α-Intersectional Fairness also has limitations. One of the primary ones is that it assumes a minimum number of examples for each subgroup to estimate the fairness level of the model correctly. Moreover, it does not consider the data drift over time, as it assumes a static view of the problem. Thus we recommend checking the fairness level over time to account for it. Further, in this definition, setting up α is left to the practitioner and thus can be abused. In the future, we aim to develop mechanisms to validate α without access to the dataset or model.</p><p>Finally, we want to emphasize that a hypothetical perfectly fair model might not be devoid of social harm. Firstly, vectors of harm of using statistical models are not restricted to existing definitions of group fairness. Further, if some socio-economic groups are not present in a given dataset, existing fairness-inducing approaches are likely to not have any positive impact towards them when encountered upon deployment. Such is the case with commonly used datasets in the community, which over-simplify gender and race as binary features, ignoring people of mixed heritage, or non-binary gender, for example. In our experiments, we too have used these datasets, owing to their prevalence, and we urge the community to create dataset with non-binary attributes. That said, our measure works with non-binary sensitive attributes, with no modifications.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Group-wise performance range comparison. The range of group-wise performances of models h θ and model h θ are respectively [w, b] and [ w, b]. Note that the difference x (resp. y) between the best (resp. worst) group-wise performances of h θ and h θ can be positive or negative.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure2: Test results over the worst-off group on CelebA, Twitter Hate Speech, and Numeracy by varying the number of sensitive axes. For p binary sensitive axis in the dataset, the total number of sensitive groups are p 3 -1. Note that in FPR, lower the value better it is, while for TPR opposite is true.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Value of IF α on the test set of CelebA, and Numeracy datasets for varying α ∈ [0, 1].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Test results on (a) CelebA, and (b) Anxiety using False Positive Rate while optimizing for DF. The utility of various approaches is measured by balanced accuracy (BA), whereas fairness is measured by differential fairness DF and intersectional fairness IF α=0.5</figDesc><table><row><cell>Method</cell><cell>BA ↑</cell><cell>Best Off ↓ Worst Off ↓</cell><cell>DF ↓</cell><cell>IF α=0.5 ↓</cell></row><row><cell cols="5">Unconstrained 0.81 + 0.0 0.08 + 0.01 0.36 + 0.04 0.36 +/-0.06 0.31 +/-0.02</cell></row><row><cell>Adversarial</cell><cell>0.8 + 0.0</cell><cell cols="3">0.07 + 0.02 0.32 + 0.02 0.31 +/-0.12 0.28 +/-0.04</cell></row><row><cell>FairGrad</cell><cell cols="4">0.77 + 0.01 0.14 + 0.01 0.39 + 0.01 0.34 +/-0.03 0.4 +/-0.02</cell></row><row><cell>INLP</cell><cell>0.8 + 0.0</cell><cell cols="3">0.09 + 0.01 0.34 + 0.04 0.32 +/-0.03 0.32 +/-0.01</cell></row><row><cell>Fair MixUp</cell><cell>0.8 + 0.0</cell><cell cols="3">0.08 + 0.01 0.37 + 0.02 0.38 +/-0.04 0.3 +/-0.01</cell></row><row><cell></cell><cell></cell><cell>(a) Results on CelebA</cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell>BA ↑</cell><cell>Best Off ↓ Worst Off ↓</cell><cell>DF ↓</cell><cell>IF α=0.5 ↓</cell></row><row><cell cols="5">Unconstrained 0.63 + 0.01 0.27 + 0.04 0.5 + 0.03 0.38 +/-0.05 0.55 +/-0.06</cell></row><row><cell>Adversarial</cell><cell cols="4">0.62 + 0.01 0.28 + 0.05 0.53 + 0.09 0.43 +/-0.04 0.55 +/-0.06</cell></row><row><cell>FairGrad</cell><cell cols="4">0.63 + 0.01 0.33 + 0.04 0.59 + 0.06 0.49 +/-0.05 0.61 +/-0.03</cell></row><row><cell>INLP</cell><cell cols="4">0.63 + 0.01 0.27 + 0.04 0.49 + 0.03 0.36 +/-0.03 0.56 +/-0.05</cell></row><row><cell>Fair MixUp</cell><cell cols="4">0.61 + 0.02 0.3 + 0.03 0.61 + 0.07 0.58 +/-0.03 0.56 +/-0.03</cell></row><row><cell></cell><cell></cell><cell>(b) Results on Anxiety</cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>We note that, in their extension to parity in False Positive Rates,<ref type="bibr" target="#b35">Morina et al. (2019)</ref> did not account for the fact that higher FPR means lower performance, hence harming all groups always leads to better fairness. Our general formulation in Definition 1 fixes this problem through the convention that higher m corresponds to better performance.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>https://www.law.cornell.edu/cfr/text/29/1607.4</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>source code is available here: https://github.com/ saist1993/BenchmarkingIntersectionalBias</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Design Choices</head><p>In this section, we discuss our design choices for ∆ abs and ∆ rel .</p><p>Choice of ∆ rel An alternate choice of ∆ rel is to utilize the performance difference between the groups instead of the above mentioned ratio. However, we advocate for the ratio as a superior choice for the following reasons:</p><p>• Scale-Invariant Comparison: The ratio enables comparing two models without the influence of the scale by normalizing the relative performance of a model. For instance, assume two models h θ and h θ ′ with the worst and the best group's performance for h θ as 0.01 and 0.02 respectively, and 0.1 and 0.2 for h θ ′ . In this setting, the ∆ rel as the difference would always assign h θ as fairer, even though both models are twice worse for the worst group compared to the best group. Note that our overall fairness measure accounts for the effect of scale through the inclusion of ∆ abs . This is in-contrast to DFwhich does not take scale into account.</p><p>• Alignment with the 80% rule: The ratio aligns with the well-known 80% rule <ref type="bibr">(Commission et al., 1990)</ref>, which states that there exists legal evidence of discrimination if the ratio of the probabilities for a favorable outcome between the disadvantaged sensitive group and the advantaged sensitive group is less than 0.8. By adopting the ratio as ∆ rel , our metric adheres to this established criterion.</p><p>• Influence of worst-case group: If ∆ rel represents the difference in performance, then at α = 0.5 the model with better worst-case performance will always have a lower γ than the one with worse worst-case performance. In other words, at α = 0.5, ∆ abs would always dominate ∆ rel . However, this contradicts the intuitive understanding that, at α = 0.5, both ∆ rel and ∆ abs should exert an equal influence.</p><p>Choice of ∆ abs An alternate choice we explored for ∆ abs was the average performance of the two groups involved instead of just the worstperforming one. However, Proposition 1 does not hold in the average case. This implies that a pair of groups can exist for which I α is larger than the pair of groups consisting of the worst and bestperforming groups. Moreover, Proposition 1 is an essential building block for intersectional property which is described later.  . For both fairness definition, lower is better, while for balanced accuracy, higher is better. The Best Off and Worst Off, in both cases higher is better, represents the min TPR and max TPR. Results have been averaged over 5 different runs. We have also highlighted methods which showcase leveling down using cyan ( ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Intersectional Property</head><p>In this section, we prove the intersectional property stated in Section 4. The proof follows the same procedure as described by <ref type="bibr" target="#b10">Foulds et al. (2020)</ref>. The intersectional property states that:</p><p>Proposition. Let the model h θ be (α, γ)intersectionally fair over the set of groups defined by</p><p>and P = A s 1 × • • • A s k be the Cartesian product of the sensitive axes where s j ∈ N + . Then, h θ is (α, γ)-intersectionally fair over P.</p><p>The essential idea of the proof is to show that the maximum and the minimum group wise performance in P is bounded by the maximum and the minimum group wise performance in G. After proving the above, then using Proposition 1, we  . For both fairness definition, lower is better, while for balanced accuracy, higher is better. The Best Off and Worst Off, in both cases lower is better, represents the min FPR and max FPR. Results have been averaged over 5 different runs. We have also highlighted methods which showcase leveling down using cyan ( ).</p><p>can show that IF α over G is higher than IF α over P.</p><p>the Cartesian product of the protected attributes included in G but not in P. Then for any model h θ , y ∈ Range(h θ ), max g∈P:P (g|θ)&gt;0</p><p>By a similar argument, min g∈P:P (g|θ)&gt;0 P h θ (h θ (x) = y|P = g) ≥ min g ′ ∈G:P (g ′ |θ)&gt;0 P h θ (h θ (x) = y|g ′ ). Applying Corollary 1, we hence bound γ in P by the γ in G</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Extended Experiments</head><p>In this section, we detail the additional results. Table 2 provides results for the True Positive Rate (TPR) fairness measure, as outlined in the Experiment Section 5.2. In Figure <ref type="figure">6</ref>, we vary the number of sensitive axes and plot the worst-case performance for Anxiety in FPR and TPR settings. Finally, Table <ref type="table">3</ref> displays results related to the FPR parity fairness measure, focusing on the Twitter Hate Speech and Numeracy datasets. Notably, for TPR, each method exhibits leveling down in at least one dataset. For example, Adversarial shows leveling down in the Numeracy dataset, whereas INLP does so in both the Twitter Hate Speech and Anxiety datasets. Similarly, as with FPR, DF does not consistently identify leveling down. As evidence, while both FairGrad and INLP demonstrate leveling down, they show a better fairness level than Unconstrained. For p binary sensitive axis in the dataset, the total number of sensitive groups are p 3 -1. Note that in FPR, lower the value better it is, while for TPR opposite is true.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Constructing a psychometric testbed for fair natural language processing</title>
		<author>
			<persName><forename type="first">Ahmed</forename><surname>Abbasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">G</forename><surname>Dobolyi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">P</forename><surname>Lalor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">G</forename><surname>Netemeyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kendall</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.emnlp-main.304</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event<address><addrLine>Punta Cana, Dominican Republic</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021-07-11">2021. 7-11 November, 2021</date>
			<biblScope unit="page" from="3748" to="3758" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A reductions approach to fair classification</title>
		<author>
			<persName><forename type="first">Alekh</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alina</forename><surname>Beygelzimer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miroslav</forename><surname>Dudík</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Langford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanna</forename><surname>Wallach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="60" to="69" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Gender shades: Intersectional accuracy disparities in commercial gender classification</title>
		<author>
			<persName><forename type="first">Joy</forename><surname>Buolamwini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timnit</forename><surname>Gebru</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Fairness, Accountability and Transparency, FAT 2018</title>
		<title level="s">Proceedings of Machine Learning Research</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-02">2018. February 2018</date>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="page" from="77" to="91" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName><surname>Pmlr</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Smoteboost: Improving prediction of the minority class in boosting</title>
		<author>
			<persName><forename type="first">V</forename><surname>Nitesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandar</forename><surname>Chawla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lawrence</forename><forename type="middle">O</forename><surname>Lazarevic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><forename type="middle">W</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName><surname>Bowyer</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-540-39804-2_12</idno>
	</analytic>
	<monogr>
		<title level="m">Knowledge Discovery in Databases: PKDD 2003, 7th European Conference on Principles and Practice of Knowledge Discovery in Databases</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Cavtat-Dubrovnik, Croatia</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2003-09-22">2003. September 22-26, 2003</date>
			<biblScope unit="volume">2838</biblScope>
			<biblScope unit="page" from="107" to="119" />
		</imprint>
	</monogr>
	<note>Proceedings</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Fair mixup: Fairness via interpolation</title>
		<author>
			<persName><forename type="first">Ching-Yao</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youssef</forename><surname>Mroueh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">9th International Conference on Learning Representations, ICLR 2021</title>
		<meeting><address><addrLine>Virtual Event, Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021-05-03">2021. May 3-7, 2021</date>
		</imprint>
	</monogr>
	<note>OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Uniform guidelines on employee selection procedures</title>
		<author>
			<orgName type="collaboration">Equal Employment Opportunity Commission</orgName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Fed Register</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="216" to="243" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Demarginalizing the intersection of race and sex: A black feminist critique of antidiscrimination doctrine, feminist theory and antiracist politics</title>
		<author>
			<persName><forename type="first">Kimberle</forename><surname>Crenshaw</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The University of Chicago Legal Forum</title>
		<imprint>
			<biblScope unit="volume">140</biblScope>
			<biblScope unit="page" from="139" to="167" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">Christophe</forename><surname>Denis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Romuald</forename><surname>Elie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohamed</forename><surname>Hebiri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">François</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.13642</idno>
		<title level="m">Fairness guarantee in multi-class classification</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">BERT: pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/n19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019<address><addrLine>Minneapolis, MN, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-06-02">2019. June 2-7, 2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
	<note>Long and Short Papers</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">An intersectional definition of fairness</title>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">R</forename><surname>Foulds</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rashidul</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kamrun</forename><surname>Naher Keya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shimei</forename><surname>Pan</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICDE48307.2020.00203</idno>
	</analytic>
	<monogr>
		<title level="m">36th IEEE International Conference on Data Engineering, ICDE 2020</title>
		<meeting><address><addrLine>Dallas, TX, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020-04-20">2020. April 20-24, 2020</date>
			<biblScope unit="page" from="1918" to="1921" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">A survey on intersectional fairness in machine learning: Notions, mitigation, and challenges</title>
		<author>
			<persName><forename type="first">Usman</forename><surname>Gohar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Cheng</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2305.06969</idno>
		<idno>CoRR, abs/2305.06969</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Optimising fairness through parametrised data sampling</title>
		<author>
			<persName><forename type="first">Vladimiro</forename><surname>González-Zelaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julián</forename><surname>Salas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dennis</forename><surname>Prangle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paolo</forename><surname>Missier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EDBT</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="445" to="450" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Equality of opportunity in supervised learning</title>
		<author>
			<persName><forename type="first">Moritz</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nati</forename><surname>Srebro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems 2016</title>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-12-05">2016. December 5-10, 2016</date>
			<biblScope unit="page" from="3315" to="3323" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Multicalibration: Calibration for the (computationally-identifiable) masses</title>
		<author>
			<persName><forename type="first">Úrsula</forename><surname>Hébert-Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">P</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Reingold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guy</forename><forename type="middle">N</forename><surname>Rothblum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsmässan</title>
		<meeting>the 35th International Conference on Machine Learning, ICML 2018, Stockholmsmässan<address><addrLine>Stockholm, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-07-10">2018. July 10-15, 2018</date>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="1944" to="1953" />
		</imprint>
	</monogr>
	<note>Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title/>
		<author>
			<persName><surname>Pmlr</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Unequal access to higher education: Student loan debt disproportionately impacts minority students</title>
		<author>
			<persName><forename type="first">Elisa</forename><forename type="middle">Reyes</forename><surname>Hinojosa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scholar</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page">63</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Multilingual Twitter corpus and baselines for evaluating demographic bias in hate speech recognition</title>
		<author>
			<persName><forename type="first">Xiaolei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linzi</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Franck</forename><surname>Dernoncourt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Paul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twelfth Language Resources and Evaluation Conference</title>
		<meeting>the Twelfth Language Resources and Evaluation Conference<address><addrLine>Marseille, France</addrLine></address></meeting>
		<imprint>
			<publisher>European Language Resources Association</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Classifying without discriminating</title>
		<author>
			<persName><forename type="first">Faisal</forename><surname>Kamiran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Toon</forename><surname>Calders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 2nd international conference on computer, control and communication</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Classification with no discrimination by preferential sampling</title>
		<author>
			<persName><forename type="first">Faisal</forename><surname>Kamiran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Toon</forename><surname>Calders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 19th Machine Learning Conf</title>
		<meeting>19th Machine Learning Conf<address><addrLine>Belgium and The Netherlands</addrLine></address></meeting>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Preventing fairness gerrymandering: Auditing and learning for subgroup fairness</title>
		<author>
			<persName><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seth</forename><surname>Kearns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Neel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiwei</forename><forename type="middle">Steven</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning, ICML 2018</title>
		<title level="s">Proceedings of Machine Learning Research</title>
		<meeting>the 35th International Conference on Machine Learning, ICML 2018<address><addrLine>Stockholmsmässan, Stockholm, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-07-10">2018. July 10-15, 2018</date>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="2569" to="2577" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title/>
		<author>
			<persName><surname>Pmlr</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations, ICLR 2015</title>
		<title level="s">Conference Track Proceedings</title>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-07">2015. May 7-9, 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Bias outof-the-box: An empirical analysis of intersectional occupational biases in popular generative language models</title>
		<author>
			<persName><forename type="first">Rose</forename><surname>Hannah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yennie</forename><surname>Kirk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Filippo</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haider</forename><surname>Volpin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elias</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frédéric</forename><forename type="middle">A</forename><surname>Benussi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandar</forename><surname>Dreyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuki</forename><forename type="middle">M</forename><surname>Shtedritski</surname></persName>
		</author>
		<author>
			<persName><surname>Asano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021</title>
		<meeting><address><addrLine>NeurIPS; virtual</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021-12-06">2021. 2021. December 6-14, 2021</date>
			<biblScope unit="page" from="2611" to="2624" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Benchmarking intersectional biases in NLP</title>
		<author>
			<persName><forename type="first">John</forename><surname>Lalor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kendall</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicole</forename><surname>Forsgren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmed</forename><surname>Abbasi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.naacl-main.263</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL 2022</title>
		<meeting>the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL 2022<address><addrLine>Seattle, WA, United States</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022-07-10">2022. July 10-15, 2022</date>
			<biblScope unit="page" from="3598" to="3609" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Towards robust and privacy-preserving text representations</title>
		<author>
			<persName><forename type="first">Yitong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Baldwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P18-2005</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="25" to="30" />
		</imprint>
	</monogr>
	<note>Short Papers</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep learning face attributes in the wild</title>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2015.425</idno>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Computer Vision, ICCV 2015</title>
		<meeting><address><addrLine>Santiago, Chile</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2015-12-07">2015. December 7-13, 2015</date>
			<biblScope unit="page" from="3730" to="3738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Too relaxed to be fair</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Lohaus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michaël</forename><surname>Perrot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ulrike</forename><surname>Von</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luxburg</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="6360" to="6369" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title/>
		<author>
			<persName><surname>Pmlr</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Fair NLP models with differentially private text encoders</title>
		<author>
			<persName><forename type="first">Gaurav</forename><surname>Maheshwari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Denis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikaela</forename><surname>Keller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aurélien</forename><surname>Bellet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2022</title>
		<meeting><address><addrLine>Abu Dhabi, United Arab Emirates</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022-12-07">2022. December 7-11, 2022</date>
			<biblScope unit="page" from="6913" to="6930" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Fairgrad: Fairness aware gradient descent</title>
		<author>
			<persName><forename type="first">Gaurav</forename><surname>Maheshwari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michaël</forename><surname>Perrot</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2206.10923</idno>
		<idno>CoRR, abs/2206.10923</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A survey on bias and fairness in machine learning</title>
		<author>
			<persName><forename type="first">Ninareh</forename><surname>Mehrabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fred</forename><surname>Morstatter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nripsuta</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Lerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aram</forename><surname>Galstyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM computing surveys (CSUR)</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1" to="35" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A survey on bias and fairness in machine learning</title>
		<author>
			<persName><forename type="first">Ninareh</forename><surname>Mehrabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fred</forename><surname>Morstatter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nripsuta</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Lerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aram</forename><surname>Galstyan</surname></persName>
		</author>
		<idno type="DOI">10.1145/3457607</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Comput. Surv</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">35</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">The unfairness of fair machine learning: Levelling down and strict egalitarianism by default</title>
		<author>
			<persName><forename type="first">D</forename><surname>Brent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandra</forename><surname>Mittelstadt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Wachter</surname></persName>
		</author>
		<author>
			<persName><surname>Russell</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2302.02404</idno>
		<idno>CoRR, abs/2302.02404</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Auditing and achieving intersectional fairness in classification problems</title>
		<author>
			<persName><forename type="first">Giulio</forename><surname>Morina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Viktoriia</forename><surname>Oliinyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Waton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ines</forename><surname>Marusic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Konstantinos</forename><surname>Georgatzis</surname></persName>
		</author>
		<idno>CoRR, abs/1911.01468</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Financial assistance: Minority enrollments and persistence</title>
		<author>
			<persName><forename type="first">Amaury</forename><surname>Nora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fran</forename><surname>Horvath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Education and Urban Society</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="299" to="311" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Null it out: Guarding protected attributes by iterative nullspace projection</title>
		<author>
			<persName><forename type="first">Shauli</forename><surname>Ravfogel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanai</forename><surname>Elazar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hila</forename><surname>Gonen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Twiton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.647</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-07-05">2020. July 5-10, 2020</date>
			<biblScope unit="page" from="7237" to="7256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">The risk of racial bias in hate speech detection</title>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Sap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dallas</forename><surname>Card</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saadia</forename><surname>Gabriel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th annual meeting of the association for computational linguistics</title>
		<meeting>the 57th annual meeting of the association for computational linguistics</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1668" to="1678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Fairness and abstraction in sociotechnical systems</title>
		<author>
			<persName><forename type="first">Danah</forename><surname>Andrew D Selbst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sorelle</forename><forename type="middle">A</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suresh</forename><surname>Friedler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Janet</forename><surname>Venkatasubramanian</surname></persName>
		</author>
		<author>
			<persName><surname>Vertesi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the conference on fairness, accountability, and transparency</title>
		<meeting>the conference on fairness, accountability, and transparency</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="59" to="68" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Evaluating debiasing techniques for intersectional biases</title>
		<author>
			<persName><forename type="first">Shivashankar</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xudong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Baldwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lea</forename><surname>Frermann</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.emnlp-main.193</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event<address><addrLine>Punta Cana, Dominican Republic</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021-07-11">2021. 7-11 November, 2021</date>
			<biblScope unit="page" from="2492" to="2498" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Assessing social and intersectional biases in contextualized word representations</title>
		<author>
			<persName><forename type="first">Yi</forename><surname>Chern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tan</forename></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">Elisa</forename><surname>Celis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019</title>
		<meeting><address><addrLine>NeurIPS; Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12-08">2019. 2019. December 8-14, 2019</date>
			<biblScope unit="page" from="13209" to="13220" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Fairness with overlapping groups; a probabilistic perspective</title>
		<author>
			<persName><forename type="first">Forest</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mouhamadou</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oluwasanmi</forename><surname>Koyejo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020</title>
		<meeting><address><addrLine>NeurIPS; virtual</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-12-06">2020. 2020. December 6-12, 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Probably approximately metric-fair learning</title>
		<author>
			<persName><forename type="first">Gal</forename><surname>Yona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guy</forename><forename type="middle">N</forename><surname>Rothblum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning, ICML 2018</title>
		<title level="s">Proceedings of Machine Learning Research</title>
		<meeting>the 35th International Conference on Machine Learning, ICML 2018<address><addrLine>Stockholmsmässan, Stockholm, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-07-10">2018. July 10-15, 2018</date>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="5666" to="5674" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title/>
		<author>
			<persName><surname>Pmlr</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Fairness constraints: Mechanisms for fair classification</title>
		<author>
			<persName><forename type="first">Muhammad</forename><surname>Bilal Zafar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Isabel</forename><surname>Valera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manuel</forename><surname>Gomez Rogriguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krishna</forename><forename type="middle">P</forename><surname>Gummadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="962" to="970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title/>
		<author>
			<persName><surname>Pmlr</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Scaling fair learning to hundreds of intersectional groups</title>
		<author>
			<persName><forename type="first">Eric</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">De-An</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anqi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anima</forename><surname>Anandkumar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Leveling down in computer vision: Pareto inefficiencies in fair deep classifiers</title>
		<author>
			<persName><forename type="first">Dominik</forename><surname>Zietlow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Lohaus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guha</forename><surname>Balakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthäus</forename><surname>Kleindessner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francesco</forename><surname>Locatello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Russell</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR52688.2022.01016</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2022</title>
		<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2022-06-18">2022. June 18-24, 2022</date>
			<biblScope unit="page" from="10400" to="10411" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
