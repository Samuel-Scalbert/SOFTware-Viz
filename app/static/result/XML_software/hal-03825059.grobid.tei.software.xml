<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ON THE LINK BETWEEN EMOTION, ATTENTION AND CONTENT IN VIRTUAL IMMERSIVE ENVIRONMENTS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><roleName>Florent</roleName><forename type="first">Quentin</forename><surname>Guimard</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Camille</forename><surname>Bauce</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Université Côte d&apos;Azur</orgName>
								<orgName type="institution" key="instit2">Inria</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Aldric</forename><surname>Ducreux</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Lucile</forename><surname>Sassatelli</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Institut Universitaire de France</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hui-Yin</forename><surname>Wu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Université Côte d&apos;Azur</orgName>
								<orgName type="institution" key="instit2">Inria</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Marco</forename><surname>Winckler</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Université Côte d&apos;Azur</orgName>
								<orgName type="institution" key="instit2">Inria</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Auriane</forename><surname>Gros</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution" key="instit1">Université Côte d&apos;Azur</orgName>
								<orgName type="institution" key="instit2">CHU de Nice</orgName>
								<orgName type="institution" key="instit3">CoBTeK</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="laboratory">I3S</orgName>
								<orgName type="institution" key="instit1">Université Côte d&apos;Azur</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">ON THE LINK BETWEEN EMOTION, ATTENTION AND CONTENT IN VIRTUAL IMMERSIVE ENVIRONMENTS</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">D68B16FECA812F91A3339E558CFD6C9D</idno>
					<idno type="DOI">10.1109/ICIP46576.2022.9897903</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-04-12T14:52+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>• videos</term>
					<term>saliency maps</term>
					<term>emotions</term>
					<term>physiological signals</term>
					<term>gaze</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>While immersive media have been shown to generate more intense emotions, saliency information has been shown to be a key component for the assessment of their quality, owing to the various portions of the sphere (viewports) a user can attend. In this article, we investigate the tri-partite connection between user attention, user emotion and visual content in immersive environments. To do so, we present a new dataset enabling the analysis of different types of saliency, both lowlevel and high-level, in connection with the user's state in 360 • videos. Head and gaze movements are recorded along with self-reports and continuous physiological measurements of emotions. We then study how the accuracy of saliency estimators in predicting user attention depends on user-reported and physiologically-sensed emotional perceptions. Our results show that high-level saliency better predicts user attention for higher levels of arousal. We discuss how this work serves as a first step to understand and predict user attention and intents in immersive interactive environments.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Immersive media and environments are on the rise with increased affordability of virtual reality (VR) equipment and the deployment of popular platforms for 360 • streaming or advanced interaction in the Metaverse 1 . This new type of content questions the design of compelling immersive experiences, as well as the technical choices for storage and distribution. Visual quality assessment by humans is key to enable efficient storage and distribution of content with high perceptual quality <ref type="bibr" target="#b0">[1]</ref>. To apply quality assessment-driven processes This work has been partly supported by the French government, through the UCA JEDI and EUR DS4H Investments in the Future projects ANR-15-IDEX-0001 and ANR-17-EURE-0004. This work was partly supported by EU Horizon 2020 project AI4Media, under contract no. 951911 (https: //ai4media.eu/). 1 https://www.cnbc.com/2021/12/27/metas-oculus-virtual-realityheadsets-were-a-popular-holiday-gift.html (such as compression, streaming decisions, etc.) to any video content, it is crucial to automate quality assessment with quality estimators associated to the content. For immersive media specifically, quality assessment depends on the sequence of viewports attended by the human rater, which can be significantly different from one rater to the other. For example, Xu et al. <ref type="bibr" target="#b1">[2]</ref> consider content saliency and viewport preference to extend the PSNR and SSIM metrics to 360 • content. The accuracy of quality estimators for immersive content therefore strongly depends on the accuracy of content-based saliency estimators predicting patterns of human attention. Saliency is hence a key component of quality assessment of immersive media. On the other hand, immersive content have been shown to elicit more intense emotions compared with regular flat-screen presentations <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5]</ref>.</p><p>In this article, we investigate how emotions are associated with head motion to impact the accuracy of content-based saliency estimators. To the best of our knowledge, this is the first article to investigate the tri-partite connection between user attention, user emotion and visual content in immersive environments. Our contributions are:</p><p>• A new dataset from user experiments recording head and gaze movements, along with self-reports and continuous physiological measurements of emotions. The stimuli are 360 • videos selected to enable the analysis of different types of saliency in connection with the user's state. We verify the consistency of our results. The dataset is publicly available 2 .</p><p>• An investigation into how emotions affect the accuracy of saliency estimators, both with low-level and high-level saliency, in 360 • videos. Our results show that high-level saliency better predicts user attention for higher levels of arousal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RELATED WORK</head><p>Sensing and analyzing emotions in immersive environments has spurred interest (see, e.g., <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6]</ref>), more recently coupled with motion recordings and analysis <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11]</ref>.</p><p>Human emotions are commonly decomposed along two main dimensions: valence, representing the negative or posi-tive nature of an emotion (unpleasant-pleasant), and arousal, representing the intensity of the perceived emotion (calmexcited) <ref type="bibr" target="#b11">[12]</ref>. The first reference database <ref type="bibr" target="#b6">[7]</ref> providing emotional ratings and motion recordings of 360 • videos is made of 73 VR videos on which 95 users rated valence and arousal using the self-assessment manikin (SAM) tool <ref type="bibr" target="#b12">[13]</ref> after experiencing each video. Their head positions were continuously recorded. A dataset of self-reported emotions of 19 users watching thirty-six 360 • images is collected by Tang et al. <ref type="bibr" target="#b7">[8]</ref>, with eye motion recorded. However, ratings made in retrospect cannot represent the variety of states a user goes through during the experience <ref type="bibr" target="#b5">[6]</ref>, limiting potential analyses and interpretations. Recent works have therefore proposed tools enabling a continuous collection of self-reports inside the immersive environment <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref>. The data collected in these recent works also comprise physiological measurements of heart rate and electrodermal activity (EDA, as skin conductance), which has been shown to reliably represent user instantaneous arousal <ref type="bibr" target="#b13">[14]</ref>. Understanding how different types and levels of emotions correspond to specific types of motion has already been investigated <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b10">11]</ref>. Results from Li et al. <ref type="bibr" target="#b6">[7]</ref> show some level of correlation between (time) average arousal and average pitch angle, and between yaw angle standard deviation and valence, while results from Tang et al. <ref type="bibr" target="#b7">[8]</ref> show a significant impact of negative images on eye behavior.</p><p>While above works have focused on the analysis of user emotion and motion based on coarse-grained categorization of the entire content (high/low positive/negative valence and high/low arousal), other works have focused on the impact of specific regions on the user's attention, described with lowlevel (LL) saliency or emotional aspects. LL saliency refers to pixel-level features (e.g., edges, luminance, motion). Cerf et al. <ref type="bibr" target="#b14">[15]</ref> showed that human eye movements are influenced both by LL and high-level (HL) saliency (related to higher semantic concepts such as objects and faces). Chaabouni et al. <ref type="bibr" target="#b15">[16]</ref> showed that normalizing fixations density with LL saliency significantly improves the interest estimators based on gaze data. Hedger et al. <ref type="bibr" target="#b16">[17]</ref> re-examined previous results suggesting that emotional faces in an image attract more user attention/fixations outside awareness. They showed that facial expressions had no effect on attentional allocation, which can instead be explained by the higher LL saliency.</p><p>In this article, we present a first step towards understanding the connection between user emotion and predictability of motion from content saliency. Specifically,we analyze how the accuracy of LL and HL saliency estimators depend on the user's self-reported and physiologically-sensed emotional perceptions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">MATERIAL AND METHODS</head><p>Dataset We present the dataset we have collected to analyze saliency accuracy in Sec. 4. This dataset is publicly available <ref type="foot" target="#foot_0">2</ref> . It is composed of user head and eye movements recorded while watching 360 • videos in a VR headset, along with users' EDA and heart rate (HR) during viewing, and valence and arousal ratings collected at the end of every clip. The user experiment has been approved by the university ethics committee. Stimuli In order to investigate the differences in accuracy of LL and HL saliencies depending on user emotions, we select seven videos from the database provided by Li et al. <ref type="bibr" target="#b6">[7]</ref> that meet two criteria. First, the videos must span an as broad as possible range of (valence, arousal). Video details are provided here 2 . Second, HL and LL saliencies must not always overlap. LL and HL saliencies are computed on 100 "patches" made of overlapping projections centered on points uniformly sampled from each frame of the video. LL saliency is computed using the Itti model <ref type="bibr" target="#b17">[18]</ref> combined with optical flow between consecutive frames on these individual patches. Inspiring from Chopra et al. <ref type="bibr" target="#b18">[19]</ref>, HL saliency is obtained from YOLOv4 object detector on these same patches, object bounding boxes are used as binary saliency maps. These patches are then back-projected on the equirectangular frame, by addition of the overlapping patches. To select videos where the HL and LL saliencies do not overlap systematically but are rather balanced inside and outside objects, we compare (i) the number of pixels inside and outside objects, and (ii) the per-pixel LL saliency (ranging between 0 and 255), computed as the total LL saliency inside and outside objects normalized with the corresponding number of pixels. Fig. <ref type="figure" target="#fig_0">1</ref> exemplifies that in video 12. The number of pixels with such minimum LL saliency inside and outside objects is equivalent over time, as is the per-pixel LL saliency in both areas. Fig. <ref type="figure" target="#fig_1">2</ref> shows a frame where regions with high LL saliency can be seen outside the detected objects. Methodology Recordings of head and eye movements have been made with a FOVE headset, equipped with an eyetracker with a 120Hz acquisition rate, and tethered to a desktop computer. The video is played in an application developed in Unity with the FOVE SDK. Recordings of EDA and optical pulse have been made with a Shimmer3 GSR+ unit with a frequency range of 15.9Hz. All of the measurements were resampled to 100Hz for analysis. The lab experiment involved 31 users (10f, 20m, 1nb; 18-29 years old, M=24, SD=3.26). First, participants were presented a pre-questionnaire to assess their background with VR and checking for visual deficiencies. Next, the VR experiment systematically started with a low-arousal (relaxing) video (ID 32) to bring EDA and HR levels to a user-relative baseline. Then, the remaining six VR videos were experienced in a random order by every user. Finally, after each video, the headset was taken off and the SAM scale presented for arousal and valence rating. An at least 1-min break outside the headset was observed between every video. All the videos were played without sound. Preliminary analysis After visual inspection to remove erroneous EDA data, the phasic component is extracted using cvxEDA as implemented by Neurokit <ref type="bibr" target="#b19">[20]</ref>. The skin conductance response (SCR) is finally obtained as the absolute value of the first derivative of the phasic component <ref type="bibr" target="#b9">[10]</ref>. The reliability of arousal and valence ratings is assessed by the intraclass correlation coefficient (ICC), where a class of measurements corresponds to a stimulus (360 • video). ICC estimates based on mean ratings with a two-way mixed effects model are 0.96 (95% CI 0.87-0.99) for arousal and 0.88 (95% CI 0.72-0.98) for valence. The median root square difference of valence-arousal ratings with the corresponding values available in the original dataset <ref type="bibr" target="#b6">[7]</ref> is 1.17 (range: 1-9), showing the agreement between both. Finally, we look at the correspondence between SCR and arousal ratings. Similar to Toet et al. <ref type="bibr" target="#b8">[9]</ref>, for each video, we compute the mean (resp. median) of SCR across users. The results show that the video rankings according to mean arousal and to mean SCR are very close (not shown here due to space limitation).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">ANALYSIS</head><p>Our objective is to compare the accuracy of both types of saliency maps, HL and LL, to match the users' fixations over every frame of the 360 • video. To do so, we compute the normalized scanpath saliency (NSS), which measures the amount of saliency around fixations <ref type="bibr" target="#b20">[21]</ref>. We consider segments of 5 sec. to average the saliency maps of all frames and aggregate the user's fixations in this interval, hence obtaining an NSS value for both saliency types NSS HL u,v,i and NSS LL u,v,i for every user u, video v, and interval i. The averages over intervals (resp. users) are denoted by NSS u,v and NSS v , respectively. We analyze the association between NSS HL u,v and NSS LL u,v with mean centered SCR denoted cSCR u,v and graded arousal GA u,v . SCR is centered per user with cSCR u,v = SCR u,v -E v [SCR u,v ] because the preliminary analysis has shown that the absolute levels of SCR vary significantly across users, but intra-user variations across videos are consistent with the ordering of each user's arousal ratings. To analyze the difference in accuracy of both types of saliency depending on the user's arousal, we consider in Fig. <ref type="figure" target="#fig_2">3</ref> the difference NSS Dif f u,v = NSS HL u,v -NSS LL u,v plotted against cSCR u,v (left) and graded arousal GA u,v (right) for all u, v, the points being colored per video. The major finding is the increasing trend of NSS Dif f with EDA and graded arousal. Specifically, the PCC between NSS Dif f and EDA cSCR is 0.25 (p &lt; 10 -3 ), and the PCC between NSS Dif f and graded arousal GA u,v is 0.41 (p &lt; 10 -9 ). These estimates are obtained over 217 (u, v) samples. According to Walline [22, Appendix 6C, page 79], such levels of correlation are significant for 123 and 44 samples, respectively (see <ref type="bibr" target="#b22">[23]</ref>).</p><p>We then analyze the same associations averaged per video in Fig. <ref type="figure" target="#fig_4">4</ref>, where the x-axis of the first row is cSCR v and that of the second row is GA v , with v in the set of video indices. The columns are numbered from the left. We first confirm from the leftmost column that ordering and appearance of NSS Dif f v against EDA or graded arousal are close. Second, we observe a clear increasing trend confirming the above positive significant correlation results.</p><p>To investigate the reasons for this trend, we decompose NSS Dif f v into its individual components NSS HL v and NSS LL v depicted in columns 2 and 3. Owing to the similarity of trends against EDA and graded arousal, we conduct the analysis only on the latter. We first observe an increasing trend of NSS HL v . It could have been even clearer considering that underwater objects in video 12 (brown dot) are often missed by the ob-  ject detector (large shark), hence under-estimating NSS HL  12 . We can then question whether this increase is due to users focusing more when more aroused, or to intrinsic features of the videos, where larger objects would appear in higher arousal videos. We verify in the last column that the increase in NSS HL with arousal cannot be entirely attributed to a relatively larger area occupied by objects. Second, column 3 shows no clear trend. The variation in NSS LL does not appear to be related to EDA or graded arousal.</p><p>We can therefore conclude that the increasing trend of NSS Dif f with arousal is mainly due to higher NSS HL for higher-arousal videos. A first conclusion we may draw is that the relative weight of HL saliency should vary in a saliency model depending on the user's arousal state. Sensing the user's state may hence help predict their attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">DISCUSSION</head><p>We cannot claim causation on whether users focus because they are more aroused by the content, or if they are more aroused because they focus on objects. A first question is whether significantly different levels of arousal occur for users on the same video. This is part of future work. This would mean that the video content alone is not informative enough to adapt the relative weights of HL and LL saliency to users. On the contrary, if the video content is sufficient, then one can think of leveraging arousal (physiological or subjective) measurements in quality assessment sessions to serve as an auxiliary loss to train (deep) saliency models.</p><p>While arousal and valence are major dimensions to describe user emotion of a given content like a video, the richer experience of an immersive and possibly interactive environment is described over various additional dimensions, partic-ularly presence, immersion, agency, engagement, flow, usability, skill or judgement <ref type="bibr" target="#b23">[24]</ref>. Recently, valence, arousal and agency have been shown to interact in non-trivial ways to produce presence <ref type="bibr" target="#b24">[25]</ref>. In 6DoF environments, which we are currently investigating for rehabilitation scenarios and where engagement, skills and judgments are major outcomes, it is crucial to adapt the environment's content to provide proper adaptive guidance to the user. This requires an understanding and the prediction of the user's attention and intents, which depend on the user's emotional state. This work in 3DoF immersive low-interaction environment hence serves as a baseline for immersive interactive environments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">CONCLUSION</head><p>In this article, we have first introduced a new dataset of user head and gaze movements in 360 • videos with valence and arousal ratings, and continuous physiological measurements of skin conductance and heart rate. The stimuli have been specifically selected to enable a spatio-temporal analysis in relation to content, user motion and emotions. We have presented first results comparing HL and LL saliency accuracy depending on user arousal, showing that the accuracy of HL saliency increases when user arousal increases.</p><p>Next steps will consist in investigating finer temporal associations of saliency and attention locations with arousal/EDA, as well as structural modeling of possible interacting factors (e.g., valence, fear, agency) in the production of head and gaze patterns. Also, the accuracy of more refined saliency models such as deep neural networks explicitly or implicitly combining saliency levels will be assessed to better understand motion predictability depending on the user's emotional state.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: HL and LL saliency characterization of Video 12. Left: number of pixels inside and outside objects. Right: average LL saliency per pixel inside and outside objects.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Fig. 2: HL and LL saliency visualization for frame 1545 of video 12. Left: the frame. Center: HL saliency (detected objects are elephants). Right: LL saliency.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>Fig. 3: NSS Dif f u,v against cSCR u,v and GA u,v for every user u and video v. The black line shows a linear regression model fitted on the data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 :</head><label>4</label><figDesc>Fig. 4: From left to right: NSS Dif f v</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>https://gitlab.com/PEM360/PEM360</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Qualinet White Paper on Definitions of Quality of Experience</title>
		<author>
			<persName><forename type="first">Kjell</forename><surname>Brunnström</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergio</forename><forename type="middle">Ariel</forename><surname>Beker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katrien</forename><surname>De Moor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ann</forename><surname>Dooms</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Egger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marie-Neige</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tobias</forename><surname>Hossfeld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Satu</forename><surname>Jumisko-Pyykkö</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Keimel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohamed-Chaker</forename><surname>Larabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bob</forename><surname>Lawlor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><forename type="middle">Le</forename><surname>Callet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Möller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manuela</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Perkis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jesenka</forename><surname>Pibernik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Raake</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Reichl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ulrich</forename><surname>Reiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raimund</forename><surname>Schatz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Schelkens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lea</forename><surname>Skorin-Kapov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominik</forename><surname>Strohmeier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Timmerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Varela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ina</forename><surname>Wechsung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junyong</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrej</forename><surname>Zgank</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013-03-12">Mar. 2013. March 12, 2013</date>
			<pubPlace>Novi Sad</pubPlace>
		</imprint>
	</monogr>
	<note>Qualinet White Paper on Definitions of Quality of Experience Output from the fifth Qualinet meeting</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Assessing visual quality of omnidirectional videos</title>
		<author>
			<persName><forename type="first">Mai</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenzhong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zulin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenyu</forename><surname>Guan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3516" to="3530" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Presence and emotions in virtual environments: The influence of stereoscopy</title>
		<author>
			<persName><forename type="first">Rosa</forename><surname>María Baños</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cristina</forename><surname>Botella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Isabel</forename><surname>Rubió</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soledad</forename><surname>Quero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Azucena</forename><surname>García-Palacios</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mariano</forename><surname>Luis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alcañiz</forename><surname>Raya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">multimedia and virtual reality on behavior and society</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="8" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
	<note>Cyberpsychology &amp; behavior : the impact of the Internet</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Is virtual reality emotionally arousing? investigating five emotion inducing virtual park scenarios</title>
		<author>
			<persName><forename type="first">Anna</forename><surname>Felnhofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oswald</forename><forename type="middle">D</forename><surname>Kothgassner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mareike</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna-Katharina</forename><surname>Heinzle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leon</forename><surname>Beutl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Helmut</forename><surname>Hlavacs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilse</forename><surname>Kryspin-Exner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Hum.-Comput. Stud</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="page" from="48" to="56" />
			<date type="published" when="2015-10">oct 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Gaming in virtual reality: What changes in terms of usability, emotional response and sense of presence compared to non-immersive video games?</title>
		<author>
			<persName><forename type="first">Federica</forename><surname>Pallavicini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Pepe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maria</forename><forename type="middle">Eleonora</forename><surname>Minissi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Simulation &amp; Gaming</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="136" to="159" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Comparing Emotional States Induced by 360 • Videos Via Head-Mounted Display and Computer Screen</title>
		<author>
			<persName><forename type="first">Jan-Niklas</forename><surname>Voigt-Antons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eero</forename><surname>Lehtonen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andres</forename><surname>Pinilla Palacios</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danish</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tanja</forename><surname>Kojic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Möller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 Twelfth International Conference on Quality of Multimedia Experience (QoMEX)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A Public Database of Immersive VR Videos with Corresponding Ratings of Arousal, Valence, and Correlations between Head Movements and Self Report Measures</title>
		<author>
			<persName><forename type="first">Benjamin</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeremy</forename><forename type="middle">N</forename><surname>Bailenson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Pines</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Walter</forename><forename type="middle">J</forename><surname>Greenleaf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leanne</forename><forename type="middle">M</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in Psychology</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">2116</biblScope>
			<date type="published" when="2017-12">Dec. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Influence of Emotions on Eye Behavior in Omnidirectional Content</title>
		<author>
			<persName><forename type="first">Wei</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiyi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Toinon</forename><surname>Vigier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthieu Perreira Da</forename><surname>Silva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 Twelfth International Conference on Quality of Multimedia Experience (QoMEX)</title>
		<meeting><address><addrLine>Athlone, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020-05">May 2020</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">An Immersive Self-Report Tool for the Affective Appraisal of 360 • VR Videos</title>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Toet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabienne</forename><surname>Heijn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anne-Marie</forename><surname>Brouwer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tina</forename><surname>Mioch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><forename type="middle">B F</forename><surname>Van Erp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in Virtual Reality</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">552587</biblScope>
			<date type="published" when="2020-09">Sept. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">CEAP-360VR: A Continuous Physiological and Behavioral Emotion Annotation Dataset for 360 VR Videos</title>
		<author>
			<persName><forename type="first">Abdallah</forename><forename type="middle">El</forename><surname>Tong Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyi</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gangyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pablo</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><surname>Cesar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Investigating the Relationship between Momentary Emotion Self-reports and Head and Eye Movements in HMD-based 360 • VR Video Watching</title>
		<author>
			<persName><forename type="first">Abdallah</forename><forename type="middle">El</forename><surname>Tong Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gangyi</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pablo</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><surname>Cesar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems</title>
		<meeting><address><addrLine>Yokohama Japan</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2021-05">May 2021</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Discrete emotions or dimensions? The role of valence focus and arousal focus</title>
		<author>
			<persName><forename type="first">Lisa</forename><surname>Feldman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barrett</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognition and Emotion</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="579" to="599" />
			<date type="published" when="1998">1998</date>
			<publisher>Taylor &amp; Francis</publisher>
			<pubPlace>Place</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Measuring emotion: The selfassessment manikin and the semantic differential</title>
		<author>
			<persName><forename type="first">Margaret</forename><forename type="middle">M</forename><surname>Bradley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Lang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Behavior Therapy and Experimental Psychiatry</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="49" to="59" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">Wolfram</forename><surname>Boucsein</surname></persName>
		</author>
		<title level="m">Electrodermal activity</title>
		<meeting><address><addrLine>New York, NY, US</addrLine></address></meeting>
		<imprint>
			<publisher>Springer Science + Business Media</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page">618</biblScope>
		</imprint>
	</monogr>
	<note>Electrodermal activity. 2nd ed.</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Predicting human gaze using low-level saliency combined with face detection</title>
		<author>
			<persName><forename type="first">Moran</forename><surname>Cerf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Harel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wolfgang</forename><surname>Einhaeuser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christof</forename><surname>Koch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Platt</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Koller</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Singer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Roweis</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Impact of Saliency and Gaze Features on Visual Control: Gaze-Saliency Interest Estimator</title>
		<author>
			<persName><forename type="first">Souad</forename><surname>Chaabouni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frederic</forename><surname>Precioso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM International Conference on Multimedia</title>
		<meeting>the 27th ACM International Conference on Multimedia<address><addrLine>Nice France</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019-10">Oct. 2019</date>
			<biblScope unit="page" from="1367" to="1374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Do emotional faces capture attention, and does this depend on awareness? Evidence from the visual probe paradigm</title>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Hedger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Garner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wendy</forename><forename type="middle">J</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: Human Perception and Performance</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="790" to="802" />
			<date type="published" when="2019-06">June 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A model of saliencybased visual attention for rapid scene analysis</title>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Itti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christof</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ernst</forename><surname>Niebur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1254" to="1259" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">PARIMA: Viewport Adaptive 360-Degree Video Streaming</title>
		<author>
			<persName><forename type="first">Lovish</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarthak</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhijit</forename><surname>Mondal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandip</forename><surname>Chakraborty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Web Conference 2021</title>
		<meeting>the Web Conference 2021</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2379" to="2391" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">NeuroKit2: A python toolbox for neurophysiological signal processing</title>
		<author>
			<persName><forename type="first">Dominique</forename><surname>Makowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tam</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><forename type="middle">C</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName><surname>Brammer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hung</forename><surname>Franc ¸ois Lespinasse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Schölzel</surname></persName>
		</author>
		<author>
			<persName><surname>Annabel Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behavior Research Methods</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1689" to="1696" />
			<date type="published" when="2021-02">feb 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Methods for comparing scanpaths and saliency maps: strengths and weaknesses</title>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meur</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Thierry</forename><surname>Baccino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behavior Research Methods</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="251" to="266" />
			<date type="published" when="2013-03">Mar. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Designing Clinical Research: an Epidemiologic Approach, 2nd Ed</title>
		<author>
			<persName><forename type="first">Jeffrey</forename><forename type="middle">J</forename><surname>Walline</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Optometry and Vision Science</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Sample size calculators for designing clinical research</title>
		<author>
			<persName><surname>Ucsf</surname></persName>
		</author>
		<ptr target="https://sample-size.net/correlation-sample-size/" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Proposition and Validation of a Questionnaire to Measure the User Experience in Immersive Virtual Environments</title>
		<author>
			<persName><forename type="first">Katy</forename><surname>Tcha-Tokey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Christmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emilie</forename><surname>Loup-Escande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Richir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Virtual Reality</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="33" to="48" />
			<date type="published" when="2016-01">Jan. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Effects of Emotion and Agency on Presence in Virtual Reality</title>
		<author>
			<persName><forename type="first">Crescent</forename><surname>Jicol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chun</forename><surname>Hin Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Doling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caitlin</forename><forename type="middle">H</forename><surname>Illingworth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinha</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charlotte</forename><surname>Headey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christof</forename><surname>Lutteroth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karin</forename><surname>Proulx</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eamonn O'</forename><surname>Petrini</surname></persName>
		</author>
		<author>
			<persName><surname>Neill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems</title>
		<meeting>the 2021 CHI Conference on Human Factors in Computing Systems<address><addrLine>Yokohama Japan</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2021-05">May 2021</date>
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
