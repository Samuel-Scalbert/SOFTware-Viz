<TEI xmlns="http://www.tei-c.org/ns/1.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xml:space="preserve" xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Provenance Data in the Machine Learning Lifecycle in Computational Science and Engineering</title>
				<funder>
					<orgName type="full">FAPERJ</orgName>
				</funder>
				<funder>
					<orgName type="full">Inria Associated Team SciDISC</orgName>
				</funder>
				<funder>
					<orgName type="full">CNPq</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher />
				<availability status="unknown"><licence /></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Renan</forename><surname>Souza</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">IBM Research</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Federal Univ. of Rio de Janeiro</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Leonardo</forename><surname>Azevedo</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">IBM Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Vítor</forename><surname>Lourenc ¸o §</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">IBM Research</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Federal Univ. of Rio de Janeiro</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Elton</forename><surname>Soares</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">IBM Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Raphael</forename><surname>Thiago</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">IBM Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Rafael</forename><surname>Brandão</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">IBM Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Daniel</forename><surname>Civitarese</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">IBM Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Emilio</forename><surname>Vital Brazil</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">IBM Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Marcio</forename><surname>Moreno</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">IBM Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Patrick</forename><surname>Valduriez #</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Inria &amp; LIRMM</orgName>
								<orgName type="institution">U. Montpellier</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Marta</forename><surname>Mattoso</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Federal Univ. of Rio de Janeiro</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Renato</forename><surname>Cerqueira</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">IBM Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Marco</forename><forename type="middle">A S</forename><surname>Netto</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">IBM Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Provenance Data in the Machine Learning Lifecycle in Computational Science and Engineering</title>
					</analytic>
					<monogr>
						<imprint>
							<date />
						</imprint>
					</monogr>
					<idno type="MD5">824061B730278DC62B36DCEDEB00732D</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-04-12T14:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid" />
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Machine Learning Lifecycle</term>
					<term>Workflow Provenance</term>
					<term>Computational Science and Engineering</term>
				</keywords>
			</textClass>
			<abstract>
<div><p>Machine Learning (ML) has become essential in several industries. In Computational Science and Engineering (CSE), the complexity of the ML lifecycle comes from the large variety of data, scientists' expertise, tools, and workflows. If data are not tracked properly during the lifecycle, it becomes unfeasible to recreate a ML model from scratch or to explain to stackholders how it was created. The main limitation of provenance tracking solutions is that they cannot cope with provenance capture and integration of domain and ML data processed in the multiple workflows in the lifecycle, while keeping the provenance capture overhead low. To handle this problem, in this paper we contribute with a detailed characterization of provenance data in the ML lifecycle in CSE; a new provenance data representation, called PROV-ML, built on top of W3C PROV and ML Schema; and extensions to a system that tracks provenance from multiple workflows to address the characteristics of ML and CSE, and to allow for provenance queries with a standard vocabulary. We show a practical use in a real case in the O&amp;G industry, along with its evaluation using 48 GPUs in parallel.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div><head>I. INTRODUCTION</head><p>Machine Learning (ML) has been fundamentally changing Computational Science and Engineering (CSE) in various ways <ref type="bibr" target="#b0">[1]</ref>. Techniques, such as statistical relational learning and deep learning, have been used to extract knowledge from data, with application domains ranging from Computational Physics to Agriculture and Oil and Gas (O&amp;G) <ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref>. Obtaining reliable ML models involves a complex ML lifecycle <ref type="bibr" target="#b5">[6]</ref>, which is critical in large-scale CSE projects <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b6">7]</ref>. The ML lifecycle in CSE depends on transforming raw data into trained models, which requires multiple, distributed workflows that use a wide variety of algorithms, data, data processing tools and data stores; demands execution in machines ranging from standalone servers to cloud or HPC clusters; and is carried out by multidisciplinary teams, including domain scientists, computational scientists and engineers, and ML specialists. Given the heterogeneous nature of the lifecycle, it is difficult to track, in an integrated way, the data transformations that occur throughout the lifecycle while keeping the execution overhead low, which is a major concern among CSE users. In practice, tracking the data in the data transformations is often done manually, which is time consuming and error prone. This is problematic for several reasons, ranging from scientific (e.g., jeopardizes reproducibility) to business (e.g., users may be less likely to apply a trained model, even with best performance, if they do not understand the transformations in the lifecycle).</p><p>Data lineage (i.e., data provenance) helps reproducing, tracing, assessing, and understanding data and their transformation processes <ref type="bibr" target="#b7">[8]</ref>. Solutions for provenance data tracking for ML have been proposed <ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref>, but with focus on learning phases only, thus limiting an integrated view of domain-specific data, processed in the early phases of the lifecycle, with ML data. Besides, users need to migrate their workflows to a different software ecosystem or change the way they develop, which may compromise adoption in CSE. Another approach is to add provenance tracking to workflows, reducing the need to change the development practice <ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref>. Nonetheless, solutions following this approach do not support the lifecycle, which requires three main capabilities: provenance tracking in multiple workflows that use heterogeneous data and stores; a provenance data representation with ML-specific vocabulary; and providing for integrated data analysis through provenance while keeping the capture overhead low. Another solution following this approach is <software ContextAttributes="used">ProvLake</software> <ref type="bibr" target="#b16">[17]</ref>, which has low capture overhead in multiple workflows that use heterogeneous data stores. However, similarly to the other solutions, its provenance data representation is based on W3C PROV only, thus lacking ML-specific vocabulary, limiting its support for the ML lifecycle in CSE. Allowing for such provenance analysis that integrates both ML data and domain-specific data while keeping capture overhead low in HPC workflows is hard.</p><p>In this paper, we propose an end-to-end solution for tracking data transformations that occur in the ML lifecycle in CSE, from the curation of raw data until the generation of trained models, by providing efficient provenance capture and data analysis through provenance queries. Our approach is to model the lifecycle as multiple workflows interconnected with data and to track provenance as the workflows execute. By adding provenance capture calls in the workflows, users can perform ML monitoring (e.g., the evolution of model performance as the training iterates) and more comprehensive provenance analyses that join domain-specific data with ML data generated in the lifecycle. The main contributions of the paper are:</p><p>(i) a characterization of provenance data in the ML lifecycle in CSE (Sec. II);</p><p>(ii) PROV-ML: a new data representation, which combines W3C PROV <ref type="bibr" target="#b17">[18]</ref> with W3C ML Schema <ref type="bibr" target="#b18">[19]</ref>, for prove-R. <ref type="bibr">Souza</ref>  nance of the ML lifecycle in CSE (Sec. III);</p><p>(iii) extensions of the <software ContextAttributes="used">ProvLake</software> <ref type="bibr" target="#b16">[17]</ref> to support provenance tracking and analysis following the PROV-ML (Sec. IV).</p><p>Experiments show its practical use in a real O&amp;G case in a testbed of 48 GPUs (Sec. V).</p></div>
<div><head>II. WORKFLOW PROVENANCE FOR ML IN CSE</head><p>This work focuses on provenance in workflows formed by chained data transformations composing the ML lifecycle in CSE, aiming at supporting the data analysis in a large-scale CSE project. Before we characterize the data analysis through provenance (Sec. II-C), we first characterize the lifecycle's personas (Sec. II-A) to provide for analysis addressing the users' needs, then we describe the lifecycle (Sec. II-B).</p></div>
<div><head>A. Personas in the ML Lifecycle in CSE</head><p>Large-scale CSE projects are often multidisciplinary, with collaborating users with different skills on the domain data, e.g., mathematics, physics, statistics, computational methods, and ML. These users perform distinct types of analysis and have different provenance requirements. In order to position the personas and their primary activities in the ML lifecycle in CSE, we adapt background work on traditional scientific workflows <ref type="bibr" target="#b19">[20]</ref> and ML <ref type="bibr" target="#b5">[6]</ref>. Fig. <ref type="figure" target="#fig_4">1</ref> illustrates how expertise, representative personas, and primary activities fall under an expertise spectrum ranging from scientific-domain only (fully white on the left) to ML only (fully black on the right).</p></div>
<div><head>Main Expertise</head><p>Domain ML Computational scientists and engineers. They have high computational skills, often with abilities to develop parallel scripts and execute them in HPC clusters. Examples are computational physicists, engineers. They are highly knowledgeable in the domain, although not as in-depth as the domain scientists. They are familiar with traditional numerical simulations that require HPC, which need complex scientific data analyses to guide the fine-tuning of parameters <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b20">21]</ref>.</p></div>
<div><head>Representative Persona</head></div>
<div><head>Domain</head><p>In the lifecycle, they are often the ML model trainers, who tune parameters, a very usual task when training ML models. They use their knowledge on the domain to make decisions, e.g., to filter relevant parts of the training datasets that are guaranteed to respect the physical constraints of the problem. Some users with more in-depth knowledge of ML techniques, i.e., those who are more towards the black portion of the spectrum in Fig. <ref type="figure" target="#fig_4">1</ref>, can design new ML models. They can, for instance, design Physically-informed Neural Networks (PINNs) <ref type="bibr" target="#b2">[3]</ref>, which embed domain-specific physical constraints in the ML models. These users can be responsible for validating the ML model and, more experienced users with considerable ML and domain knowledge, help in the overall analyses of the produced models, their quality, how they were used, etc.</p><p>ML scientists and engineers. They have in-depth knowledge of statistics, ML algorithms, and software engineering. They design new ML models and develop scripts typically using ML libraries like <software>TensorFlow</software>, <software ContextAttributes="used">PyTorch</software>, and <software ContextAttributes="used">Scikit Learn</software>. They are familiar with software engineering techniques (e.g., continuous integration, test-driven development, cloud deployment) and can use different kinds of DBMSs to store data. They often train the ML models they design, often in HPC clusters. Moreover, to be able to develop effective models, they also have some domain knowledge.</p><p>Provenance specialists. In addition to those three main personas, Provenance specialists play an essential role in a CSE project by managing data provenance in the lifecycle. They design the provenance schema for applications and guide other users to add provenance capture calls to the workflows. Thus, they need knowledge in the scientific domain and ML. They also support other personas to analyze provenance, domainspecific, execution, and ML data.</p></div>
<div><head>B. The ML lifecycle in CSE</head><p>We can divide the ML lifecycle in CSE into three major phases: data curation, learning data preparation, and learning (Fig. <ref type="figure" target="#fig_0">2</ref> -dashed arrows are data flows and solid arrows are interactions between phases). Our view of the lifecycle is inspired by Polyzotis et al.' survey <ref type="bibr" target="#b5">[6]</ref>. Although they proposed a generic view, which can be applied to CSE, there remains the need for a focused view on the problems inherent to CSE. We grouped the inner phases into major phases, organizing the activities according to the scientific data manipulated and the personas involved in the major phases.</p><p>Data curation. It is the most complex phase of the lifecycle, especially because of the nature of the scientific data. To achieve automated knowledge extraction from scientific data promoted by ML, much manual and highly specialized work is performed by the users (mainly domain scientists). There is a huge gap between raw scientific data and useful data for consumption (e.g., data to serve as input to train ML models). Datasets are typically large, up to terabytes in a single file.  They may contain geospatial-temporal data, stored as huge matrices in well-known scientific formats, like HDF5. Also, some files are stored using domain-specific formats, e.g., SEG-Y format for seismic data, widely used in the O&amp;G industry. Specialized formats in CSE domains may require industryspecific software and domain-specific knowledge to inspect, visualize, and understand the data. In addition, users can use metadata and textual reports to annotate the data with extra domain-specific knowledge, without which would be nearly impossible to make the data useful for ML algorithms.</p><p>Considering the heterogeneous nature of the data, "it is unreasonable to assume that data lives in a single source" (e.g., a single file system or DBMS) <ref type="bibr" target="#b5">[6]</ref>. For instance, raw files can be stored in file systems or cloud stores, domain-specific annotations can be stored in a <software ContextAttributes="used">Semantic Graph</software> DBMS (e.g., Triple Store) with domain ontologies, and curated data can be stored in a NoSQL DBMS, such as Document DBMSs. Then, computational scientists and engineers write data-intensive workflow scripts to clean, filter, and validate the data. For instance, they check if the geolocalization of the data files is consistent. These scripts transform the raw data into curated data by consuming and generating data from those data stores. Each of these inner phases inside the data curation phase is highly interactive, manual, and may execute independently. In other words, users may run different scripts to execute these phases, several times, in an ad-hoc way and any order. Also, they run these scripts in different machines, such as in an HPC cluster or in the cloud, or even on the users' desktop. These phases occur in a cycle, which stops when the users consider the data "curated". These curated data are significantly more organized and easier to analyze and understand. In the context of ML, it is ready to be transformed into training data.</p><p>Learning data preparation. Model trainers select relevant parts of the curated data to be used for learning. For instance, if the ML task is to classify geological structures <ref type="bibr" target="#b4">[5]</ref>, seismic images will need to be correlated with annotations -seismic interpretation-, creating annotated samples. After selecting the data, model designers develop scripts that transform the data into training datasets. Typical transformations include image crop, quantization, scale, among others. In this phase, users frequently use domain-specific libraries to manipulate raw scientific data. Due to data complexity, oftentimes data need to be manually inspected before it can be used as input for the learning phase. </p></div>
<div><head>C. Characterizing Provenance Analysis in ML for CSE</head><p>Provenance data in workflows contain a structured record of the data derivation paths within chained data transformations, along with the parameterization of each transformation <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b20">21]</ref>. Provenance data are usually represented as a directed graph where: vertices are instances of entities (typically data) or activities (typically the data transformations) or agents (typically the users); and, edges are instances of relationships between vertices <ref type="bibr" target="#b17">[18]</ref>. Scientists use provenance data for reproducibility and result understanding <ref type="bibr" target="#b7">[8]</ref>. This kind of provenance consumption, which often occurs post mortem, i.e., after workflow execution, is characterized as offline provenance analysis. A characterization of provenance analysis to leverage ML in support of workflows is surveyed by Deelman et al. <ref type="bibr" target="#b6">[7]</ref>. We propose here a taxonomy to classify provenance analysis in support of ML, by considering three classes: data, execution timing, and training timing. We provide the characterization based on the data being analyzed, using query examples (listed in Table <ref type="table" target="#tab_3">I</ref>) in our use case in O&amp;G.</p><p>Use case. The use case addresses seismic surveys, which are indirect measures of the earth subsurface that can be organized into slices (images). These surveys cover hundreds of square kilometers and help to interpret the geology and find possible hydrocarbons accumulations. The seismic data have a very complex workflow and can suffer from many problems, like noise and shadows (regions with low signal). Also, the geological structures vary from point to point in the earth, imposing significant challenges to the ML algorithms. Next, we characterize the data involved in the lifecycle.</p><p>Data class includes domain-specific, machine learning, and execution. Provenance data may be augmented with these data, increasing the scope of provenance analysis.</p><p>Domain-specific data are the main data processed in the data curation phase (Sec. II-B). Approaches to add domain data into provenance analysis include, e.g., raw data extraction <ref type="bibr" target="#b14">[15]</ref> and utilization of semantic domain databases associated to provenance databases <ref type="bibr" target="#b16">[17]</ref>. For raw data extraction, quantities of interest are extracted from large raw data files, and for domain databases, domain scientists may provide relevant information and metadata about the raw data and store them in knowledge data graphs (e.g., in Triple Stores).</p><p>Machine learning data include training data and generated trained models, which are more related to the learning data preparation (e.g., Q1) and learning (e.g., Q2 and Q3) phases (Fig. <ref type="figure" target="#fig_0">2</ref>). These queries exemplify that the parametrization within the data transformations and relevant metadata of the generated data (both training data and trained model) are important for provenance analysis.</p><p>Execution data. Besides model performance metrics (e.g., accuracy), users need to assess execution time and resources consumption of their workflows. They need to inspect if a critical block in their workflow (e.g., the one that demands high parallelism) is taking longer than usual or if other parts are consuming more memory than expected. For this, provenance systems can capture system performance metrics and timestamps (e.g., Q4). Metadata such as data store metadata (e.g., host address), HPC cluster name and nodes in use, etc. can be captured and associated with the provenance of the data transformations for extended analysis.</p><p>Hybrid. Users can combine these data. For instance, in Q5, the analysis queries data processed in workflows in the learning data preparation and learning phases, whereas Q6 uses the same data generated in the learning data preparation to analyze the raw files curated in the data curation phase.</p><p>Execution timing refers to if the analysis is done online, i.e., while at least a workflow is running, or offline.</p><p>Offline analysis. The typical use of offline provenance analysis is to support reproducibility and historical results understanding, e.g., understand the data curation phase of raw scientific files and relate with the generated trained ML models. The queries Q1-Q6 can be executed offline.</p><p>Online analysis. Users can use online provenance analysis to monitor, debug or inspect the data transformations while they are still running (e.g., see the status, see how the intermediate results are evolving as the input parameters vary). The problem of adding low provenance data capture overhead is more challenging for provenance systems that allow for online analysis <ref type="bibr" target="#b16">[17]</ref>. Queries Q3-Q5 exemplify queries that can be executed online, e.g., while a training process is running.</p><p>Training timing refers to whether the analysis performs intratraining-i.e., to inspect one training process, e.g., a training job running on an HPC cluster, or inter-training-i.e., analyses comprehending results of several training processes.</p><p>Intra-training. In an offline intra-training analysis, users are interested in understanding how well trained models generated in a given training process perform. All queries, Q1-Q6, could be executed either online or offline, but Q3 and Q4 are more likely to be performed as online intra-training analysis.</p><p>Inter-training. This analysis refers to comprehensive queries to understand multiple training processes, e.g., how each of them performed, which training datasets were used, how the training processes were parameterized. This is very important in the lifecycle, as it supports activities like Model Validation, Management, Training, and Design. Usually, they are used offline, but may also be performed online. Queries Q1-Q6 fit this class when analyzing multiple trained models generated in different training processes.</p><p>Further characterization. Other classes worth mentioning for provenance analysis for ML in CSE are: data store-data are distributed onto multiple stores, like file systems, cloud stores (e.g., IBM Cloud Object Storage), Relational or NoSQL DBMSs <ref type="bibr" target="#b16">[17]</ref>; provenance data granularity-provenance of files (i.e., references to files consumed and generated in a <software ContextAttributes="used">script</software>), functions calls (arguments and outputs), blocks of code, and stack traces <ref type="bibr" target="#b13">[14]</ref>; and provenance analysis direction: forward or backward-generally, forward queries analyze from raw scientific files or training datasets to trained models (e.g., Q3-Q5), whereas backward queries analyze from trained models to training datasets or raw files (e.g., Q1, Q2, Q6).</p></div>
<div><head>III. ML PROVENANCE DATA REPRESENTATION</head><p>There are many workflow provenance tracking solutions <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b16">17]</ref>, but they are often based on W3C PROV <ref type="bibr" target="#b17">[18]</ref> (and extensions) only. Thus, they are too generic in terms of provenance data representation and analysis, which makes the adoption for ML more difficult. An existing work on ML data representation is the W3C ML Schema (MLS) <ref type="bibr" target="#b21">[22]</ref>. Although the MLS has some provenance representation, a MLS-based only representation does not meet the needs either. It does not have a clear distinction between prospective and retrospective provenance data, which compromises query capabilities because prospective provenance provides the abstraction layer to specify provenance analysis over data generated in workflows' execution (i.e., retrospective provenance). Also, MLS does not separate the learning stages (training, validation and test), which would enable finer analysis based on specific stages. Finally, it is not designed to allow for representation of domain-specific data generated at early phases of the lifecycle (e.g., data curation).</p><p>To address these problems, this section introduces PROV-ML. To the best of our knowledge, it is the first provenance  data representation for workflows in the ML lifecycle in CSE. It is compliant with both W3C PROV and MLS, and extends <software ContextAttributes="used">ProvLake</software>'s workflow provenance data representation <ref type="bibr" target="#b22">[23]</ref>, which is an extension of PROV.</p><p>PROV-ML provides detailed support for the learning and learning data preparation phases of lifecycle. It inherits the benefits of <software ContextAttributes="used">ProvLake</software>, enabling the integration of provenance of domain-specific data processed by workflows in the curation phase. PROV-ML is depicted in Fig. <ref type="figure" target="#fig_2">3</ref>, where Fig. <ref type="figure" target="#fig_2">3(A)</ref> shows the relation of the learning phase with the input data and the goal of a ML workflow (i.e., a workflow in the learning phase); and Fig. <ref type="figure" target="#fig_2">3</ref>(B) represents the relation of the learning phase with its technique and parameters. Classes in white background represent prospective provenance; light gray, retrospective; and dark gray represents specific concepts inherited, as is, from MLS. PROV-ML classes are described on Table <ref type="table" target="#tab_4">II</ref>. Further details on PROV-ML are online <ref type="bibr" target="#b22">[23]</ref>.</p></div>
<div><head>IV. PROVLAKE IN THE ML LIFECYCLE IN CSE</head><p>To address provenance tracking and analysis throughout the ML lifecycle in CSE, our approach is to model it as multiple workflows with chained data transformations, where the workflows are interconnected through data. Provenance tracking comprises provenance capture, the creation of the provenance relationships (e.g., associations between the processes and the consumed and generated data), and storage of the provenance data. In our view, provenance tracking systems that can be coupled to workflows <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b16">17]</ref> provide the flexibility needed in large-scale CSE projects, as opposed to moving workflows' executions and data to be managed by a single orchestration system, like a Workflow Management System. Workflow provenance capture systems usually address <software ContextAttributes="used">scripts</software> as workflows with chained functions, method, or library calls that execute data transformations, while capturing input arguments and output values from these calls. Among these solutions, <software ContextAttributes="used">ProvLake</software> <ref type="bibr" target="#b16">[17]</ref> has been applied to capture provenance from multiple distributed workflows that consume and generate data from and to heterogeneous data stores, while keeping provenance capture overhead low. While these workflows execute, provenance data are captured and stored in a single provenance database, available for integrated analysis of the data generated throughout the lifecycle. This section describes <software ContextAttributes="used">ProvLake</software> architecture and deployment in support of the lifecycle.</p><p>Architecture. It has five main components (Fig. <ref type="figure" target="#fig_3">4</ref>): <software ContextAttributes="used">ProvLake</software> Library (<software ContextAttributes="used">PLLib</software>); <software ContextAttributes="used">ProvTracker</software>; <software ContextAttributes="used">ProvManager</software>; <software ContextAttributes="used">PolyProv-QueryEngine</software>; and <software ContextAttributes="used">Prov DBMS</software> (the DBMS that manages the provenance database). The workflows are instrumented with <software ContextAttributes="used">PLLib</software>, imported as a library in the <software ContextAttributes="used">scripts</software>, which is responsible for the provenance data capture. In an offline manner and following the methodology described in a previous work <ref type="bibr" target="#b16">[17]</ref> to specify the workflows using prospective provenance data standards <ref type="bibr" target="#b7">[8]</ref>, users add provenance data capture calls using the <software ContextAttributes="used">PLLib</software>. A provenance capture task happens when a data transformation executes, which typically occurs in a function call, in a </p></div>
<div><head>Experiment</head><p>The set of analyses (e.g., research questions), that drives the ML workflow.</p></div>
<div><head>LearningProcessExecution</head><p>An ML workflow execution. This is equivalent to mls:Run and was renamed to explicitly preserve the aspects of retrospective provenance, which are not explicitly handled in MLS.</p></div>
<div><head>LearningTask and LearningTaskValue</head><p>Defines the goal of a learning process, i.e., the ML task (e.g., LearningTask: Classification; LearningTaskValue: Seismic Stratigraphic Classification).</p></div>
<div><head>LearningStageType</head><p>A stage in the learning process. It is one of: training, testing or validation.</p></div>
<div><head>LearningDatasetReference</head><p>Defines the dataset to be used by a LearningStage and LearningDatasetRe-ferenceValue is the dataset reference used in a LearningStageExectution.</p></div>
<div><head>DatasetCharacteristic and DatasetCharacteristcValue</head><p>Defines metadata about the Learning-DatasetReference (e.g., #instances), and DatasetCharacteristcValue relates with a LearningDatasetReferenceValue (e.g., #instances =8).</p></div>
<div><head>FeatureSet and FeatureSetData</head><p>Defines the features FeatureExtraction to generate over LearningDatasetReference and, FeatureSetData is the generated values in the execution.</p></div>
<div><head>FeatureSetCharacteristic</head><p>Defines the set of metadata that describes the FeatureSet (e.g., number of features, features' type).</p></div>
<div><head>FeatureExtraction and FeatureExtractionExecution</head><p>Defines the features retrieval process.</p></div>
<div><head>Software</head><p>Defines a collection of ML techniques' implementations (e.g., <software>Scikit-Learn</software>).</p></div>
<div><head>Algorithm</head><p>ML technique with no associated technology, software or implementation (e.g., k-means clustering technique).</p></div>
<div><head>Implementation</head><p>Defines the retrospective aspect of an Algorithm, i.e., an ML technique's implementation in a software (e.g., <software>Scikit-Learn</software>'s k-means implementation).</p></div>
<div><head>ImplementationCharacteristic</head><p>Defines the implementation's set of metadata, (e.g., version, git hash).</p></div>
<div><head>LearningHyperParameter</head><p>Defines the prior parameter of an Algorithm used by a LearningStage.</p></div>
<div><head>LearningHyperParameter Setting</head><p>Defines the parameter values of an execution (e.g., the k value in a k-means clustering technique, range of epochs in a neural network training).</p></div>
<div><head>ModelSchema</head><p>The scope of the resulting model.</p></div>
<div><head>ModelReference and ModelReferenceValue</head><p>The resulting model of a LearningStage should generate and the generated value (e.g., the trained model after the training stage).</p></div>
<div><head>ModelHyperParameter and ModelHyperParameterValue</head><p>Hyperparameters a LearningStage generates and the resulting model with their values (e.g., the epoch which the resulting model was generated), respectively.</p></div>
<div><head>DataStoreInstance</head><p>Resulting model (i.e., ModelReferencevalue) storage.</p></div>
<div><head>EvaluationMeasure and ModelEvaluation</head><p>A measure a LearningStage should evaluate and its associated value generated in execution (e.g., the precision of classifier model).</p></div>
<div><head>EvaluationSpecification and EvaluationProcedure</head><p>Classes directly inherited from MLS, with their semantics preserved.</p><p>program execution, or in an iteration in an iterative workflow.</p><p>As shown in a simplified pseudocode of a deep learning training (Algorithm 1), a provenance task is delimited within blocks of code in the scripts, illustrated with prov.in() and prov.out(), each generating a provenance capture event.</p><p><software ContextAttributes="used">PLLib</software> design has two goals: (i) to keep execution overhead low and (ii) to avoid major modifications in the user code while preserving the provenance data analytical capabilities. Because of the workflow specification using prospective provenance data, kept external to the workflow scripts and loaded only once at the beginning (Line 2 in Algorithm 1), the code modification and data to be sent to <software ContextAttributes="used">ProvTracker</software> are reduced. Design principles such as queuing provenance requests, asynchronicity (i.e., the workflow scripts do not wait for the provenance requests to be fully processed-the pipeline from the <software ContextAttributes="used">PLLib</software> to <software ContextAttributes="used">ProvTracker</software>, <software ContextAttributes="used">ProvManager</software>, and <software ContextAttributes="used">Prov DBMS</software>), and reduction of system calls help reducing capture overhead <ref type="bibr" target="#b16">[17]</ref>. Provenance capture requests are queued and the maximum queue size is a configurable parameter. Moreover, users choose to store provenance data on disk only, rather than sending to <software ContextAttributes="used">ProvTracker</software>, but in this case, online provenance analysis is not supported. Then, if disk only is not specified, when the scripts execute, provenance data are captured and sent to <software ContextAttributes="used">ProvTracker</software>.</p><p><software ContextAttributes="used">ProvTracker</software> uses prospective provenance data to provide for the tracking by creating the relationships of retrospective provenance data being continuously sent by <software ContextAttributes="used">PLLib</software>, from multiple distributed workflows. <software ContextAttributes="used">ProvTracker</software> gives unique identifiers to every data value captured by the <software ContextAttributes="used">PLLib</software>, so when a data transformation consumes data produced by another, <software ContextAttributes="used">ProvTracker</software> will track such relationship and populate the data graph. When the data values are data references (e.g., references to files or identifiers in a database table or any analogous data reference), it creates an edge between the data value and the data store <ref type="bibr" target="#b16">[17]</ref>. Data transformations that are specific and standard in ML workflows, e.g., training, validation, and testing are defined beforehand following PROV-ML (III). <software ContextAttributes="used">ProvTracker</software> also allows users to specify, in the   prospective provenance specification, that certain parameters or output values have ML-specific semantics, following PROV-ML, to be stored in the provenance database. Moreover, <software ContextAttributes="used">ProvTracker</software> has work queues to group provenance requests before sending retrospective provenance data to <software ContextAttributes="used">ProvManager</software>. <software ContextAttributes="used">ProvManager</software> is a RESTful service that receives provenance data using PROV-ML vocabulary, and transforms the data into RDF triples (the data model of the DBMS in use by <software ContextAttributes="used">ProvLake</software> in this current implementation) and inserts them in a bulk. Provenance queries are provided by the <software ContextAttributes="used">PolyProv-QueryEngine</software>. The characterization (Section II-C) and typical queries (e.g., Q1-Q6) are used to influence the implementation of parameterized RESTful endpoints using PROV-ML terms. Variations of this endpoint, using terms available in PROV-ML, are used to specify the inputs for the queries. If an endpoint is not implemented for a specific query, users can still write raw queries and submit them to <software ContextAttributes="used">PolyProvQueryEngine</software> directly, which redirects the query to the <software ContextAttributes="used">Prov DBMS</software>.</p><p>Execution Strategies on HPC Clusters. <software ContextAttributes="used">ProvLake</software> uses a microservice architecture to achieve high flexibility when specifying how the components are deployed to reduce performance penalties. Fig. <ref type="figure" target="#fig_3">4</ref> shows a deployment of <software ContextAttributes="used">ProvLake</software> onto two clusters, one for I/O-intensive workflows like the data processing ones (Data Proc. Workflows in the figure-used for the data curation and learning data preparation phases of the lifecycle) and the other for compute-intensive workflows, like the training workflows (for the learning phase). <software ContextAttributes="used">PLLib</software> is the only component in direct contact with the users' workflows running in the clusters, shielding the workflows from possible slowness from other components. To reduce communication cost between the users' workflow and the <software ContextAttributes="used">PLLib</software>, <software ContextAttributes="used">ProvTracker</software> is deployed inside the cluster. To avoid competition (which increases overhead) with the users' workflows, <software ContextAttributes="used">ProvTracker</software> is started on a separate node in the cluster. The other architectural components are deployed externally to the clusters because they are not in direct contact with the <software ContextAttributes="used">PLLib</software>, thus not increasing the communication cost in the workflow scripts. This avoids using extra computing resources only for provenance tracking and analysis, leaving more resources for the users' workflows; and avoids operational work to install more software, such as a DBMS, inside a compute-intensive cluster.</p></div>
<div><head>V. EXPERIMENTAL VALIDATION</head><p>In this section, we provide an experimental validation of <software>ProvLake</software> in support of the ML lifecycle in CSE. As execution overhead is a major concern among CSE users, we first present a performance analysis of parallel provenance data capture in Section V-A, then we show a running example of which data are captured during the lifecycle of our case study to answer the exemplary queries Q1-Q6 in Section V-B.</p><p>Hardware setup. We use two clusters: a learning cluster, which has 393 Intel and Power8 nodes, each with 24 to 48 CPU cores, 256 to 512 GB RAM, interconnected via InfiniBand, sharing about 3.45 PB in a GPFS, and using in total 946 GPUs (NVIDIA Tesla K40 and K80, each with 2880 and 4992 CUDA cores respectively); and a data processing cluster, which has 12 nodes, each with 128 GB RAM, two Intel CPUs with 40 cores, sharing a GPFS with 24 TB, interconnected via an InfiniBand. Software setup. <software ContextAttributes="used">ProvManager</software>, <software ContextAttributes="used">PolyProvQueryEngine</software>, and <software ContextAttributes="used">Prov DBMS</software> are deployed on a virtual Kubernetes cluster with two nodes with 4 vCores, 16 GB RAM each, virtualized on top of the data processing cluster. As in Fig. <ref type="figure" target="#fig_3">4</ref>, the <software ContextAttributes="used">ProvTracker</software> service is started on a separate node on each of the two clusters. <software ContextAttributes="used">ProvLake</software>'s services are implemented using Python and deployed with <software ContextAttributes="used">uWSGI</software> with C++ <software ContextAttributes="used">Cython</software> plugin with multiprocess and multi-thread parallelism enabled. <software ContextAttributes="used">ProvManager</software>'s queue is set to 50 and <software ContextAttributes="used">ProvTracker</software> threads are set to 120. The workflow <software ContextAttributes="used">scripts</software> of our use case are implemented in Python using multiple libraries, such as to manipulate raw seismic files and for learning (<software ContextAttributes="used">PyTorch</software> V1.1).</p></div>
<div><head>A. Performance Analysis</head><p>In our use case for training an autonomous identifier of geological structures (c.f. Sec II-C), the learning phase generates a large amount of provenance data at a high frequency to stress <software ContextAttributes="used">ProvLake</software> services. In the deep learning model training, there are two provenance capture calls (for the beginning and end) at each batch iteration, in each learning epoch (c.f. Algorithm 1). In this test, each training workflow executes about 35 iterations for each learning epoch and up to 300 epochs, generating about 15,000 provenance capture events per workflow run. <software ContextAttributes="used">ProvTracker</software> runs on one node in the learning cluster with 24 CPU cores, whereas the training workflows run in parallel and distributed on up to 8 nodes, each with 28 Intel CPU cores and 6 GPUs (K80). While running the workflows, <software ContextAttributes="used">PLLib</software> captures data at runtime and sends them to <software ContextAttributes="used">ProvTracker</software> which in turn sends them to <software ContextAttributes="used">ProvManager</software> service deployed externally on the virtual Kubernetes cluster, which finally stores them in the <software ContextAttributes="used">Prov DBMS</software>. A provenance capture overhead analysis of <software ContextAttributes="used">ProvLake</software> using synthetic workloads to highly stress the system and comparison with a competing system has been presented in a previous work <ref type="bibr" target="#b16">[17]</ref>. Here, we first present a performance analysis testing different settings for provenance analysis, and then a scalability analysis, both using real ML workloads. We measure the overall execution time of the training workflow <software ContextAttributes="used">script</software>, repeating each test at least 10 times and we plot the boxplots of the repetitions and the numeric values used in-text refer to the median of the repetitions. Experiment 1: varying provenance capture settings. For a baseline, we first execute the training without any provenance capture, then we vary the queue size in <software ContextAttributes="used">PLLib</software> (i.e., amount of provenance capture requests accumulated in <software ContextAttributes="used">PLLib</software>), diskless vs. diskful (i.e., saving or not provenance data in a log file on disk), and online vs. offline (i.e., storing or not provenance data in the DBMS, available for online provenance queries during the execution). As for the training datasets, we use a curated and labeled real seismic dataset using a specific range of seismic slices (corresponding to a regional section of a seismic cube) defined by the model trainer. The results are in Fig. <ref type="figure">5</ref>, where the fastest result is for Queue Size = 50, To analyze the queue size, we compare Settings A-C with D-F and we see larger queues provide faster provenance capture since there is less but larger communication with <software ContextAttributes="used">ProvTracker</software> service. For instance, Setting A is about 7% slower than D. However, very large queues have drawbacks as they introduce higher latency between the event being captured in the workflow execution and the provenance record being stored in the database, caused by the retention of provenance capture events in <software ContextAttributes="used">PLLib</software>'s queue. Nevertheless, for the settings with queue size 50 (D-F), a latency of less than 5 seconds between the actual occurrence of the event and its provenance being registered in the database, available for queries, can be considered near real-time and good enough even for training monitoring. To analyze diskless vs. diskful settings, we compare Setting A with B and C; and D with E and F. Diskless is faster than diskful, as the latter introduces more I/O operations at runtime. However, comparing only the medians, the difference is negligible (less than 0.1%). Thus, because of a higher fault-tolerance provided by a diskful setting, it may be interesting to append provenance data onto a file on disk, locally in the cluster where the workflow runs. Similarly, comparing the medians, we observe that the difference between online vs. offline (e.g., setting B vs. C or E vs. F) is also small, about 1%. Therefore, despite (D) being the fastest setting, (E) may be preferred because its performance is nearly the same as (D) and it has the advantage of backup storage for provenance data, which is quite important as provenance is used for quality assessment and reproducibility. Experiment 2: scalability analysis. In this experiment, we want to confirm if the execution strategies on an HPC cluster are keeping the overhead low in a real ML workload, running multiple training workflows in parallel. We run a weak scalability test by increasing the number of processing units while increasing the data size. We use the fastest setting of the previous experiment (i.e., D) and the same seismic cube. To set up the training datasets, the trainer selects up to 8 different sets of seismic slices, where each set has the same length (i.e., nearly the same data size). Thus, for x ∈ {1, 2, 4, 8}, there are x workflows running on x nodes in parallel, summing 28x Intel CPU cores, 6x GPUs, 4992 * 6x CUDA GPU cores, using in total an input dataset with size x * datasize, where datasize is the size of a dataset formed by 1 set of seismic slices. The results are in Fig. <ref type="figure" target="#fig_7">6</ref>, where we illustrate the linear scalability as a horizontal line passing through the median of the smallest setting (x = 1). Ideally, the medians should be near this line. If they are not, it means that <software ContextAttributes="used">ProvTracker</software> is taking too long to answer, caused by high stress in the system due to too many provenance capture requests, adding latency to the training. However, we see that even in the largest setting (i.e., x = 8), the execution time remains close to the linear curve. The boxes remain within a small margin of 0.2 min (or 0.9% of the x = 1 median) between 21.4 and 21.6 min, meaning that the system delivers a constant and predictable behavior even at larger scales. We note though that the variance grows with the scale, caused by the larger number of parallel tasks. Therefore, we conclude that at least for this scale (up to 48 K80 GPUs), the provenance capture system delivers good scalability.</p></div>
<div><head>B. Use Case Validation</head><p>We explain how <software ContextAttributes="used">ProvLake</software> supports queries Q1-Q6 in the O&amp;G use case, illustrated in Fig. <ref type="figure" target="#fig_9">7</ref> and described in Section II-C. As shown in the figure, the phases of the ML lifecycle in CSE are interconnected, as data generated in a phase are consumed in another. Essentially, <software ContextAttributes="used">ProvLake</software> tracks and maintains such interconnections in a provenance data graph as millions of RDF triples (about 30M in total after all models have been trained in this use case) as the chained data transformations in the multiple, distributed workflows composing the inner phases of the major phases of the lifecycle run. The data in the figure are represented as RDF resources, i.e., instances that extend prov:Entity and PROV-ML specializations. Each of these instances receives a URI, which works as a global identifier throughout the lifecycle. Each trained model generated in the learning phase is represented as an RDF resource, as well as the model hyperparameters of each trained model, the evaluation metrics, and a reference (file path) to the actual model file stored in the file system. Execution data, such as file system metadata, cluster's hostname and node names used in the HPC jobs, job ids in the cluster scheduler, and start and end timestamps of each block of provenance capture events are associated to the trained models in the provenance data graph.  Similarly, in the learning data preparation phase, there are several data transformations in a data pipeline that transform the curated and annotated scientific data into training, validation, and test datasets. Each data transformation is parameterized. Parameters specify, for instance, noise filter thresholds, size (in pixels) of tiles which will serve a seismic image classifier, ranges of regions (called seismic slices, e.g., inline and crossline slices) of the seismic cube that will form the training dataset, etc. Each value of these parameters, the name of the transformation, execution data, and data references to input and output data physically stored in the data stores are captured and represented in <software ContextAttributes="used">ProvLake</software>'s provenance data graph. For the data curation phase, <software ContextAttributes="used">ProvLake</software> captures provenance when the data-intensive <software ContextAttributes="used">scripts</software> that clean, filter, and create auxiliary data run. When processing raw scientific files, important data which will help to answer the queries are extracted, such as geographic coordinates embedded as metadata in raw SEG-Y seismic files (represented as a netherlands.sgy in the figure), associated to the file's URI, and stored in the provenance database. Yet, geoscientists input important annotations into some of those scripts including associated oil fields, basins, oil wells, and pieces of texts from PDF documents with survey information related to the geological data acquisition process. These annotations are stored in a domain-specific database, externally to the provenance database, stored in a Triple Store. In this case, <software ContextAttributes="used">ProvLake</software>'s ability to keep track of data distributed in multiple stores helps to maintain the data relationships between the raw files in the file system and the structured knowledge stored in another database. Auxiliary data, such as polygons of the seismic cube are stored in the Document DBMS, and the data references are similarly tracked and related to the raw files. Other data, such as implementation details, software name and version, are captured and stored in the provenance database, following the PROV-ML, but, for simplicity, we do not show them in the figure . As the data and their relationships are properly tracked while the workflows execute, <software ContextAttributes="used">ProvLake</software> enables answering online, offline, intra-and inter-training provenance queries to analyze ML data, domainspecific data, and execution data throughout the phases of the lifecycle, exemplified by the queries Q1-Q6.</p></div>
<div><head>Data Curation</head><note type="other">Triple Store File System</note></div>
<div><head>Learning Data Preparation</head></div>
<div><head>Training, Validation, Test Data Creation Pipelines</head><note type="other">File System Learning Training, Validation, Test Training Hyperparameters</note><p>To submit queries, the user sends a GET or POST request to one of <software>PolyProvQueryEngine</software>'s endpoints. Then, <software ContextAttributes="used">PolyProv-QueryEngine</software> sends requests to <software ContextAttributes="used">ProvManager</software>. Most of the queries are answered with simple graph traversals using standard SPARQL features. For instance, to answer Q1, the user provides a trained model URI (generated in the learning phase) and the query should traverse in the provenance data graph backward until the raw seismic file's URI (processed in the data curation phase). To return the geographic coordinates and number of seismic slices, the query uses the extracted data related to the seismic file. To return the oil basin and oil field information, the query retrieves data from the resource, in the Triple Store, that represents structured knowledge about the seismic file. For Q2 and Q6, similar graph traversal is executed. Other queries require analytical operators, such as Q3, which requires finding the trained model with least (using min() native SPARQL operator) loss, and returning its hyperparameters. Q4 and Q5 make use of execution data to provide basic statistics (min(), max(), avg() operators) about the execution time of training iterations.</p></div>
<div><head>VI. RELATED WORK</head><p>Some works have addressed provenance tracking in the ML context <ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b23">24]</ref>. However, they are mainly focused on the learning and learning data preparation phases, failing to trace back from the trained models until the raw domainspecific data curated in workflows in the curation phase. These solutions often come with one single system to manage execution, data, and provenance of the whole lifecycle, but in order to do so, users need to develop their workflows in such a system. Although it is a good fit for simple projects (e.g., the same user designs ML models, curates, prepares the data and trains the ML models), it is not for CSE, which is considerably more complex and heterogeneous. It is unrealistic to expect that all phases, their execution, and the processed data will be managed by one single system. Alternatively, provenance tracking systems <ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref> can be coupled to a CSE workflow, providing provenance support while not significantly changing the way CSE users develop their applications. However, these solutions fail to track the interconnections between workflows and fail to track data processed in multiple heterogeneous stores. Also, some of them <ref type="bibr" target="#b11">[12]</ref> add high provenance capture overhead, preventing their adoption in CSE. Finally, none of them has a provenance data representation capable of representing ML-specific and domain-specific data, as we propose with PROV-ML, with extensions of W3C PROV <ref type="bibr" target="#b17">[18]</ref> and MLS <ref type="bibr" target="#b21">[22]</ref>.</p><p>On new provenance data representations for ML, some works addressed the gap between the experiments of a ML workflow execution and a standard representation to provide reproducible experiments <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b24">25]</ref>. Esteves et al. <ref type="bibr" target="#b24">[25]</ref> introduce W3C PROV-compliant provenance in these workflows in ML. They provide a machine-readable vocabulary and a common schema for reproducibility in various frameworks and workflow systems. However, it lacks details of the ML phases itself. Publio et al. <ref type="bibr" target="#b18">[19]</ref> present a new ML data representation based on MEX vocabulary <ref type="bibr" target="#b24">[25]</ref> to improve processes on ML workflows. Nonetheless, they lack a explicit separation between prospective and retrospective provenance, limiting provenance data understanding. Moreover, these works are focused on the learning phases of the lifecycle, whereas the interconnections with workflows in prior phases, like for data curation, are not provided. Finally, none of these solutions has a provenance tracking system as we are proposing.</p></div>
<div><head>VII. CONCLUSIONS</head><p>In this work, we addressed the problem of tracking the data transformations in the ML lifecycle, focusing on CSE. We showed that heterogeneity in several dimensions, including different human expertise, workflows, data stores, execution machines, among others, adds a significant complexity that must be addressed to support provenance tracking in the ML lifecycle; end-to-end from raw scientific data files to trained models. To the best of our knowledge, this is the first work that characterizes provenance as an essential aspect to be managed for the track of data in the ML lifecycle in CSE.</p><p>Although existing provenance tracking solutions that can be coupled with workflows contribute with the flexibility needed in CSE projects, they fail to support the heterogeneous nature of the lifecycle. After the practical experience of extending <software>ProvLake</software> for the lifecycle, we draw the following lessons:</p><p>(i) The characterization of provenance in the lifecycle allows for an understanding of the different needs of different persona as it drives the provenance tracking to answer key online and offline, intra-and inter-training provenance queries capable of analyzing, in an integrated way, ML data, domainspecific data, and execution data, throughout the data curation, data preparation and learning phases of the lifecycle. We observed that the data curation step, which is often neglected by ML systems, is the most complex part in CSE and needs to be addressed carefully for provenance analysis.</p><p>(ii) In CSE, it is necessary to integrate provenance from multiple workflows that process domain-specific data in the data curation phase and ML data in the learning phases of the ML lifecycle; otherwise, important data are not tracked properly. In practice, this is often done manually, which is time consuming and error prone. To achieve this integration, it was key to create a representation that leverages ML and domain-specific data. Therefore, we created PROV-ML, which is compliant to W3C definitions, namely PROV and ML Schema. We hope such representation can be adopted by other systems in this area.</p><p>(iii) Architectural design decisions, such as a microservice architecture and a lightweight provenance capture library (with less than 1% of overhead), are essential for efficient tracking enabling comprehensive provenance analysis. We observed this finding through an O&amp;G use case running on a testbed of 48 GPUs.</p></div><figure xml:id="fig_0"><head>Fig. 2 :</head><label>2</label><figDesc>Fig. 2: The ML lifecycle in CSE.</figDesc></figure>
<figure xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>Fig. 3: PROV-ML: a W3C PROV-and W3C ML Schema-compliant workflow provenance data representation.</figDesc></figure>
<figure xml:id="fig_3"><head>üMLFig. 4 :</head><label>4</label><figDesc>Fig. 4: ProvLake architecture on an exemplary deployment on two clusters: for data preprocessing and for learning.</figDesc></figure>
<figure xml:id="fig_4"><head>Algorithm 1 :</head><label>1</label><figDesc>Provenance capture in a training script. Input: training hyperparameters, input data sets 1 import PLLib as prov 2 prov.init(prospective provenance) 3 ... 4 prov.in( training , training hyperprms, input data ref erences) 5 for e = 1 .. max epochs do 6 prov.in( epoch , e) 7 ...</figDesc></figure>
<figure xml:id="fig_5"><head>8 for</head><label>8</label><figDesc>batch id in data batches do 9 prov.in( batch , batch id) 10 ...</figDesc></figure>
<figure xml:id="fig_6"><head /><label /><figDesc>11 prov.out( batch , loss value) 12 prov.out( epoch , conf usion matrix, model hyperprms, model perf , model ref )</figDesc></figure>
<figure xml:id="fig_7"><head>Fig. 6 :</head><label>6</label><figDesc>Fig. 6: Weak scalability analysis.</figDesc></figure>
<figure xml:id="fig_8"><head>Filtering</head><label /><figDesc>URI: &lt;http://ibm.com/netherlands.sgy&gt; -associated oil field -associated oil basin -associated PDF files (e.g., survey info) -nearby oil wells -curated and annotated data references -seismic slice ranges -noise threshold: 0.30 -tile size: 40 Data Reference Tracking -in: curated and annotated data references -out: /input/{train,test,valid}.hdf5</figDesc></figure>
<figure xml:id="fig_9"><head>Fig. 7 :</head><label>7</label><figDesc>Fig. 7: Provenance data tracking in an O&amp;G use case for the ML lifecycle in CSE.</figDesc><graphic coords="10,407.19,144.70,90.88,53.13" type="bitmap" /></figure>
<figure type="table" xml:id="tab_3"><head>TABLE I :</head><label>I</label><figDesc>Examples of provenance queries in ML for CSE.</figDesc><table><row><cell /><cell>Given a trained model, what are the geographic coordinates, oil</cell></row><row><cell>Q1</cell><cell>basin and field, and the number of seismic slices of the seismic in</cell></row><row><cell /><cell>the training dataset?</cell></row><row><cell /><cell>Given a trained model, what is the tile size, the noise filter threshold,</cell></row><row><cell>Q2</cell><cell>and the ranges of seismic slices that were selected to generate the</cell></row><row><cell /><cell>training set used to adjust this model?</cell></row><row><cell /><cell>Given a training set, what are the values for all hyperparameters</cell></row><row><cell>Q3</cell><cell>and the evaluation measure values associated with the trained model</cell></row><row><cell /><cell>with least loss?</cell></row><row><cell /><cell>What are the average, min, and max execution times of each batch</cell></row><row><cell>Q4</cell><cell>iteration inside each epoch of the deep neural network training,</cell></row><row><cell /><cell>given a training dataset?</cell></row><row><cell /><cell>What is the execution time on average per batch iteration, per epoch,</cell></row><row><cell>Q5</cell><cell>and what are the evaluation metrics of the trained models that used</cell></row><row><cell /><cell>the training dataset generated for a given range of seismic slices?</cell></row><row><cell>Q6</cell><cell>Given the training dataset used in Q5, what was the seismic data file used, along with its number of slices, related oil basin, and field?</cell></row></table><note><p>Learning. In this phase, model trainers select the input training datasets, optionally they choose validation datasets, and choose training parameters (e.g., in deep learning they can choose ranges of epochs and learning rates) that will be optimized in the training process. Trainers can use their domain knowledge to discard input training datasets that will unlikely provide good results. The training process is computeintensive, typically executed as a job submitted in an HPC cluster. One single training process often generates multiple trained models, among which one is chosen as the "best" depending on evaluation metrics (e.g., MSE, accuracy, or any other user-defined metric). Moreover, as the training process takes a long time, trainers need to monitor it by, e.g., inspecting how the evaluation metrics are evolving while the training process iterates. They can wait until completion or interrupt the training process, change parameters, re-submit the training in an iterative way until satisfied with results.</p></note></figure>
<figure type="table" xml:id="tab_4"><head>TABLE II :</head><label>II</label><figDesc>PROV-ML data representation classes.</figDesc><table><row><cell>Class</cell><cell>Description</cell></row><row><cell>Study</cell><cell>Investigation (e.g., research hypothesis) leading ML workflow definitions.</cell></row></table></figure>
			<note place="foot" n="13" xml:id="foot_0"><p>prov.out( training )  </p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGMENT</head><p>The authors would like to thank <rs type="person">Marcelo Costalonga</rs> and <rs type="person">Daniela Szwarcman</rs> for their help. This work was partially funded by <rs type="funder">CNPq</rs>, <rs type="funder">FAPERJ</rs>, and <rs type="funder">Inria Associated Team SciDISC</rs>.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Hesthaven</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Karniadakis</surname></persName>
		</author>
		<ptr target="https://icerm.brown.edu/events/ht19-1-sml" />
		<title level="m">Scientific machine learning workshop</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Intelligent systems for geosciences: an essential research agenda</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Pierce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Babaie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CACM</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Physics-informed neural networks: a deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations</title>
		<author>
			<persName><forename type="first">M</forename><surname>Raissi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perdikaris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Karniadakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Comp. Physics</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">DeepDownscale: a deep learning strategy for high-resolution weather forecast</title>
		<author>
			<persName><forename type="first">E</forename><surname>Rodrigues</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Oliveira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cunha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE eScience</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Efficient classification of seismic textures</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Chevitarese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Szwarcman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">V</forename><surname>Brazil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCNN</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Data lifecycle challenges in production machine learning: a survey</title>
		<author>
			<persName><forename type="first">N</forename><surname>Polyzotis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Whang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD Rec</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The role of machine learning in scientific workflows</title>
		<author>
			<persName><forename type="first">E</forename><surname>Deelman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mandal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. HPC</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A survey on provenance: What for? what form? what from?</title>
		<author>
			<persName><forename type="first">M</forename><surname>Herschel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Diestelkmper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ben Lahmar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">VLDB J</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Towards unified data and lifecycle management for deep learning</title>
		<author>
			<persName><forename type="first">H</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDE</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Diagnosing machine learning pipelines with fine-grained lineage</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">R</forename><surname>Sparks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Franklin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HPDC</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Accelerating the machine learning lifecycle with MLflow</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zaharia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Davidson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Data Eng. Bulletin</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Big provenance stream processing for data intensive computations</title>
		<author>
			<persName><forename type="first">I</forename><surname>Suriarachchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Withana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Plale</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE eScience</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A PROV-compliant approach to script-to-workflow process</title>
		<author>
			<persName><forename type="first">L</forename><surname>Carvalho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Belhajjame</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Medeiros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Sem. Web J</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A survey on collecting, managing, and analyzing provenance from scripts</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pimentel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Freire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Murta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Surv</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">DfAnalyzer: runtime dataflow analysis of scientific applications using provenance</title>
		<author>
			<persName><forename type="first">V</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Oliveira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Valduriez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PVLDB</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Capturing provenance for runtime data analysis in computational science and engineering applications</title>
		<author>
			<persName><forename type="first">V</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Camata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IPAW</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Efficient runtime capture of multiworkflow data using provenance</title>
		<author>
			<persName><forename type="first">R</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Azevedo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Thiago</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE eScience</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Moreau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Missier</surname></persName>
		</author>
		<ptr target="https://www.w3.org/TR/prov-dm/" />
		<title level="m">PROV-DM: the PROV data model</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">ML Schema: exposing the semantics of machine learning with schemas and ontologies</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">C</forename><surname>Publio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Esteves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ławrynowicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Reproducibility in ML@ICML</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Data reduction in scientific workflows using provenance monitoring and user steering</title>
		<author>
			<persName><forename type="first">R</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Coutinho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">FGCS</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Keeping track of user steering actions in dynamic workflows</title>
		<author>
			<persName><forename type="first">R</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Camata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">FGCS</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">A</forename></persName>
		</author>
		<ptr target="https://www.w3.org/community/ml-schema/" />
		<title level="m">Machine learning schema community group</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<ptr target="https://ibm.biz/provlake" />
		<title level="m">Provlake website</title>
		<imprint />
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Model selection management systems: the next frontier of advanced analytics</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mccann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Naughton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGMOD Rec</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">MEX vocabulary: a lightweight interchange format for machine learning experiments</title>
		<author>
			<persName><forename type="first">D</forename><surname>Esteves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Moussallem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">B</forename><surname>Neto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SEMANTICS</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>