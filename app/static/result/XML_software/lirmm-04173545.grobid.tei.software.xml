<TEI xmlns="http://www.tei-c.org/ns/1.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xml:space="preserve" xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd">
	<teiHeader xml:lang="fr">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Life Science Workflow Services (LifeSWS): motivations and architecture</title>
				<funder ref="#_CRXpq6K">
					<orgName type="full">MaCS4Plants CIRAD</orgName>
				</funder>
				<funder ref="#_6eFCzBk">
					<orgName type="full">EU</orgName>
				</funder>
				<funder ref="#_YWHCQH8 #_MGruS8h #_dF2JPVh">
					<orgName type="full">unknown</orgName>
				</funder>
				<funder>
					<orgName type="full">CNPq</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher />
				<availability status="unknown"><licence /></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Reza</forename><surname>Akbarinia</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">LIRMM</orgName>
								<orgName type="institution" key="instit1">Inria</orgName>
								<orgName type="institution" key="instit2">Univ Montpellier</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Christophe</forename><surname>Botella</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">LIRMM</orgName>
								<orgName type="institution" key="instit1">Inria</orgName>
								<orgName type="institution" key="instit2">Univ Montpellier</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Alexis</forename><surname>Joly</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">LIRMM</orgName>
								<orgName type="institution" key="instit1">Inria</orgName>
								<orgName type="institution" key="instit2">Univ Montpellier</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Florent</forename><surname>Masseglia</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">LIRMM</orgName>
								<orgName type="institution" key="instit1">Inria</orgName>
								<orgName type="institution" key="instit2">Univ Montpellier</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Marta</forename><surname>Mattoso</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Federal University of Rio de Janeiro</orgName>
								<address>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Eduardo</forename><surname>Ogasawara</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">CEFET/RJ</orgName>
								<address>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Daniel</forename><surname>De Oliveira</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Fluminense Federal University</orgName>
								<address>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Esther</forename><surname>Pacitti</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">LIRMM</orgName>
								<orgName type="institution" key="instit1">Inria</orgName>
								<orgName type="institution" key="instit2">Univ Montpellier</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Fabio</forename><surname>Porto</surname></persName>
							<affiliation key="aff4">
								<orgName type="laboratory">LNCC</orgName>
								<address>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Christophe</forename><surname>Pradal</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">LIRMM</orgName>
								<orgName type="institution" key="instit1">Inria</orgName>
								<orgName type="institution" key="instit2">Univ Montpellier</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff6">
								<orgName type="department">INRAE</orgName>
								<orgName type="laboratory">CIRAD</orgName>
								<orgName type="institution" key="instit1">AGAP Institute</orgName>
								<orgName type="institution" key="instit2">Univ Montpellier</orgName>
								<orgName type="institution" key="instit3">Institut Agro Montpellier</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Dennis</forename><surname>Shasha</surname></persName>
							<affiliation key="aff5">
								<orgName type="institution">New York University</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Patrick</forename><surname>Valduriez</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">LIRMM</orgName>
								<orgName type="institution" key="instit1">Inria</orgName>
								<orgName type="institution" key="instit2">Univ Montpellier</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Life Science Workflow Services (LifeSWS): motivations and architecture</title>
					</analytic>
					<monogr>
						<imprint>
							<date />
						</imprint>
					</monogr>
					<idno type="MD5">A6D06C1081AD1C3625A3CC525E83958A</idno>
					<idno type="DOI">10.1007/978-3-662-68100-8_1</idno>
					<note type="submission">Submitted on 29 Jul 2023</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-04-12T14:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid" />
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Data driven science</term>
					<term>Life science</term>
					<term>Data science</term>
					<term>Workflows</term>
					<term>Model life cycle</term>
					<term>Service-based architecture</term>
				</keywords>
			</textClass>
			<abstract>
<div><p>à la diffusion de documents scientifiques de niveau recherche, publiés ou non, émanant des établissements d'enseignement et de recherche français ou étrangers, des laboratoires publics ou privés.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="fr">
		<body>
<div><head n="1">Introduction</head><p>Data driven science such as agronomy, astronomy, environmental, and life science must deal with overwhelming amounts of complex data, e.g., coming from sensors and scientific instruments, or produced by simulation. Increasingly, scientific breakthroughs will be enabled by advanced techniques from data science <ref type="bibr" target="#b22">[23]</ref> that help researchers manipulate and explore these massive datasets <ref type="bibr" target="#b13">[14]</ref>.</p><p>Life science is the study of living organisms (plants, humans, micro-organisms, . . . ) and their association with internal or external conditions. It is an interdisciplinary domain including agronomy, biology, and botany. The data in life science comes from many different data sources produced by modern platforms, e.g., high-throughput phenotyping, next-generation sequencing, remote sensing, etc., or readily available as international databases, such as Data.World, GenomeHub, <software>AgMIP</software>, EMPHASIS, etc. Such data is used to help producing/training models (statistical models, machine learning (ML) models, etc.) to derive information and knowledge or to make predictions using complex workflows. Since models are tailored to specific research questions, they are typically produced by different research groups and take various forms that reflect the researchers' approaches with their data.</p><p>Data processing with models typically involves complex data analysis workflows (workflows, for short hereafter). Unlike business workflow systems, e.g., new order processing, these workflows are compute-and data-intensive, may take hours or even days, but are often deterministic, and do not involve fine-grained transactions. They allow domain scientists (specialized in a science domain, e.g., plant biology), to express multi-step computational activities, such as loading input data files, processing the data, running analyses, and aggregating the results. Workflows have been implemented on top of scientific workflow systems such as <software ContextAttributes="created">Galaxy</software> <ref type="bibr" target="#b0">[1]</ref> and <software ContextAttributes="created">OpenAlea</software> <ref type="bibr" target="#b29">[30]</ref>. They frequently make use of data analytics engines such as <software ContextAttributes="created">Spark</software> <ref type="bibr" target="#b35">[36]</ref> and <software ContextAttributes="created">Flink</software> <ref type="bibr" target="#b5">[6]</ref>, as well as Machine Learning (ML) libraries such as <software ContextAttributes="created">PyTorch</software> <ref type="bibr" target="#b24">[25]</ref> and <software ContextAttributes="created">Scikit-learn</software> <ref type="bibr" target="#b25">[26]</ref>. In order to scale to massive datasets, they make increasing use of distributed and parallel execution environments in the cloud.</p><p>While this paper (and project) focuses on data-driven life science, we believe the project can provide a framework for other application domains with similar requirements.</p></div>
<div><head n="1.1">Use Cases</head><p>Let us illustrate the requirements for managing such data and models with real application examples from life science. In the context of climate change, agroecosystems face multiple challenges, including adaptation, resilience, epidemics, land-use conflicts, and the need for biodiversity conservation. Examples of practical questions that end-users might ask are:</p><p>-How to select or breed new plant varieties that are adapted to my local environmental conditions (e.g., drought, flooding, high temperature, disease)? -Which treatments should be deployed on my farm depending on climatic conditions and geographical proximity to disease hot spots?</p><p>Addressing these questions requires multiscale modeling, e.g., modeling plants at different scales (e.g., organ, plant, crop, land surface, region) to predict the impact from heterogeneous data, e.g., data on plants, environment (weather, soil), and remote sensing. These models are the outcome of workflows, whose activities typically involve data extraction, data cleaning, machine learning, and visualization. Often the output of one workflow is the input to another.</p><p>With One Health, an approach that recognizes that the health of people is closely connected to the health of animals and our shared environment, understanding epidemic propagation at various levels (local, regional, national, global) has become critical for health authorities. The major problem is how to select the best prediction model for a given region by combining propagation models from different regions as well as integrating various data sources (epidemic, climate, socio-economic, etc.) along some common dimensions, e.g., time, location, etc.</p><p>The practical difficulty to achieve such integration is that it is hard to relate models and data, which are typically produced by different people with different methods, formats and tools. International repositories for scientific data and models are useful but they tend to be specialized for specific purposes and research communities, e.g., genomics, phenotyping, and epidemiology. Similarly, the workflow systems to manipulate data and models are specialized for a research domain, e.g., <software>OpenAlea</software> for plant phenotyping, <software ContextAttributes="created">Galaxy</software> for genomics. Thus, there is a pressing need for integrated data and model management in order to achieve consistency and ease of use through generic workflow services with the ultimate goal of improving model accuracy and predictions.</p></div>
<div><head n="1.2">The Centrality of Workflows</head><p>In this paper, we propose an open service-based architecture, called Life Science Workflow Services (<software>LifeSWS</software>). The main objective of LifeSWS is to help managing complex workflows by organizing massive and heterogeneous data, in connection with models and making workflow artifacts (datasets, models, metadata, workflow components, etc.) easy to search, debug, and parallelize.</p><p>In many ways, workflows are to scientific data processing what queries are to business data processing. In business data processing, queries must be written (with some reuse of other queries), debugged and optimized (sometimes through parallelization), and should work across distributed servers, hardware, and operating systems. Scientific data processing is much more complex so workflows replace queries. The issues however are much the same. Workflows in scientific data processing must be written (with some reuse of other workflow components), debugged (often benefiting from provenance), optimized (often through parallelization and caching), and should work across distributed servers and operating systems. In addition, workflows should be fault tolerant and workflow component versions should be kept up-to-date. Thus, a technical goal of this project is to make workflows work as seamlessly with data as queries do in business processing.</p><p><software ContextAttributes="created">LifeSWS</software> capitalizes on our previous experience in developing major systems for scientific applications such as: polystores with <software ContextAttributes="created">CloudMdSQL</software> [17], workflows with <software ContextAttributes="created">OpenAlea</software> <ref type="bibr" target="#b29">[30]</ref>, model management with <software ContextAttributes="created">Gypscie</software> <ref type="bibr" target="#b34">[35]</ref>  <ref type="bibr" target="#b37">[38]</ref>, querying data across distributed services with <software ContextAttributes="created">DfAnalyzer</software> <ref type="bibr" target="#b31">[32]</ref> and <software ContextAttributes="created">Provlake</software> <ref type="bibr" target="#b32">[33]</ref>, monitoring and debugging applications implemented in big data frameworks such as <software ContextAttributes="created">Apache Spark</software> <ref type="bibr" target="#b11">[12]</ref>, and debugging workflows with <software ContextAttributes="created">BugDoc</software> <ref type="bibr" target="#b17">[18]</ref> and <software ContextAttributes="created">VersionClimber</software> <ref type="bibr" target="#b28">[29]</ref>.</p></div>
<div><head n="1.3">Paper Outline</head><p>The paper is organized as follows. </p></div>
<div><head n="2">Motivating Examples</head><p>In this section, we introduce examples from real-life science applications that will serve as motivation for our work and as the basis for use cases with <software>LifeSWS</software>. These examples are in agro-ecosystems in the context of climate change and epidemic modeling. These examples share common requirements but have specific features that will show different uses of <software ContextAttributes="created">LifeSWS</software>.</p></div>
<div><head n="2.1">High-Throughput Phenotyping in the Context of Climate Change</head><p>As observed above, agro-ecosystems face multiple challenges, including climate change, epidemics, land-use conflicts, and the need to conserve biodiversity. To enhance the resilience of agro-ecosystems, interdisciplinary efforts are required, ranging from a detailed biological understanding of the physiology of plants with multiple stresses (e.g., drought, temperature, decease), agronomy to adapt agroecosystems to future challenges, as well as sociology, economy and politics to understand the impacts of changing public policy. This challenge requires mobilizing all possible levers of plant adaptation, including the genotype, phenotype, and their interactions with the environment. The genetic/genomic revolution has allowed us to sequence and manipulate genes at a low cost and to generate an avalanche of information. But understanding genome-to-phenotype relationships is crucial. While national and international phenotyping platforms allow the capture of phenotypes at high-throughput, most of the traits that contribute to the performance of agro-ecosystems are environment-dependent. The performance of a variety that thrives in a particular environment may perform poorly in a different one. Thus, it is important to capture phenotypes in various environments using various sensors at different scales (IoT, images from drones, 3D point clouds from Lidars and remote sensing images from satellites).</p><p>Major efforts have been invested in crop breeding to improve crop yield for food security. However, profiling the crop phenome by considering the structure and function of plants associated with genetics and environments remains a technical challenge <ref type="bibr" target="#b33">[34]</ref>.</p><p>In the past decade, high-throughput phenotyping platforms have emerged, enabling the collection of quantitative data on thousands of plants under controlled environmental conditions. A good example is the French Phenome project, with seven facilities producing 200 Terabytes of diverse, multiscale data annually, including images, environmental conditions, and sensor outputs from different sites <ref type="bibr" target="#b12">[13]</ref>.</p><p>To support high throughput phenotyping, many workflows have been developed using <software ContextAttributes="created">OpenAlea</software> to analyze, reconstruct, and visualize the spatial and temporal development of the geometry and topology of thousands of plants in various environmental conditions. For instance, the Phenomenal workflow supports the reconstruction in 3D and the segmentation of plant organs <ref type="bibr" target="#b1">[2]</ref>. The PhenoTrack workflow, which is based on Phenomenal, allows the 3D reconstruction of plants with the temporal tracking of the growth of each organ for the entire developmental cycle <ref type="bibr" target="#b8">[9]</ref>. Finally, <software ContextAttributes="created">RootSystemTracker</software> provides a workflow for the automatic structural and developmental 2D root phenotyping of Arabidopsis plants in Petri dishes <ref type="bibr" target="#b9">[10]</ref>.</p><p>Figure <ref type="figure" target="#fig_0">1</ref> shows a) the Phenomenal workflow implemented in <software ContextAttributes="created">OpenAlea</software>; b) 3D organ tracking of a maize plant with PhenoTrack3D <ref type="bibr" target="#b8">[9]</ref>; and c) a reconstructed root system architecture through time using <software ContextAttributes="created">RootSystemTracker</software> <ref type="bibr" target="#b9">[10]</ref>. These workflows need to process large volumes of data on distributed infrastructures. To execute these workflows, we need to: 1) transfer large image datasets from a data center close to the phenotyping platforms to computing servers in the cloud; 2) distribute the execution on a cloud or grid infrastructure; 3) capture the provenance of the execution and cache intermediate results for later use; 4) rerun workflows with new processes and parameters; 5) provide execution results using dashboards to check the execution.</p><p>Furthermore, to understand the genotype-to-phenotype relationships, we need to be able to relate plant traits computed by phenotyping workflows (e.g., with <software>OpenAlea</software>) with genetic information using genotyping workflows such as genomewide association studies (e.g., with <software ContextAttributes="created">Galaxy</software>). Thus, we need to integrate heterogeneous workflows and be able to schedule their execution.</p><p>Finally, through time, phenotyping workflows evolve with new processes implemented in various libraries whose versions and dependencies change quickly. Parameters need to be calibrated on new phenotyping platforms which have new sensors, different light conditions, or new plant species. Furthermore, it is important to identify problems in the workflow specification (e.g., that may lead to deadlocks) before executing them in HPC environments to avoid sparing resources. Finally, the workflows need to be debugged to identify problems occurring in new settings.</p></div>
<div><head n="2.2">Epidemic Modeling</head><p>Each year, dengue, zika, chikungunya, and other arboviruses disseminated by the Aedes Aegypti vector exert an extreme burden on populations' health, especially in low-income countries.</p><p>General statistical models that try to explain or predict dengue in large areas usually do not consider the diversity of the territory and the different or even contradictory relations that predictors can preserve with the outcome. Conversely, creating individual models for every possible geographic location is impractical and unfeasible. In addition, there are regions for which we don't have enough data to create prediction models.</p><p>Therefore, a more effective approach is to develop Machine Learning models tailored to the unique characteristics of each region, considering the specific meteorological, socio-economic, and sanitary conditions that affect the epidemic transmission in that area. Then, these models can be used for predicting the transmission in similar regions for which constructing specific models is not possible (due to lack of data). For efficient modeling of dengue and other arboviruses, we need tools that can facilitate the selection of ML models that are most suitable for predicting these viruses. By utilizing these tools, more accurate predictive models can be selected and used to better understand and prepare for the transmission of viruses in the given region.</p><p>However, providing these tools is challenging. The reason is that we need to gather different datasets and models, and develop novel algorithms to enhance the accuracy and reliability of the prediction models. The required datasets and models are as follows: 1) propagation datasets that contain information about the spread of disease; 2) climate datasets that provide crucial insights into environmental factors like temperature and precipitation that may impact disease transmission; 3) socio-economic datasets that help us to understand the social and economic factors that could influence disease spread; 4) prediction models generated for some regions. By utilizing these diverse datasets and models, we should be able to select more accurate and reliable models for query regions, which can ultimately contribute to better disease control and prevention strategies.</p><p>Figure <ref type="figure" target="#fig_1">2</ref> shows the Dengue cases in Brazil spanning from 2000 to 2019 (a), along with the geographical distribution of cases across various regions (b and c) <ref type="bibr" target="#b4">[5]</ref>. For predicting the Epidemy in each region, we need to select a model (or set of models ) that takes into account the specific meteorological and socio-economic characteristics of the region. In this section, we introduce the service-based architecture of LifeSWS, with its functional architecture and three layers of services (presentation and directory, workflow and data management services).</p></div>
<div><head n="3.1">Functional Architecture</head><p>Our design choices are guided by the requirements of our users. The main potential users of <software>LifeSWS</software> are: the domain scientists who wish to analyze the data using different models and workflows; the workflow providers who create, maintain or enhance workflows for domain scientists using their workflow tools; the model providers who build models; and the data providers who supply data sources to the workflows.</p><p>Our architecture capitalizes on the latest advances in web-oriented architectures, microservices, containers and distributed and parallel data management <ref type="bibr" target="#b23">[24]</ref>. We adopt the main following design choices and principles:</p><p>-Ease of use through web interfaces, which are easy to develop and specialize for different kinds of users; -Open architecture with open source services and tools, and well-defined APIs to foster services interoperability (like cloud web services);</p><p>-Distributed architecture to provide performance, scalability and ease of use in the cloud using distributed database principles; -Support for various databases (SQL, <software>NoSQL</software>, SciDB, etc.) and scientific file types (e.g., HDF and NetCDF); -Integrated services on top of various databases, which can be local (in the same data center) or remote (in remote data centers) and access to various tools and execution environments.  </p></div>
<div><head n="3.2">Presentation and Directory Services</head><p>Presentation and directory services provide users and applications with secure ways of accessing <software>LifeSWS</software> services. Presentation services include a Web dashboard service, a Web <software ContextAttributes="created">API</software> and a directory service.</p><p>The Web dashboard service allows <software>LifeSWS</software> developers to build specific dashboards for different types of users (domain scientists, workflow providers, model providers and data providers). These dashboards allow users to analyze and display real-time data as charts and reports. They offer the following capabilities to developers: (1) a directory to publish and/or find data sources and workflow components; (2) tools for assembling workflows easily; (3) tools for debugging; and (4) scheduling workflows using workflow systems.</p><p>The directory stores data about <software>LifeSWS</software> users, access rights, dashboards and services. As a user directory, it helps register users, find out about them as well as authenticate them when accessing <software ContextAttributes="created">LifeSWS</software> services. As a service directory, it provides a single place to publish, discover, and connect <software ContextAttributes="created">LifeSWS</software> services as well as external services that can be distributed over the network. Additional network security, e.g., firewall, can be provided at this layer.</p><p>As an alternative to Web dashboards, the Web <software>API</software> is a server-side <software ContextAttributes="created">API</software> that allows <software ContextAttributes="created">LifeSWS</software> developers to access <software ContextAttributes="created">LifeSWS</software> services from more general Web applications. This <software ContextAttributes="created">API</software> consists of one or more publicly exposed endpoints that specify where and how to access the services with a request-response protocol, typically in JSON.</p><p>Finally, <software ContextAttributes="created">LifeSWS</software> offers an external data view to ease the development of dashboards and workflows, which integrates observational and predictive data. This external view can be represented by a knowledge graph <ref type="bibr" target="#b14">[15]</ref> extended to support the representation of observation time-series and predictive information metadata, such as: error estimate, multi-class prediction probabilities and etc..</p></div>
<div><head n="3.3">Workflow Services</head><p>Workflow services make it easy for scientists to develop, debug and optimize their workflows for doing their scientific experiments and data analyses. The services should also support the of sharing data, models and workflow components. Because the users want to be able to use their familiar tools (e.g., workflow systems such as <software>Galaxy</software> or <software ContextAttributes="created">OpenAlea</software>) and data sources (e.g., Data World), this layer provides efficient services to register and manage data and models, and allow model execution using different tools and data sources. The primary services provided at this layer are: catalog (including version management), provenance and cache, data analytics, and data management.</p><p>Catalog. The catalog is the central place to find out about all artifacts and tools of interest for <software>LifeSWS</software> users: data sources, datasets, models, workflows and code libraries. Artifacts can be found outside <software ContextAttributes="created">LifeSWS</software> and thus accessed through some <software ContextAttributes="created">API</software>, or stored within <software ContextAttributes="created">LifeSWS</software> for efficient reuse. Each artifact has associated metadata that describes it and allows access to it, either locally if it is stored in <software ContextAttributes="created">LifeSWS</software>, or through its URI if it is an external resource (tool or data source). With the catalog, one may register artifacts, change them or provide a new version. The catalog also knows about tools (e.g., <software ContextAttributes="created">OpenAlea</software>, <software ContextAttributes="created">Spark</software>) and code libraries that implement models (e.g., Phenomenal workflow). Finally, the catalog comes with a search capability that allows users to navigate through the hierarchy of artifacts.</p><p>Model Management. Various types of models are used in life sciences. Datadriven machine learning models adopt a learning strategy that updates a set of weights that approximates a function to the behavior of the learned phenomenon given by the patterns extracted from the input data. Typical tasks executed by machine learning models include solving classification and numerical regression problems, which one may generalize as prediction tasks. Another relevant type of model extensively used in life science are mechanistic models, which refer to computational artifacts derived from the mathematical modeling of a phenomenon. The product of running mechanistic models for a certain number of time steps is referred to as a phenomenon simulation. For instance, crop simulation models reproduce the main functions of plants such as the evolution of plant architecture, light interception, photosynthesis, and water/nitrogen balance in the crop and soil <ref type="bibr" target="#b20">[21]</ref>.</p><p>The management of such life science model artifacts requires model life cycle management and model deployment, using specific tools that can be accessed through <software>LifeSWS</software>. Through a unified view of different model artifacts (produced with different tools), <software ContextAttributes="created">LifeSWS</software> can improve model selection and allow for model integration.</p><p>Model selection allows the user to easily search for model artifacts of interest so they can be used for reproduction or integration. Searching can be done based on different criteria such as scientific domain or subdomain, metadata, format, tools and keywords. This capability uses the catalog of artifacts.</p><p>The performance monitoring of models in operation by the model management service is important to assess prediction quality and point to model updates. In particular, if the input data distribution changes, models built on past historical data must be flagged so they can be updated. For machine learning models, a concept-drift component must detect variations in input data patterns and launch alerts for downstream model updates. The latter are processed according to application requirements. <software>LifeSWS</software> supports complete and automatic model retraining, using ML tools, or it can delegate the model update process to components that implement a more sophisticated update procedure, involving for instance a fast training of a simple surrogate model, while the main model is updated.</p><p>Model integration allows combining different models, possibly produced using different tools. It can take different forms, depending on the model types and the integration objective. In machine learning, model integration may take the form of an ensemble of models <ref type="bibr" target="#b36">[37]</ref>. An ensemble considers a set of models aiming at the prediction of the same target. The integration process is modeled as a pipeline that runs each individual participant model, possibly across different tools, over the same input and combines the individual results into an integrated one, often using a linear combination of the results. Using the DJEnsemble method <ref type="bibr" target="#b26">[27]</ref>, ensembles can be computed automatically by a <software ContextAttributes="created">LifeSWS</software> platform such as <software ContextAttributes="created">Gypscie</software> (see Section 4), so that the selection of participant models follows a cost-based selection approach. Model integration takes a different form in mechanistic models as they typically use scientific workflows for simulating the phenomenon. For example, to visualize a 3D model of plant growth in a local environment, the 3D plant structure and biophysical models such as light interception, carbon allocation and water and mineral uptake can be simulated by a workflow modeled in <software ContextAttributes="created">OpenAlea</software>.</p><p>Workflow Integration. This service provides support for integrating and efficiently executing workflows on different workflow systems using the <software>Workflow Access</software> APIs. It shares some similar goals and functions found in data integration. The main functions provided by this service are workflow definition and execution, provenance and cache.</p><p>For workflow definition and execution, we plan to rely on the Common Workflow Language (CWL) <ref type="bibr" target="#b7">[8]</ref>, an open standard aiming to enable scientists to share complex data analysis and machine learning workflows. CWL supports connecting command line tools to create workflows that are portable across a variety of CWL-compliant platforms, from a single developer's laptop up to a massively parallel cluster in the cloud. The CWL project produces free and open standards for describing command-line tool based workflows. These standards are implemented in many popular workflow systems such as <software ContextAttributes="created">Galaxy</software>, Pegasus, Streamflow, and <software ContextAttributes="created">CWL-Airflow</software>. To enable portability and reusability, CWL is explicit about inputs/outputs to form the workflow, data locations and execution models, which can be deployed using software container technologies, such as Docker and <software ContextAttributes="created">Singularity</software>.</p><p>Within the CWL project, we can contribute to the definition of integrated workflows that span multiple workflows and workflow systems. Once an integrated workflow has been defined and its mappings registered using CWL, it can be executed using a <software>LifeSWS</software> scheduler that orchestrates execution across different workflow systems, in connection with these systems' schedulers.</p><p>Provenance (also referred to as lineage) management helps to reproduce, trace, assess, understand, and explain how datasets have been produced. This is a useful underlying functionality for several strategic capabilities, including experimental reproducibility, user steering (i.e., runtime monitoring, interactive data analysis, runtime fine-tuning) and data analysis. These capabilities are essential building blocks towards the goal of storing and sharing results of executions that can be useful later (by the same or different users perhaps on very different platforms).</p><p>In addition to provenance management, this service includes cache management, using information about cache data, as well as the location of the cache data (e.g., files, Spark RDDs, . . . ). Caching datasets improves performance when they are produced at multiple times by different users or distributed at various sites. The decision whether to cache an intermediate result can be explicit (i.e., decided by the user) or made automatic based on workflow fragment analysis <ref type="bibr" target="#b12">[13]</ref>.</p><p>All information for this service is stored in a database that is relatively small. In particular, the cache itself is small (only references) and the cached data can be managed using the underlying execution environments accessed through the <software>Workflow Access</software> APIs. Using a database for this service provides the traditional advantages of data sharing, integrity and querying using an SQL-like language.</p><p>Analytics. This service allows scientists, through their specific dashboards, to perform analytics on the data produced by their workflows using the other workflow services. Using the Catalog and Model Management services, the user is able to select models and datasets of interest, execute the selected models using various workflow execution environments (using the <software>Workflow Access</software> APIs), and analyze the results. It also allows the user to cache and explain the results, and reproduce executions using the Provenance and Cache services.</p><p>This service also facilitates the analysis of different types of data such as time series and spatial data, by incorporating advanced analytical techniques like anomaly detection, similarity search and clustering.</p></div>
<div><head n="3.4">Data Management Services</head><p>These services make it easy for <software>LifeSWS</software> users to manage their artifacts (datasets, models, metadata, etc.) and session data (logs, intermediate datasets, etc.) with high-level capabilities using the Data Source, Model/Workflow and Data Store APIs. An important capability is moving data between different data sources, databases and execution environments using simple import-export functions. Another useful capability, using the Data Source APIs, is subscribing to some data sources that provide a publish <software ContextAttributes="created">API</software>, to get warned of the new versions.</p><p>More advanced capabilities, similar to distributed databases <ref type="bibr" target="#b23">[24]</ref> and polystores <ref type="bibr" target="#b2">[3]</ref>, could be provided at this level for integrating data from different data sources. In particular, the <software ContextAttributes="created">CloudMdsQL</software> polystore [17] is efficient for querying multiple heterogeneous data sources (e.g. files, relational and <software ContextAttributes="created">NoSQL</software>) in the cloud. A <software ContextAttributes="created">CloudMdsQL</software> query may contain nested subqueries, and each subquery addresses directly a particular data store and may contain embedded invocations to the data store native query interface. Thus, the major innovation is that a <software ContextAttributes="created">CloudMdsQL</software> query can exploit the full power of local data stores, by simply allowing some local data store native queries to be called as functions, and at the same time be optimized based on a simple cost model, <software ContextAttributes="created">CloudMd-sQL</software> can also access address distributed processing frameworks such as <software ContextAttributes="created">Apache Spark</software> by enabling the ad-hoc usage of user defined map/filter/reduce operators as subqueries.</p><p>Data Store APIs. These APIs allow storing and accessing data in different data stores (SQL, <software>NoSQL</software>, <software ContextAttributes="created">Savime</software>, files, . . . ), to support specific requirements. For instance, the catalog, provenance, and cache databases are typically in an SQL database such as <software ContextAttributes="created">PostgreSQL</software>. By contrast, external data could be stored in the original format, e.g., JSON in a <software ContextAttributes="created">NoSQL</software> document database, and extracted using data-specific APIs. Datasets produced using tools or cache data could be stored in files, e.g., Parquet, etc., or in a scientific database like <software ContextAttributes="created">Savime</software>. The data store APIs should be based on standard APIs, such as JDBC, file system APIs, . . . ). Data Source APIs. These APIs allow connecting to various web data sources, such as Data World, <software ContextAttributes="created">AgMIP</software>, etc., and performing various tasks (search for datasets, extract a dataset's metadata, import a dataset, get changes, etc.).</p><p>They can be used to build user-friendly dashboards for domain scientists with semantic-based search capabilities, as for instance in the ontology-driven Phenotyping Hybrid Information System (PHIS <ref type="bibr" target="#b21">[22]</ref>).</p><p>Workflow Access APIs. These APIs allow accessing and manipulating models and workflows as structured objects with their own semantics, execute them in their own execution environments, such as <software>Pytorch</software> (ML models), <software ContextAttributes="created">OpenAlea</software> (workflows) and <software ContextAttributes="created">Spark</software> (e.g., SparkSQL queries). These APIs make it possible to simply perform various tasks using models and workflows, such as import or export of models, executing them using a dataset, saving the result data (using the Data Store <software ContextAttributes="created">API</software>, . . . ).</p></div>
<div><head n="4">LifeSWS Platforms</head><p><software>LifeSWS</software> services can be implemented and deployed in various platforms (using different software and hardware infrastructures) to address the specific requirements of vertical applications. Examples of platforms would be some <software ContextAttributes="created">LifeSWS</software> services deployed in the cloud (public, private or hybrid) or on-premise clusters of servers, reusing existing software components that (partially) implement the services.</p><p>A good example of a <software ContextAttributes="created">LifeSWS</software> platform is <software ContextAttributes="created">Gypscie</software> <ref type="bibr" target="#b37">[38]</ref>, which provides services to develop, share, improve and publish scientific artifacts (datasets, models, etc.). <software ContextAttributes="created">Gypscie</software>'s services are available through two different interfaces. Figure <ref type="figure" target="#fig_4">4</ref> shows the interface that enables interactive access to services, including artifacts registration and service requests. The same functionality is available through a REST <software ContextAttributes="created">API</software> based on the HTTP protocol.</p><p>These services make it easy for model providers and scientists to:</p><p>-Collect, curate and integrate heterogeneous data; -Support the complete ML model life cycle, from model building to deployment, monitoring and policy enforcement; -Find ready-to-use models that best fit a particular prediction problem; -Compare and ensemble models; -Execute models with various tools: ML engines, workflow systems, . . . -Use specific hardware infrastructures and corresponding algorithms according to a desired task, e.g., use a distributed training algorithm for a particular GPU-based server for training a large deep neural network.</p><p>Let us illustrate how <software>LifeSWS</software> services would be supported by <software ContextAttributes="created">Gypscie</software> services for presentation &amp; directory, model management, and data management.</p></div>
<div><head n="4.1">Presentation &amp; Directory</head><p><software>Gypscie</software> offers a web interface that eases ML model management. It also offers a notebook interface for direct python <software ContextAttributes="created">scripts</software> integration with the <software ContextAttributes="created">Flask</software> framework. Furthermore, <software ContextAttributes="created">Gypscie</software> enables other tools to access its services through a </p></div>
<div><head n="4.2">Model Management</head><p>The core functionalities of <software>Gypscie</software> cover the services needed to support the full ML life cycle. Regarding model management, the <software ContextAttributes="created">Gypscie</software> data model fosters the reuse of all artifacts involved during the model's life cycle. As such, the user can publish the scripts involving the data preparation and model fitting for a particular learning algorithm (hereafter, denoted learner ). For ease of browsing, learners are aggregated into learner family. We use the learning artifact to build models by providing the necessary training dataset. In addition, <software ContextAttributes="created">Gypscie</software> allows models built in other external systems to be imported and registered into it. Thus, models can be automatically registered when built using a known learner and the <software ContextAttributes="created">Gypscie</software> training functionality, or they can be manually imported. In both cases, once registered they are ready to be called for inference. The functionalities involving model training and inferencing are both implemented using <software ContextAttributes="created">MLFlow</software>. The latter enables <software ContextAttributes="created">Gypscie</software> to communicate with the most frequently used ML engines, such as <software ContextAttributes="created">Pytorch</software> and <software ContextAttributes="created">Scikit-learn</software>. <software ContextAttributes="created">Gypscie</software> instruments the running scripts to register in <software ContextAttributes="created">MLFlow</software> the values of performance metrics, which the system collects and stores into its catalog, once the job has finished its execution.</p><p>A particular feature of <software ContextAttributes="created">Gypscie</software> is its ability to deal with spatio-temporal data, which are extremely common in scientific applications. <software ContextAttributes="created">Gypscie</software> implements the DJEnsemble <ref type="bibr" target="#b26">[27]</ref> inference approach. The idea is to automatically select trained spatio-temporal models with performance guarantees for the scientific predictions. The approach considers a set of registered models for the execution of a certain task, for instance, rainfall prediction. The algorithm uses a costbased strategy that strikes a balance between prediction precision and execution cost to select the best set of models that infer the rainfall prediction in a region of the space. Additionally, it specifies how to spatially allocate the selected models to cover the query region. <software ContextAttributes="created">Gypscie</software> runs the optimization process, executes the selected models, and composes the final result. This is a very complex task that is completely abstracted from the final user, showing the potential of <software ContextAttributes="created">LifeSWS</software> to create an easy ML environment.</p></div>
<div><head n="4.3">Data Management</head><p>Data management involves the following services: (1) accessing registered data;</p><p>(2) gathering provenance information, and (3) exploring the content of datasets. <software>Gypscie</software> registers metadata in its catalog for accessing data stored in an external data source, such as Databricks Delta Lake or Lustre file system. When a scheduled workflow requires a dataset, the dataset is automatically transferred from its stored location to a file system supporting the workflow execution environment.</p><p>As a workflow applies transformations on a dataset, <software>Gypscie</software> stores the provenance information regarding the operation. Thus, within <software ContextAttributes="created">Gypscie</software> a user can always review the lineage of transformation that led to the dataset's current version.</p><p>Finally, <software ContextAttributes="created">Gypscie</software> integrates the SAVIME in-memory multi-dimensional array database system <ref type="bibr" target="#b19">[20]</ref>. SAVIME supports the expression of SQL-like queries over raw datasets. The query language enables the registration of prebuilt ML models that can be invoked over the results of a query expression.</p><p><software>Gypscie</software> has been deployed on a server at LNCC, and interfaced with two execution environments (see Section 5 : the Santos Dumont HPC system at LNCC which could be used in large model training (e.g., using <software ContextAttributes="created">PyTorch</software>) and a <software ContextAttributes="created">Spark</software> shared-nothing cluster to perform large-scale data transformation.</p></div>
<div><head n="5">Use Cases with LifeSWS</head><p>In this section, we show how the <software>LifeSWS</software> services could be used to support the requirements of the two motivating examples, referring to the services of the previous section.</p></div>
<div><head n="5.1">High-Throughput Phenotyping</head><p>In this use case, <software ContextAttributes="created">LifeSWS</software> is used by domain scientists in order to analyze, process and visualise High Throughput Phenotyping (HTP) experiments, with the following workflows, models and datasets: -<software ContextAttributes="created">OpenAlea</software> workflows that implement a specific phenotyping processing such as 3D maize segmentation, organ tracking, or root system reconstruction; -A <software ContextAttributes="created">Galaxy</software> workflow for a Genome-Wide Association Study (GWAS); -Image analysis algorithms to segment the background, to reconstruct the plant in 3D using space carving, semantic segmentation and tracking of the organs; -Functional-structural plant models that are used either to compute nonobserved information like light interception by leaves <ref type="bibr" target="#b1">[2]</ref> or water fluxes inside the root system <ref type="bibr" target="#b3">[4]</ref>, or to generate synthetic data for training or validating methods of plant reconstruction; -Raw image datasets obtained from the phenotyping platform, which contain timeseries of several images per plant; -Outputs of plant traits (e.g., leaf angle, light use efficiency, or biomass accumulation) for each genotype are saved in the Phenotyping Hybrid Information System (PHIS) <ref type="bibr" target="#b21">[22]</ref>.</p><p>A domain scientist, often a plant biologist, searches for a specific <software>OpenAlea</software> phenotyping workflow based on its metadata. Then, she edits, visualizes, and executes the workflow on a small dataset. For instance she can reconstruct in 3D the growth and development of a maize plant during a growing season. Finally, she selects a full dataset from an existing phenotyping experiment and executes the workflow to obtain a set of plants traits specific to each genotype and to environmental conditions such as drought or temperature. Finally, to breed varieties tolerant to drought, some outputs of the workflow will be associated with genetic markers using a <software ContextAttributes="created">Galaxy</software> workflow that implements a Genome-Wide Association Study (GWAS). GWAS allows to correlate favorable traits (e.g., responsible for drought tolerance) with a genomic region and thus to breed new varieties.</p><p>For instance, the Phenomenal workflow (Figure <ref type="figure" target="#fig_0">1</ref>.a) is composed of different fragments, i.e. reusable sub-workflows: binarization, images calibration, 3D volume reconstruction, and organ segmentation. The intermediate datasets are also shown in the Figure . The raw data is produced by the Phenoarch platform, which has a capacity of managing 2500 plants within a controlled environment (e.g., temperature, humidity, irrigation) and automatic imaging through time. The total size of the raw image dataset for one experiment is 15 Terabytes. The raw data is stored on a server close to the experimental platform, but also referenced to the PHIS with all metadata.</p><p>To execute the Phenomenal workflow, the <software>OpenAlea</software> scheduler checks, using provenance data, whether some fragments have already been executed and are present in the cache. <software ContextAttributes="created">OpenAlea</software> then schedules the execution of the workflow on a distributed infrastructure. After execution, the user can visualize the results either as classical plots or as 3D plots to inspect the reconstructed plants. The results are automatically stored in the PHIS, to associate each plant of each genotype with environmental data and the computed traits. These results are used as input of the <software ContextAttributes="created">Galaxy</software> workflow to make a complete GWAS study.</p><p>Let us now explain how this use case can be realized using the services provided by <software>LifeSWS</software>. Required workflows of the use case can be searched and found using the Catalog service. This allows to navigate among <software ContextAttributes="created">OpenAlea</software> and <software ContextAttributes="created">Galaxy</software> workflows and to select the Phenomenal and GWAS workflows. Both workflows are composed of versioned tools, models, and workflow fragments that are retrieved from the Model Management service. Workflows are visualized and parameters are set via workflow dashboards of the Presentation and Directory services. After edition, new versions are stored using the Model Management service. Datasets of the Phenomenal experiment can be retrieved and accessed using the Data Source APIs with a connection to the PHIS.</p><p><software>LifeSWS</software> looks up the provenance and cache, and triggers <software ContextAttributes="created">OpenAlea</software>'s distributed execution. Provenance supports the determination of whether a workflow fragment has been already computed with the same parameters and datasets. The cache enables the retrieval of previous intermediate results rather than recomputing them again. The cache and provenance information are updated during the execution of <software ContextAttributes="created">OpenAlea</software> workflows using the <software ContextAttributes="created">Workflow Access</software> <software ContextAttributes="created">API</software>. <software ContextAttributes="created">LifeSWS</software> provides Data Services to feed the <software ContextAttributes="created">Galaxy</software> workflow with the output of <software ContextAttributes="created">OpenAlea</software> workflow when execution has been done. Then, it triggers the execution of <software ContextAttributes="created">Galaxy</software> on distributed resources.</p><p>The visualization of the intermediate and final results is done in a workflow dashboard and some specific outputs (e.g., plant traits) are updated in the PHIS using the Data Source APIs. Moreover, <software ContextAttributes="created">OpenAlea</software> models and workflows can be upgraded automatically using <software ContextAttributes="created">VersionClimber</software> <ref type="bibr" target="#b28">[29]</ref> from the Workflow Integration services to find the latest compatible versions of all the modela and libraries the workflows depend on.</p></div>
<div><head n="5.2">Epidemic Modeling</head><p>In this use case, <software>LifeSWS</software> is used by scientists in order to select ML models that would perform best to predict the transmission of the Dengue virus in a particular region.</p><p>In this scenario, the following inputs may be used by <software>LifeSWS</software>:</p><p>-Propagation datasets obtained from public health institutions, which are likely to contain information on the spread of disease. -Publicly available climate datasets, which could be used to identify environmental factors, such as temperature and precipitation, that may impact disease transmission. -Socio-economic datasets publicly available for specific countries and regions, which may provide insight into social and economic factors that could influence the spread of disease. -Prediction models provided by the system's users for different regions.</p><p>Let r be a region in a country given by a user, the objective is to find the best models that can predict the dengue transmission for r. If there are available models for the given region, then they are returned to the user. Otherwise, the system should execute the following workflow activities to analyze the characteristics of region r, and find the appropriate prediction models. In this case, the system performs a similarity search between the variables representing r (e.g., meteorological and socio-economical variables) and those of the regions for which it has predictive models in its database. Using this similarity search, <software>LifeSWS</software> identifies the most similar region to r. Finally, it retrieves the predictive models associated with the most similar region and returns them as the best models for predicting Dengue transmission in r.</p><p>Let us now explain how this use case can be realized using the services provided by LifeSWS. The required datasets of the use case, mainly propagation, climate and socio-economic datasets can be accessed using the Data Source APIs. The prediction models are given to <software>LifeSWS</software> via the dashboards of the Presentation and Directory services. Then, they are stored using the Model Management service. The Catalog is used to index the metadata of the given datasets and models.</p><p>The whole workflow for finding the prediction models of the given region is executed by the scheduler of the Workflow Integration services. The users send their region r to <software>LifeSWS</software> via their dashboard. Then, the system checks the Catalog to determine whether there are any predicting models for the given region. If the answer is positive, the models are retrieved from the Model Management service. Otherwise, <software ContextAttributes="created">LifeSWS</software> needs to find the similar regions to r and return their predicting models. For this, <software ContextAttributes="created">LifeSWS</software> first uses the Data Store service, to find the metadata of the query r and other regions. Then, the Data Analytics service is used to perform a similarity search in order to find the most similar regions to r, which we denote as R. <software ContextAttributes="created">LifeSWS</software> uses its Catalog to select the best models for predicting the transmission of the Dengue virus in the regions similar to r. Then, it calls the Cache management service to retrieve the selected models if they are available in the cache. Otherwise, the Model Management service is used to access the selected models, which are then returned to the user using the dashboards.</p></div>
<div><head n="6">Related Work</head><p>To address our objectives, many different approaches and solutions could be used with different trade-offs between development and maintenance cost, generality and efficiency. In this section, we discuss the main practical approaches and related technologies in a wide spectrum from generic to specific: cloud services, scientific workflow systems, heterogeneous data management systems, model lifecycle frameworks, and science platforms.</p><p>At one end of the spectrum (the most generic approaches), we have cloud services from major vendors (Amazon, Google, Microsoft, IBM, Oracle, . . . ). They provide many ready-to-use services within a Platform-as-a-Service (PaaS) to build applications that deal with Web data and enterprise data. They focus on ease-of-use, elasticity and interoperability through well-defined APIs that allow to use proprietary as well as open-source software. For instance, Amazon Web Services (AWS) is a large cloud computing platform, offering 200+ services, from basic services (storage, computing, database, containers, security, . . . ) to more advanced services (machine learning, data warehouse, data lake, search, . . . ). However, a first reason that prevents the use of such cloud platforms for <software>LifeSWS</software> is the lack of services directly available for scientific applications (workflows, provenance, numerical simulation, interface to HPC systems, . . . ). Another important reason for scientific organizations is that they prefer to rely on open vendor-neutral vendors.</p><p>At the other end of the spectrum (the most specific approaches), we have scientific workflows systems, such as <software ContextAttributes="created">Galaxy</software> <ref type="bibr" target="#b0">[1]</ref>, <software ContextAttributes="created">Kepler</software> <ref type="bibr" target="#b18">[19]</ref> and <software ContextAttributes="created">OpenAlea</software> <ref type="bibr" target="#b29">[30]</ref>, which are designed to help scientists developing complex applications. They typically include tools to model, design, debug, share and execute workflows, with interactive visualization of the results. To support result analysis and explaining and experiment reproducibility, workflow systems often support provenance, which captures the derivation history of a dataset, including the original data sources, intermediate datasets, and the computational steps that were applied to produce this dataset. Workflows are also often data-intensive, i.e., process, manage or produce huge amounts of data. Thus, in order to be executed in reasonable time, they require deployment in High Performance Computing (HPC) environments such as supercomputers, computer clusters or grids. For instance, <software ContextAttributes="created">DfAnalyzer</software> <ref type="bibr" target="#b31">[32]</ref> is a tool that enables monitoring, debugging, steering, and analysis of dataflows while the data is being generated by scientific applications. Most workflow systems are also open-source, providing access to community shared resources such as models, code libraries, and datasets. Thus, they tend to be specialized for some scientific domains. For instance, <software ContextAttributes="created">Galaxy</software> is quite popular in bioinformatics, while <software ContextAttributes="created">OpenAlea</software> is specialized in plant phenotyping. <software ContextAttributes="created">Kepler</software> <ref type="bibr" target="#b18">[19]</ref> addresses other scientific domains such as chemistry, ecology, geology, molecular biology and oceanography.</p><p>More generic, we have the popular big data analytics engines such as <software ContextAttributes="created">Spark</software> <ref type="bibr" target="#b35">[36]</ref> and <software ContextAttributes="created">Flink</software> <ref type="bibr" target="#b5">[6]</ref> which allow for batch or realtime data processing, and ML libraries such as <software ContextAttributes="created">PyTorch</software> <ref type="bibr" target="#b24">[25]</ref> and <software ContextAttributes="created">Scikit-learn</software> <ref type="bibr" target="#b25">[26]</ref> with workflows to collect training data, preprocess data (cleaning, formatting, . . . ), build datasets, train and refine models and evaluate.</p><p>As workflows are getting used a lot in practice, the problem of debugging has become important. It is difficult since there are many potential sources of errors including: bugs in the code, input data, software updates, and improper parameter settings. To address this problem, <software ContextAttributes="created">BugDoc</software> <ref type="bibr" target="#b17">[18]</ref> automatically infers the root causes and derive succinct explanations of failures for black-box pipelines using the results from previous runs. <software ContextAttributes="created">VersionClimber</software> <ref type="bibr" target="#b28">[29]</ref> is another automated system that deals with the problem of the pipelines that apply multiple packages, each of which evolves independently, to one or several data sources. <software ContextAttributes="created">VersionClimber</software> automatically discovers newer versions of these packages that are compatible.</p><p>For the applications envisioned in <software ContextAttributes="created">LifeSWS</software>, these systems will help, because we may want to combine different workflows (e.g., <software ContextAttributes="created">Galaxy</software>, <software ContextAttributes="created">OpenAlea</software> and <software ContextAttributes="created">Spark</software>), debug them with a tool like <software ContextAttributes="created">BugDoc</software> with some other data analytics services (e.g., time series analysis) and keep versions up-to-date. To integrate and execute heterogeneous workflows, we plan to rely on the Common Workflow Language (CWL) standard <ref type="bibr" target="#b7">[8]</ref>, which helps creating portable workflows.</p><p>Heterogeneous data management systems provide capabilities to access heterogeneous different data sources, which are important for our objectives. The problem of querying heterogeneous data sources, i.e., managed by different data management systems such as relational or XML database systems, has long been studied in the context of multidatabase systems <ref type="bibr" target="#b23">[24]</ref>. However, multidatabase systems have not been designed for the cloud, with a large variety of data stores such as SQL, <software ContextAttributes="created">NoSQL</software>, NewSQL and HDFS. Furthermore, operating in a cloud infrastructure provides more control over where the system components can be installed, which makes it possible to design more efficient architectures. These differences have motivated the design of polystores (or multistore systems) that provide integrated access to a number of cloud data stores. For instance, <software ContextAttributes="created">Cloud-MdsQL</software> [17] supports a functional SQL-like language, capable of querying multiple heterogeneous data stores within a single query that may contain embedded invocations to each data store's native query interface.</p><p>Spurred by the growing use of machine learning in all kinds of applications, many new model lifecycle systems have been proposed. Different from traditional software engineering, the development of ML applications is more iterative and explorative, yielding a variety of artifacts, such as datasets, models, features, hyperparameters, metrics, software code and pipelines. The objective of these new systems is to enable explainability, reproducibility, and traceability of ML executions by supporting the storage, management and reuse of these artifacts. The systematic literature review of more than 60 ML lifecycle management systems <ref type="bibr" target="#b30">[31]</ref> shows that there is no precise functional scope, thus making comparison be-tween systems difficult. Some systems focus on the management of ML artifacts only while some others add capabilities for the development of ML applications. The most complete systems come from cloud providers, e.g., Microsoft <software ContextAttributes="created">Azure ML</software>, Amazon <software ContextAttributes="created">SageMaker</software> and <software ContextAttributes="created">Google Vertex AI</software>, as ML as a service (MLaaS) platforms. In contrast, open-source systems tend to be more focused. For instance, MLflow <ref type="bibr" target="#b6">[7]</ref> focuses on capturing, storing, managing, and deploying ML artifacts using a standard format to store models and project code. It provides APIs to access ML development tools, such as <software ContextAttributes="created">PyTorch</software>, <software ContextAttributes="created">Scikit-learn</software> and <software ContextAttributes="created">Tensorflow</software>. Also motivated by the objective of providing a holistic view to support the lifecycle of scientific ML, <software ContextAttributes="created">ProvLake</software> <ref type="bibr" target="#b32">[33]</ref> is a provenance data management system capable of capturing, integrating, and querying data across multiple distributed services, programs, databases, stores, and computational workflows by leveraging provenance data.</p><p>Science platforms are facilities that provide services and resources for research communities to perform collaborative research, observation and experimentation. They may include major scientific equipment, sometimes HPC machines, scientific datasets, data and research papers, code libraries and models. A common particular case is the science gateway (or science portal), which is a community-developed set of tools, applications, and data that are integrated through a web-based portal or a suite of applications. Science platforms are more or less specialized for some particular science, e.g., <software>InfraPhenoGrid</software>, PHIS, Plntnet and <software ContextAttributes="created">CyVerse</software>.</p><p><software ContextAttributes="created">InfraPhenoGrid</software> <ref type="bibr" target="#b27">[28]</ref> is a grid-based platform to efficiently manage datasets produced by the PhenoArch plant phenomics platform in Montpellier and deploy scientific workflows using a middleware that hides complexity.</p><p>PHIS <ref type="bibr" target="#b21">[22]</ref> is a rich Phenotyping Hybrid Information System complementary to <software ContextAttributes="created">InfraPhenoGrid</software> designed for plant phenomics. It allows storing and managing heterogeneous data (e.g., images, spectra, growth curves) and multi-spatial and temporal scale data (leaf to canopy level) coming from multiple sources (field, greenhouse). Its ontology-driven architecture is a powerful tool for integrating and managing data from multiple experiments and platforms including field and greenhouse. PHIS allows to enrich datasets with knowledge and metadata enabling the reuse of data and meta-analyses. In contrast, <software ContextAttributes="created">LifeSWS</software> addresses a wider spectrum of applications in life sciences and provides key services such as user-specific Web dashboards, model management, provenance and cache, and workflow integration. <software ContextAttributes="created">Pl@ntNet</software> <ref type="bibr" target="#b15">[16]</ref> is a participatory platform and information system dedicated to the production and sharing of botanical data in order to study biodiversity. The main application performs deep learning-based plant identification on a smartphone and returns, given a plant image, the ranked list of the most likely species and asks for interactive validation by the users.</p><p><software ContextAttributes="created">CyVerse</software> <ref type="bibr" target="#b10">[11]</ref> is a platform for life sciences with services and resources to deal with huge datasets and complex data analyses. It includes a Web-based platform with data management services (storage, analysis, visualization, exploration), shared data and science APIs to access supercomputing resources. <software ContextAttributes="created">CyVerse</software> is the closest to <software ContextAttributes="created">LifeSWS</software>, but lacks key services such as user-specific Web dashboards, model management, provenance and cache, and Model/Workflow APIs.</p></div>
<div><head n="7">Conclusion</head><p>In this paper, we proposed <software>LifeSWS</software>, an open service-based architecture that implements data analysis workflow services for life sciences. The main objective of <software ContextAttributes="created">LifeSWS</software> is to support the construction and maintenance of high quality, scalable and efficient workflows by organizing and making workflow artifacts (datasets, models, metadata, workflow components, etc.) easy to search and manipulate using various workflow systems.</p><p>Our architecture capitalizes on the latest advances in web-oriented architectures, microservices and distributed and parallel data management. It relies on open source services and tools, and well-defined APIs to foster services interoperability (like cloud web services). <software>LifeSWS</software> provides three main layers of services (presentation and directory, workflow and data management) and APIs to interface with different workflow systems, data sources and data stores.</p><p><software>LifeSWS</software> services can be implemented and deployed in various platforms to address the specific requirements of vertical applications. We illustrated a <software ContextAttributes="created">LifeSWS</software> platform with <software ContextAttributes="created">Gypscie</software>, which provides services to develop, share, improve and publish scientific artifacts (datasets, models, etc.).</p><p>We also illustrated our proposed architecture with real use cases from life science. These examples are in agro-ecosystems in the context of climate change and epidemic modeling. These examples share common requirements but have specific features that show different uses of LifeSWS.</p><p><software>LifeSWS</software> capitalizes on our previous experience in developing major systems for scientific applications. However, there are still major issues. To understand the research issues, let us consider two important scenarios in which LifeSDS should be able to help: (1) domain scientists build one or more datasets which may be in a variety of formats (relational database, csv files, etc.); (2) they also build workflows that make use of these datasets. LifeSDS comes to play a role in two major ways: (1) improve the maintenance and performance of existing workflows; (2) allow authenticated and efficient access and management of multiple workflows and datasets.</p><p>Based on our experience and application use cases, the main issues we plan to work on involve:</p><p>-Making it easy to integrate and run heterogeneous workflows defined using the Common Workflow Language (CWL), while providing reuse and reproducibility; -Providing efficient execution of heterogeneous workflows by caching intermediate results and performing cache-aware scheduling; -Making it easy for domain scientists to manage the model life cycle, perform model selection and model integration for different types of models managed using different tools;</p><p>-Assisting scientists in analyzing diverse data types, such as time series, through the integration of advanced analytical methods for different analytical requirements such as clustering, anomaly detection, kNN search, etc; -Keeping track of the provenance of both data sources and software components, both to aid in debugging using tools such as <software ContextAttributes="created">BugDoc</software> <ref type="bibr" target="#b17">[18]</ref> and to enhance the reproducibility of these computational experiments.</p></div><figure xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Spatial and Temporal Workflows of Maize Shoot and Arabidopsis Root System Architecture</figDesc><graphic coords="6,137.60,295.66,340.16,170.08" type="bitmap" /></figure>
<figure xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Dengue Epidemy in Brazil</figDesc><graphic coords="8,194.30,162.63,226.76,170.08" type="bitmap" /></figure>
<figure xml:id="fig_2"><head>LifeSWS's functional architecture is shown in Figure 3 .</head><label>3</label><figDesc>It has three main layers of services: (1) presentation and directory, (2) workflow and (3) data management. Each layer can use the services of the same layer or the layers below. To interface with different systems, services can also use three kinds of APIs: Worflow Access APIs, Data Source APIs and Data Store APIs.</figDesc></figure>
<figure xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. LifeSWS Architecture</figDesc><graphic coords="9,137.60,291.94,340.17,170.08" type="bitmap" /></figure>
<figure xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Gypscie Web Interface</figDesc><graphic coords="15,134.77,115.84,368.49,190.55" type="bitmap" /></figure>
<figure xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. LifeSWS platform with Gypscie</figDesc><graphic coords="17,137.60,115.84,340.16,170.08" type="bitmap" /></figure>
<figure type="table" xml:id="tab_0"><head /><label /><figDesc>Section 2 develops our motivating examples from real life science applications. Section 3 presents our open, service-based architecture for LifeSWS. Section 4 discusses platforms and infrastructures that can implement LifeSWS. Section 5 shows the use of LifeSWS with use cases from our motivating examples. Section 6 discusses related work. Section 7 concludes and discusses open research issues.</figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgement</head><p>This work is within the context of the HPDaSc associated team between <rs type="institution">Inria and Brazil</rs>. Some of us are supported by <rs type="funder">CNPq</rs> research productivity fellowships. <rs type="person">C. Pradal</rs> has support from the <rs type="funder">MaCS4Plants CIRAD</rs> network, initiated from the <rs type="programName">AGAP Institute</rs> and <rs type="programName">AMAP joint research units</rs>, and <rs type="funder">EU</rs>'s <rs type="programName">Horizon 2020 research and innovation program</rs> (<rs type="programName">IPM Decisions</rs> project No. <rs type="grantNumber">817617</rs>, <rs type="projectName">Breeding-Value</rs> project No. <rs type="grantNumber">101000747</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_CRXpq6K">
					<orgName type="program" subtype="full">AGAP Institute</orgName>
				</org>
				<org type="funding" xml:id="_6eFCzBk">
					<orgName type="program" subtype="full">AMAP joint research units</orgName>
				</org>
				<org type="funding" xml:id="_YWHCQH8">
					<orgName type="program" subtype="full">Horizon 2020 research and innovation program</orgName>
				</org>
				<org type="funded-project" xml:id="_MGruS8h">
					<idno type="grant-number">817617</idno>
					<orgName type="project" subtype="full">Breeding-Value</orgName>
					<orgName type="program" subtype="full">IPM Decisions</orgName>
				</org>
				<org type="funding" xml:id="_dF2JPVh">
					<idno type="grant-number">101000747</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The galaxy platform for accessible, reproducible and collaborative biomedical analyses: 2022 update</title>
		<author>
			<persName><forename type="first">E</forename><surname>Afgan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic Acids Research</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">W1</biblScope>
			<biblScope unit="page" from="345" to="351" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Phenomenal: an automatic open source library for 3d shoot architecture reconstruction and analysis for image-based plant phenotyping</title>
		<author>
			<persName><forename type="first">S</forename><surname>Artzet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chopard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Brichet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mielewczik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cohen-Boulakia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cabrera-Bosquet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Tardieu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fournier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Pradal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BioRxiv</title>
		<imprint>
			<biblScope unit="page">805739</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Query processing in multistore systems: an overview</title>
		<author>
			<persName><forename type="first">C</forename><surname>Bondiombouy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Valduriez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Cloud Computing</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="309" to="346" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Phenotyping and modeling of root hydraulic architecture reveal critical determinants of axial water transport</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Boursiac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Pradal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bauget</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Delivorias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Godin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Maurel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Plant Physiology</title>
		<imprint>
			<biblScope unit="volume">190</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1289" to="1306" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Lying in wait: the resurgence of dengue virus after the zika epidemic in brazil</title>
		<author>
			<persName><forename type="first">A</forename><surname>Brito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Machado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Oidtman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J L</forename><surname>Siconelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Tran Minh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fauver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Carvalho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Dezordi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Castro Jorge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Minto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kalinich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Petrone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>España</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cummings</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Baele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Grubaugh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Communications</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">05</biblScope>
			<biblScope unit="page">2619</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Apache flink: Stream and batch processing in a single engine</title>
		<author>
			<persName><forename type="first">P</forename><surname>Carbone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Katsifodimos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ewen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Markl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Haridi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Tzoumas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Data Engineering Bulletin</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="28" to="38" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Developments in MLflow: a system to accelerate the machine learning lifecycle</title>
		<author>
			<persName><forename type="first">A</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Data Management for End-To-End Machine Learning (DEEM@SIGMOD)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Methods included: standardizing computational reuse and portability with the common workflow language</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Crusoe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="54" to="63" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Phenotrack3d: an automatic high-throughput phenotyping pipeline to track maize organs over time</title>
		<author>
			<persName><forename type="first">B</forename><surname>Daviet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fernandez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cabrera-Bosquet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Pradal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fournier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Plant Methods</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">130</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">High-throughput and automatic structural and developmental root phenotyping on arabidopsis seedlings</title>
		<author>
			<persName><forename type="first">R</forename><surname>Fernandez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Crabos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Maillard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Nacry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Pradal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Plant Methods</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="19" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The iplant collaborative: Cyberinfrastructure for plant biology</title>
		<author>
			<persName><forename type="first">S</forename><surname>Goff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in Plant Science</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Capturing and analyzing provenance from sparkbased scientific workflows with samba-rap</title>
		<author>
			<persName><forename type="first">T</forename><surname>Guedes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">B</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L F</forename><surname>Falci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A C S</forename><surname>Ocaña</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mattoso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">V N</forename><surname>Bedo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>De Oliveira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Future Generation Computing Systems</title>
		<imprint>
			<biblScope unit="volume">112</biblScope>
			<biblScope unit="page" from="658" to="669" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Cache-aware scheduling of scientific workflows in a multisite cloud</title>
		<author>
			<persName><forename type="first">G</forename><surname>Heidsieck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>De Oliveira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Pacitti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Pradal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Tardieu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Valduriez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Future Generation Computer Systems</title>
		<imprint>
			<biblScope unit="volume">122</biblScope>
			<biblScope unit="page" from="172" to="186" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">The Fourth Paradigm: Data-Intensive Scientific Discovery</title>
		<author>
			<persName><forename type="first">T</forename><surname>Hey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tansley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Tolle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gray</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009-10">October 2009</date>
		</imprint>
		<respStmt>
			<orgName>Microsoft Research</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Knowledge graphs</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hogan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Blomqvist</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cochez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Melo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">D</forename><surname>Gutierrez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kirrane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gayo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E L</forename><surname>Navigli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Neumaier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ngomo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C N</forename><surname>Polleres</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rashid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Rula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Schmelzeisen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sequeda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Staab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zimmermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename></persName>
		</author>
		<idno type="DOI">10.1145/3447772</idno>
		<ptr target="https://doi.org/10.1145/3447772" />
	</analytic>
	<monogr>
		<title level="j">ACM Comput. Surv</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2021-07">jul 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Interactive plant identification based on social image data</title>
		<author>
			<persName><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Bakić</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Barbe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Selmi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Yahiaoui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Carré</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Mouysset</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Molino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Boujemaa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Barthélémy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ecological Informatics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="22" to="34" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note>special Issue on Multimedia in Ecology and Environment 17</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The cloudmdsql multistore system</title>
		<author>
			<persName><forename type="first">B</forename><surname>Kolev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bondiombouy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Valduriez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jiménez-Peris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGMOD International Conference on Management of Data</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2113" to="2116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Bugdoc: iterative debugging and explanation of pipeline</title>
		<author>
			<persName><forename type="first">R</forename><surname>Lourenço</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Freire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Shasha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">VLDB Journal</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="75" to="101" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Scientific workflow management and the kepler system</title>
		<author>
			<persName><forename type="first">B</forename><surname>Ludäscher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Altintas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Berkley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Jaeger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">B</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">A</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Concurrency and Computation: Practice &amp; Experience</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1039" to="1065" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">SAVIME: An Array DBMS for Simulation Analysis and ML Models Predictions</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">L S</forename><surname>Lustosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Da Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">N R</forename><surname>Da Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Valduriez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">A M</forename><surname>Porto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Information and Data Management</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="247" to="264" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Plant and crop simulation models: powerful tools to link physiology, genetics, and phenomics</title>
		<author>
			<persName><forename type="first">B</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Martre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Botany</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2339" to="2344" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Dealing with multi-source and multi-scale information in plant phenomics: the ontology-driven phenotyping hybrid information system</title>
		<author>
			<persName><forename type="first">P</forename><surname>Neveu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">New Phytologist</title>
		<imprint>
			<biblScope unit="volume">221</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="588" to="601" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Data science: A systematic treatment</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Özsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="106" to="116" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Principles of Distributed Database Systems</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Özsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Valduriez</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
	<note>Fourth Edition</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Conference on Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Scikit-learn: Machine learning in python</title>
		<author>
			<persName><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Djensemble: a cost-based selection and allocation of a disjoint ensemble of spatio-temporal models</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">M</forename><surname>Souto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Da Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zorilla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Tsan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">S</forename><surname>Ogasawara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ziviani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Porto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Scientific and Statistical Database Management (SSDBM)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="226" to="231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">InfraPhenoGrid: A scientific workflow infrastructure for Plant Phenomics on the Grid</title>
		<author>
			<persName><forename type="first">C</forename><surname>Pradal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Artzet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chopard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Dupuis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fournier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mielewczik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Negre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Neveu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Parigot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Valduriez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cohen-Boulakia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Future Generation Computer Systems</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="page" from="341" to="353" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">VersionClimber: version upgrades without tears</title>
		<author>
			<persName><forename type="first">C</forename><surname>Pradal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cohen-Boulakia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Valduriez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shasha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computing in Science and Engineering</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="87" to="93" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Openalea: scientific workflows combining data analysis and simulation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Pradal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fournier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Valduriez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Boulakia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Scientific and Statistical Database Management (SSDBM)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Management of machine learning lifecycle artifacts: A survey</title>
		<author>
			<persName><forename type="first">M</forename><surname>Schlegel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sattler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGMOD Record</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="18" to="35" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">DfAnalyzer: Runtime Dataflow Analysis of Scientific Applications using Provenance</title>
		<author>
			<persName><forename type="first">V</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>De Oliveira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Valduriez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mattoso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the VLDB Endowment (PVLDB)</title>
		<meeting>the VLDB Endowment (PVLDB)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="2082" to="2085" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Workflow provenance in the lifecycle of scientific machine learning</title>
		<author>
			<persName><forename type="first">R</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">G</forename><surname>Azevedo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Lourenço</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">F S</forename><surname>Soares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Thiago</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Brandão</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Civitarese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">V</forename><surname>Brazil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Moreno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Valduriez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mattoso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cerqueira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A S</forename><surname>Netto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Concurrency and Computation: Practice &amp; Experience</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">14</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Plant phenomics, from sensors to knowledge</title>
		<author>
			<persName><forename type="first">F</forename><surname>Tardieu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cabrera-Bosquet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pridmore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bennett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Current Biology</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">15</biblScope>
			<biblScope unit="page" from="R770" to="R783" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Data and Machine Learning Model Management with Gypscie</title>
		<author>
			<persName><forename type="first">P</forename><surname>Valduriez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Porto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CARLA Workshop on HPC and Data Sciences meet Scientific Computing</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1" to="2" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Spark: Cluster computing with working sets</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zaharia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chowdhury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Franklin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shenker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Stoica</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX Workshop on Hot Topics in Cloud Computing (HotCloud)</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<title level="m">Ensemble Machine Learning, Methods and Applications</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A data-driven model selection approach to spatio-temporal prediction</title>
		<author>
			<persName><forename type="first">R</forename><surname>Zorrilla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">S</forename><surname>Ogasawara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Valduriez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Porto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Brazilian Symposium on Databases (SBBD)</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>