<TEI xmlns="http://www.tei-c.org/ns/1.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xml:space="preserve" xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Differential Privacy at Risk: Bridging Randomness and Privacy Budget</title>
				<funder>
					<orgName type="full">National Research Foundation</orgName>
					<orgName type="abbreviated">NRF</orgName>
				</funder>
				<funder ref="#_RtA6xjH">
					<orgName type="full">Singapore Telecommunications Ltd</orgName>
				</funder>
				<funder>
					<orgName type="full">National University of Singapore</orgName>
				</funder>
				<funder ref="#_8k4Mwe3">
					<orgName type="full">Agence Nationale de la Recherche</orgName>
					<orgName type="abbreviated">ANR</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher />
				<availability status="unknown"><licence /></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Ashish</forename><surname>Dandekar</surname></persName>
							<email>adandekar@ens.fr</email>
						</author>
						<author>
							<persName><forename type="first">Debabrota</forename><surname>Basu</surname></persName>
							<email>basud@chalmers.se</email>
						</author>
						<author>
							<persName><forename type="first">Stéphane</forename><surname>Bressan</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="laboratory">DI ENS</orgName>
								<orgName type="institution" key="instit1">ENS</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<orgName type="institution" key="instit3">PSL University &amp; Inria</orgName>
								<address>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Dept. of Computer Sci. and Engg</orgName>
								<orgName type="institution">Chalmers University of Technology</orgName>
								<address>
									<settlement>Göteborg</settlement>
									<country key="SE">Sweden</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">National University of Singapore</orgName>
								<address>
									<settlement>Singa-pore</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Differential Privacy at Risk: Bridging Randomness and Privacy Budget</title>
					</analytic>
					<monogr>
						<imprint>
							<date />
						</imprint>
					</monogr>
					<idno type="MD5">27CE8622CF8AB1B4EDC2651D5D005341</idno>
					<idno type="DOI">10.2478/popets-2021-0005</idno>
					<note type="submission">Received ..; revised ..; accepted ...</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-04-12T14:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid" />
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Differential privacy</term>
					<term>cost model</term>
					<term>Laplace mechanism</term>
				</keywords>
			</textClass>
			<abstract>
<div><p>The calibration of noise for a privacypreserving mechanism depends on the sensitivity of the query and the prescribed privacy level. A data steward must make the non-trivial choice of a privacy level that balances the requirements of users and the monetary constraints of the business entity. Firstly, we analyse roles of the sources of randomness, namely the explicit randomness induced by the noise distribution and the implicit randomness induced by the data-generation distribution, that are involved in the design of a privacy-preserving mechanism. The finer analysis enables us to provide stronger privacy guarantees with quantifiable risks. Thus, we propose privacy at risk that is a probabilistic calibration of privacypreserving mechanisms. We provide a composition theorem that leverages privacy at risk. We instantiate the probabilistic calibration for the Laplace mechanism by providing analytical results. Secondly, we propose a cost model that bridges the gap between the privacy level and the compensation budget estimated by a GDPR compliant business entity. The convexity of the proposed cost model leads to a unique fine-tuning of privacy level that minimises the compensation budget. We show its effectiveness by illustrating a realistic scenario that avoids overestimation of the compensation budget by using privacy at risk for the Laplace mechanism. We quantitatively show that composition using the cost optimal privacy at risk provides stronger privacy guarantee than the classical advanced composition. Although the illustration is specific to the chosen cost model, it naturally extends to any convex cost model. We also provide realistic illustrations of how a data steward uses privacy at risk to balance the tradeoff between utility and privacy.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div><head n="1">Introduction</head><p>Dwork et al. <ref type="bibr" target="#b11">[12]</ref> quantify the privacy level ε in εdifferential privacy (or ε-DP) as an upper bound on the worst-case privacy loss incurred by a privacy-preserving mechanism. Generally, a privacy-preserving mechanism perturbs the results by adding the calibrated amount of random noise to them. The calibration of noise depends on the sensitivity of the query and the specified privacy level. In a real-world setting, a data steward must specify a privacy level that balances the requirements of the users and monetary constraints of the business entity. For example, Garfinkel et al. <ref type="bibr" target="#b13">[14]</ref> report on issues encountered when deploying differential privacy as the privacy definition by the US census bureau. They highlight the lack of analytical methods to choose the privacy level. They also report empirical studies that show the loss in utility due to the application of privacypreserving mechanisms.</p><p>We address the dilemma of a data steward in two ways. Firstly, we propose a probabilistic quantification of privacy levels. Probabilistic quantification of privacy levels provides a data steward with a way to take quantified risks under the desired utility of the data. We refer to the probabilistic quantification as privacy at risk. We also derive a composition theorem that leverages privacy at risk. Secondly, we propose a cost model that links the privacy level to a monetary budget. This cost model helps the data steward to choose the privacy level constrained on the estimated budget and vice versa. Convexity of the proposed cost model ensures the existence of a unique privacy at risk that would minimise the budget. We show that the composition with an optimal privacy at risk provides stronger privacy guarantees than the traditional advanced composition <ref type="bibr" target="#b11">[12]</ref>. In the end, we illustrate a realistic scenario that exemplifies how the data steward can avoid overestimation of the budget by using the proposed cost model by using privacy at risk.</p><p>The probabilistic quantification of privacy levels depends on two sources of randomness: the explicit randomness induced by the noise distribution and the implicit randomness induced by the data-generation distribution. Often, these two sources are coupled with each other. We require analytical forms of both sources of randomness as well as an analytical representation of the query to derive a privacy guarantee. Computing the probabilistic quantification of different sources of randomness is generally a challenging task. Although we find multiple probabilistic privacy definitions in the literature <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b26">27]</ref> <ref type="foot" target="#foot_0">1</ref> , we miss an analytical quantification bridging the randomness and privacy level of a privacypreserving mechanism. We propose a probabilistic quantification, namely privacy at risk, that further leads to analytical relation between privacy and randomness. We derive a composition theorem with privacy at risk for mechanisms with the same as well as varying privacy levels. It is an extension of the advanced composition theorem <ref type="bibr" target="#b11">[12]</ref> that deals with a sequential and adaptive use of privacy-preserving mechanisms. We also prove that privacy at risk satisfies convexity over privacy levels and a weak relaxation of the post-processing property. To the best of our knowledge, we are the first to analytically derive the proposed probabilistic quantification for the widely used Laplace mechanism <ref type="bibr" target="#b9">[10]</ref>.</p><p>The privacy level proposed by the differential privacy framework is too abstract a quantity to be integrated in a business setting. We propose a cost model that maps the privacy level to a monetary budget. The proposed model is a convex function of the privacy level, which further leads to a convex cost model for privacy at risk. Hence, it has a unique probabilistic privacy level that minimises the cost. We illustrate this using a realistic scenario in a GDPR-compliant business entity that needs an estimation of the compensation budget that it needs to pay to stakeholders in the unfortunate event of a personal data breach. The illustration, which uses the proposed convex cost model, shows that the use of probabilistic privacy levels avoids overestimation of the compensation budget without sacrificing utility. The illustration naturally extends to any convex cost model.</p><p>In this work, we comparatively evaluate the privacy guarantees using privacy at risk of the Laplace mechanism. We quantitatively compare the composition under the optimal privacy at risk, which is estimated using the cost model, with traditional composition mechanismsbasic and advanced mechanisms <ref type="bibr" target="#b11">[12]</ref>. We observe that it gives stronger privacy guarantees than the ones obtained by the advanced composition without sacrificing on the utility of the mechanism.</p><p>In conclusion, benefits of the probabilistic quantification i.e., of the privacy at risk are twofold. It not only quantifies the privacy level for a given privacypreserving mechanism but also facilitates decisionmaking in problems that focus on the privacy-utility trade-off and the compensation budget minimisation.</p></div>
<div><head n="2">Background</head><p>We consider a universe of datasets D. We explicitly mention when we consider that the datasets are sampled from a data-generation distribution G with support D. Two datasets of equal cardinality x and y are said to be neighbouring datasets if they differ in one data point. A pair of neighbouring datasets is denoted by x ∼ y. In this work, we focus on a specific class of queries called numeric queries. A numeric query f is a function that maps a dataset into a real-valued vector, i.e. f : D → R k . For instance, a sum query returns the sum of the values in a dataset.</p><p>In order to achieve a privacy guarantee, researchers use a privacy-preserving mechanism, or mechanism in short, which is a randomised algorithm that adds noise to the query from a given family of distributions. Thus, a privacy-preserving mechanism of a given family, M(f, Θ), for the query f and the set of parameters Θ of the given noise distribution, is a function i.e. M(f, Θ) : D → R. In the case of numerical queries, R is R k . We denote a privacy-preserving mechanism as M, when the query and the parameters are clear from the context. Definition 1 (Differential Privacy <ref type="bibr" target="#b11">[12]</ref>). A privacypreserving mechanism M, equipped with a query f and with parameters Θ, is (ε, δ)-differentially private if for all Z ⊆ Range(M) and x, y ∈ D such that x ∼ y:</p><formula xml:id="formula_0">P(M(f, Θ)(x) ∈ Z) ≤ e ε × P(M(f, Θ)(y) ∈ Z) + δ.</formula><p>An (ε, 0)-differentially private mechanism is also simply said to be ε-differentially private. Often, ε-differential privacy is referred to as pure differential privacy whereas (ε, δ)-differential privacy is referred as approximate differential privacy.</p><p>A privacy-preserving mechanism provides perfect privacy if it yields indistinguishable outputs for all neighbouring input datasets. The privacy level ε quantifies the privacy guarantee provided by ε-differential privacy. For a given query, the smaller the value of the ε, the qualitatively higher the privacy. A randomised algorithm that is ε-differentially private is also ε -differential private for any ε &gt; ε.</p><p>In order to satisfy ε-differential privacy, the parameters of a privacy-preserving mechanism requires a calculated calibration. The amount of noise required to achieve a specified privacy level depends on the query. If the output of the query does not change drastically for two neighbouring datasets, then a small amount of noise is required to achieve a given privacy level. The measure of such fluctuations is called the sensitivity of the query. The parameters of a privacy-preserving mechanism are calibrated using the sensitivity of the query that quantifies the smoothness of a numeric query.</p></div>
<div><head>Definition 2 (Sensitivity). The sensitivity of a query</head><formula xml:id="formula_1">f : D → R k is defined as ∆ f max x,y∈D x∼y f (x) -f (y) 1 .</formula><p>The Laplace mechanism is a privacy-preserving mechanism that adds scaled noise sampled from a calibrated Laplace distribution to the numeric query.</p></div>
<div><head>Definition 3 ([35]</head><p>). The Laplace distribution with mean zero and scale b &gt; 0 is a probability distribution with probability density function</p><formula xml:id="formula_2">Lap(b) 1 2b exp - |x| b ,</formula><p>where x ∈ R. We write Lap(b) to denote a random variable X ∼ Lap(b) Definition 4 (Laplace Mechanism <ref type="bibr" target="#b9">[10]</ref>). Given any function f : D → R k and any x ∈ D, the Laplace Mechanism is defined as</p><formula xml:id="formula_3">L ∆ f ε (x) M f, ∆ f ε (x) = f (x) + (L 1 , ..., L k ),</formula><p>where L i is drawn from Lap Theorem 1 <ref type="bibr">([10]</ref>). The Laplace mechanism, L</p><formula xml:id="formula_4">∆ f ε0 , is ε 0 -differentially private.</formula></div>
<div><head n="3">Privacy at Risk: A Probabilistic Quantification of Randomness</head><p>The parameters of a privacy-preserving mechanism are calibrated using the privacy level and the sensitivity of the query. A data steward needs to choose an appropriate privacy level for practical implementation. Lee et al. <ref type="bibr" target="#b24">[25]</ref> show that the choice of an actual privacy level by a data steward in regard to her business requirements is a non-trivial task. Recall that the privacy level in the definition of differential privacy corresponds to the worst case privacy loss. Business users are however used to taking and managing risks, if the risks can be quantified. For instance, Jorion <ref type="bibr" target="#b20">[21]</ref> defines Value at Risk that is used by risk analysts to quantify the loss in investments for a given portfolio and an acceptable confidence bound. Motivated by the formulation of Value at Risk, we propose to use the use of probabilistic privacy level. It provides us with a finer tuning of an ε 0differentially private privacy-preserving mechanism for a specified risk γ.</p><p>Definition 5 (Privacy at Risk). For a given data generating distribution G, a privacy-preserving mechanism M, equipped with a query f and with parameters Θ, satisfies ε-differential privacy with a privacy at risk 0 ≤ γ ≤ 1 if, for all Z ⊆ Range(M) and x, y sampled from G such that x ∼ y:</p><formula xml:id="formula_5">P ln P(M(f, Θ)(x) ∈ Z) P(M(f, Θ)(y) ∈ Z) &gt; ε ≤ γ, (<label>1</label></formula><formula xml:id="formula_6">)</formula><p>where the outer probability is calculated with respect to the probability space Range(M • G) obtained by applying the privacy-preserving mechanism M on the datageneration distribution G.</p><p>If a privacy-preserving mechanism is ε 0 -differentially private for a given query f and parameters Θ, for any privacy level ε ≥ ε 0 , the privacy at risk is 0. We are interested in quantifying the risk γ with which an ε 0 -differentially private privacy-preserving mechanism also satisfies a stronger ε-differential privacy, i.e., with ε &lt; ε 0 .</p><p>Unifying Probabilistic and Random DP Interestingly, Equation (1) unifies the notions of probabilistic differential privacy and random differential privacy by accounting for both sources of randomness in a privacy-preserving mechanism. Machanavajjhala et al. <ref type="bibr" target="#b26">[27]</ref> define probabilistic differential privacy that incorporates the explicit randomness of the noise distribution of the privacy-preserving mechanism, whereas Hall et al. <ref type="bibr" target="#b15">[16]</ref> define random differential privacy that incorporates the implicit randomness of the data-generation distribution. In probabilistic differential privacy, the outer probability is computed over the sample space of Range(M) and all datasets are equally probable.</p></div>
<div><head>Connection with Approximate DP</head><p>Despite a resemblance with probabilistic relaxations of differential privacy <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b26">27]</ref> due to the added parameter δ, (ε, δ)-differential privacy (Definition 1) is a nonprobabilistic variant <ref type="bibr" target="#b28">[29]</ref> of regular ε-differential privacy. Indeed, unlike the auxiliary parameters in probabilistic relaxations, such as γ in privacy at risk (ref.</p><p>Definition 5), the parameter δ of approximate differential privacy is an absolute slack that is independent of the sources of randomness. For a specified choice of ε and δ, one can analytically compute a matching value of δ for a new value of ε 2 . Therefore, as other probabilistic relaxations, privacy at risk cannot be directly related to approximate differential privacy. An alternative is to find out a privacy at risk level γ for a given privacy level (ε, δ) while the original noise satisfies (ε 0 , δ).</p><p>Theorem 2. If a privacy preserving mechanism satisfies (ε, γ) privacy at risk, it also satisfies (ε, γ) approximate differential privacy.</p><p>We obtain this reduction as the probability measure induced by the privacy preserving mechanism and data generating distribution on any output set Z ⊆ Range(M) is additive. 3 The proof of the theorem is in Appendix A.</p></div>
<div><head n="3.1">Composition theorem</head><p>The application of ε-differential privacy to many realworld problem suffers from the degradation of privacy guarantee, i.e., privacy level, over the composition. The basic composition theorem <ref type="bibr" target="#b11">[12]</ref> dictates that the privacy guarantee degrades linearly in the number of evaluations of the mechanism. The advanced composition theorem <ref type="bibr" target="#b11">[12]</ref> provides a finer analysis of the privacy loss 2 For any 0 &lt; ε ≤ ε, any (ε, δ)-differentially private mechanism also satisfies (ε , (e εe ε + δ))-differential privacy.</p><p>3 The converse is not true as explained before.</p><p>over multiple evaluations with a square root dependence on the number of evaluations. In this section, we provide the composition theorem for privacy at risk. Definition 6 (Privacy loss random variable). For a privacy-preserving mechanism M : D → R, any two neighbouring datasets x, y ∈ D and an output r ∈ R, the value of the privacy loss random variable C is defined as:</p><formula xml:id="formula_7">C(r) ln P(M(x) = r) P(M(y) = r)</formula><p>.</p><p>Lemma 1. If a privacy-preserving mechanism M satisfies ε 0 -differential privacy, then</p><formula xml:id="formula_8">P[|C| ≤ ε 0 ] = 1.</formula><p>Theorem 3. For all ε 0 , ε, γ, δ &gt; 0, the class of ε 0differentially private mechanisms, which satisfy (ε, γ)privacy at risk under a uniform data-generation distribution, are (ε , δ)-differential privacy under n-fold composition where</p><formula xml:id="formula_9">ε = ε 0 2n ln 1 δ + nµ, where µ = 1 2 [γε 2 + (1 -γ)ε 2 0 ]. Proof. Let, M 1...n : D → R 1 × R 2 × ... × R n denote the n-fold composition of privacy-preserving mechanisms {M i : D → R i } n i=1</formula><p>. Each ε 0 -differentially private M i also satisfies (ε, γ)-privacy at risk for some ε ≤ ε 0 and appropriately computed γ. Consider any two neighbouring datasets x, y ∈ D. Let,</p><formula xml:id="formula_10">B = (r 1 , ..., r n ) n i=1 P(M i (x) = r i ) P(M i (y) = r i ) &gt; e ε</formula><p>Using the technique in <ref type="bibr" target="#b11">[12,</ref><ref type="bibr">Theorem 3.20]</ref>, it suffices to show that P(M 1...n (x) ∈ B) ≤ δ.</p><formula xml:id="formula_11">Consider ln P(M 1...n (x) = (r 1 , ..., r n )) P(M 1...n (y) = (r 1 , ..., r n )) = ln n i=1 P(M i (x) = r i ) P(M i (y) = r i ) = n i=1 ln P(M i (x) = r i ) P(M i (y) = r i ) n i=1 C i (2)</formula><p>where C i in the last line denotes the privacy loss random variable related to M i . Consider an ε-differentially private mechanism M ε and ε 0 -differentially private mechanism M ε0 . Let M ε0 satisfy (ε, γ)-privacy at risk for ε ≤ ε 0 and appropriately computed γ. Each M i can be simulated as the mechanism M ε with probability γ and the mechanism M ε0 otherwise. Therefore, the privacy loss random variable for each mechanism M i can be written as</p><formula xml:id="formula_12">C i = γC i ε + (1 -γ)C i ε0</formula><p>where C i ε denotes the privacy loss random variable associated with the mechanism M ε and C i ε0 denotes the privacy loss random variable associated with the mechanism M ε0 . Using <ref type="bibr" target="#b4">[5,</ref><ref type="bibr">Remark 3.4]</ref>, we can bound the mean of every privacy loss random variable as:</p><formula xml:id="formula_13">µ E[C i ] ≤ 1 2 [γε 2 + (1 -γ)ε 2 0 ].</formula><p>We have a collection of n independent privacy random variables C i 's such that P |C i | ≤ ε 0 = 1. Using Hoeffding's bound <ref type="bibr" target="#b17">[18]</ref> on the sample mean for any β &gt; 0,</p><formula xml:id="formula_14">P 1 n i C i ≥ E[C i ] + β ≤ exp - nβ 2 2ε 2 0 .</formula><p>Rearranging the inequality by renaming the upper bound on the probability as δ, we get:</p><formula xml:id="formula_15">P i C i ≥ nµ + ε 0 2n ln 1 δ ≤ δ.</formula><p>Theorem 3 is an analogue, in the privacy at risk setting, of the advanced composition of differential privacy <ref type="bibr" target="#b11">[12,</ref><ref type="bibr">Theorem 3.20]</ref> under a constraint of independent evaluations. Note that if one takes γ = 0, then we obtain the exact same formula as in <ref type="bibr" target="#b11">[12,</ref><ref type="bibr">Theorem 3.20]</ref>. It provides a sanity check for the consistency of composition using privacy at risk.</p></div>
<div><head>Corollary 1 (Heterogeneous Composition</head><p>). For all ε l , ε, γ l , δ &gt; 0 and l ∈ {1, . . . , n}, the composition of {ε l } n l=1 -differentially private mechanisms, which satisfy (ε, γ l )-privacy at risk under a uniform data-generation distribution, also satisfies (ε , δ)-differential privacy where</p><formula xml:id="formula_16">ε = 2 n l=1 ε 2 l ln 1 δ + µ, where µ = 1 2 [ε 2 ( n l=1 γ l ) + n l=1 (1 -γ l )ε 2 l ].</formula><p>Proof. The proof follows from the same argument as that of Theorem 3 of bounding the loss random variable at step l using γ l C l ε + (1 -γ l )C l ε l and then applying the concentration inequality.</p><p>A detailed discussion and analysis of proving such heterogeneous composition theorems is available in <ref type="bibr" target="#b21">[22,</ref><ref type="bibr">Section 3.3]</ref>.</p><p>In fact, if we consider both sources of randomness, the expected value of the loss function must be computed by using the law of total expectation.</p><formula xml:id="formula_17">E[C] = E x,y∼G [E[C]|x, y]</formula><p>Therefore, the exact computation of privacy guarantees after the composition requires access to the datageneration distribution. We assume a uniform datageneration distribution while proving Theorem 3. We can obtain better and finer privacy guarantees accounting for data-generation distribution, which we keep as a future work.</p></div>
<div><head n="3.2">Convexity and Post-Processing</head><p>We show that privacy at risk satisfies the convexity property and does not satisfy the post-processing property.</p><p>Lemma 2 (Convexity). For a given ε 0 -differentially private privacy-preserving mechanism, privacy at risk satisfies the convexity property.</p><p>Proof. Let M be a mechanism that satisfies ε 0differential privacy. By the definition of the privacy at risk, it also satisfies (ε 1 , γ 1 )-privacy at risk as well as (ε 2 , γ 2 )-privacy at risk for some ε 1 , ε 2 ≤ ε 0 and appropriately computed values of γ 1 and γ 2 . Let M 1 and M 2 denote the hypothetical mechanisms that satisfy (ε 1 , γ 1 )-privacy at risk and (ε 2 , γ 2 )-privacy at risk respectively. We can write privacy loss random variables as follows:</p><formula xml:id="formula_18">C 1 ≤ γ 1 ε 1 + (1 -γ 1 )ε 0 C 2 ≤ γ 2 ε 2 + (1 -γ 2 )ε 0</formula><p>where C 1 and C 2 denote privacy loss random variables for M 1 and M 2 .</p><p>Let us consider a privacy-preserving mechanism M that uses M 1 with a probability p and M 2 with a probability (1-p) for some p ∈ [0, 1]. By using the techniques in the proof of Theorem 3, the privacy loss random variable C for M can be written as:</p><formula xml:id="formula_19">C = pC 1 + (1 -p)C 2 ≤ γ ε + (1 -γ )ε 0 where ε = pγ 1 ε 1 + (1 -p)γ 2 ε 2 pγ 1 + (1 -p)γ 2 γ = (1 -pγ 1 -(1 -p)γ 2 )</formula><p>Thus, M satisfies (ε , γ )-privacy at risk. This proves that privacy at risk satisfies convexity [23, Axiom 2.1.2].</p><p>Meiser <ref type="bibr" target="#b28">[29]</ref> proved that a relaxation of differential privacy that provides probabilistic bounds on the privacy loss random variable does not satisfy post-processing property of differential privacy. Privacy at risk is indeed such a probabilistic relaxation.</p><p>Corollary 2 (Post-processing). Privacy at risk does not satisfy the post-processing property for every possible mapping of the output.</p><p>Though privacy at risk is not preserved after postprocessing, it yields a weaker guarantee in terms of approximate differential privacy after post-processing. The proof involves reduction of privacy at risk to approximate differential privacy and preservation of approximate differential privacy under post-processing.</p><formula xml:id="formula_20">Lemma 3 (Weak Post-processing). Let M : D → R ⊆</formula><p>R k be a mechanism that satisfy (ε, γ)-privacy at risk and f : R → R be any arbitrary data independent mapping. Then, f • M : D → R would also satisfy (ε, γ)approximate differential privacy.</p><p>Proof. Let us fix a pair of neighbouring datasets x and y, and also an event Z ⊆ R . Let us define pre-image of Z as Z {r ∈ R : f (r) ∈ Z}. Now, we get</p><formula xml:id="formula_21">P(f • M(x) ∈ Z ) = P(M(x) ∈ Z) ≤ (a) e ε P(M(y) ∈ Z) + γ = e ε P(f • M(y) ∈ Z ) + δ (a) is a direct consequence of Theorem 2.</formula></div>
<div><head n="4">Privacy at Risk for Laplace Mechanism</head><p>The Laplace and Gaussian mechanisms are widely used privacy-preserving mechanisms in the literature. The Laplace mechanism satisfies pure ε-differential privacy whereas the Gaussian mechanism satisfies approximate (ε, δ)-differential privacy. As previously discussed, it is not straightforward to establish a connection between the non-probabilistic parameter δ of approximate differential privacy and the probabilistic bound γ of privacy at risk. Therefore, we keep privacy at risk for Gaussian mechanism as the future work.</p><p>In this section, we instantiate privacy at risk for the Laplace mechanism in three cases: two cases involving two sources of randomness and a third case involving the coupled effect. These three different cases correspond to three different interpretations of the confidence level, represented by the parameter γ, corresponding to three interpretations of the support of the outer probability in Definition 5. In order to highlight this nuance, we denote the confidence levels corresponding to the three cases and their three sources of randomness as γ 1 , γ 2 , and γ 3 , respectively.</p></div>
<div><head n="4.1">The Case of Explicit Randomness</head><p>In this section, we study the effect of the explicit randomness induced by the noise sampled from Laplace distribution. We provide a probabilistic quantification for fine tuning for the Laplace mechanism. We fine-tune the privacy level for a specified risk under by assuming that the sensitivity of the query is known a priori.</p><p>For a Laplace mechanism L ∆ f ε0 calibrated with sensitivity ∆ f and privacy level ε 0 , we present the analytical formula relating privacy level ε and the risk γ 1 in Theorem 4. The proof is available in Appendix B.</p></div>
<div><head>Theorem 4. The risk</head><formula xml:id="formula_22">γ 1 ∈ [0, 1] with which a Laplace Mechanism L ∆ f ε0 , for a numeric query f : D → R k sat- isfies a privacy level ε ≥ 0 is given by γ 1 = P(T ≤ ε) P(T ≤ ε 0 ) , (<label>3</label></formula><formula xml:id="formula_23">)</formula><p>where T is a random variable that follows a distribution with the following density function.</p><formula xml:id="formula_24">P T (t) = 2 1-k t k-1 2 K k-1 2 (t)ε 0 √ 2πΓ(k)∆ f where K n-1 2</formula><p>is the Bessel function of second kind.</p><p>Figure <ref type="figure">1a</ref> shows the plot of the privacy level against risk for different values of k and for a Laplace mechanism L 1.0 1.0 . As the value of k increases, the amount of noise added in the output of numeric query increases. Therefore, for a specified privacy level, the privacy at risk level increases with the value of k.</p><p>The analytical formula representing γ 1 as a function of ε is bijective. We need to invert it to obtain the privacy level ε for a privacy at risk γ 1 . However the analytical closed form for such an inverse function is not explicit. We use a numerical approach to compute privacy level for a given privacy at risk from the analytical formula of Theorem 4.</p><p>Result for a Real-valued Query. For the case k = 1, the analytical derivation is fairly straightforward. In this case, we obtain an invertible closed-form of a privacy level for a specified risk. It is presented in Equation <ref type="formula" target="#formula_25">4</ref>.</p><formula xml:id="formula_25">ε = ln 1 1 -γ 1 (1 -e -ε0 )<label>(4)</label></formula><p>Remarks on ε 0 . For k = 1, Figure <ref type="figure">1b</ref> shows the plot of privacy at risk level ε versus privacy at risk γ 1 for the Laplace mechanism L 1.0 ε0 . As the value of ε 0 increases, the probability of Laplace mechanism generating higher value of noise reduces. Therefore, for a fixed privacy level, privacy at risk increases with the value of ε 0 . The same observation is made for k &gt; 1.</p></div>
<div><head n="4.2">The Case of Implicit Randomness</head><p>In this section, we study the effect of the implicit randomness induced by the data-generation distribution to provide a fine tuning for the Laplace mechanism. We fine-tune the risk for a specified privacy level without assuming that the sensitivity of the query.</p><p>If one takes into account randomness induced by the data-generation distribution, all pairs of neighbouring datasets are not equally probable. This leads to estimation of sensitivity of a query for a specified datageneration distribution. If we have access to an analytical form of the data-generation distribution and to the query, we could analytically derive the sensitivity distribution for the query. In general, we have access to the datasets, but not the data-generation distribution that generates them. We, therefore, statistically estimate sensitivity by constructing an empirical distribution. We call the sensitivity value obtained for a specified risk from the empirical cumulative distribution of sensitivity the sampled sensitivity (Definition 7). However, the value of sampled sensitivity is simply an estimate of the sensitivity for a specified risk. In order to capture this additional uncertainty introduced by the estimation from the empirical sensitivity distribution rather than the true unknown distribution, we compute a lower bound on the accuracy of this estimation. This lower bound yields a probabilistic lower bound on the specified risk. We refer to it as empirical risk. For a specified absolute risk γ 2 , we denote by γ2 corresponding empirical risk.</p><p>For the Laplace mechanism L ∆ S f ε calibrated with sampled sensitivity ∆ S f and privacy level ε, we evaluate the empirical risk γ2 . We present the result in Theorem 5. The proof is available in Appendix C. </p><formula xml:id="formula_26">f : D → R k is γ2 ≥ γ 2 (1 -2e -2ρ 2 n ) (<label>5</label></formula><formula xml:id="formula_27">)</formula><p>where n is the number of samples used for estimation of the sampled sensitivity and ρ is the accuracy parameter. γ 2 denotes the specified absolute risk.</p><p>The error parameter ρ controls the closeness between the empirical cumulative distribution of the sensitivity to the true cumulative distribution of the sensitivity. Lower the value of the error, closer is the empirical cumulative distribution to the true cumulative distribution. Mathematically,</p><formula xml:id="formula_28">ρ ≥ sup ∆ |F n S (∆) -F S (∆)|,</formula><p>where F n S is the empirical cumulative distribution of sensitivity after n samples and F S is the actual cumulative distribution of sensitivity.</p><p>Figure <ref type="figure">2</ref> shows the plot of number of samples as a function of the privacy at risk and the error parameter. Naturally, we require higher number of samples in order to have lower error rate. The number of samples reduces as the privacy at risk increases. The lower risk demands precision in the estimated sampled sensitivity, which in turn requires larger number of samples.</p><p>If the analytical form of the data-generation distribution is not known a priori, the empirical distribution of sensitivity can be estimated in two ways. The first way is to fit a known distribution on the available data and later use it to build an empirical distribution of the sensitivities. The second way is to sub-sample from a large dataset in order to build an empirical distribution of the sensitivities. In both of these ways, the empirical distribution of sensitivities captures the inherent randomness in the data-generation distribution. The first way suffers from the goodness of the fit of the known distribution to the available data. An ill-fit distribution does not reflect the true data-generation distribution and hence introduces errors in the sensitivity estimation. Since the second way involves subsampling, it is immune to this problem. The quality of sensitivity estimates obtained by sub-sampling the datasets depend on the availability of large population. Let, G denotes the data-generation distribution, either known apriori or constructed by subsampling the available data. We adopt the procedure of <ref type="bibr" target="#b37">[38]</ref> to sample two neighbouring datasets with p data points each. We sample p -1 data points from G that are common to both of these datasets and later two more data points, independently. From those two points, we allot one data point to each of the two datasets.</p><p>Let, S f = f (x) -f (y) 1 denotes the sensitivity random variable for a given query f , where x and y are two neighbouring datasets sampled from G. Using n pairs of neighbouring datasets sampled from G, we construct the empirical cumulative distribution, F n , for the sensitivity random variable. Definition 7. For a given query f and for a specified risk γ 2 , sampled sensitivity, ∆ S f , is defined as the value of sensitivity random variable that is estimated using its empirical cumulative distribution function, F n , constructed using n pairs of neighbouring datasets sampled from the data-generation distribution G.</p><formula xml:id="formula_29">∆ S f F -1 n (γ 2 )</formula><p>If we knew analytical form of the data generation distribution, we could analytically derive the cumulative distribution function of the sensitivity, F , and find the sensitivity of the query as ∆ f = F -1 (1). Therefore, in order to have the sampled sensitivity close to the sensitivity of the query, we require the empirical cumulative distributions to be close to the cumulative distribution of the sensitivity. We use this insight to derive the analytical bound in the Theorem 5.</p></div>
<div><head n="4.3">The Case of Explicit and Implicit Randomness</head><p>In this section, we study the combined effect of both explicit randomness induced by the noise distribution and implicit randomness in the data-generation distribution respectively. We do not assume the knowledge of the sensitivity of the query. We estimate sensitivity using the empirical cumulative distribution of sensitivity. We construct the empirical distribution over the sensitivities using the sampling technique presented in the earlier case. Since we use the sampled sensitivity (Definition 7) to calibrate the Laplace mechanism, we estimate the empirical risk γ3 .</p><p>For Laplace mechanism L ∆ S f ε0 calibrated with sampled sensitivity ∆ S f and privacy level ε 0 , we present the analytical bound on the empirical sensitivity γ3 in Theorem 6 with proof in the Appendix D.</p></div>
<div><head>Theorem 6. Analytical bound on the empirical risk</head><formula xml:id="formula_30">γ3 ∈ [0, 1] to achieve a privacy level ε &gt; 0 for Laplace mechanism L ∆ S f ε0 with sampled sensitivity ∆ S f of a query f : D → R k is γ3 ≥ γ 3 (1 -2e -2ρ 2 n ) (<label>6</label></formula><formula xml:id="formula_31">)</formula><p>where n is the number of samples used for estimating the sensitivity, ρ is the accuracy parameter. γ 3 denotes the specified absolute risk defined as:</p><formula xml:id="formula_32">γ 3 = P(T ≤ ε) P(T ≤ ηε 0 ) • γ 2</formula><p>Here, η is of the order of the ratio of the true sensitivity of the query to its sampled sensitivity.</p><p>The error parameter ρ controls the closeness between the empirical cumulative distribution of the sensitivity to the true cumulative distribution of the sensitivity. Figure <ref type="figure" target="#fig_3">3</ref> shows the dependence of the error parameter on the number of samples. In Figure <ref type="figure" target="#fig_3">3a</ref>, we observe that for a fixed number of samples and a privacy level, the privacy at risk decreases with the value of error parameter. For a fixed number of samples, smaller values of the error parameter reduce the probability of similarity between the empirical cumulative distribution of sensitivity and the true cumulative distribution. Therefore, we observe the reduction in the risk for a fixed privacy level. In Figure <ref type="figure" target="#fig_3">3b</ref>, we observe that for a fixed value of error parameter and a fixed level of privacy level, the risk increases with the number of samples. For a fixed value of the error parameter, larger values of the sample size increase the probability of similarity between the empirical cumulative distribution of sensitivity and the true cumulative distribution. Therefore, we observe the increase in the risk for a fixed privacy level.</p><p>Effect of the consideration of implicit and explicit randomness is evident in the analytical expression for γ 3 in Equation <ref type="formula" target="#formula_33">7</ref>. Proof is available in Appendix D. The privacy at risk is composed of two factors whereas the second term is a privacy at risk that accounts for inherent randomness. The first term takes into account the implicit randomness of the Laplace distribution along with a coupling coefficient η. We define η as the ratio of the true sensitivity of the query to its sampled sensitivity. We provide an approximation to estimate η in the absence of knowledge of the true sensitivity. It can be found in Appendix D.</p><formula xml:id="formula_33">γ 3 P(T ≤ ε) P(T ≤ ηε 0 ) • γ 2<label>(7)</label></formula></div>
<div><head n="5">Minimising Compensation Budget for Privacy at Risk</head><p>Many service providers collect users' data to enhance user experience. In order to avoid misuse of this data, we require a legal framework that not only limits the use of the collected data but also proposes reparative measures in case of a data leak. General Data Protection Regulation (GDPR) 4 is such a legal framework. Section 82 in GDPR states that any person who suffers from material or non-material damage as a result of a personal data breach has the right to demand compensation from the data processor. Therefore, every GDPR compliant business entity that either holds or processes personal data needs to secure a certain budget in the scenario of the personal data breach. In order to reduce the risk of such an unfortunate event, the business entity may use privacy-preserving mechanisms that provide provable privacy guarantees while publishing their results. In order to calculate the compensation budget for a business entity, we devise a cost model that maps the privacy guarantees provided by differential privacy and privacy at risk to monetary costs. The discussions demonstrate the usefulness of probabilistic quantification of differential privacy in a business setting.</p><p>4 https://gdpr-info.eu/</p></div>
<div><head n="5.1">Cost Model for Differential Privacy</head><p>Let E be the compensation budget that a business entity has to pay to every stakeholder in case of a personal data breach when the data is processed without any provable privacy guarantees. Let E dp ε be the compensation budget that a business entity has to pay to every stakeholder in case of a personal data breach when the data is processed with privacy guarantees in terms of ε-differential privacy.</p><p>Privacy level, ε, in ε-differential privacy is the quantifier of indistinguishability of the outputs of a privacypreserving mechanism when two neighbouring datasets are provided as inputs. When the privacy level is zero, the privacy-preserving mechanism outputs all results with equal probability. The indistinguishability reduces with increase in the privacy level. Thus, privacy level of zero bears the lowest risk of personal data breach and the risk increases with the privacy level. E dp ε needs to be commensurate to such a risk and, therefore, it needs to satisfy the following constraints.</p><formula xml:id="formula_34">1. For all ε ∈ R ≥0 , E dp ε ≤ E. 2. E dp ε is a monotonically increasing function of ε. 3. As ε → 0, E dp</formula><p>ε → E min where E min is the unavoidable cost that business entity might need to pay in case of personal data breach even after the privacy measures are employed. 4. As ε → ∞, E dp ε → E.</p><p>There are various functions that satisfy these constraints. In absence of any further constraints, we model E dp ε as defined in Equation ( <ref type="formula" target="#formula_35">8</ref>).</p><formula xml:id="formula_35">E dp ε E min + Ee -c ε . (<label>8</label></formula><formula xml:id="formula_36">)</formula><p>E dp ε has two parameters, namely c &gt; 0 and E min ≥ 0. c controls the rate of change in the cost as the privacy level changes and E min is a privacy level independent bias. For this study, we use a simplified model with c = 1 and E min = 0.</p></div>
<div><head n="5.2">Cost Model for Privacy at Risk</head><p>Let, E par ε0 (ε, γ) be the compensation that a business entity has to pay to every stakeholder in case of a personal data breach when the data is processed with an ε 0 -differentially private privacy-preserving mechanism along with a probabilistic quantification of privacy level. Use of such a quantification allows us to provide a stronger privacy guarantee viz. ε &lt; ε 0 for a specified privacy at risk at most γ. Thus, we calculate E par ε0 using Equation <ref type="formula">9</ref>.</p><formula xml:id="formula_37">E par ε0 (ε, γ) γE dp ε + (1 -γ)E dp ε0 (9)</formula><p>Note that the analysis in this section is specific to the cost model in Equation <ref type="formula" target="#formula_35">8</ref>. It naturally extends to any choice of convex cost model.</p></div>
<div><head n="5.2.1">Existence of Minimum Compensation Budget</head><p>We want to find the privacy level, say ε min , that yields the lowest compensation budget. We do that by minimising Equation <ref type="formula">9</ref>with respect to ε. <ref type="formula" target="#formula_35">8</ref>,</p></div>
<div><head>Lemma 4. For the choice of cost model in Equation</head><formula xml:id="formula_38">E par ε0 (ε, γ) is a convex function of ε.</formula><p>By Lemma 4, there exists a unique ε min that minimises the compensation budget for a specified parametrisation, say ε 0 . Since the risk γ in Equation <ref type="formula">9</ref>is itself a function of privacy level ε, analytical calculation of ε min is not possible in the most general case. When the output of the query is a real number, i. e. k = 1, we derive the analytic form (Equation <ref type="formula" target="#formula_25">4</ref>) to compute the risk under the consideration of explicit randomness. In such a case, ε min is calculated by differentiating Equation <ref type="formula">9</ref>with respect to ε and equating it to zero. It gives us Equation 10 that we solve using any root finding technique such as Newton-Raphson method <ref type="bibr" target="#b36">[37]</ref> to compute ε min .</p><formula xml:id="formula_39">1 ε -ln 1 - 1 -e ε ε 2 = 1 ε 0 (10)</formula></div>
<div><head n="5.2.2">Fine-tuning Privacy at Risk</head><p>For a fixed budget, say B, re-arrangement of Equation <ref type="formula">9</ref>gives us an upper bound on the privacy level ε. We use the cost model with c = 1 and E min = 0 to derive the upper bound. If we have a maximum permissible expected mean absolute error T , we use Equation <ref type="formula">12</ref>to obtain a lower bound on the privacy at risk level. Equation 11 illustrates the upper and lower bounds that dictate the permissible range of ε that a data publisher can promise depending on the budget and the permissible error constraints.</p><formula xml:id="formula_40">1 T ≤ ε ≤ ln γE B -(1 -γ)E dp ε0 -1<label>(11)</label></formula><p>Thus, the privacy level is constrained by the effectiveness requirement from below and by the mone- tary budget from above. <ref type="bibr" target="#b18">[19]</ref> calculate upper and lower bound on the privacy level in the differential privacy. They use a different cost model owing to the scenario of research study that compensates its participants for their data and releases the results in a differentially private manner. Their cost model is different than our GDPR inspired modelling.</p></div>
<div><head n="5.3">Illustration</head><p>Suppose that the health centre in a university that complies to GDPR publishes statistics of its staff health checkup, such as obesity statistics, twice in a year. In January 2018, the health centre publishes that 34 out of 99 faculty members suffer from obesity. In July 2018, the health centre publishes that 35 out of 100 faculty members suffer from obesity. An intruder, perhaps an analyst working for an insurance company, checks the staff listings in January 2018 and July 2018, which are publicly available on website of the university. The intruder does not find any change other than the recruitment of John Doe in April 2018. Thus, with high probability, the intruder deduces that John Doe suffers from obesity. In order to avoid such a privacy breach, the health centre decides to publish the results using the Laplace mechanism. In this case, the Laplace mechanism operates on the count query.</p><p>In order to control the amount of noise, the health centre needs to appropriately set the privacy level. Suppose that the health centre decides to use the expected mean absolute error, defined in Equation <ref type="formula">12</ref>, as the mea-sure of effectiveness for the Laplace mechanism.</p><formula xml:id="formula_41">E |L 1 ε (x) -f (x)| = 1 ε (12)</formula><p>Equation 12 makes use of the fact that the sensitivity of the count query is one. Suppose that the health centre requires the expected mean absolute error of at most two in order to maintain the quality of the published statistics. In this case, the privacy level has to be at least 0.5.</p><p>In order to compute the budget, the health centre requires an estimate of E. Moriarty et al. <ref type="bibr" target="#b29">[30]</ref> show that the incremental cost of premiums for the health insurance with morbid obesity ranges between $5467 to $5530. With reference to this research, the health centre takes $5500 as an estimate of E. For the staff size of 100 and the privacy level 0.5, the health centre uses Equation 8 in its simplified setting to compute the total budget of $74434.40.</p><p>Is it possible to reduce this budget without degrading the effectiveness of the Laplace mechanism? We show that it is possible by fine-tuning the Laplace mechanism. Under the consideration of the explicit randomness introduced by the Laplace noise distribution, we show that ε 0 -differentially private Laplace mechanism also satisfies ε-differential privacy with risk γ, which is computed using the formula in Theorem 4. Fine-tuning allows us to get a stronger privacy guarantee, ε &lt; ε 0 that requires a smaller budget. In Figure <ref type="figure" target="#fig_4">4</ref>, we plot the budget for various privacy levels. We observe that the privacy level 0.274, which is same as ε min computed by solving Equation 10, yields the lowest compensation budget of $37805.86. Thus, by using privacy at risk, the health centre is able to save $36628.532 without sacrificing the quality of the published results.</p></div>
<div><head n="5.4">Cost Model and the Composition of Laplace Mechanisms</head><p>Convexity of the proposed cost function enables us to estimate the optimal value of the privacy at risk level. We use the optimal privacy value to provide tighter bounds on the composition of Laplace mechanism. In Figure <ref type="figure" target="#fig_1">5</ref>, we compare the privacy guarantees obtained by using basic composition theorem <ref type="bibr" target="#b11">[12]</ref>, advanced composition theorem <ref type="bibr" target="#b11">[12]</ref> and the composition theorem for privacy at risk. We comparatively evaluate them for composition of Laplace mechanisms with privacy levels 0.1, 0.5 and 1.0. We compute the privacy level after composition by setting δ to 10 -5 . Basic Compositio <ref type="bibr" target="#b9">[10]</ref> Adva ced Compositio <ref type="bibr" target="#b9">[10]</ref> Compositio (ith Privac) at Risk (c) L 1 1.0 satisfies (0.42, 0.54)-privacy at risk.</p></div>
<div><head>Fig. 5.</head><p>Comparing the privacy guarantee obtained by basic composition and advanced composition <ref type="bibr" target="#b11">[12]</ref> with the composition obtained using optimal privacy at risk that minimises the cost of Laplace mechanism L 1 ε 0 . For the evaluation, we set δ = 10 -5 .</p><p>We observe that the use of optimal privacy at risk provided significantly stronger privacy guarantees as compared to the conventional composition theorems. Advanced composition theorem is known to provide stronger privacy guarantees for mechanism with smaller εs. As we observe in Figure <ref type="figure" target="#fig_1">5c</ref> and Figure <ref type="figure" target="#fig_1">5b</ref>, the composition provides strictly stronger privacy guarantees than basic composition, in the cases where the advanced composition fails.</p></div>
<div><head>Comparison with the Moment Accountant</head><p>Papernot et al. <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b33">34]</ref> empirically showed that the privacy guarantees provided by the advanced composition theorem are quantitatively worse than the ones achieved by the state-of-the-art moment accountant <ref type="bibr" target="#b0">[1]</ref>. The moment accountant evaluates the privacy guarantee by keeping track of various moments of privacy loss random variables. The computation of the moments is performed by using numerical methods on the specified dataset. Therefore, despite the quantitative strength of privacy guarantee provided by the moment accountant, it is qualitatively weaker, in a sense that it is specific to the dataset used for evaluation, in constrast to advanced composition.</p><p>Papernot et al. <ref type="bibr" target="#b32">[33]</ref> introduced the PATE framework that uses the Laplace mechanism to provide privacy guarantees for a machine learning model trained in an ensemble manner. We comparatively evaluate the privacy guarantees provided by their moment accountant on MNIST dataset with the privacy guarantees obtained using privacy at risk. We do so by using privacy at risk while computing a data dependent bound <ref type="bibr" target="#b32">[33,</ref><ref type="bibr">Theorem 3]</ref>. Under the identical experimental setup, we use a 0.1-differentially private Laplace mechanism, which optimally satisfies (0.08, 0.8)-privacy at risk. We list the calculated privacy guarantees in Table <ref type="table">1</ref>. The reported privacy guarantee is the mean privacy guarantee over 30 experiments.</p></div>
<div><head n="6">Balancing Utility and Privacy</head><p>In this section, we empirically illustrate and discuss the steps that a data steward needs to take and the issues that she needs to consider in order to realise a required privacy at risk level ε for a confidence level γ when seeking to disclose the result of a query.</p><p>We consider a query that returns the parameter of a ridge regression <ref type="bibr" target="#b30">[31]</ref> for an input dataset. It is a basic and widely used statistical analysis tool. We use the privacy-preserving mechanism presented by Ligett et al. <ref type="bibr" target="#b25">[26]</ref> for ridge regression. It is a Laplace mechanism that induces noise in the output parameters of the ridge regression. The authors provide a theoretical upper bound on the sensitivity of the ridge regression, which we refer as sensitivity, in the experiments.</p></div>
<div><head n="6.1">Dataset and Experimental Setup.</head><p>We conduct experiments on a subset of the 2000 US census dataset provided by Minnesota Population Center in its Integrated Public Use Microdata Series <ref type="bibr" target="#b38">[39]</ref>. The census dataset consists of 1% sample of the original census data. It spans over 1.23 million households with records of 2.8 million people. The value of several attributes is not necessarily available for every household. We have therefore selected 212, 605 records, corresponding to the household heads, and 6 attributes, namely,  <ref type="table">1</ref>. Comparative analysis of privacy levels computed using three composition theorems when applied to 0.1-differentially private Laplace mechanism, which optimally satisfies (0.08, 0.8)-privacy at risk. The observations for the moment accountant on MNIST datasets are taken from <ref type="bibr" target="#b32">[33]</ref>.</p><p>Age, Gender, Race, Marital Status, Education, Income, whose values are available for the 212, 605 records.</p><p>In order to satisfy the constraint in the derivation of the sensitivity of ridge regression <ref type="bibr" target="#b25">[26]</ref>, we, without loss of generality, normalise the dataset in the following way. We normalise Income attribute such that the values lie in [0, 1]. We normalise other attributes such that l 2 norm of each data point is unity.</p><p>All experiments are run on <software>Linux</software> machine with 12-core 3.60GHz Intel ® Core i7™processor with 64GB memory. Python ® 2.7.6 is used as the scripting language.</p></div>
<div><head n="6.2">Result Analysis</head><p>We train ridge regression model to predict Income using other attributes as predictors. We split the dataset into the training dataset (80%) and testing dataset (20%). We compute the root mean squared error (RMSE) of ridge regression, trained on the training data with regularisation parameter set to 0.01, on the testing dataset. We use it as the metric of utility loss. Smaller the value of RMSE, smaller the loss in utility. For a given value of privacy at risk level, we compute 50 runs of an experiment of a differentially private ridge regression and report the means over the 50 runs of the experiment.</p><p>Let us now provide illustrative experiments under the three different cases. In every scenario, the data steward is given a privacy at risk level ε and the confidence level γ and wants to disclose the parameters of a ridge regression model that she trains on the census dataset. She needs to calibrate the Laplace mechanism by estimating either its privacy level ε 0 (Case 1) or sensitivity (Case 2) or both (Case 3) to achieve the privacy at risk required the ridge regression query.</p><p>The Case of Explicit Randomness (cf. Section 4.1). In this scenario, the data steward knows the sensitivity for the ridge regression. She needs to compute the privacy level, ε 0 , to calibrate the Laplace mechanism. She uses Equation 3 that links the desired privacy at risk level ε, the confidence level γ 1 and the privacy level of noise ε 0 . Specifically, for given ε and γ 1 , she computes ε 0 by solving the equation:</p><formula xml:id="formula_42">γ 1 P(T ≤ ε 0 ) -P(T ≤ ε) = 0.</formula><p>Since the equation does not give an analytical formula for ε 0 , the data steward uses a root finding algorithm such as Newton-Raphson method <ref type="bibr" target="#b36">[37]</ref> to solve the above equation. For instance, if she needs to achieve a privacy at risk level ε = 0.4 with confidence level γ 1 = 0.6, she can substitute these values in the above equation and solve the equation to get the privacy level of noise ε 0 = 0.8.</p><p>Figure <ref type="figure" target="#fig_6">6</ref> shows the variation of privacy at risk level ε and confidence level γ 1 . It also depicts the variation of utility loss for different privacy at risk levels in Figure <ref type="figure" target="#fig_6">6</ref>.</p><p>In accordance to the data steward's problem, if she needs to achieve a privacy at risk level ε = 0.4 with confidence level γ 1 = 0.6, she obtains the privacy level of noise to be ε 0 = 0.8. Additionally, we observe that the choice of privacy level 0.8 instead of 0.4 to calibrate the Laplace mechanism gives lower utility loss for the data steward. This is the benefit drawn from the risk taken under the control of privacy at risk. Thus, she uses privacy level ε 0 and the sensitivity of the function to calibrate Laplace mechanism.</p><p>The Case of Implicit Randomness (cf. Section 4.2). In this scenario, the data steward does not know the sensitivity of ridge regression. She assesses that she can afford to sample at most n times from the population dataset. She understands the effect of the uncertainty introduced by the statistical estimation of the sensitivity. Therefore, she uses the confidence level for empirical privacy at risk γ2 .</p><p>Given the value of n, she chooses the value of the accuracy parameter using Figure <ref type="figure">2</ref>. For instance, if the number of samples that she can draw is 10 4 , she chooses the value of the accuracy parameter ρ = 0.01. Next, she uses Equation 13 to determine the value of probabilistic tolerance, α, for the sample size n. For instance, if the data steward is not allowed to access more than 15, 000  samples, for the accuracy of 0.01 the probabilistic tolerance is 0.9.</p><formula xml:id="formula_43">α = 1 -2e (-2ρ 2 n) (<label>13)</label></formula><p>She constructs an empirical cumulative distribution over the sensitivities as described in Section 4.2. Such an empirical cumulative distribution is shown in Figure <ref type="figure" target="#fig_7">7</ref>.</p><p>Using the computed probabilistic tolerance and desired confidence level γ2 , she uses equation in Theorem 5 to determine γ 2 . She computes the sampled sensitivity using the empirical distribution function and the confidence level for privacy ∆ S f at risk γ 2 . For instance, using the empirical cumulative distribution in Figure <ref type="figure" target="#fig_7">7</ref> she calculates the value of the sampled sensitivity to be approximately 0.001 for γ 2 = 0.4 and approximately 0.01 for γ 2 = 0.85 Thus, she uses privacy level ε, sets the number of samples to be n and computes the sampled sensitivity ∆ S f to calibrate the Laplace mechanism.</p><p>The Case of Explicit and Implicit Randomness (cf. Section 4.3). In this scenario, the data steward does not know the sensitivity of ridge regression. She is not allowed to sample more than n times from a population dataset. For a given confidence level γ 2 and the privacy at risk ε, she calibrates the Laplace mechanism using illustration for Section 4.3. The privacy level in this calibration yields utility loss that is more than her requirement. Therefore, she wants to re-calibrate the Laplace mechanism in order to reduce utility loss.</p><p>For the re-calibration, the data steward uses privacy level of the pre-calibrated Laplace mechanism, i.e. ε, as the privacy at risk level and she provides a new confidence level for empirical privacy at risk γ3 . Using Equation 25 and Equation <ref type="formula">23</ref>, she calculates:</p><formula xml:id="formula_44">γ3 P(T ≤ ηε 0 ) -αγ 2 P(T ≤ ε) = 0</formula><p>She solves such an equation for ε 0 using the root finding technique such as Newton-Raphson method <ref type="bibr" target="#b36">[37]</ref>. For instance, if she needs to achieve a privacy at risk level ε = 0.4 with confidence levels γ3 = 0.9 and γ 2 = 0.9, she can substitute these values and the values of tolerance parameter and sampled sensitivity, as used in the previous experiments, in the above equation. Then, solving the equation leads to the privacy level of noise ε 0 = 0.8.</p><p>Thus, she re-calibrates the Laplace mechanism with privacy level ε 0 , sets the number of samples to be n and sampled sensitivity ∆ S f .</p></div>
<div><head n="7">Related Work</head><p>Calibration of mechanisms. Researchers have proposed different privacy-preserving mechanisms to make different queries differentially private. These mechanisms can be broadly classified into two categories. In one category, the mechanisms explicitly add calibrated noise, such as Laplace noise in the work of <ref type="bibr" target="#b10">[11]</ref> or Gaussian noise in the work of <ref type="bibr" target="#b11">[12]</ref>, to the outputs of the query. In the other category, <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b40">41]</ref> propose mechanisms that alter the query function so that the modified function satisfies differentially privacy. Privacy-preserving mechanisms in both of these categories perturb the original output of the query and make it difficult for a malicious data analyst to recover the original output of the query. These mechanisms induce randomness us-ing the explicit noise distribution. Calibration of these mechanisms require the knowledge of the sensitivity of the query. Nissim et al. <ref type="bibr" target="#b31">[32]</ref> consider the implicit randomness in the data-generation distribution to compute an estimate of the sensitivity. The authors propose the smooth sensitivity function that is an envelope over the local sensitivities for all individual datasets. Local sensitivity of a dataset is the maximum change in the value of the query over all of its neighboring datasets. In general, it is not easy to analytically estimate the smooth sensitivity function for a general query. Rubinstein et al. <ref type="bibr" target="#b37">[38]</ref> also study the inherent randomness in the datageneration algorithm. We adopt their approach of sampling the sensitivity from the empirical distribution of the sensitivity. They use order statistics to choose a particular value of the sensitivity. We use the risk, which provides a mediation tool for business entities to assess the actual business risks, on the sensitivity distribution to estimate the sensitivity.</p><p>Refinements of differential privacy. In order to account for both sources of randomness, refinements of ε-differential privacy are proposed in order to bound the probability of occurrence of worst case scenarios. Machanavajjhala et al. <ref type="bibr" target="#b26">[27]</ref> propose probabilistic differential privacy that considers upper bounds of the worst case privacy loss for corresponding confidence levels on the noise distribution. Definition of probabilistic differential privacy incorporates the explicit randomness induced by the noise distribution and bounds the probability over the space of noisy outputs to satisfy the ε-differential privacy definition. Dwork et al. <ref type="bibr" target="#b12">[13]</ref> propose Concentrated differential privacy that considers the expected values of the privacy loss random variables for the corresponding. Definition of concentrated differential privacy incorporates the explicit randomness induced by the noise distribution but considering only the expected value of privacy loss satisfying ε-differential privacy definition instead of using the confidence levels limits its scope.</p><p>Hall et al. <ref type="bibr" target="#b16">[17]</ref> propose random differential privacy that considers the privacy loss for corresponding confidence levels on the implicit randomness in the datageneration distribution. Definition of random differential privacy incorporates the implicit randomness induced by the data-generation distribution and bounds the probability over the space of datasets generated from the given distribution to satisfy the ε-differential privacy definition. Dwork et al. <ref type="bibr" target="#b8">[9]</ref> define approximate differential privacy by adding a constant bias to the privacy guarantee provided by the differential privacy. It is not a probabilistic refinement of the differential privacy.</p><p>Around the same time of our work, Triastcyn et al. <ref type="bibr" target="#b39">[40]</ref> independently propose Bayesian differential privacy that takes into account both of the sources of randomness. Despite this similarity, our works differ in multiple dimensions. Firstly, they have shown the reduction of their definition to a variant of Renyi differential privacy. The variant depends on the data-generation distribution. Secondly, they rely on the moment accountant for the composition of the mechanisms. Lastly, they do not provide a finer case-by-case analysis of the source of randomness, which leads to analytical solutions for the privacy guarantee.</p><p>Kifer et al. <ref type="bibr" target="#b23">[24]</ref> define Pufferfish privacy framework, and its variant by Bassily et al. <ref type="bibr" target="#b3">[4]</ref>, that considers randomness due to data-generation distribution as well as noise distribution. Despite the generality of their approach, the framework relies on the domain expert to define a set of secrets that they want to protect.</p><p>We refer interested readers to <ref type="bibr" target="#b7">[8]</ref> for an extensive review of the differential privacy and its refinements.</p><p>Composition theorem. Recently proposed technique of the moment accountant <ref type="bibr" target="#b0">[1]</ref> has become the state-of-the-art of composing mechanisms in the area of privacy-preserving machine learning. <ref type="bibr">Abadi et al.</ref> show that the moment accountant provides much strong privacy guarantees than the conventional composition mechanisms. It works by keeping track of various moments of privacy loss random variable and use the bounds on them to provide privacy guarantees. The moment accountant requires access to data-generation distribution to compute the bounds on the moment. Hence, the privacy guarantees are specific to the dataset.</p><p>Cost models. <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b14">15]</ref> propose game theoretic methods that provide the means to evaluate the monetary cost of differential privacy. Pejó et al. <ref type="bibr" target="#b35">[36]</ref> also propose a game theoretic cost model in the setting of private collaborative learning. Our approach is inspired by the approach in the work of Hsu et al. <ref type="bibr" target="#b18">[19]</ref>. They model the cost under a scenario of a research study wherein the participants are reimbursed for their participation. Our cost modelling is driven by the scenario of securing a compensation budget in compliance with GDPR. Our requirement differs from the requirements of <ref type="bibr" target="#b18">[19]</ref>, particularly in our model participants do not have any monetary incentive to share their data.</p></div>
<div><head n="8">Conclusion and Future Works</head><p>In this paper, we provide a means to fine-tune the privacy level of a privacy-preserving mechanism by analysing various sources of randomness. Such a finetuning leads to probabilistic quantification on privacy levels with quantified risks, which we call as privacy at risk. We also provide composition theorem that leverages privacy at risk. We analytical calculate privacy at risk for Laplace mechanism. We propose a cost model that bridges the gap between the privacy level and the compensation budget estimated by a GDPR compliant business entity. Convexity of the cost function ensures existence of unique privacy at risk that minimises compensation budget. The cost model helps in not only reinforcing the ease of application in a business setting but also providing stronger privacy guarantees on the composition of mechanism.</p><p>It is possible to instantiate privacy at risk for Gaussian mechanism. The mechanism is (ε, δ)-differential private for γ = 0 and a non-zero risk calculated by accounting for the sources of randomness. We save it for future work. Privacy at risk may be fully analytically computed in cases where the data-generation, or the sensitivity distribution, the noise distribution and the query are analytically known and take convenient forms. We are now looking at such convenient but realistic cases.</p></div>
<div><head>A Proof of Theorem 2 (Section 3)</head><p>Proof. Let us fix a pair of neighbouring datasets x and y, and also fix a subset of outputs Z ⊆ Range(M). Choose OUT ⊆ Z such that</p><formula xml:id="formula_45">OUT z ∈ Z P(M(f, Θ)(x) = z) P(M(f, Θ)(y) = z) &gt; e ε</formula><p>Thus, we also obtain a complementary set of outputs Z \ OUT, where the privacy loss is upper bounded i.e.</p><p>P(M(f, Θ)(x) = z) ≤ e ε P(M(f, Θ)(y) = z). Thus, </p><formula xml:id="formula_46">P(M(f, Θ)(x) ∈ Z) = P(M(f, Θ)(x) ∈ Z\OUT) + P(M(f, Θ)(x) ∈ OUT) ≤ (a) e ε P(M(f, Θ)(y) ∈ Z\OUT) + P(M(f, Θ)(x) ∈ OUT) ≤ (b) e ε P(M(f, Θ)(y) ∈ Z \ OUT) + γ ≤ e ε P(M(f, Θ)(y) ∈ Z) + γ (a)</formula></div>
<div><head>B Proof of Theorem 4 (Section 4.1)</head><p>Although a Laplace mechanism L ∆ f ε induces higher amount of noise on average than a Laplace mechanism L ∆ f ε0 for ε &lt; ε 0 , there is a non-zero probability that L ∆ f ε induces noise commensurate to L ∆ f ε0 . This non-zero probability guides us to calculate the privacy at risk γ 1 for the privacy at risk level ε. In order to get an intuition, we illustrate the calculation of the overlap between two Laplace distributions as an estimator of similarity between the two distributions. Definition 8. [Overlap of Distributions, <ref type="bibr" target="#b34">[35]</ref>] The overlap, O, between two probability densities P 1 , P 2 with support X is defined as</p><formula xml:id="formula_47">O = X min[P 1 (x), P 2 (x)] dx.</formula></div>
<div><head>Lemma 5. The overlap O between two probability distributions, Lap(</head><formula xml:id="formula_48">∆ f ε1 ) and Lap( ∆ f ε2 ), such that ε 2 ≤ ε 1 , is given by O = 1 -(exp (-µε 2 /∆ f ) -exp (-µε 1 /∆ f )), where µ = ∆ f ln (ε1/ε2) ε1-ε2</formula><p>.</p><p>Using the result in Lemma 5, we note that the overlap between two distributions with ε 0 = 1 and ε = 0.6 is 0.81. Thus, L ∆ f 0.6 induces noise that is more than 80% times similar to the noise induced by L ∆ f 1.0 . Therefore, we can loosely say that at least 80% of the times a Laplace Mechanism L ∆ f 1.0 will provide the same privacy as a Laplace Mechanism L ∆ f 0.8 . Although the overlap between Laplace distributions with different scales offers an insight into the relationship between different privacy levels, it does not capture the constraint induced by the sensitivity. For a given query f , the amount of noise required to satisfy differential privacy is commensurate to the sensitivity of the query. This calibration puts a constraint on the noise that is required to be induced on a pair of neighbouring datasets. We state this constraint in Lemma 6, which we further use to prove that the Laplace Mechanism L </p><formula xml:id="formula_49">→ R k . For any output z ∈ R k of the Laplace Mechanism L ∆ f ε0 , k i=1 (|f (y i ) -z i | -|f (x i ) -z i |) ≤ k i=1 (|f (x i ) -f (y i )|) ≤ ∆ f .</formula><p>We use triangular inequality in the first step and Definition 2 of sensitivity in the second step.</p><p>We write Exp(b) to denote a random variable sampled from an exponential distribution with scale b &gt; 0. We write Gamma(k, θ) to denote a random variable sampled from a gamma distribution with shape k &gt; 0 and scale θ &gt; 0. </p></div>
<div><head>Lemma 7. [[35]] If a random variable</head><formula xml:id="formula_50">P T (t; n, θ) = 2 2-n t n-1 2 K n-1 2 (t) √ 2πΓ(n)θ where K n-1 2</formula><p>is the modified Bessel function of second kind.</p><p>Proof. Let X 1 and X 2 be two i.i.d. Gamma(n, θ) random variables. Characteristic function of a Gamma random variable is given as</p><formula xml:id="formula_51">φ X1 (z) = φ X2 (z) = (1 -ιzθ) -n . Therefore, φ X1-X2 (z) = φ X1 (z)φ * X2 (z) = 1 (1 + (zθ) 2 ) n</formula><p>Probability density function for the random variable X 1 -X 2 is given by,</p><formula xml:id="formula_52">P X1-X2 (x) = 1 2π ∞ -∞ e -izx φ X1-X2 (z)dz = 2 1-n | x θ | n-1 2 K n-1 2 (| x θ |) √ 2πΓ(n)θ where K n-1 2</formula><p>is the Bessel function of second kind. Let</p><formula xml:id="formula_53">T = | X1-X2 θ |. Therefore, P T (t; n, θ) = 2 1-n t n-1 2 K n-1 2 (t) √ 2πΓ(n)θ</formula><p>We use <software ContextAttributes="used">Mathematica</software> <ref type="bibr" target="#b19">[20]</ref> to solve the above integral. </p><formula xml:id="formula_54">∆ f ε0 ), ε ≤ ε 0 , γ 1 P ln P(L ∆ f ε0 (x) ∈ Z) P(L ∆ f ε0 (y) ∈ Z) ≤ ε = P(T ≤ ε) P(T ≤ ε 0 )</formula><p>, where T follows the distribution in Lemma 9, P T (t; k,</p><formula xml:id="formula_55">∆ f ε0 ).</formula><p>Proof. Let, x ∈ D and y ∈ D be two datasets such that x ∼ y. Let f : D → R k be some numeric query. Let P x (z) and P y (z) denote the probabilities of getting the output z for Laplace mechanisms L ∆ f ε0 (x) and L ∆ f ε0 (y) respectively. For any point z ∈ R k and ε = 0,</p><formula xml:id="formula_56">P x (z) P y (z) = k i=1 exp -ε0|f (xi)-zi| ∆ f exp -ε0|f (yi)-zi| ∆ f = k i=1 exp ε 0 (|f (y i ) -z i | -|f (x i ) -z i |) ∆ f = exp ε ε 0 k i=1 (|f (y i ) -z i | -|f (x i ) -z i |) ε∆ f . (<label>14</label></formula><formula xml:id="formula_57">)</formula><p>By Definition 4,</p><formula xml:id="formula_58">(f (x) -z), (f (y) -z) ∼ Lap(∆ f /ε 0 ).<label>(15)</label></formula><p>Application of Lemma 7 and Lemma 8 yields,</p><formula xml:id="formula_59">k i=1 (|f (x i ) -z i |) ∼ Gamma(k, ∆ f /ε 0 ).<label>(16)</label></formula><p>Using Equations 15, 16, and Lemma 6, 10, we get</p><formula xml:id="formula_60">ε 0 ∆ f k i=1 |(|f (y i ) -z| -|f (x i ) -z|)| ∼ P T (t; k, ∆ f /ε 0 , ∆ f ). (17) since, k i=1 |(|f (y i ) -z| -|f (x i ) -z|)| ≤ ∆ f . There- fore, P ε 0 ∆ f k i=1 |(|f (y i ) -z| -|f (x i ) -z|)| ≤ ε = P(T ≤ ε) P(T ≤ ε 0 ) ,<label>(18)</label></formula><p>where T follows the distribution in Lemma 9. We use Mathematica <ref type="bibr" target="#b19">[20]</ref> to analytically compute,</p><formula xml:id="formula_61">P(T ≤ x) ∝ 1 F 2 ( 1 2 ; 3 2 -k, 3 2 ; x 2 4 ) √ π4 k x] - 2 1 F 2 (k; 1 2 + k, k + 1; x 2 4 )x 2k Γ(k)</formula><p>where 1 F 2 is the regularised generalised hypergeometric function as defined in <ref type="bibr" target="#b2">[3]</ref>. From Equation <ref type="formula" target="#formula_56">14</ref> </p><formula xml:id="formula_62">∆ S f = exp ε k i=1 (|f (y i ) -z i | -|f (x i ) -z i |) ∆ S f ≤ exp ε k i=1 |f (y i ) -f (x i )| ∆ S f = exp ε f (y) -f (x) 1 ∆ S f (<label>19</label></formula><formula xml:id="formula_63">)</formula><p>We used triangle inequality in the penultimate step. Using the trick in the work of <ref type="bibr" target="#b37">[38]</ref>, we define following events. Let, B ∆ S f denotes the set of pairs neighbouring dataset sampled from G for which the sensitivity random variable is upper bounded by ∆ S f . Let, C</p><p>∆ S f ρ denotes the set of sensitivity random variable values for which F n deviates from the unknown cumulative distribution of S, F , at most by the accuracy value ρ. These events are defined in Equation <ref type="formula">20</ref>.</p><formula xml:id="formula_64">B ∆ S f {x, y ∼ G such that f (y) -f (x) 1 ≤ ∆ S f } C ∆ S f ρ sup ∆ |F n S (∆) -F S (∆)| ≤ ρ (20)</formula><p>Thus, we obtain</p><formula xml:id="formula_65">P B ∆ S f = P B ∆ S f C ∆ S f ρ P C ∆ S f ρ + P B ∆ S f C ∆ S f ρ P C ∆ S f ρ ≥ P B ∆ S f C ∆ S f ρ P C ∆ S f ρ = F n (∆ S f )P C ∆ S f ρ ≥ γ 2 • 1 -2e -2ρ 2 n (21)</formula><p>In the last step, we use the definition of the sampled sensitivity to get the value of the first term. The last term is obtained using DKW-inequality, as defined in <ref type="bibr" target="#b27">[28]</ref>, where the n denotes the number of samples used to build empirical distribution of the sensitivity, F n . From Equation <ref type="formula" target="#formula_62">19</ref>, we understand that if f (y)f (x) 1 is less than or equals to the sampled sensitivity then the Laplace mechanism L ∆ S f ε satisfies ε-differential privacy. Equation <ref type="formula">21</ref>provides the lower bound on the probability of the event f (y) -f (x) 1 ≤ ∆ S f . Thus, combining Equation 19 and Equation 21 completes the proof.</p></div>
<div><head>D Proof of Theorem 6 (Section 4.3)</head><p>Proof of Theorem 6 builds upon the ideas from the proofs for the rest of the two cases. In addition to the events defined in Equation <ref type="formula">20</ref>, we define an additional event A</p><p>∆ S f ε0 , defined in Equation <ref type="formula">22</ref>, as a set of outputs of Laplace mechanism L ∆ S f ε0 that satisfy the constraint of ε-differential privacy for a specified privacy at risk level ε. where T follows the distribution P T (t; ∆ S f /ε 0 ) and η = ∆ f ∆ S f . For this case, we do not assume the knowledge of the sensitivity of the query. Using the empirical estimation presented in Section 4.2, if we choose the sampled sensitivity for privacy at risk γ 2 = 1, we obtain an approximation for η. |F n (F -1 n (1)) -</p><formula xml:id="formula_66">F n (F -1 (1))| ≤ |F -1 n (1) -F -1 (1)| |F n (F -1 n (1)) -F n (F -1 (1))| ≤ |∆ * S f -∆ f | ρ ≤ ∆ f -∆ * S f 1 + ρ ∆ * S f ≤ ∆ f ∆ * S f</formula><p>where we used result from Equation <ref type="formula">23</ref> where n is the number of samples used to find sampled sensitivity, ρ ∈ [0, 1] is a accuracy parameter and η =</p><formula xml:id="formula_67">∆ f ∆ S f</formula><p>. The outer probability is calculated with respect to support of the data-generation distribution G.</p><p>Proof. The proof follows from the proof of Lemma 11 and Lemma 13. Consider,</p><formula xml:id="formula_68">P(A ∆ S f ε0 ) ≥ P(A ∆ S f ε0 |B ∆ S f )P(B ∆ S f |C ∆ S f ρ )P(C ∆ S f ρ ) ≥ P(T ≤ ε) P(T ≤ ηε 0 ) • γ 2 • (1 -2e -2ρ 2 n )<label>(24)</label></formula><p>The first term in the final step of Equation <ref type="formula" target="#formula_68">24</ref>follows from the result in Corollary 4 where T follows BesselK(k,</p><formula xml:id="formula_69">∆ S f ε0</formula><p>). It is the probability with which the Laplace mechanism L ∆ S f ε0 satisfies ε-differential privacy for a given value of sampled sensitivity.</p></div>
<div><head>Probability of occurrence of event A</head><p>∆ S f ε0 calculated by accounting for both explicit and implicit sources of randomness gives the risk for privacy level ε. Thus, the proof of Lemma 13 completes the proof for Theorem 6.</p><p>Comparing the equations in Theorem 6 and Lemma 13, we observe that</p><formula xml:id="formula_70">γ 3 P(T ≤ ε) P(T ≤ ηε 0 ) • γ 2<label>(25)</label></formula><p>The privacy at risk, as defined in Equation <ref type="formula" target="#formula_70">25</ref>, is free from the term that accounts for the accuracy of sampled estimate. If we know cumulative distribution of the sensitivity, we do not suffer from the uncertainty of introduced by sampling from the empirical distribution.</p></div><figure xml:id="fig_0"><head /><label /><figDesc>the i th component of f (x).</figDesc></figure>
<figure xml:id="fig_1"><head>Theorem 5 .</head><label>5</label><figDesc>Analytical bound on the empirical risk, γ2 , for Laplace mechanism L ∆ S f ε with privacy level ε and sampled sensitivity ∆ S f for a query</figDesc></figure>
<figure xml:id="fig_2"><head>Fig. 1 .Fig. 2 .</head><label>12</label><figDesc>Fig.1. Privacy level ε for varying privacy at risk γ 1 for Laplace mechanism L 1.0 ε 0 . In Figure1a, we use ε 0 = 1.0 and different values of k. In Figure1b, for k = 1 and different values of ε 0 .</figDesc></figure>
<figure xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Dependence of error and number of samples on the privacy at risk for Laplace mechanism L ∆ S f 1.0 . For the figure on the left hand side, we fix the number of samples to 10000. For the Figure 3b we fix the error parameter to 0.01.</figDesc></figure>
<figure xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Variation in the budget for Laplace mechanism L 1 ε 0 under privacy at risk considering explicit randomness in the Laplace mechanism for the illustration in Section 5.3.</figDesc></figure>
<figure xml:id="fig_5"><head /><label /><figDesc>for ε0 = 0.10, δ = 10(5   Basic Com osition<ref type="bibr" target="#b9">[10]</ref> Advanced Com osition<ref type="bibr" target="#b9">[10]</ref> Com osition with Privacy at Risk (a) L 1 0.1 satisfies (0.08, 0.80)-privacy at risk. le(el after co position (ε ′ ) Ad(anced co position for ε0 = 0.50, δ = 10 -5 Basic Co position[10] Ad(anced Co position[10] Co position )ith Pri(acy at Risk (b) L 1 0.5 satisfies (0.27, 0.61)-privacy at risk. for ε0 = 1.00, δ = 10 -5</figDesc></figure>
<figure xml:id="fig_6"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Utility, measured by RMSE (right y-axis), and privacy at risk level ε for Laplace mechanism (left y-axis) for varying confidence levels γ 1 .</figDesc><graphic coords="15,48.31,65.20,240.95,179.66" type="bitmap" /></figure>
<figure xml:id="fig_7"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Empirical cumulative distribution of the sensitivities of ridge regression queries constructed using 15000 samples of neighboring datasets.</figDesc><graphic coords="15,302.33,66.10,228.90,177.85" type="bitmap" /></figure>
<figure xml:id="fig_8"><head /><label /><figDesc>is obtained from the fact that the privacy loss is upper bounded in the complimentary set of outputs Z \ OUT. (b) is a consequence of the definition of (ε, γ) privacy at risk. The definition upper bounds the probability measures of the subset OUT by γ.</figDesc></figure>
<figure xml:id="fig_9"><head>Lemma 6 .</head><label>6</label><figDesc>∆ f ε0satisfies (ε, γ 1 )-privacy at risk. For a Laplace Mechanism L ∆ f ε0 , the difference in the absolute values of noise induced on a pair of neighbouring datasets is upper bounded by the sensitivity of the query.Proof. Suppose that two neighbouring datasets x and y are given input to a numeric query f : D</figDesc></figure>
<figure xml:id="fig_10"><head>Corollary 3 . 2 )</head><label>32</label><figDesc>and 18, P ln P(L ∆ f ε0 (x) ∈ S) P(L ∆ f ε0 (y) ∈ S) ≤ ε = P(T ≤ ε) P(T ≤ ε 0 ) . This completes the proof of Theorem 4. Laplace Mechanism L ∆ f ε0 with f : D → R k satisfies (ε, δ)-probabilistic differentially private where δ = 1 -P(T ≤ε) P(T ≤ε0) ε ≤ ε 0 0 ε &gt; ε 0 and T follows BesselK(k, ∆ f /ε 0 ). Proof. Let, x and y be any two neighbouring datasets sampled from the data generating distribution G. Let, ∆ S f be the sampled sensitivity for query f : D → R k . Let, P x (z) and P y (z) denote the probabilities of getting the output z for Laplace mechanisms L ∆ S f ε (x) and L ∆ S f ε (y) respectively. For any point z ∈ R k and ε = 0, P x (z) P y (z) = k i=1 exp -ε|f (xi)-zi| ∆ S f exp -ε|f (yi)-zi|</figDesc></figure>
<figure xml:id="fig_11"><head>A</head><label /><figDesc>≤ ε) P(T ≤ ηε 0 )where T follows the distribution P T (t; ∆ S f /ε 0 ) in Lemma 9 and η = ∆ f ∆ S f . Proof. We provide the sketch of the proof. Proof follows from the proof of Lemma 11. For a Laplace mechanism calibrated with the sampled sensitivity ∆ S f and privacy level ε 0 , Equation 17 translates to,ε 0 ∆ S f k i=1 |(|f (y i ) -z| -|f (x i ) -z|)| ∼ P T (t; k, ∆ S f /ε 0 , ∆ S f ). since, k i=1 |(|f (y i ) -z| -|f (x i ) -z|)| ≤ ∆ f .Using Lemma 10 and Equation 18, ≤ ε) P(T ≤ ηε 0 )</figDesc></figure>
<figure xml:id="fig_12"><head>Lemma 12 .for some k ≥ 1 .</head><label>121</label><figDesc>For a given value of accuracy parameter ρ,Proof. For a given value of accuracy parameter ρ and any ∆ &gt; 0,F n (∆) -F (∆) ≤ ρSince above inequality is true for any value of ∆, let ∆ = F -1 (1). Therefore,F n (F -1 (1)) -F (F -1 (1)) ≤ ρ F n (F -1 (1)) ≤ 1 + ρ (23)Since a cumulative distribution function is 1-Lipschitz [<ref type="bibr" target="#b34">[35]</ref>],</figDesc></figure>
<figure xml:id="fig_13"><head>in step 3 Lemma 13 .</head><label>313</label><figDesc>For Laplace Mechanism L ∆ S f ε0 with sampled sensitivity ∆ S f of a query f : D → R k and for anyZ ⊆ Range(L ∆ S f ε ), P ln P(L ε0 (x) ∈ Z) P(L ε0 (y) ∈ Z) ≤ ε ≥ P(T ≤ ε) P(T ≤ ηε 0 ) γ 2 (1-2e -2ρ 2 n )</figDesc></figure>
<figure type="table" xml:id="tab_1"><head /><label /><figDesc>X follows Laplace Distribution with mean zero and scale b, |X| ∼ Exp(b). If X 1 and X 2 are two i.i.d. Gamma(n, θ) random variables, the probability density function for the random variable T = |X 1 -X 2 |/θ is given by</figDesc><table><row><cell>Lemma 9.</cell></row><row><cell>Lemma 8. [[35]] If X 1 , ..., X n are n i.i.d. random vari-</cell></row><row><cell>ables each following the Exponential Distribution with</cell></row><row><cell>scale b,</cell></row></table><note><p>n i=1 X i ∼ Gamma(n, b).</p></note></figure>
			<note place="foot" n="1" xml:id="foot_0"><p>A widely-used (ε, δ)-differential privacy is not a probabilistic relaxation of differential privacy<ref type="bibr" target="#b28">[29]</ref>.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgement</head><p>This work was supported by the <rs type="funder">National Research Foundation (NRF) Singapore</rs> under its <rs type="institution">Corporate Laboratory@University Scheme</rs>, <rs type="funder">National University of Singapore</rs>, and <rs type="funder">Singapore Telecommunications Ltd</rs>. This research was also funded in part by the <rs type="projectName">BioQOP</rs> project of the <rs type="funder">French ANR</rs> (<rs type="grantNumber">ANR-17-CE39-0006</rs>). We thank <rs type="person">Pierre Senellart</rs> for his help in reviewing derivations of privacy at risk for the Laplace mechanism.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_RtA6xjH">
					<orgName type="project" subtype="full">BioQOP</orgName>
				</org>
				<org type="funding" xml:id="_8k4Mwe3">
					<idno type="grant-number">ANR-17-CE39-0006</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep learning with differential privacy</title>
		<author>
			<persName><forename type="first">Martin</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Brendan Mcmahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Mironov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kunal</forename><surname>Talwar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security</title>
		<meeting>the 2016 ACM SIGSAC Conference on Computer and Communications Security</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="308" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Differentially private histogram publishing through lossy compres-sion</title>
		<author>
			<persName><forename type="first">Gergely</forename><surname>Acs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claude</forename><surname>Castelluccia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 12th International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012">2012. 2012</date>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
	<note>Data Mining (ICDM)</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Generalized hypergeometric functions and meijer g-function</title>
		<author>
			<persName><surname>Ra Askey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daalhuis</forename><surname>Olde</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIST handbook of mathematical functions</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="403" to="418" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Coupled-worlds privacy: Exploiting adversarial uncertainty in statistical data privacy</title>
		<author>
			<persName><forename type="first">Raef</forename><surname>Bassily</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Groce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 54th Annual Symposium on Foundations of Computer Science</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013. 2013</date>
			<biblScope unit="page" from="439" to="448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Concentrated differential privacy: Simplifications, extensions, and lower bounds</title>
		<author>
			<persName><forename type="first">Mark</forename><surname>Bun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Steinke</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Differentially private empirical risk minimization</title>
		<author>
			<persName><forename type="first">Kamalika</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claire</forename><surname>Monteleoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anand</forename><forename type="middle">D</forename><surname>Sarwate</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="1069" to="1109" />
			<date type="published" when="2011-03">Mar. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Truthful mechanisms for agents that value privacy</title>
		<author>
			<persName><forename type="first">Yiling</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Chong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><forename type="middle">A</forename><surname>Kash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tal</forename><surname>Moran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Salil</forename><surname>Vadhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Economics and Computation (TEAC)</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Sok: Differential privacies</title>
		<author>
			<persName><forename type="first">Damien</forename><surname>Desfontaines</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Balázs</forename><surname>Pejó</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings on Privacy Enhancing Technologies</title>
		<meeting>on Privacy Enhancing Technologies</meeting>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="page" from="288" to="313" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Our data, ourselves: Privacy via distributed noise generation</title>
		<author>
			<persName><forename type="first">Cynthia</forename><surname>Dwork</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krishnaram</forename><surname>Kenthapadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Mcsherry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Mironov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moni</forename><surname>Naor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eurocrypt</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">4004</biblScope>
			<biblScope unit="page" from="486" to="503" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Calibrating noise to sensitivity in private data analysis</title>
		<author>
			<persName><forename type="first">Cynthia</forename><surname>Dwork</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Mcsherry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kobbi</forename><surname>Nissim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Theory of Cryptography Conference</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="265" to="284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Calibrating Noise to Sensitivity in Private Data Analysis</title>
		<author>
			<persName><forename type="first">Cynthia</forename><surname>Dwork</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Mcsherry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kobbi</forename><surname>Nissim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Smith</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>Springer</publisher>
			<biblScope unit="page" from="265" to="284" />
			<pubPlace>Berlin Heidelberg, Berlin, Heidelberg</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The algorithmic foundations of differential privacy</title>
		<author>
			<persName><forename type="first">Cynthia</forename><surname>Dwork</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends® in Theoretical Computer Science</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="211" to="407" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">Cynthia</forename><surname>Dwork</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guy</forename><forename type="middle">N</forename><surname>Rothblum</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.01887</idno>
		<title level="m">Concentrated differential privacy</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Simson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">M</forename><surname>Garfinkel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarah</forename><surname>Abowd</surname></persName>
		</author>
		<author>
			<persName><surname>Powazek</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.02201</idno>
		<title level="m">Issues encountered deploying differential privacy</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Selling privacy at auction</title>
		<author>
			<persName><forename type="first">Arpita</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Games and Economic Behavior</title>
		<imprint>
			<biblScope unit="volume">91</biblScope>
			<biblScope unit="page" from="334" to="346" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Random differential privacy</title>
		<author>
			<persName><forename type="first">Rob</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Rinaldo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Larry</forename><surname>Wasserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Privacy and Confidentiality</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="43" to="59" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Differential privacy for functions and functional data</title>
		<author>
			<persName><forename type="first">Rob</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Rinaldo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Larry</forename><surname>Wasserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="703" to="727" />
			<date type="published" when="2013-02">Feb. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Probability inequalities for sums of bounded random variables</title>
		<author>
			<persName><forename type="first">Wassily</forename><surname>Hoeffding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Collected Works of Wassily Hoeffding</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="409" to="426" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Differential privacy: An economic method for choosing epsilon</title>
		<author>
			<persName><forename type="first">Justin</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Gaboardi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Haeberlen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjeev</forename><surname>Khanna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arjun</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><forename type="middle">C</forename><surname>Pierce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Security Foundations Symposium (CSF)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014">2014. 2014</date>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="398" to="410" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title />
	</analytic>
	<monogr>
		<title level="j">Inc. Mathematica</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<date type="published" when="2014">2014</date>
			<publisher>Champaign</publisher>
			<pubPlace>IL</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Wolfram Research</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Value at risk: The new benchmark for managing financial risk</title>
		<author>
			<persName><forename type="first">Philippe</forename><surname>Jorion</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="volume">01</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The composition theorem for differential privacy</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Kairouz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sewoong</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pramod</forename><surname>Viswanath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1376" to="1385" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">An axiomatic view of statistical privacy and utility</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Kifer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing-Rong</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Privacy and Confidentiality</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A rigorous and customizable framework for privacy</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Kifer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashwin</forename><surname>Machanavajjhala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st ACM SIGMOD-SIGACT-SIGAI symposium on Principles of Database Systems</title>
		<meeting>the 31st ACM SIGMOD-SIGACT-SIGAI symposium on Principles of Database Systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="77" to="88" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">How much is enough? choosing ε for differential privacy</title>
		<author>
			<persName><forename type="first">Jaewoo</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Clifton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Information Security</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="325" to="340" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Accuracy first: Selecting a differential privacy level for accuracy constrained erm</title>
		<author>
			<persName><forename type="first">Katrina</forename><surname>Ligett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seth</forename><surname>Neel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Waggoner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><forename type="middle">Z</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2563" to="2573" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Privacy: Theory meets practice on the map</title>
		<author>
			<persName><forename type="first">Ashwin</forename><surname>Machanavajjhala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Kifer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Abowd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Gehrke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lars</forename><surname>Vilhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDE 2008. IEEE 24th International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008">2008. 2008</date>
			<biblScope unit="page" from="277" to="286" />
		</imprint>
	</monogr>
	<note>Data Engineering</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">The tight constant in the dvoretzkykiefer-wolfowitz inequality</title>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Massart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The annals of Probability</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1269" to="1283" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Approximate and probabilistic differential privacy definitions</title>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Meiser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IACR Cryptology ePrint Archive</title>
		<imprint>
			<biblScope unit="page">277</biblScope>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">The effects of incremental costs of smoking and obesity on health care costs among adults: a 7-year longitudinal study</title>
		<author>
			<persName><forename type="first">Megan</forename><forename type="middle">E</forename><surname>James P Moriarty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kerry</forename><forename type="middle">D</forename><surname>Branda</surname></persName>
		</author>
		<author>
			<persName><surname>Olsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Nilay</surname></persName>
		</author>
		<author>
			<persName><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bijan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amy</forename><forename type="middle">E</forename><surname>Borah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><forename type="middle">S</forename><surname>Wagie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">M</forename><surname>Egginton</surname></persName>
		</author>
		<author>
			<persName><surname>Naessens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Occupational and Environmental Medicine</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="286" to="291" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Machine Learning: A Probabilistic Perspective</title>
		<author>
			<persName><forename type="first">Kevin</forename><forename type="middle">P</forename><surname>Murphy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>The MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Smooth sensitivity and sampling in private data analysis</title>
		<author>
			<persName><forename type="first">Kobbi</forename><surname>Nissim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sofya</forename><surname>Raskhodnikova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the thirty-ninth annual ACM symposium on Theory of computing</title>
		<meeting>the thirty-ninth annual ACM symposium on Theory of computing</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="75" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Semi-supervised knowledge transfer for deep learning from private training data</title>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martín</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Úlfar</forename><surname>Erlingsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kunal</forename><surname>Talwar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations, ICLR 2017</title>
		<title level="s">Conference Track Proceedings. OpenReview.net</title>
		<meeting><address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">April 24-26, 2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Scalable private learning with PATE</title>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Mironov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kunal</forename><surname>Ananth Raghunathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Úlfar</forename><surname>Talwar</surname></persName>
		</author>
		<author>
			<persName><surname>Erlingsson</surname></persName>
		</author>
		<idno>CoRR, abs/1802.08908</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Probability, random variables, and stochastic processes</title>
		<author>
			<persName><forename type="first">Athanasios</forename><surname>Papoulis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Unnikrishna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pillai</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
			<publisher>Tata McGraw-Hill Education</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Together or alone: The price of privacy in collaborative learning</title>
		<author>
			<persName><forename type="first">Balazs</forename><surname>Pejo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gergely</forename><surname>Biczok</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings on Privacy Enhancing Technologies</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="47" to="65" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName><surname>Press</surname></persName>
		</author>
		<title level="m">Numerical recipes 3rd edition: The art of scientific computing</title>
		<imprint>
			<publisher>Cambridge university press</publisher>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Pain-free random differential privacy with sensitivity sampling</title>
		<author>
			<persName><forename type="first">Francesco</forename><surname>Benjamin Ip Rubinstein</surname></persName>
		</author>
		<author>
			<persName><surname>Aldà</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2950" to="2959" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Integrated public use microdata series: Version 6</title>
		<author>
			<persName><forename type="first">Steven</forename><surname>Ruggles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katie</forename><surname>Genadek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronald</forename><surname>Goeken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josiah</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Sobek</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note>dataset</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Federated learning with bayesian differential privacy</title>
		<author>
			<persName><forename type="first">Aleksei</forename><surname>Triastcyn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boi</forename><surname>Faltings</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.10071</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Functional mechanism: regression analysis under differential privacy</title>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenjie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaokui</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marianne</forename><surname>Winslett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the VLDB Endowment</title>
		<meeting>the VLDB Endowment</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="1364" to="1375" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>