<TEI xmlns="http://www.tei-c.org/ns/1.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xml:space="preserve" xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Bandit Algorithm for Both Unknown Best Position and Best Item Display on Web Pages</title>
			</titleStmt>
			<publicationStmt>
				<publisher />
				<availability status="unknown"><licence /></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Camille-Sovanneary</forename><surname>Gauthier</surname></persName>
							<email>camille-sovanneary.gauthier@louisvuitton.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Louis Vuitton</orgName>
								<address>
									<settlement>Paris (FR)</settlement>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution" key="instit1">Univ. Rennes</orgName>
								<orgName type="institution" key="instit2">IUF</orgName>
								<orgName type="institution" key="instit3">Inria</orgName>
								<orgName type="institution" key="instit4">IRISA</orgName>
								<address>
									<settlement>Rennes</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Romaric</forename><surname>Gaudel</surname></persName>
							<email>romaric.gaudel@ensai.fr</email>
							<affiliation key="aff1">
								<orgName type="department">Ensai</orgName>
								<orgName type="institution" key="instit1">Univ. Rennes</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<orgName type="institution" key="instit3">CREST</orgName>
								<address>
									<settlement>Rennes</settlement>
									<region>FR</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Elisa</forename><surname>Fromont</surname></persName>
							<email>elisa.fromont@irisa.fr</email>
							<affiliation key="aff2">
								<orgName type="institution" key="instit1">Univ. Rennes</orgName>
								<orgName type="institution" key="instit2">IUF</orgName>
								<orgName type="institution" key="instit3">Inria</orgName>
								<orgName type="institution" key="instit4">IRISA</orgName>
								<address>
									<settlement>Rennes</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Bandit Algorithm for Both Unknown Best Position and Best Item Display on Web Pages</title>
					</analytic>
					<monogr>
						<imprint>
							<date />
						</imprint>
					</monogr>
					<idno type="MD5">F5965770051273990E8B1466D9C14349</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-04-12T14:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid" />
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Multi-armed Bandit</term>
					<term>Position-Based Model</term>
					<term>Thomson Sampling</term>
					<term>Metropolis-Hasting</term>
				</keywords>
			</textClass>
			<abstract>
<div><p>Multiple-play bandits aim at displaying relevant items at relevant positions on a web page. We introduce a new bandit-based algorithm, PB-MHB, for online recommender systems which uses the Thompson sampling framework with Metropolis-Hastings approximation. This algorithm handles a display setting governed by the positionbased model. Our sampling method does not require as input the probability of a user to look at a given position in the web page which is difficult to obtain in some applications. Experiments on simulated and real datasets show that our method, with fewer prior information, delivers better recommendations than state-of-the-art algorithms.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div><head n="1">Introduction</head><p>An online recommender systems (ORS) chooses an item to recommend to a user among a list of N potential items. The relevance of the item is measured by the users' feedback: clicks, time spent looking at the item, rating, etc. Since a feedback is only available when an item is presented to a user, the ORS needs to present both attractive items (a.k.a. exploit) to please the current user, and some items with an uncertain relevance (a.k.a. explore) to reduce this uncertainty and perform better recommendations to future users. It faces the explorationexploitation dilemma expressed by the multi-armed bandit setting <ref type="bibr" target="#b2">[3]</ref>.</p><p>On websites, online recommender systems select L items per time-stamp, corresponding to L specific positions in which to display an item. Typical examples of such systems are (i) a list of news, visible one by one by scrolling; (ii) a list of products, arranged by rows; or (iii) advertisements spread in a web page. To be selected (clicked) by a user in such context, an item neds to be relevant by itself, but also to be displayed at the right position. Several models express the way a user behaves while facing such a list of items <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b8">9]</ref> and they have been transposed to the bandit framework <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b13">14]</ref>.</p><p>Retailers often spread their commercials over a web page or display their items on several rows all at once. Thus, in this paper, we will focus on the Position-Based Model (PBM) <ref type="bibr" target="#b22">[23]</ref>. This model assumes that the probability to click on an item i in position results only from the combined impact of this item and its position: items displayed at other positions do not impact the probability to consider the item at position . PBM also gives a user the opportunity to give more than one feedback: she may click on all the items relevant for her. It means we are facing the so-called multiple-play (semi-)bandit setting <ref type="bibr" target="#b4">[5]</ref>. PBM setting is particularly interesting when the display is dynamic, as often on modern web pages, and may depend on the reading direction of the user (which varies from one country to another) and on the ever-changing layout of the page.</p><p>Contribution. We introduce PB-MHB (Position Based Metropolis-Hastings Bandit), a bandit algorithm designed to handle PBM with a Thompson sampling framework. This algorithm does not require the knowledge of the probability of a user to look at a given position: it learns this probability from past recommendations/feedback. This is a strong improvement w.r.t. previous attempts in this research line <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b16">17]</ref> as it allows the use of PB-MHB in contexts where this information is not obvious. This improvement results from the use of the Metropolis-Hastings framework <ref type="bibr" target="#b21">[22]</ref> to sample parameters given their true a posteriori distribution, even thought it is not a usual distribution. While Markov Chain Monte Carlo methods are well-known and extensively used in Bayesian statistics, they were rarely used for Thomson Sampling <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b20">[21]</ref> and it is the first time that the Metropolis-Hastings framework is used in the PBM setting. Besides this specificity, we also experimentally show that PB-MHB suffers a smaller regret than its competitors.</p><p>The paper is organized as follows: Section 2 presents the related work and Section 3 precisely defines our setting. PB-MHB is introduced in Section 4 and is experimentally compared to state-of-the-art algorithms in Section 5. We conclude in Section 6.</p></div>
<div><head n="2">Related Work</head><p>PBM <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b22">23]</ref> relies on two vectors of parameters: θ θ θ ∈ [0, 1] N and κ κ κ ∈ [0, 1] L , where θ θ θ i is the probability for the user to click on item i when she observes that item, and κ κ κ is the probability for the user to observe the position . These parameters are unknown, but they may be inferred from user behavior data: we need to first record the user feedback (click vs. no-click per position) for each set of displayed items, then we may apply an expectation-maximization framework to compute the maximum a posteriori values for (θ θ θ, κ κ κ) given these data <ref type="bibr" target="#b6">[7]</ref>.</p><p>PBM is transposed to the bandit framework in <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b16">17]</ref>. <ref type="bibr" target="#b12">[13]</ref> and <ref type="bibr" target="#b16">[17]</ref> propose two approaches based on a Thompson sampling (TS) framework, with two different sampling strategies. <ref type="bibr" target="#b16">[17]</ref> also introduce several approaches based on the optimism in face of uncertainty principle <ref type="bibr" target="#b2">[3]</ref>. However, the approaches in <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b16">17]</ref> assume κ κ κ known beforehand. <ref type="bibr" target="#b13">[14]</ref> proposes the only approach learning both θ θ θ and κ κ κ while recommending but it still requires the κ κ κ values to be organized in decreasing order, which we do not require. Note also that the corresponding approach is not based, as ours, on Thompson sampling.</p><p>While more theoretical results are obtained when applying the optimism in face of uncertainty principle to the bandit setting, approaches based on TS are known to deliver more accurate recommendations <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b11">12]</ref>. The limitation of TS is its requirement to draw from 'exotic' distributions when dealing with complex models. By limiting themselves to a setting where κ κ κ is known, <ref type="bibr" target="#b12">[13]</ref> and <ref type="bibr" target="#b16">[17]</ref> face simpler distributions than the one which arises from κ κ κ being unknown. In the following, we propose to use Metropolis-Hastings framework to handle this harder distribution. <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b23">24]</ref> investigate a large range of distribution approximation strategies to apply TS framework to the distributions arising from the contextual bandit setting, and <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b20">21]</ref> apply approximate sampling to other settings. Overall, these articles handle a pure bandit setting while we are in a semi-bandits setting: for each recommendation we receive as reward a list of 1 or 0 (click or not). As most of commercial website can track precisely on which product each client clicks, we aim at exploiting that fine-grain information.</p><p>The cascading model (CM) <ref type="bibr" target="#b8">[9]</ref> is another popular model of user behavior. It assumes that the positions are observed in a known order and that the user leaves the website as soon as she clicks on an item<ref type="foot" target="#foot_0">4</ref> . More specifically, if she clicks on the item in position , she will not look at the following positions: + 1, . . . , L. This setting has been extensively studied within the bandit framework <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b26">27]</ref>. However, the assumption of CM regarding the order of observation is irrelevant when considering items spread in a page or on a grid (especially as reading direction of the user may varies from one country to another).</p><p>A few approaches simultaneously handle CM and PBM <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b25">26]</ref>. Their genericity is their strength and their weakness: they do not require the knowledge of the behavioral model, but they cannot use that model to speed-up the process of learning the user preferences. Moreover, these algorithms assume that the best recommendation consists in ordering the items from the more attractive to the less attractive ones. In the context of PBM, this is equivalent to assuming κ κ κ to be sorted in decreasing order. Our algorithm does not make such assumption. Anyhow, we also compare PB-MHB to <software ContextAttributes="used">TopRank</software> <ref type="bibr" target="#b17">[18]</ref> in Section 5 in order to ensure that our model benefits from the knowledge of the click model.</p></div>
<div><head n="3">Recommendation Setting</head><p>The proposed approach handles the following online recommendations setting: at each time-stamp t, the ORS chooses a list i i i(t) = (i i i 1 (t), . . . , i i i L (t)) of L distinct items among a set of N items. The user observes each position with a probability κ κ κ , and if the position is observed, the user clicks on the item i i i with a probability θ θ θ i i i . We denote r r r (t) ∈ {0, 1} the reward in position obtained by proposing i i i at time t, namely 1 if the user did observe the position and clicked on item i i i (t), and 0 otherwise. We assume that each draw is independent, meaning r r r (t) | i i i (t)</p><p>iid.</p><p>∼ Bernoulli (θ θ θ i i i κ κ κ ) , or in other words</p><formula xml:id="formula_0">P (r r r (t) = 1 | i i i (t)) = θ θ θ i i i κ κ κ , P (r r r (t) = 0 | i i i (t)) = 1 -θ θ θ i i i κ κ κ .</formula><p>The ORS aims at maximizing the cumulative reward, namely the total number of clicks gathered from time-stamp 1 to time-stamp T :</p><formula xml:id="formula_1">T t=1 L =1 r r r (t).</formula><p>Without loss of generality, we assume that max κ κ κ = 1. <ref type="foot" target="#foot_1">5</ref> To keep the notations simple, we also assume that <ref type="foot" target="#foot_2">6</ref> The best recommendation is then i i i * = (1, 2, . . . , L), which leads to the expected instantaneous reward µ * = L =1 θ θ θ κ κ κ . The pair (θ θ θ, κ κ κ) is unknown from the ORS. It has to infer the best recommendation from the recommendations and the rewards gathered at previous timestamps, denoted D(t) = {(i i i(1), r r r(1)), . . . , (i i i(t -1), r r r(t -1))}. This corresponds to the bandit setting where it is usual to consider the (cumulative pseudo-)regret</p><formula xml:id="formula_2">θ θ θ 1 &gt; θ θ θ 2 &gt; • • • &gt; θ θ θ N , and κ κ κ 1 = 1 &gt; κ κ κ 2 &gt; • • • &gt; κ κ κ L .</formula><formula xml:id="formula_3">R T def = T t=1 L =1 E [r r r | i i i * ] - T t=1 L =1 E [r r r (t) | i i i (t)] = T µ * - T t=1 L =1 θ θ θ i i i (t) κ κ κ . (1)</formula><p>The regret R T denotes the cumulative expected loss of the ORS w.r.t. the oracle recommending the best items at each time-stamp. Hereafter we aim at an algorithm which minimizes the expectation of R T w.r.t. its choices.</p></div>
<div><head n="4">PB-MHB Algorithm</head><p>We handle the setting presented in the previous section with the online recommender system depicted by Algorithm 1 and referred to as PB-MHB (for Position Based Metropolis-Hastings Bandit). This algorithm is based on the Thompson sampling framework <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b1">2]</ref>. First, we look at rewards with a fully Bayesian point of view: we assume that they follow the statistical model depicted in Section 3, and we choose a uniform prior on the parameters θ θ θ and κ κ κ. Therefore the posterior probability for these parameters given the previous observations D(t) is</p><formula xml:id="formula_4">P (θ θ θ, κ κ κ|D(t)) ∝ N i=1 L =1 (θ θ θ i κ κ κ ) S i, (t) (1 -θ θ θ i κ κ κ ) F i, (t) ,<label>(2)</label></formula><p>where</p><formula xml:id="formula_5">S i, (t) = t-1 s=1 1 i i i (s)=i 1 r r r (s)=1</formula><p>denotes the number of times the item i has been clicked while being displayed in position from time-stamp 1 to t -1, and F i, (t) = t-1 s=1 1 i i i (s)=i 1 r r r (s)=0 denotes the number of times the item i has not been clicked while being displayed in position from time-stamp 1 to t -1.</p><p>Second, we choose the recommendation i i i(t) at time-stamp t according to its posterior probability of being the best arm. To do so, we denote ( θ θ θ, κ κ κ) a sample of parameters (θ θ θ, κ κ κ) according to their posterior probability, we keep the best items given θ θ θ, and we display them in the right order given κ κ κ. The posterior probability <ref type="bibr" target="#b1">(2)</ref> does not correspond to a well-known distribution. <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b16">[17]</ref> tackle this problem by considering that κ κ κ is known in order to manipulate N independent simpler distributions P i (θ θ θ i |θ θ θ -i , κ κ κ, D(t)). By having κ κ κ and θ θ θ both unknown, we have to handle a law for which the components θ θ θ 1 , . . . , θ θ θ N and κ κ κ 1 , . . . , κ κ κ L are correlated (see Equation <ref type="formula" target="#formula_4">2</ref>). We handle it thanks to a carefully designed Metropolis-Hastings algorithm <ref type="bibr" target="#b21">[22]</ref> (cf. Algorithm 2). This algorithm consists in building a sequence of m samples (θ θ θ (1) , κ κ κ (1) ), . . . , (θ θ θ (m) , κ κ κ (m) ) such that (θ θ θ (m) , κ κ κ (m) ) follows a good approximation of the targeted distribution. It is based on a Markov chain on parameters (θ θ θ, κ κ κ) which admits the targeted probability distribution as its unique stationary distribution.</p><p>At iteration s, the sample (θ θ θ (s) , κ κ κ (s) ) moves toward sample (θ θ θ (s+1) , κ κ κ (s+1) ) by applying (N + L -1) transitions: one per item and one per position except for κ κ κ 1 . Let's start by focusing on the transition regarding item i (Lines 5-9) and denote (θ θ θ, κ κ κ) the sample before the transition.</p><p>The algorithm aims at sampling a new value for θ θ θ i according to its posterior probability given other parameters and the previous observations D(t): until 0 θ θ θ 1 8: with prob. min 1,</p><formula xml:id="formula_6">P i( θ θ θ|θ θ θ -i ,κ κ κ,D(t)) P i( θ θ θ i |θ θ θ -i ,κ κ κ,D(t)) ∆Φσ (θ θ θ i ) ∆Φσ ( θ θ θ) 9: θ θ θi ← θ θ θ 10:</formula><p>end for 11:</p><p>for = 2, . . . , L do 12: repeat 13: draw κ κ κ ∼ N (κ κ κ , σ) 14:</p><p>until 0 κ κ κ 1 15: with prob. min 1, P (κ κ κ|θ θ θ,κ κ κ -,D(t))</p><formula xml:id="formula_7">P (κ κ κ |θ θ θ,κ κ κ -,D(t)) ∆Φσ (κ κ κ ) ∆Φσ (κ κ κ) 16: κ κ κ ← κ κ κ 17:</formula><p>end for 18: end for 19: return (θ θ θ, κ κ κ)</p></div>
<div><head n="4.2">Overall Complexity</head><p>The computational complexity of Algorithm 1 is driven by the number of randomwalk steps done per recommendation: m(N + L -1), which is controlled by the parameter m. This parameter corresponds to the burning period: the number of iterations required by the Metropolis-Hastings algorithm to draw a point (θ θ θ (m) , κ κ κ (m) ) almost independent from the initial one. While the requirement for a burning period may refrain us from using a Metropolis-Hasting algorithm in such recommendation setting, we demonstrate in the following experiments that the required value for m remains reasonable. We drastically reduce m by starting the Metropolis-Hasting call from the point used to recommend at previous time-stamp. This corresponds to replacing Line 1 in Algorithm 2 by: 1: (θ θ θ, κ κ κ) ← ( θ θ θ, κ κ κ) used for the previous recommendation .</p></div>
<div><head n="5">Experiments</head><p>In this section we demonstrate the benefit of the proposed approach both on two artificial and two real-life datasets. Note that whatever real-life data we are using, we can only use them to compute the parameters θ θ θ and κ κ κ and simulate at each time-stamp a "real" user feedback (i.e. clicks) by applying PBM with these parameters. This is what is usually done in the literature since the recommendations done by a bandit are very unlikely to match the recommendations logged in the ground truth data and without a good matching, it would be impossible to compute a relevant reward for each interaction. Code and data for replicating our experiments are available at https://github.com/gaudel/ranking bandits.</p></div>
<div><head n="5.1">Datasets</head><p>In the experiments, the online recommender systems are required to deliver T consecutive recommendations, their feedbacks being drawn from a PBM distribution. We consider two settings denoted purely simulated and behavioral in the remaining. With the purely simulated setting, we choose the value of the parameters (θ θ θ, κ κ κ) to highlight the stability of the proposed approach even for extreme settings. Namely, we consider N = 10 items, L = 5 positions, and κ κ κ = [1, 0.75, 0.6, 0.3, 0.1]. The range of values for θ θ θ is either close to zero (θ θ θ -= [10 -3 , 5.10 -4 , 10 -4 , 5.10 -5 , 10 -5 , 10 -6 , . . . , 10 -6 ]) or close to one (θ θ θ + = [0.99, 0.95, 0.9, 0.85, 0.8, 0.75, . . . , 0.75]).</p><p>With the behavioral setting, the values for κ κ κ and θ θ θ are obtained from true users behavior as in <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b13">[14]</ref>. Two behavioral datasets are considered and presented hereafter. The first one is KDD Cup 2012 track 2 dataset, which consists of session logs of soso.com, a Tencent's search engine. It tracks clicks and displays of advertisements on a search engine result web-page, w.r.t. the user query. For each query, 3 positions are available for a various number of ads to display.</p><p>To follow previous works, instead of looking for the probability to be clicked per display, we target the probability to be clicked per session. This amounts to discarding the information Impression. We also filter the logs to restrict the analysis to (query, ad) couples with enough information: for each query, ads are excluded if they were displayed less than 1,000 times at any of the 3 possible positions. Then, we filter queries that have less than 5 ads satisfying the previous condition. We end up with 8 queries and from 5 to 11 ads per query. Finally, for each query q, the parameters (θ θ θ [q] , κ κ κ [q] ) are set from the Singular Value Decomposition (SVD) of the matrix M M M [q] ∈ R N ×L which contains the probability to be clicked for each item in each position. By denoting ζ [q] , the greatest singular value of M M M [q] , and u u u [q] (respectively v v v [q] ) the left (resp. right) singular vector as-</p><formula xml:id="formula_8">sociated to ζ [q] , we set θ θ θ [q] def = v v v [q] 1 ζ [q] u u u [q] and κ κ κ [q] def = v v v [q] /v v v [q]</formula><p>1 , such that κ κ κ</p><p>[q] 1 = 1, and θ θ θ [q] T κ κ κ [q] = ζu u u [q] T v v v [q] . This leads to θ θ θ i values ranging from 0.004 to 0.149, depending on the query.</p><p>The second behavioral dataset is Yandex 7 . As in <ref type="bibr" target="#b17">[18]</ref>, we select the 10 most frequent queries, and for each query the ORS displays 5 items peeked among the 10 most attractive ones. As for KDD dataset, the parameters (θ θ θ [q] , κ κ κ [q] ) are set from an SVD. This leads to θ θ θ i values ranging from 0.070 to 0.936, depending on the query.</p></div>
<div><head n="5.2">Competitors</head><p>We compare the performance of PB-MHB with the performance of PMED <ref type="bibr" target="#b13">[14]</ref>, <software ContextAttributes="used">TopRank</software> <ref type="bibr" target="#b17">[18]</ref>, and the standard baseline ε n -greedy <ref type="bibr" target="#b2">[3]</ref>.</p><p>PMED is designed to match a lower-bound on the expected regret of any reasonable algorithm under the PBM assumption. Let us recall that it assumes κ κ κ values to be decreasing, which means the ORS knows in advance which is the most observed position, which is the second most observed position, and so on. PB-MHB does not require this ordering, it learns it from interactions with users. PMED uniform-exploration parameter α is fixed to 1. Due to its very high time complexity, experiments with PMED are stopped after 5 hours for each dataset (which corresponds to about 10 5 recommendations).</p><p><software ContextAttributes="used">TopRank</software> handles a wider range of click models than PB-MHB, but it also assumes the knowledge of the order on positions. <software ContextAttributes="used">TopRank</software> hyper-parameter δ is set to 1/T as recommended by Theorem 1 in <ref type="bibr" target="#b17">[18]</ref>.</p><p>Finally, we compare PB-MHB to n -Greedy. At each time-stamp t, an estimation ( θ θ θ, κ κ κ) of parameters (θ θ θ, κ κ κ) is obtained applying SVD to the collected data. Let us denote î i i(t) the recommendation with the highest expected reward given the inferred values ( θ θ θ, κ κ κ). A greedy algorithm would recommend î i i(t). Since this algorithm never explores, it may end-up recommending a sub-optimal affectation. n -Greedy counters this by randomly replacing each item of the recommendation with a probability ε(t) = c/t, where c is a hyper-parameter to be tuned. In the following, we plot the results obtained with the best possible value for c, while trying c in {10 0 , 10 1 , . . . , 10 6 }. Note that the best value for c varies a lot from a dataset to another.</p><p>In the experiments presented hereafter the requirements of each algorithm are enforced. Namely, κ κ κ values are decreasing when running experiments with PMED and <software>TopRank</software>.</p></div>
<div><head n="5.3">Results</head><p>We compare the previously presented algorithms on the basis of the cumulative regret (see Equation ( <ref type="formula">1</ref>)), which is the sum, over T consecutive recommendations, of the difference between the expected reward of the best possible answer and of the answer of a given recommender system. The regret will be plotted with respect to T on a log-scale basis. The best algorithm is the one with the lowest regret. The regret plots are bounded by the regret of the oracle (0) and the regret of a recommender system choosing the items uniformly at random. We average the results of each algorithm over 20 independant sequences of recommendations per query.</p><p>PB-MHB Hyper-parameters PB-MHB behavior is affected by two hyperparameters: the width c/ √ t of the Gaussian random-walk steps, and the number m of Metropolis-Hastings iterations per recommendation. Overall, when Metropolis-Hastings runs start from the couple ( θ θ θ, κ κ κ) from the previous timestamp, we show in Figure <ref type="figure" target="#fig_1">1a</ref> that PB-MHB exhibits the smallest regret on Yandex as soon as c &gt; 1000 and m = 1. Note that m = 1 is also the setting which minimizes the computation time of PB-MHB. These hyper-parameter choices also hold for the 3 other datasets (not shown here for lack of space).</p><p>We now discuss the impact of choosing other hyper-parameter values on Yandex data. The regret is the smallest as soon as c is large enough (c &gt;= 100). In Figure <ref type="figure" target="#fig_1">1b</ref>, we illustrate the impact of m. It yields a high regret only when c and m are both too small (full blue curve): when the random-walk steps are too small the Metropolis-Hasting algorithm requires more iterations to get uncorrelated  samples ( θ θ θ, κ κ κ). For reasonable values of c, m has no impact on the regret. Figure <ref type="figure" target="#fig_1">1b</ref> also shows the impact of keeping the parameters from the previous timestamp compared to a purely random start. Starting from a new randomly drawn set of parameters would require more than m = 10 iterations to obtain the same result, meaning a computation budget more than 10 times higher. This behavior is explained by the gap between the uniform law (which is used to draw the starting set of parameters) and the targeted law (a posteriori law of these parameters) which concentrates around its MAP. Even worse, this gap increases while getting more and more data since the a posteriori law concentrates with the increase of data. As a consequence, the required value for m increases along time when applying a standard Metropolis-Hasting initialisation, which explains why the dotted red line diverges from the solid one around time-stamp 30.</p><p>Comparison with Competitors Figure <ref type="figure" target="#fig_0">2</ref> compares the regret obtained by PB-MHB and its competitors on datasets with various click and observation probabilities. Up to time-stamp 10 4 , PMED and PB-MHB exhibit the smallest regret on all settings. Thereafter, PMED is by far the algorithm with the smallest regret. Regarding computation time, apart from PMED all the algorithms require less than 20 ms per recommendation which remains affordable: <software ContextAttributes="used">n -Greedy</software> is the fastest with less than 0.5ms per recommendation 8 ; then <software ContextAttributes="used">TopRank</software> and PB-MHB require 10 to 40 times more computation time than <software ContextAttributes="used">n -Greedy</software>; finally, PMED requires more than 150ms per recommendation which makes it impractical regardless of its low regret.</p></div>
<div><head n="6">Conclusion</head><p>We have introduced a new bandit-based algorithm, PB-MHB, for online recommender systems in the PBM which uses a Thompson sampling framework to learn the κ κ κ and θ θ θ parameters of this model instead of assuming them given. Experiments on simulated and real datasets show that our method (i) suffers a smaller regret than its competitors having access to the same information, and (ii) suffers a similar regret as its competitors using more prior information.</p><p>These results are still empirical but we plan to formally prove them in future work. Indeed, <ref type="bibr" target="#b20">[21]</ref> upper-bounded the regret of a Thompson Sampling bandit algorithm, while using Langevin Monte-Carlo to sample posterior values of parameters. That could be a good starting point for theoretically studying the convergence of PB-MHB. We also would like to improve our algorithm by further working on the proposal law to draw candidates for the sampling part. The proposal is currently a truncated random walk. By managing it differently (with a logit transformation for instance) we could improve both the time and precision performance. On the other hand, with a better understanding of the evolution of the target distribution, we could also improve the sampling part. Moreover, we would like to apply PB-MHB to environments where κ κ κ is evolving with time (with new marketing trends) and where our learning setting could develop its full potential.</p></div><figure xml:id="fig_0"><head>Algorithm 2</head><label>2</label><figDesc>Metropolis-Hastings applied to the distribution of Equation (2) Require: D(t): previous recommendations and rewards Require: σ = c/ √ t: Gaussian random-walk steps width Require: m: number of iterations 1: draw (θ θ θ, κ κ κ) after uniform distribution 2: κ κ κ1 ← 1 3: for s = 1, . . . , m do 4: for i = 1, . . . , N do 5: repeat 6: draw θ θ θ ∼ N (θ θ θi, σ) 7:</figDesc></figure>
<figure xml:id="fig_1"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: Cumulative regret w.r.t. time-stamp on Yandex data. Impact of the width c/ √ t of Gaussian random-walk steps (left). Impact of the use of the parameters from the previous time-stamp to warm-up the Metropolis-Hasting algorithm and of the number m of Metropolis-Hastings iterations per recommendation (right). The shaded area depicts the standard error of our regret estimates.</figDesc></figure>
<figure xml:id="fig_3"><head>Fig. 2 :</head><label>2</label><figDesc>Fig.2: Cumulative regret w.r.t. time-stamp on five different settings for all competitors. The plotted curves correspond to the average over 20 independant sequences of recommendations per query (in total: 20 sequences for simulated data, 160 sequences for KDD and 200 sequences for Yandex). The shaded area depicts the standard error of our regret estimates. For n -Greedy, c is set to 10 4 for KDD and Yandex settings, to 10 5 when θ θ θ is close to 0, and to 10 3 when θ θ θ is close to 1.</figDesc></figure>
<figure type="table" xml:id="tab_0"><head /><label /><figDesc>Algorithm 1 PB-MHB, Metropolis-Hastings based bandit for Position-Based Model</figDesc><table><row><cell>D(1) ← {}</cell></row><row><cell>for t = 1, . . . do</cell></row><row><cell>draw ( θ θ θ, κ κ κ) ∼ P (θ θ θ, κ κ κ|D(t)) using Algorithm 2 display the L items with greatest value in θ θ θ, ordered by decreasing values of κ κ κ</cell></row><row><cell>get rewards r r r(t)</cell></row><row><cell>D(t + 1) ← D(t) ∪ (i i i(t), r r r(t))</cell></row><row><cell>end for</cell></row><row><cell>4.1 Sampling w.r.t. the Posterior Distribution</cell></row></table></figure>
			<note place="foot" n="4" xml:id="foot_0"><p>Some refined models assume a probability to leave. With these models, the user may click on several items.</p></note>
			<note place="foot" n="5" xml:id="foot_1"><p>As stated in<ref type="bibr" target="#b13">[14]</ref>, we may replace (θ θ θ, κ κ κ) by (θ θ θ. max κ κ κ , κ κ κ/ max κ κ κ ).</p></note>
			<note place="foot" n="6" xml:id="foot_2"><p>Our algorithm and the experiments only assume κ κ κ1 = 1.</p></note>
			<note place="foot" xml:id="foot_3"><p>Pi( θ θ θ|θ θ θ-i,κ κ κ,D(t))Pi(θ θ θi|θ θ θ-i,κ κ κ,D(t)) measures how likely the candidate value is compared to the previous one, w.r.t. the posterior distribution,</p></note>
			<note place="foot" n="8" xml:id="foot_4"><p>Computation time for a sequence of 10 7 recommendations vs. the first query of Yandex data, on an Intel Xeon E5640 CPU2.67GHz with 50 GB RAM. The algorithms are implemented in Python.</p></note>
		</body>
		<back>
			<div type="annex">
<div><p>The transition regarding parameter κ κ κ involves the same framework: the proposal is a truncated Gaussian random-walk step and aims at the probability</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Thompson sampling for contextual bandits with linear payoffs</title>
		<author>
			<persName><forename type="first">S</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proc. of the 30th Int. Conf. on Machine Learning. ICML'13</title>
		<meeting>of the 30th Int. Conf. on Machine Learning. ICML'13</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Near-optimal regret bounds for thompson sampling</title>
		<author>
			<persName><forename type="first">S</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Jour. of the ACM, JACM</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1" to="30" />
			<date type="published" when="2017-09">Sep 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Finite-time analysis of the multiarmed bandit problem</title>
		<author>
			<persName><forename type="first">P</forename><surname>Auer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Cesa-Bianchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="235" to="256" />
			<date type="published" when="2002-05">May 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">An empirical evaluation of thompson sampling</title>
		<author>
			<persName><forename type="first">O</forename><surname>Chapelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS'11</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">24</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Combinatorial multi-armed bandit: General framework and applications</title>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proc. of the 30th Int. Conf. on Machine Learning. ICML'13</title>
		<meeting>of the 30th Int. Conf. on Machine Learning. ICML'13</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A thompson sampling algorithm for cascading bandits</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">C</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proc. of the 22nd Int. Conf. on Artificial Intelligence and Statistics. AISTATS'19</title>
		<meeting>of the 22nd Int. Conf. on Artificial Intelligence and Statistics. AISTATS'19</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Click Models for Web Search</title>
		<author>
			<persName><forename type="first">A</forename><surname>Chuklin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Markov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>Morgan &amp; Claypool Publishers</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning to rank: Regret lower bounds and efficient algorithms</title>
		<author>
			<persName><forename type="first">R</forename><surname>Combes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Magureanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Proutière</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Laroche</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proc. of the 2015 ACM SIGMETRICS Int. Conf. on Measurement and Modeling of Computer Systems</title>
		<meeting>of the 2015 ACM SIGMETRICS Int. Conf. on Measurement and Modeling of Computer Systems</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">An experimental comparison of click position-bias models</title>
		<author>
			<persName><forename type="first">N</forename><surname>Craswell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Zoeter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ramsey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proc. of the 2008 Int. Conf. on Web Search and Data Mining. WSDM '08</title>
		<meeting>of the 2008 Int. Conf. on Web Search and Data Mining. WSDM '08</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Pg-ts: Improved thompson sampling for logistic contextual bandits</title>
		<author>
			<persName><forename type="first">B</forename><surname>Dumitrascu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Engelhardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS'18</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Dcm bandits: Learning to rank with multiple clicks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Katariya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kveton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szepesvári</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proc. of the Int. Conf. on Machine Learning. ICML</title>
		<meeting>of the Int. Conf. on Machine Learning. ICML</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Efficient thompson sampling for online matrix factorization recommendation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kawale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">H</forename><surname>Bui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kveton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Tran-Thanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chawla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS'15</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">28</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Optimal regret analysis of thompson sampling in stochastic multi-armed bandit problem with multiple plays</title>
		<author>
			<persName><forename type="first">J</forename><surname>Komiyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Honda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Nakagawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proc. of the 32nd Int. Conf. on Machine Learning. ICML'15</title>
		<meeting>of the 32nd Int. Conf. on Machine Learning. ICML'15</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Position-based multiple-play bandit problem with unknown position bias</title>
		<author>
			<persName><forename type="first">J</forename><surname>Komiyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Honda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Takeda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS'17</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Cascading bandits: Learning to rank in the cascade model</title>
		<author>
			<persName><forename type="first">B</forename><surname>Kveton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szepesvári</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ashkan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proc. of the 32nd Int. Conf. on Machine Learning. ICML'15</title>
		<meeting>of the 32nd Int. Conf. on Machine Learning. ICML'15</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Combinatorial cascading bandits</title>
		<author>
			<persName><forename type="first">B</forename><surname>Kveton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ashkan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szepesvári</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS'15</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">28</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Multiple-play bandits in the position-based model</title>
		<author>
			<persName><forename type="first">P</forename><surname>Lagrée</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Vernade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Cappé</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30. NIPS'16</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">TopRank: A practical algorithm for online stochastic ranking</title>
		<author>
			<persName><forename type="first">T</forename><surname>Lattimore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kveton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szepesvari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 31. NIPS'18</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">BubbleRank: Safe online learning to re-rank via implicit click feedback</title>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kveton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lattimore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Markov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szepesvári</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zoghi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proc. of the 35th Uncertainty in Artificial Intelligence Conference. UAI'19</title>
		<meeting>of the 35th Uncertainty in Artificial Intelligence Conference. UAI'19</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Contextual combinatorial cascading bandits</title>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proc. of the 33rd Int. Conf. on Machine Learning. ICML'16</title>
		<meeting>of the 33rd Int. Conf. on Machine Learning. ICML'16</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">On thompson sampling with langevin algorithms</title>
		<author>
			<persName><forename type="first">E</forename><surname>Mazumdar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pacchiano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">L</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proc. of the 37th Int. Conf. on Machine Learning. ICML'20</title>
		<meeting>of the 37th Int. Conf. on Machine Learning. ICML'20</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Probabilistic inference using markov chain monte carlo methods</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Neal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993-09">09 1993</date>
		</imprint>
		<respStmt>
			<orgName>University of Zurich, Department of Informatics</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. rep.</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Predicting clicks: Estimating the click-through rate for new ads</title>
		<author>
			<persName><forename type="first">M</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Dominowska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ragno</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proc. of the 16th International World Wide Web Conference. WWW '07</title>
		<meeting>of the 16th International World Wide Web Conference. WWW '07</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep bayesian bandits showdown: An empirical comparison of bayesian deep networks for thompson sampling</title>
		<author>
			<persName><forename type="first">C</forename><surname>Riquelme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proc. of the Int. Conf. on Learning Representations. ICLR'18</title>
		<meeting>of the Int. Conf. on Learning Representations. ICLR'18</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">On the likelihood that one unknown probability exceeds another in view of the evidence of two samples</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">R</forename><surname>Thompson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">3/4</biblScope>
			<biblScope unit="page" from="285" to="294" />
			<date type="published" when="1933">1933</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Online learning to rank in stochastic click models</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zoghi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tunys</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ghavamzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kveton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szepesvari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proc. of the 34th Int. Conf. on Machine Learning. ICML'17</title>
		<meeting>of the 34th Int. Conf. on Machine Learning. ICML'17</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Cascading bandits for large-scale recommendation problems</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">R</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kveton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proc. of the 32nd Conference on Uncertainty in Artificial Intelligence. UAI '16</title>
		<meeting>of the 32nd Conference on Uncertainty in Artificial Intelligence. UAI '16</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>