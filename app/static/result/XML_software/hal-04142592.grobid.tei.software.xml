<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Precise Segmentation for Children Handwriting Analysis by Combining Multiple Deep Models with Online Knowledge</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Simon</forename><surname>Corbillé</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Univ Rennes</orgName>
								<orgName type="institution" key="instit2">IRISA</orgName>
								<address>
									<postCode>35000</postCode>
									<settlement>Rennes</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Éric</forename><surname>Anquetil</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">INSA Rennes</orgName>
								<orgName type="institution" key="instit2">IRISA</orgName>
								<address>
									<postCode>35000</postCode>
									<settlement>Rennes</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Élisa</forename><surname>Fromont</surname></persName>
						</author>
						<title level="a" type="main">Precise Segmentation for Children Handwriting Analysis by Combining Multiple Deep Models with Online Knowledge</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">A0FD1145E75DBF01DB8B9D074594F441</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-04-12T14:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Handwriting Recognition and Segmentation</term>
					<term>R-CNN object detector</term>
					<term>Seq2Seq network</term>
					<term>French Children Handwriting</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present a strategy, called Seq2Seg, to reach both precise and accurate recognition and segmentation for children handwritten words. Reaching such high performance for both tasks is necessary to give personalized feedback to children who are learning how to write. The first contribution is to combine the predictions of an accurate Seq2Seq model with the predictions of a R-CNN object detector. The second one is to refine the bounding box predictions provided by the detector with a segmentation lattice computed from the online signal. An ablation study shows that both contributions are relevant, and their combination is efficient enough for immediate feedback and achieves state of the art results even compared to more informed systems.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The paradox of Sayre <ref type="bibr" target="#b0">[1]</ref> is a famous problem in the handwriting recognition domain. This dilemma claims that a handwritten word cannot be recognized without being segmented in letters and at the same time cannot be segmented in letters without the word being recognized. To tackle the handwriting recognition task, the systems use an analytic or a holistic approach. The analytic approach segments the handwriting and tries to recognize letters, while the holistic approach tries to recognize the whole word without explicit segmentation. Stateof-the-art methods use holistic approaches based on deep learning model. They are designed for recognition only and are efficient in solving this task. However, in a context of learning spelling, the letter segmentation provided by these approaches is not precise enough to provide a useful spatial feedback on spelling mistake to a user.</p><p>We aim at designing a support system for learning cursive handwriting at school and more particularly in a dictation context. Tackling both the challenges of recognition and segmentation of children handwriting may allow a system to provide a fine-grained analysis on the handwritten words and to deliver immediate spelling feedback to children. The children are in a learning process and therefore, their handwriting is degraded and contains misspelling errors. Line a of Figure <ref type="figure" target="#fig_0">1</ref> illustrates several examples of degraded handwriting. We can see that a distortion of the letter "e" can be interpreted as a letter "l" and vice versa for the word "elle" in the third position of this line. Line b shows several examples of phonetic errors. In the first example, where the instruction is "mes" [mε], the child writes "mai" [me], which sounds very similar in French. These homophonic errors can be anticipated in automatic systems using a language model that would take into account the contextual information. However, other types of errors in a context of learning spelling illustrated in line c such as dyslexia and out-of-vocabulary words cannot. The first example of line c shows a common mistake in French where the child confuses the letters "b" and "d" which are phonetically close.</p><p>To provide an accurate recognition and segmentation of the children handwritten words, we propose the two following contributions included in the Seq2Seg system:</p><p>-We present an original combination strategy using a model dedicated to recognition and an object detector dedicated to segmentation. The recognition model is used to recognize a word and to select the segmentation predictions of the object detector corresponding to the letters of the recognized word.</p><p>-We use an segmentation lattice <ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref> which encodes expert knowledge to refine the letter segmentation provided by the object detector and thus improve the precision of the segmentation.</p><p>This paper is organized as follows. The related works are described in Section 2. The contributions are detailed in Section 3. Section 4 presents an ablation study of our approach and compares it with the state of the art. We conclude in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>This section presents works related to the recognition and segmentation of handwritten words. The first part introduces the latest methods in the handwriting recognition domain. The second part sets out the limits of these methods for a segmentation task and presents the state of the art in children handwriting recognition and segmentation. Finally, we provide a brief presentation of object detection models to show their relevance in an handwriting segmentation context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Handwriting Recognition</head><p>The state of the art in handwriting recognition is achieved by the Sequenceto-Sequence (Seq2Seq) <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref> and Transformer <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref> networks. Seq2Seq use an encoder-decoder paradigm enhanced by an attention mechanism, while Transformers are based on a feature extractor followed by multi-head attention mechanisms. Transformers are slightly more accurate but need much more data to be optimized than Seq2Seq. This is often dealt with data generation and data augmentation techniques.</p><p>In our work, a rather small dataset is available compare to adult handwriting ones than can be found in <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref> due to the cost associated with data collection in schools and degraded handwriting annotations. We thus decided to rely upon a Seq2Seq network for word recognition because of its good compromise accuracy/need of labeled data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Handwriting Segmentation</head><p>To our knowledge, there is no (reasonable sized) public dataset for handwriting (semantic) segmentation, i.e., handwriting words with annotations at the letter pixel level. This task is particularly tedious and time-consuming but is not necessary, nowadays, to achieve excellent recognition results for the architectures mentioned above. For this reason, handwriting letter segmentation methods are difficult to compare quantitatively. For the networks designed for handwriting recognition, the letter segmentation can be computed from the position of the receptive field associated to the letter prediction. The width and height of the receptive field being fixed, this approach, which lacks flexibility, does not provide a precise segmentation. Furthermore, most networks are trained with the connectionist temporal classification (CTC) <ref type="bibr" target="#b10">[11]</ref> approach. CTC manages the alignment between an input data sequence and an output sequence of frames of variable size. CTC is known to have a peaky behavior <ref type="bibr" target="#b11">[12]</ref> i.e., it predict one frame per letter. This impacts the segmentation performance since a frame has a fixed size while a handwritten letter has a variable one. In <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref>, the authors modified CTC to enforce a better alignment between the frames and the real letters. However, despite these efforts, the segmentation was still lacking precision.</p><p>The authors of <ref type="bibr" target="#b3">[4]</ref> and its extensions <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16]</ref> use an analytic approach to reach the state of the art in children handwriting recognition and segmentation. The letter recognition is made from letter splitting hypotheses coming from a segmentation lattice <ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref>. Then, the method selects the best path of the lattice where his associated word is closest to the instruction or to a phonetically close word. However, this system uses a language model to guide the analysis of children handwriting using assumptions of probable phonetic errors. It is thus specific and dedicated to the French language and cannot be easily adapted to other languages. Moreover, as already shown in the Introduction in Figure <ref type="figure" target="#fig_0">1</ref>, some children errors cannot be prevented using a language model. In this work, we want to achieve results on par with <ref type="bibr" target="#b14">[15]</ref> without relying on a language model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Object Detection</head><p>We rely on an existing, very successful, two-stage deep learning-based object detector <ref type="bibr" target="#b16">[17]</ref> to perform a precise localization of the letters in the handwritten words. Two-stage detectors <ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref> are known to be a little bit more precise for localization than their one-stage counterparts <ref type="bibr" target="#b19">[20]</ref><ref type="bibr" target="#b20">[21]</ref><ref type="bibr" target="#b21">[22]</ref><ref type="bibr" target="#b22">[23]</ref> even though they are usually slower. Object detectors provide a joint classification of objects into classes and a regression of the bounding boxes that best localize each object in an image (or in a video frame). In a two-stage detector, candidate regions are generated by a RPN (region-proposal network) and processed to perform the detection task.</p><p>Based on the output of the object detector (i.e. labeled bounding boxes), the final segmentation is obtained using all the handwriting pixels within the predicted bounding box. Note that in our work, the image of the handwriting comes from an online signal, therefore, this image is noise-free both on the background and on the handwriting pixels. This makes it possible to extract the letter segmentation from the bounding box coordinates. Also note that we could have used the semantic segmentation output of an instance-based semantic segmentation network such as Mask-RCNN <ref type="bibr" target="#b16">[17]</ref> to directly segment the letters. However, the complexity in terms of parameters of such segmentation networks (the semantic segmentation part of the network is usually independent from the object detection part), and the limited number of realistic labeled children words to train it, made bounding boxes of traditional object detectors better candidates to tackle our segmentation problem when the segmentation target is too ambiguous.</p><p>We propose Seq2Seg, a method to combine two deep learning models (Seq2Seq and R-CNN) and expert online knowledge to accurately segment and recognize children handwritten words. Seq2Seg, illustrated in Figure <ref type="figure" target="#fig_1">2</ref> leverages each method to provide a precise semantic segmentation of the children words. The first level (Level A) uses a model dedicated to the recognition task as an oracle to filter out the bounding box's predictions of the object detector. Level B uses an expert segmentation lattice <ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref> to refine the letter segmentation associated to the bounding boxes predicted by the object detector. The segmentation lattice use online data, while the object detector and the recognizer use online data converted to offline. In our work, we use the Seq2Seq architecture defined in <ref type="bibr" target="#b14">[15]</ref> as the text recognition model and the R-CNN architecture defined in <ref type="bibr" target="#b16">[17]</ref> as the object detector. The Seq2Seq performs well on the recognition task but provides an imprecise segmentation while R-CNN performs well on the segmentation task but is less accurate in recognition than the Seq2Seq (see Table <ref type="table">2</ref> in Section 4 for the detailed results).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Level A: Filtering Bounding Boxes Predictions with an Accurate Recognition Model</head><p>The recognition model is trained solely on the recognition task and outputs a word. From this word, one can deduce, in particular, the number of letters to be segmented. This information makes it possible to select a fixed number of object detector segmentation predictions during the inference and use the more accurate recognition result of the recognition model. The process of selecting the predictions from the object detector can be difficult and ambiguous in certain cases, as illustrated in Figure <ref type="figure" target="#fig_2">3</ref>: e.g., in a letter "m", two letters "n" can be recognized but they cannot be both true at the same time so here, a more global view is necessary to choose the right segmentation. The use of the precise recognition model, providing that it does not introduce other errors, makes it possible to remove these ambiguities. The object detector has several output x-ordered predictions. The goal is to select the object detector prediction corresponding to the letter segmentation. This method illustrated in Figure <ref type="figure" target="#fig_3">4</ref> is broken down into three steps: (Step 1) we compute all object detector prediction sequences; (Step 2) we filter the sequences according to the length of the word recognized by the recognition model; (Step 3) we compute the score associated with each sequence. The final selected sequence is the one with the highest score. R-CNN natively includes two Non-Maximum Suppression (NMS) phases to filter out its predictions. The first is applied to the regions proposals to reduce the number of proposals to consider, while the second is applied to predictions (bounding boxes and labels) to keep the best prediction for the objects predictions with the same label. In a letter-in-word detection context, there is little overlap between letters unlike a classic COCO-style object detection. In order to handle the cases where several letter predictions are nested as emphasized in Figure <ref type="figure" target="#fig_2">3</ref>, we have added an NMS on the predictions of the model which is independent of the class. Our method uses the predictions before the last NMS to have a wide variety of prediction to filter with segmentation ambiguities. The purpose of the method is to remove these ambiguities. (1) Compute all the prediction sequences: consider a directed graph G(V, E), where V and E correspond to the sets of vertices and edges. For each prediction of the object detector ordered by x min from the bounding box coordinates, a vertex is added in G as illustrated in Figure <ref type="figure" target="#fig_3">4</ref>. The weight of an edge e ij = (v i , v j ) ∈ E is computed as e ij = 1 -IoU P ixel between the predictions ordered by x min associated to the vertices. IoU P ixel stands for the Intersection Over Union of the handwriting pixels contained in the two bounding boxes corresponding to the two vertices: the predictions with the higher overlap have a weaker link. A sequence of predictions ordered by x min corresponds to a graph path, i.e. a list of connected vertices in the graph.</p><p>(2) Filter the sequences according to the length of the word predicted by the recognition model: there are three selection scenarios (Table <ref type="table" target="#tab_2">3</ref> in Section 4 details the result of each type of scenario):</p><p>-Perfect matching: The number of predicted letters of the object detector and of the recognition model is equal. In this case, we expect our filtering to only improve the recognition part of the object detector (that we do not use explicitly). -Matching: The number of predicted letters of the object detector and of the recognition model is different but there is at least one possible matching in the solutions. In this case, we expect that the use of the word classifier as an oracle will help to remove some ambiguities for the object detector. This may improve both the recognition and the segmentation.</p><p>-No matching: The number of predicted letters of the object detector and of the recognition model is different and there is no possible matching in the predictors' solutions. In this case, both the classification and the segmentation of the object detector are used (the Seq2Seq is ignored). In practice, in this case, we noticed that the Seq2seq was either predicting an additional letter or was missing one. It is thus important for the object detector to be able to ignore the oracle prediction when there is a strong conflict between both models. This filtering might thus improve the overall recognition results since the object detector will take over the Seq2Seq but only for the most difficult predictions.</p><p>(3) Compute the score associated with each sequence: the score of a sequence of size N a takes into account the degree of overlap between all the bounding boxes involved in the sequence. In particular, it minimizes the interletter overlap and also includes a coverage criterion to ensure a good coverage of the entire handwritten text. The overlapping score, s overlap , is the product of all edge weights weight v in the path of the graph G(V, E) corresponding to a sequence:</p><formula xml:id="formula_0">s overlap = Π Na i=1 weight v i<label>(1)</label></formula><p>The larger the overlap, the lower the score is. On the contrary to classic COCOstyle object detection contexts <ref type="bibr" target="#b23">[24]</ref>, in the handwriting context, there is almost no overlap between objects to detect except for the ligature area between the letters. To compute the coverage score and to count each pixel only once, we add the number of pixels contained in each prediction and the number of pixels contained in the intersection of the two predictions is subtracted from the number of pixels contained in each prediction. Then, the predicted number of pixels is divided by the total number of pixels as follows:</p><formula xml:id="formula_1">s cover = (Σ Na i=1 N p pred i -Σ Na i=2 N p inter(pred i-1 , pred i ))/N p total<label>(2)</label></formula><p>The final s alignment score is defined as:</p><formula xml:id="formula_2">s alignement = s overlap + s cover<label>(3)</label></formula><p>The output of the Seq2Seg model is the semantic segmentation computed from the bounding boxes of the sequence with the highest s alignment score together with the predictions of the Seq2Seq model for each letter. We can note that the efficiency of this method in terms of computation time depends on the size of the generated graphs. In our children handwriting context, the graphs associated with the words are small because the words are smaller than 10 letters. The computation time of this method is therefore low enough to provide immediate feedback.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Level B: Use of a Segmentation Lattice Based on Online Handwriting</head><p>The online handwriting can be split a priori into different segmentation hypotheses grouped in a segmentation lattice using heuristics (and without letter recognition). This process is detailed in <ref type="bibr" target="#b1">[2]</ref> and consolidated in <ref type="bibr" target="#b3">[4]</ref>. Furthermore, the online signal makes it possible to obtain a first automatic semantic segmentation for each hypothesis where two classes are considered: background and handwriting. Our goal is to use this lattice to find the "nearest" hypotheses of the segmentation lattice associated to the bounding boxes predicted by the object detector as illustrated in Figure <ref type="figure" target="#fig_4">5</ref>. The similarity between the lattice nodes and the bounding boxes is computed with an IoU P ixel i.e. an Intersection-over-Union between the handwriting pix-els contained in a bounding box (easily accessible as explained before) and the ones in a node of the segmentation lattice. By associating the hypotheses of the lattice with the bounding boxes, this method refines the coordinates of the bounding boxes and thus increase the precision of the segmentation of the object detector. Moreover, this approach also provides a better segmentation for slant handwriting than the "bounding box to segmentation" trivial correspondence proposed in Section 2.3. This is illustrated in Figure <ref type="figure" target="#fig_4">5</ref> for the letter "l" and "o".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset</head><p>Children cursive handwriting is acquired on pen-based tablet at schools. The French handwritten words are acquired as an online signal encoded by a sequence of points represented by two coordinates (x, y), a pressure and a timestamp. The online signal is used to compute the segmentation lattice presented in the previous section and is converted into an image with linked points and a thickness of 2 on the links. The input images are padded (x axis) and resized (y axis) at 1 280 × 128 pixels to fit the used deep learning models. Table <ref type="table" target="#tab_0">1</ref> details the datasets used to train/test the deep learning models (which are all variants of the acquired children words). The original dataset is composed of 8 054 French handwritten cursive words annotated at the word level that are useful to train the Seq2Seq network. Besides, 2 126 words are annotated (i.e. segmented) at the letter level to train the R-CNN object detector. The number of letter-annotated data being limited, we redefined the splits compared to <ref type="bibr" target="#b14">[15]</ref> and between the two models, to better train the object detector. The children writers are different for the training and the testing and the test set is the same for all models. Due to GDPR restrictions on children's private data, this dataset is not public. To better train the R-CNN model (that is data greedy), we perform data augmentation only on the training set (called "with synthesis" in the table). Among a list of usual offline deformations (stretching, slant) and more recent ones (stroke stretching, curvature <ref type="bibr" target="#b24">[25]</ref>), each word is augmented 30 times with random parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation and Evaluation Metrics</head><p>Implementation: The Seq2Seq model follows the same architecture and training protocol as in <ref type="bibr" target="#b14">[15]</ref>. The model performs poorly with only children handwriting dataset and to our knowledge, there is no other children handwriting dataset available. Therefore the model is pre-trained on an adult handwriting dataset <ref type="bibr" target="#b9">[10]</ref> and then fine-tuned on the children handwriting dataset. The model is trained during 200 epochs with a batch size of 16. The RMS prop optimizer is used with a learning rate of 0.001. Since the test set is different, we reevaluate the method from <ref type="bibr" target="#b14">[15]</ref> on our dataset. The R-CNN with a ResNet-FPN backbone is trained during 60 epochs with a batch size of 4. The AdamW <ref type="bibr" target="#b25">[26]</ref> optimizer is used with a learning rate of 0.0001. The R-CNN parameters are indicated in the original article <ref type="bibr" target="#b16">[17]</ref> except that we ignore the Mask branch, we use the Complete Intersection Over Union (CIoU) <ref type="bibr" target="#b26">[27]</ref> criterion to match the ground truths and the predictions during the training phase. We also add an NMS filtering independent of the class on the outputs to handle nested predictions as explained in Section 3.1. We set all parameters of the R-CNN model on the validation set using the Mean Average Precision (MAP) performance score before evaluating the best model on the test set.</p><p>Metrics: To evaluate the performance of our Seq2Seg approach, we use the usual Character Error Rate (CER) and Word Error Rate (WER) with a Damerau Leveinshtein <ref type="bibr" target="#b27">[28]</ref> distance for the recognition performance. We use Intersection Over Union (IoU) and IoU at pixel level to evaluate the segmentation performance. As explained before, the IoU P ixel focuses on the handwriting lines and ignores the (mostly white) background.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Quantitative Results</head><p>We first perform an ablation study to measure the impact of our different contributions as well as the choice of features extractor backbone in object detector. Then, we compare our approach to the state-of-the-art models on our data. All experiments are evaluated in terms of recognition, segmentation and computing speed on the test set. While the networks are trained on GPU, the processing time is computed on a laptop with an Intel Core i7-8665U CPU. Indeed, education applications are run on a pen-based tablet, where an internet connection is not always available. Therefore, timing analysis is more relevant on a CPU-equiped laptop. We consider acceptable an analysis time lower than 2 seconds to deliver immediate feedback to the children.</p><p>Table <ref type="table">2</ref> shows the results of the ablation study, where Level A corresponds to the filtering method of the object detector predictions with the result of a recognition model presented in Section 3.1 and Level B corresponding to the refinement of the bounding boxes coordinates of the object detector with a segmentation lattice presented in Section 3.2. We denote Seq2Seq as the result of the encoder part and use only the encoder result in this work, as recommended in <ref type="bibr" target="#b14">[15]</ref>. We can see in the table that the choice of a deeper backbone (we tried 18 to 101 layers) in the object detector (R-CNN) improves the performance in Table <ref type="table">2</ref>. Ablation study of the object detector backbone and impact of our contributions. Recognition is evaluated with Character Error Rate (CER) and Word Error Rate (WER) (lower values are better). Segmentation is evaluated with Intersection Over Union (IoU) and IoU P ixel (higher values are better). The Average time is the averaged number of seconds for a method to analyze a word.  ). We chose the backbone ResNet-34 FPN for the next experiments due to its speed and slightly better performance in recognition.</p><p>Level A: filtering the object detector's predictions with the results of the Seq2Seq allows us to obtain slightly better results in recognition (CER of 5%) than the Seq2Seq alone (CER of 5.3%). The reasons for this are given in the "no matching case" of the second step of the first contribution presented in Section 3.1. Furthermore, this method selects the bounding boxes to maximize the coverage and minimize the overlap of the handwriting and thus improves the segmentation of the object detector. Table <ref type="table" target="#tab_2">3</ref> details the different scenarios of filtering and their contributions to the performance compared to the object detector and the Seq2Seq performance alone:</p><p>-In the scenario where the number of predictions of the two models is equal, the Level A improves only the recognition performance as expected. This scenario concerns most of the words. -In the scenario where the number of predictions is different and a matching exist, the gain is the highest. Indeed, the strategy makes it possible to filter the bad predictions of the object detector.</p><p>-For a few words, nothing is filtered out and thus this contribution does not improve the object detector performance. In practice, this corresponds to words for which the recognizer makes more mistakes than the object detector. Level B: refining the bounding boxes coordinates by the use of a segmentation lattice improves the R-CNN segmentation performance for a small computing cost.</p><p>The results of the competitors are shown in Table <ref type="table" target="#tab_3">4</ref>. The best recognition and segmentation performance on our dataset are given by <ref type="bibr" target="#b14">[15]</ref> with a small margin compared to Seq2Seg (+0.1% CER, +1.6% IoU P ixel ), a high computation cost (5,07s, +3,45s compared to Seq2Seg) and using a language model. To overcome this computation cost, the authors of <ref type="bibr" target="#b14">[15]</ref> have proposed a pruning strategy (shown in the second line). This strategy degrades the recognition performance as well as the segmentation one which makes it significantly lower than Seq2Seg for recognition and segmentation (-2.6% CER, -4.3% WER, +1.3% IoU, +2.6% IoU P ixel ). The following section presents a qualitative analysis of the results obtained and shows the limits associated with the children handwriting recognition and segmentation tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Qualitative Results</head><p>This section presents a qualitative analysis of the results obtained by Seq2Seg. The goal is to visualize the effect of each contribution, i.e. the impact of Level A and Level B contributions. In these visualization examples, the output of the object detector corresponds to the predictions before the last NMS was performed independently of the class of the predicted bounding box. Figure <ref type="figure" target="#fig_5">6</ref> emphasizes the relevance of Level A contribution. The filtering process by the recognition model selects the correct number of letters by minimizing the overlap and maximizing the coverage rate. Moreover, the use of the segmentation lattice in Level B contribution produces a precise segmentation of the handwriting words especially in example 1 where the bounding boxes of the letters "i" and "t" overlap. Figure <ref type="figure" target="#fig_6">7</ref> illustrates examples where the recognition model makes errors. In example 1, there is no matching between the prediction of the recognition model and the object detector. We can see that the Seq2Seq makes recognition errors and therefore its associated filtering would be wrong. In this case, the bounding boxes and labels predicted by the object detector are used and provide an accurate result in recognition and segmentation. Example 2 shows a case where the filtering by the recognition model leads to a segmentation error. In addition, we can note the omission of the drawing of the point of the "i" in example 2 which is quite common in a context of learning how to write.</p><p>Note that evaluating the quality of the handwriting segmentation with the currently used metrics is difficult. Indeed, it is not easy to define an absolute segmentation ground truth for some letters due to the ligature area between letters. Thus, a prediction can have an IoU lower than 100% with the ground truth while the associated segmentation is correct. Moreover, the ground truth class associated with a degraded letter can vary according to the annotator (confusion between the letter "e" and the letter "l", "a" and "o" ...). Taking into account the uncertainty in the predictions might be helpful to know when a (human) teacher should take over the automated system to provide a more useful advice to the children.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We presented Seq2Seg, an original combination strategy which uses a model dedicated to recognition as an oracle to filter out the segmentation predictions of an object detector and then refines the segmentation using an expert segmentation lattice. Seq2Seg produces the best of both worlds: the accurate recognition of a Seq2Seq and the precise segmentation provided by an R-CNN object detector. Seq2Seg is efficient enough to provide immediate feedback to children learning how to write and it outperforms the state of the art results on this task without the use of a language model. This last point makes Seq2Seg much more flexible to other learning contexts. Our future work will focus on evaluating and improving the quality of the feedback in school contexts. In particular, we plan to better leverage the uncertainty of the decisions (both for the Seq2Seq and the object detector), for example by allowing the system to reject hypotheses, to prevent giving erroneous feedback to the children.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Examples of cursive children handwriting. The oral French instruction given to the children is provided in orange and examples of feedback are drawn in red. Line a shows some degraded handwriting, line b, phonetic errors and line c shows other types of errors in a context of learning spelling.</figDesc><graphic coords="3,134.77,115.83,345.83,174.22" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Summary of levels A and B contributions.</figDesc><graphic coords="6,134.77,279.40,345.84,266.64" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Examples of ambiguity in object detector predictions: the correct prediction is in full line and the wrong ones in dash.</figDesc><graphic coords="7,165.40,306.38,284.57,85.04" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Level A: Example the three steps of the process of filtering object detector predictions with the result of a recognition model.</figDesc><graphic coords="8,134.77,115.83,345.83,301.23" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Example of bounding boxes refinement with the online segmentation lattice. IoU P ixel is used to select the best lattice hypothesis.</figDesc><graphic coords="10,134.77,343.45,345.84,241.25" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Examples with an accurate recognition and a precise segmentation.</figDesc><graphic coords="15,134.77,285.97,345.83,198.48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Examples where the recognition model makes errors: in example 1, there is no matching between the recognition model and the object detector. In example 2 the filtering leads to an under-segmentation error.</figDesc><graphic coords="16,134.77,115.84,345.83,197.08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Details on the data used to train the deep learning models.</figDesc><table><row><cell>Models</cell><cell cols="4">Annotation type Training Validation Test Total</cell></row><row><cell>Seq2Seq</cell><cell>Words</cell><cell>6 022</cell><cell>1 000</cell><cell>1 032 8 054</cell></row><row><cell>R-CNN</cell><cell>Letters</cell><cell>918</cell><cell>176</cell><cell>1 032 2 126</cell></row><row><cell cols="2">R-CNN with synthesis Letters</cell><cell cols="2">27 540 176</cell><cell>1 032 28 748</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Number of words by scenario of filtering between R-CNN and Seq2Seq.</figDesc><table><row><cell cols="8">Performance of models alone and level A contribution. R-CNN uses ResNet34-FPN</cell></row><row><cell>backbone.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">R-CNN</cell><cell cols="2">Seq2Seq</cell><cell cols="2">Level A</cell></row><row><cell>Filtering type</cell><cell cols="7">#Words CER (%) IoU (%) CER (%) IoU (%) CER (%) IoU (%)</cell></row><row><cell cols="2">Perfect Matching 857</cell><cell>8.0</cell><cell>85.6</cell><cell>3.8</cell><cell>50.2</cell><cell>3.8</cell><cell>85.6</cell></row><row><cell>Matching</cell><cell>164</cell><cell>34.7</cell><cell>47.9</cell><cell>11.5</cell><cell>40.0</cell><cell>11.5</cell><cell>64.6</cell></row><row><cell>No Matching</cell><cell>11</cell><cell>1.8</cell><cell>87.2</cell><cell>36.6</cell><cell>31.3</cell><cell>1.8</cell><cell>87.2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Comparison to state-of-the-art approaches. Recognition is evaluated with Character Error Rate (CER) and Word Error Rate (WER) (lower values are better). Segmentation is evaluated with Intersection Over Union (IoU) and IoU P ixel (higher values are better). The Average time is the averaged number of seconds for a method to analyze a word. LM stand for "Language Model".</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Recognition</cell><cell cols="2">Segmentation</cell><cell>Time</cell></row><row><cell>Method</cell><cell cols="6">LM CER (%) WER (%) IoU (%) IoUP ixel (%) Average Time (s)</cell></row><row><cell>Fusion competition [15]</cell><cell>Yes</cell><cell>4.9</cell><cell>16.1</cell><cell>89.2</cell><cell>90.5</cell><cell>5.07</cell></row><row><cell cols="2">Fusion competition (pruning) [15] Yes</cell><cell>7.6</cell><cell>22.9</cell><cell>84.8</cell><cell>86.3</cell><cell>0.72</cell></row><row><cell>Seq2Seg (Our)</cell><cell>No</cell><cell>5.0</cell><cell>18.6</cell><cell>86.1</cell><cell>88.9</cell><cell>1.62</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Machine recognition of handwritten words: A project report</title>
		<author>
			<persName><forename type="first">Kenneth</forename><forename type="middle">M</forename><surname>Sayre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="213" to="228" />
			<date type="published" when="1973">1973</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Perceptual model of handwriting drawing application to the handwriting segmentation problem</title>
		<author>
			<persName><forename type="first">Éric</forename><surname>Anquetil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guy</forename><surname>Lorette</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">4th International Conference Document Analysis and Recognition (ICDAR &apos;97), 2-Volume Set</title>
		<meeting><address><addrLine>Ulm, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="1997">August 18-20, 1997. 1997</date>
			<biblScope unit="page">112</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">On-line Handwriting Character Recognition System Based on Hierarchical Qualitative Fuzzy Modelling</title>
		<author>
			<persName><forename type="first">Eric</forename><surname>Anquetil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guy</forename><surname>Lorette</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Progress in Handwriting Recognition</title>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="109" to="116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Evaluation of Children Cursive Handwritten Words for e-Education</title>
		<author>
			<persName><forename type="first">Damien</forename><surname>Simonnet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathalie</forename><surname>Girard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Éric</forename><surname>Anquetil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mickaël</forename><surname>Renault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sébastien</forename><surname>Thomas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">121</biblScope>
			<biblScope unit="page" from="133" to="139" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Evaluating sequence-to-sequence models for handwritten text recognition</title>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roger</forename><surname>Labahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tobias</forename><surname>Grüning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jochen</forename><surname>Zöllner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 International Conference on Document Analysis and Recognition, ICDAR 2019</title>
		<meeting><address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">September 20-25, 2019</date>
			<biblScope unit="page" from="1286" to="1293" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">End-to-end handwritten paragraph text recognition using a vertical attention network</title>
		<author>
			<persName><forename type="first">Denis</forename><surname>Coquenet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clement</forename><surname>Chatelain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thierry</forename><surname>Paquet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Pay attention to what you read: Non-recurrent handwritten text-line recognition</title>
		<author>
			<persName><forename type="first">Lei</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pau</forename><surname>Riba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marçal</forename><surname>Rusiñol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alicia</forename><surname>Fornés</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mauricio</forename><surname>Villegas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">129</biblScope>
			<biblScope unit="page">108766</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Transformers for Historical Handwritten Text Recognition</title>
		<author>
			<persName><forename type="first">Yann</forename><surname>Killian Barrere</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aurélie</forename><surname>Soullard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bertrand</forename><surname>Lemaitre</surname></persName>
		</author>
		<author>
			<persName><surname>Coüasnon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Doctoral Consortium -ICDAR 2021</title>
		<meeting><address><addrLine>Lausanne, Switzerland</addrLine></address></meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A full english sentence database for off-line handwriting recognition</title>
		<author>
			<persName><forename type="first">Urs-Viktor</forename><surname>Marti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Horst</forename><surname>Bunke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fifth International Conference on Document Analysis and Recognition, ICDAR 1999</title>
		<meeting><address><addrLine>Bangalore, India</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="1999-09-22">20-22 September, 1999</date>
			<biblScope unit="page" from="705" to="708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Iam-ondb -an on-line english sentence database acquired from handwritten text on a whiteboard</title>
		<author>
			<persName><forename type="first">Marcus</forename><surname>Liwicki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Horst</forename><surname>Bunke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eighth International Conference on Document Analysis and Recognition (ICDAR 2005)</title>
		<meeting><address><addrLine>Seoul, Korea</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2005-09-01">29 August -1 September 2005. 2005</date>
			<biblScope unit="page" from="956" to="961" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Santiago</forename><surname>Fernández</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Faustino</forename><forename type="middle">J</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning, Proceedings of the Twenty-Third International Conference (ICML 2006)</title>
		<title level="s">ACM International Conference Proceeding Series</title>
		<meeting><address><addrLine>Pittsburgh, Pennsylvania, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006">June 25-29, 2006</date>
			<biblScope unit="volume">148</biblScope>
			<biblScope unit="page" from="369" to="376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Why does CTC result in peaky behavior?</title>
		<author>
			<persName><forename type="first">Albert</forename><surname>Zeyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ralf</forename><surname>Schlüter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
		<idno>CoRR, abs/2105.14849</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Connectionist temporal classification with maximum entropy regularization</title>
		<author>
			<persName><forename type="first">Hu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changshui</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Hanna</forename><forename type="middle">M</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Kristen</forename><surname>Grauman</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Nicolò</forename><surname>Cesa-Bianchi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Roman</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="839" to="849" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Reinterpreting CTC training as iterative fitting</title>
		<author>
			<persName><forename type="first">Hongzhu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiqiang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">105</biblScope>
			<biblScope unit="page">107392</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Combination of explicit segmentation with Seq2Seq recognition for fine analysis of children handwriting</title>
		<author>
			<persName><forename type="first">Omar</forename><surname>Krichen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Corbillé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Éric</forename><surname>Anquetil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathalie</forename><surname>Girard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Élisa</forename><surname>Fromont</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pauline</forename><surname>Nerdeux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal on Document Analysis and Recognition</title>
		<imprint>
			<date type="published" when="2022-09">September 2022</date>
			<publisher>IJDAR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Online analysis of children handwritten words in dictation context</title>
		<author>
			<persName><forename type="first">Omar</forename><surname>Krichen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Corbillé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Éric</forename><surname>Anquetil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathalie</forename><surname>Girard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pauline</forename><surname>Nerdeux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Document Analysis and Recognition, ICDAR 2021 Workshops</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">Elisa</forename><forename type="middle">H Barney</forename><surname>Smith</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Umapada</forename><surname>Pal</surname></persName>
		</editor>
		<meeting><address><addrLine>Lausanne, Switzerland</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">September 5-10, 2021. 2021</date>
			<biblScope unit="volume">12916</biblScope>
			<biblScope unit="page" from="125" to="140" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Mask R-CNN</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision, ICCV 2017</title>
		<meeting><address><addrLine>Venice, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2017">October 22-29, 2017. 2017</date>
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Faster R-CNN: towards real-time object detection with region proposal networks</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><forename type="middle">B</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">Corinna</forename><surname>Cortes</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Neil</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Daniel</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Masashi</forename><surname>Sugiyama</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Roman</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Libra R-CNN: towards balanced learning for object detection</title>
		<author>
			<persName><forename type="first">Jiangmiao</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huajun</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition, CVPR</title>
		<imprint>
			<publisher>Computer Vision Foundation / IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="821" to="830" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Santosh</forename><surname>Kumar Divvala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016</title>
		<meeting><address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2016">June 27-30, 2016. 2016</date>
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Yolov4: Optimal speed and accuracy of object detection</title>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Bochkovskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chien-Yao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hong-Yuan Mark</forename><surname>Liao</surname></persName>
		</author>
		<idno>CoRR, abs/2004.10934</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Yolov6: A single-stage object detection framework for industrial applications</title>
		<author>
			<persName><forename type="first">Chuyi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lulu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongliang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiheng</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifei</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zaidan</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiqiang</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiduo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yufei</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linyuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoming</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangxiang</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoming</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaolin</forename><surname>Wei</surname></persName>
		</author>
		<idno>CoRR, abs/2209.02976</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Yolov7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors</title>
		<author>
			<persName><forename type="first">Chien-Yao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Bochkovskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hong-Yuan Mark</forename><surname>Liao</surname></persName>
		</author>
		<idno>CoRR, abs/2207.02696</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Microsoft COCO: common objects in context</title>
		<author>
			<persName><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serge</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2014 -13th European Conference</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">David</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Tomás</forename><surname>Pajdla</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Tinne</forename><surname>Tuytelaars</surname></persName>
		</editor>
		<meeting><address><addrLine>Zurich, Switzerland</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">September 6-12, 2014. 2014</date>
			<biblScope unit="volume">8693</biblScope>
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Synthetic On-line Handwriting Generation by Distortions and Analogy</title>
		<author>
			<persName><forename type="first">Harold</forename><surname>Mouchère</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sabri</forename><surname>Bayoudh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Anquetil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Miclet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">13th Conference of the International Graphonomics Society (IGS2007)</title>
		<meeting><address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-11">November 2007</date>
			<biblScope unit="page" from="10" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations, ICLR 2019</title>
		<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">May 6-9, 2019. 2019</date>
		</imprint>
	</monogr>
	<note>OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Iciou: Improved loss based on complete intersection over union for bounding box regression</title>
		<author>
			<persName><forename type="first">Xufei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeong-Young</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="105686" to="105695" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A technique for computer detection and correction of spelling errors</title>
		<author>
			<persName><forename type="first">Fred</forename><surname>Damerau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="171" to="176" />
			<date type="published" when="1964">1964</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
