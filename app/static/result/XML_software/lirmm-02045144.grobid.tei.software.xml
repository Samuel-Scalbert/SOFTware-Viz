<TEI xmlns="http://www.tei-c.org/ns/1.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xml:space="preserve" xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Parallel Computation of PDFs on Big Spatial Data Using Spark</title>
				<funder ref="#_aAQrnAN">
					<orgName type="full">EU</orgName>
				</funder>
				<funder>
					<orgName type="full">MCTI/RNP-Brazil</orgName>
				</funder>
				<funder>
					<orgName type="full">FAPERJ</orgName>
				</funder>
				<funder>
					<orgName type="full">Inria Associated Team SciDISC</orgName>
				</funder>
				<funder>
					<orgName type="full">CNPq</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher />
				<availability status="unknown"><licence /></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Ji</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Noel</forename><surname>Moreno Lemus</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Esther</forename><surname>Pacitti</surname></persName>
							<email>esther.pacitti@lirmm.fr</email>
						</author>
						<author>
							<persName><forename type="first">Fabio</forename><surname>Porto</surname></persName>
							<email>fporto@lncc.br</email>
						</author>
						<author>
							<persName><forename type="first">Patrick</forename><surname>Valduriez</surname></persName>
							<email>patrick.valduriez@inria.fr</email>
						</author>
						<author>
							<persName><forename type="first">N</forename><surname>Moreno</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Inria and LIRMM</orgName>
								<orgName type="institution">Univ. of Montpelier</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<address>
									<settlement>Petrópolis</settlement>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Inria and LIRMM</orgName>
								<orgName type="institution">Univ. of Montpelier</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<address>
									<settlement>Porto</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">LNCC Petrópolis</orgName>
								<address>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="department">Inria and LIRMM</orgName>
								<orgName type="institution">Univ. of Montpelier</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Parallel Computation of PDFs on Big Spatial Data Using Spark</title>
					</analytic>
					<monogr>
						<imprint>
							<date />
						</imprint>
					</monogr>
					<idno type="MD5">0745960ECE459893D0150667F4493724</idno>
					<idno type="DOI">10.1007/s10619-019-07260-3</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-04-12T14:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid" />
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Spatial data</term>
					<term>big data</term>
					<term>parallel processing</term>
					<term>Spark</term>
				</keywords>
			</textClass>
			<abstract>
<div><p>We consider big spatial data, which is typically produced in scientific areas such as geological or seismic interpretation. The spatial data can be produced by observation (e.g. using sensors or soil instruments) or numerical simulation programs and correspond to points that represent a 3D soil cube area. However, errors in signal processing and modeling create some uncertainty, and thus a lack of accuracy in identifying geological or seismic phenomenons. Such uncertainty must be carefully analyzed. To analyze uncertainty, the main solution is to compute a Probability Density Function (PDF) of each point in the spatial cube area. However, computing PDFs on big spatial data can be very time consuming (from several hours to even months on a computer cluster). In this paper, we propose a new solution to efficiently compute such PDFs in parallel using <software>Spark</software>, with three methods: data grouping, machine learning prediction and sampling. We evaluate our solution by extensive experiments on different computer clusters using big data ranging from hundreds of GB to several TB. The experimental results show that our solution scales up very well and can reduce the execution time by a factor of 33 (in the order of seconds or minutes) compared with a baseline method.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div><head n="1">Introduction</head><p>Big spatial data is now routinely produced and used in scientific areas such as geological or seismic interpretation <ref type="bibr" target="#b9">[10]</ref>. The spatial data are produced by observation, using sensors <ref type="bibr" target="#b45">[46]</ref>, <ref type="bibr" target="#b11">[12]</ref> or soil instruments <ref type="bibr" target="#b24">[25]</ref>, or numerical simulation, using mathematical models <ref type="bibr" target="#b14">[15]</ref>. These spatial data allow identifying some phenomenon over a spatial reference <ref type="bibr" target="#b18">[19]</ref>. For instance, the Fig. <ref type="figure">1</ref>: The distribution of a point. spatial reference may be a three dimensional soil cube area and the phenomenon a seismic fault, represented as quantities of interest (QOIs) of sampled points (or points for short) in the cube space. The cube area is composed of multiple horizontal slices, each slice having multiple lines and each line having multiple points. A single simulation produces a spatial dataset whose points represent a 3D soil cube area.</p><p>However, errors in signal processing and modeling create some uncertainty, and thus a lack of accuracy when identifying phenomenons. Such uncertainty must be carefully quantified. In order to understand uncertainty, several simulation runs with different input parameters are usually conducted, thus generating multiple spatial datasets that can be very big, e.g. hundreds of GB or TB. Within multiple spatial datasets, each point in the cube area is associated to a set of different observation values. The observation values are captured by sensors, or generated from simulation, at a specific point of the spatial area.</p><p>Uncertainty quantification of spatial data is of much importance for geological or seismic scientists <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b44">[45]</ref>. It is the process of quantifying the uncertainty error of each point in the spatial cube space, which requires computing a Probability Density Function (PDF) of each point <ref type="bibr" target="#b25">[26]</ref>. The PDF is composed of the distribution type (e.g. normal, exponential) and necessary statistical parameters (e.g. the mean and standard deviation values for normal and rate for exponential).</p><p>Figure <ref type="figure">1</ref> shows that the set of observation values at a point may be modelled by four distribution types, i.e. uniform (a), normal (b), exponential (c), and log-normal (d). The horizontal axis represents the values (V) and the vertical axis represents the frequency (F). The green bars represent the frequency of the observation values in value intervals and the red outline represents the calculated PDF. Given a resulting PDF modeling the data distribution at a point, there may be some error between the distribution of the observation values and one produced by a PDF.</p><p>During the process of fitting data to a PDF, we aim at reducing such PDF error (or error for short in the rest of the paper) to a minimum. For instance, the set of observation values corresponding to the QOI at a point obeys a normal distribution shown in Figure <ref type="figure">1 (b)</ref>. The mean value of the set of observation values (see Equation <ref type="formula" target="#formula_0">1</ref>) may be used as a representative of QOI since it has the highest chance to be the QOI. However, the distribution can be different from normal. And analyzing uncertainty just using the mean or standard deviation values is difficult and imprecise. For instance, if the distribution type of simulated values is exponential (see Figure <ref type="figure">1 (c</ref>)), we should take the value zero (different from the mean value of the simulated values) as the QOI value since it has the highest frequency. Thus, once the PDF of observations in a point has been computed, we can calculate the QOI value that has the highest probability. The QOI value can be assumed to represent the imprecision among the point observations, and computed across the whole spatial dataset as a measure of point observations uncertainty.</p><p>Calculating the PDF that best fits the observation values at each point can be time consuming. For instance, one simulation of an area of 10km (distance) * 10km (depth) * 5km (width) corresponds to 2.4 TB data with 10000 measurements at each point <ref type="bibr" target="#b1">[2]</ref>. This area contains 6.25 * 10 8 points. The time to calculate the PDF with consideration of 4 distribution types (normal, uniform, exponential and log-normal) can be up to several days or months using a computer cluster (or cluster for short).</p><p>The problem we address is how to efficiently compute PDFs under bounded error constraints. There are three main challenges, i.e. big data, long and complex computation and error bound. First, the input spatial datasets can be very big. Imagine that the size of each spatial dataset is several hundred MB, then 10,000 spatial datasets correspond to several TB, which makes it hard to process. Second, computing PDFs is complex and takes time. The PDF computation with the datasets of several TB in a single computer or a small cluster (6 nodes, each with 32 CPU cores) may take up to several months. Third, the PDF computation should not have unacceptable error, i.e. under error bound. For instance, if the error is bigger than 50%, then the result will not be representative.</p><p>Solutions for computing PDFs are now available in popular programming environments for numerical and statistical analysis, such as <software ContextAttributes="used">MATLAB</software> and R. <software ContextAttributes="used">MATLAB</software> provides libraries <ref type="bibr" target="#b30">[31]</ref> <ref type="bibr" target="#b35">[36]</ref> that use a baseline method for PDF fitting. R provides the <software ContextAttributes="used">GLDEX</software> library that supports the Generalized Lambda Distribution (GLD) family of PDFs. However, these implementations do not address the problem of efficient computation of thousands or millions of PDFs that may lead to redundant computation. Machine Learning (ML) techniques are widely used to process big data <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b13">14]</ref> for the sake of prediction and classification <ref type="bibr" target="#b43">[44]</ref>. They can be used for reducing the dimension of data <ref type="bibr" target="#b23">[24]</ref>, reducing the time for repeating work <ref type="bibr" target="#b7">[8]</ref> and discovering knowledge based on big data <ref type="bibr" target="#b20">[21]</ref>. However, unlike the traditional approach, we do not use ML to reduce the data dimension. Instead, we use ML to discover new knowledge in order to reduce redundant computation of PDFs. Furthermore, we use data aggregation to reduce the quantity of data to process.</p><p>In this paper, we propose a new solution to efficiently compute PDFs in parallel using <software ContextAttributes="used">Spark</software> <ref type="bibr" target="#b48">[49]</ref>, a popular in-memory big data processing framework (see <ref type="bibr" target="#b28">[29]</ref> for a survey on big data systems). The main reason to choose <software ContextAttributes="used">Spark</software> is that, unlike <software ContextAttributes="used">MPI</software>, it makes it easy to parallelize application code in a cluster. Futhermore, it makes efficient use of main memory to deal with intermediate data (RDD) that is used repeatedly, as with ML algorithms. In addition to deploying PDF computation over <software ContextAttributes="used">Spark</software>, we propose three new methods to efficiently compute PDFs: data grouping, ML prediction and sampling. Data grouping consists in grouping similar points to compute the PDF. In the original input data, the data corresponding to some points may be the same or very similar to the data corresponding to a common point. ML prediction uses ML classification methods to predict the distribution type of each point. Sampling method enables to efficiently compute statistical parameters of a region by sampling a fraction of the total number of points to reduce the computation space.</p><p>To validate our solution, we use the spatial data generated from simulations based on the models from the seismic benchmark of the HPC4e project between Europe and Brazil for oil and gas exploration <ref type="bibr" target="#b1">[2]</ref>. This benchmark includes <software ContextAttributes="used">MATLAB</software> models for seismic wave propagation. In oil and gas exploration, seismic waves are sent deep into the Earth and allowed to bounce back. Geophysicists record the waves to learn about oil and gas reservoirs located beneath Earth's surface.</p><p>This paper makes the following contributions:</p><p>-A scalable approach and architecture to compute PDFs of QOIs in large spatial datasets using <software>Spark</software>; -Three new methods to reduce the time of computing PDFs, i.e. data grouping, ML prediction and sampling; -An extensive experimental evaluation based on the implementation of the methods in a <software ContextAttributes="used">Spark</software>/HDFS cluster and big datasets ranging from 235 GB to 2.4 TB. The experimental results show that our methods scale up very well and reduce the execution time by a factor of 33 (in the order of seconds or minutes) compared with a baseline method;</p><p>The three new methods, i.e. data grouping, ML prediction and sampling, are the main contributions of the paper. In addition, the layered architecture and the optimization methods (see details in Section 5.2) are also contributions for the basic processing of PDFs. The architecture is deployed in the execution environment and the optimization methods are directly implemented in a <software>Scala</software> <software ContextAttributes="used">program</software>, thus facilitating the use of the three new methods. The paper is organized as follows. Section 2 discusses related work. Section 3 introduces some background on <software ContextAttributes="used">Spark</software>. Section 4 gives the problem definition. Section 5 presents our architecture for computing PDFs with <software ContextAttributes="used">Spark</software>, with its main functions. Section 6 presents our solution to compute PDFs in parallel, with three methods, i.e. data grouping, ML prediction and sampling. Section 7 presents our experimental evaluation on different clusters with different data sizes, ranging from hundreds of GB to several TB. Section 8 concludes.</p></div>
<div><head n="2">Related work</head><p>The importance of modeling data distribution through PDFs have been acknowledged in various domains such as seismology, finance, meteorology or civil engineering <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b46">47]</ref>. There is variability in almost any value that can be measured by observation and almost all measurements are made with some intrinsic error. Thus, simple numbers are often inadequate for describing a quantity, while PDFs are more appropriate. As the number of observations increases, efficient techniques to fit a set of observations to a PDF become paramount in practical applications <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b5">6]</ref>. For instance, the fitdistr function, which implements the Maximum Likelihood Estimation (MLE) (for normal, log-normal, geometric, exponential and Poisson distributions) and Nelder-Mead (NM) <ref type="bibr" target="#b34">[35]</ref> (for the other distributions) methods in R <ref type="bibr" target="#b0">[1]</ref>, calculates the PDF with small error for a given PDF type. This function scans the input data once and the time complexity of both methods is O(n), with n input observation values for the same point <ref type="bibr" target="#b41">[42]</ref>.</p><p>In this paper, we are interested in situations where large number of observations of the same quantity of interest are registered in thousands or even millions of spatial positions. This is the case, for instance, in numerical simulations that compute the values of QOIs through a set of points that discretizes a physical domain, e.g. representing the subsurface of the ocean in a seismic simulation. In this context, Campisano et al. <ref type="bibr" target="#b8">[9]</ref> model series of observations at spatial positions in a grid as spatial-time series. The model helps supporting predictions on QOIs. This work is orthogonal to ours, as we strive to inform about the uncertainty exhibited by observations drawn from a continuous domain of values through a spatial region, which can be obtained by fitting the distribution to a PDF.</p><p>Another approach adopts the Generalized Lambda Distribution (GLD), a function for modeling a family of distributions <ref type="bibr" target="#b36">[37]</ref>, such as: normal,uniform, exponential, lognormal, gamma, etc., using λ parameters, represented as GLD(λ 1 , λ 2 , λ 3 , λ 4 ). The lambda parameters specify the location, scale and both sides of the tail of distributions, respectively, thus making it general enough to cover a variety of distributions. This generality of the GLD function is particularly interesting to avoid testing for a best fit among possible PDF function types. GLDs however cover a space of values in the λ 3 and λ 4 parameter space while being invalid in some other regions. Thus, when the distribution of a certain set of observation values is not covered by the space of λ 3 and λ 4 , the GLD error can be very big. Therefore, for some distributions, the GLD approach may not offer a good fit, or even none at all. Moreover, there are a handfull of parameter estimation (i.e. data fitting) techniques, such as Maximum log-likelihood <ref type="bibr" target="#b4">[5]</ref>. Theses techniques solve an optimization problem that starts from a random initial parametrization and numerically integrates the space of probabilities minimizing, for example, the squared error, between the GLD and the set of observations. Therefore, for very large number of points (i.e. sets of observations) for which we may want to compute the corresponding GLD, specifying strategies that reduce the number of fitting computations is still needed and is the focus of this work.</p><p>In order to tackle the redundant PDF computation problem, we apply ML techniques. ML is now widely used to process big data <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b13">14]</ref> in activities such as regression analysis and classification <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b39">40]</ref>. It has been used to reduce the number of redundant computation by clustering data and reducing data dimensions to detect parameter dependent structures <ref type="bibr" target="#b7">[8]</ref> or discover knowledge out of big data <ref type="bibr" target="#b20">[21]</ref>. More recently, ML has been used as substitutes for traditional data structures in database systems, such as B+ tree indexing and caching <ref type="bibr" target="#b26">[27]</ref>.</p><p>Considering that fitting thousands or millions of sets of observations to PDF functions is a daunting task, we use ML to save total computation cost and reduce fitting elapsed time. In particular, we use the unsupervised k-means algorithm <ref type="bibr" target="#b32">[33]</ref> to cluster similar sets of observations and the decision tree supervised algorithm to classify the distribution types using the mean and variance statistical moments of known observation sets with associated PDF types as training data <ref type="bibr" target="#b19">[20]</ref>.</p></div>
<div><head n="3">Background on Spark</head><p><software ContextAttributes="used">Spark</software> <ref type="bibr" target="#b48">[49]</ref> is an <software ContextAttributes="used">Apache</software> open-source data processing framework. It extends the MapReduce model <ref type="bibr" target="#b15">[16]</ref> for two important classes of analytics applications: iterative processing (machine learning, graph processing) and interactive data mining (with R, <software ContextAttributes="used">Excel</software> or <software ContextAttributes="used">Python</software>). Compared with MapReduce, it improves the ease of use with the <software ContextAttributes="used">Scala</software> language (a functional extension of Java) and a rich set of operators (Map, Reduce, Filter, Join, Aggregate, Count, etc.). In <software ContextAttributes="used">Spark</software>, there are two types of operations: transformations, which create a new dataset from an existing one ( e.g. Map and Filter), and actions, which return a value to the user after running a computation on the dataset (e.g. Reduce and Count). <software ContextAttributes="used">Spark</software> can be deployed on shared-nothing clusters, i.e. clusters of commodity computers with no sharing of either disk or memory among computers. In a <software ContextAttributes="used">Spark</software> cluster, a master node is used to coordinate job execution while worker nodes are in charge of executing the parallel operations.</p><p><software>Spark</software> provides an important abstraction, called resilient distributed dataset (RDD), which is a read-only and fault-tolerant collection of data elements (represented as key-value pairs) partitioned across the nodes of a shared-nothing cluster. RDDs can be created from disk-based resident data in files or intermediate data produced by transformations. They can also be made memory resident for efficient reuse across parallel operations.</p><p><software ContextAttributes="used">Spark</software> data can be stored in the <software ContextAttributes="used">Hadoop</software> Distributed File System (HDFS) <ref type="bibr" target="#b40">[41]</ref>, a popular open source file system inspired by <software ContextAttributes="used">Google File System</software> <ref type="bibr" target="#b21">[22]</ref>. Like GFS, HDFS is a highly scalable, faulttolerant file system for shared-nothing clusters. HDFS partitions files into large blocks, which are distributed and replicated on multiple nodes. An HDFS file can be represented as a <software ContextAttributes="used">Spark</software> RDD and processed in parallel.</p><p><software>Spark</software> provides a functional-style programming interface with various operations to execute a user-provided function in parallel on an RDD. We can distinguish between operations without shuffling, e.g. Map and Filter, and with shuffling, e.g. Reduce, Aggregate and Join. Shuffling is the process of redistributing the data produced by an operation, e.g. Map, so that it gets partitioned for the next operation to be done in parallel, e.g. Reduce. This process is complex and expensive and requires moving data across cluster nodes.</p><p>In this paper, we exploit <software ContextAttributes="used">Spark</software> MLlib <ref type="bibr" target="#b2">[3]</ref>, a scalable machine learning (ML) library that can handle big data <ref type="bibr" target="#b27">[28]</ref> in memory by exploiting <software ContextAttributes="used">Spark</software> RDDs and <software ContextAttributes="used">Spark</software> operations. In one method </p></div>
<div><head n="4">Problem Definition</head><p>In this section, we first present how we generate the spatial datasets, which is the input for the problem. Then, we present the challenges to process the big input data. Afterwards, we present the problem in details.</p></div>
<div><head n="4.1">Spatial Dataset Generation</head><p>A spatial dataset contains the information to generate an observation value matrix that corresponds to a three dimensional cube area. As a result of running multiple simulations, multiple spatial datasets are produced with a set of observation values at each point. The set of observation values at each point enables the computation of their mean value, standard deviation values and PDF.</p><p>Let us illustrate the problem with the spatial data generated from simulations based on the models from the seismic benchmark of the HPC4e project <ref type="bibr" target="#b1">[2]</ref>. An important parameter of the models is wave phase velocity, noted V p in electromagnetic theory, which is the rate at which the phase of the wave propagates in space. The models contain 16 layers and each layer is associated to a value of V p. The top layer delineates the topography and contains the description information of the other 15 layers. Each of the 15 layers is used to generate the observation values of points in a horizontal space of the cube area. A set of 16 layers can generate a spatial dataset, where each point corresponds to an observation value. Then, N sets of 16 layers can generate N spatial datasets, where each point corresponds to a set of N observation values.</p><p>Since our purpose is to study the uncertainty in the output as a result of the wave propagation of the input uncertainty through the model, we assume that the input value of each layer is uncertain and obeys a PDF. The distribution types for every four layers are: Normal (for the first four layers), Log-normal (for the fourth to eighth layers), Exponential (for the ninth to twelfth layers) and Uniform (for the rest). We use a Monte Carlo method to generate different sets of the 16 input parameters. For each set of input parameters, we generate a spatial dataset using the models as shown in Figure <ref type="figure" target="#fig_0">2</ref>. In Figure <ref type="figure" target="#fig_0">2</ref>, we use the Monte Carlo method to generate a set of 16 variables, each of which corresponds to the V p of each layer. Then, we execute the <software ContextAttributes="used">MATLAB</software> programs of the seismic benchmark to generate a spatial dataset. Finally, we repeat this process multiple times in order to generate multiple datasets.</p></div>
<div><head n="4.2">Challenges</head><p>Computing PDFs on spatial datasets raises three main challenges, i.e. big data, long and complex computation and error bound.</p><p>First, as the input spatial datasets can be very big, a simple scan of the input data is not enough. After scanning the data, we need to perform a join operation to merge the observation values of each point with a set of observation values. This join operation may take much time when there are many points. Since the data corresponding to the same point are distributed at different nodes in a cluster, the join operation requires shuffling, which may take much time. In addition, when the input data is very big, we cannot cache all the data in memory.</p><p>Second, computing PDFs is complex and takes time for big spatial datasets. In order to calculate the PDF of a point, we use the fitdistr function, which implements MLE and NM method (a commonly used and robust numerical method) in an R <software>program</software>. The fitdistr function optimizes a cost function using multiple iterations, which may take much time. In addition, this function is only used for calculating the PDF of a single point, which takes a vector of observation values as input and calculates the PDF of a specific distribution type. Furthermore, this function is designed for execution in a single machine, and is not scalable for a cluster environment. In our problem, we need to first extract the observation values of each point from a spatial dataset. Then, for each point, we must calculate the PDF of an appropriate type, which corresponds to the minimum error. Finally, the results must be stored in a file system. This complex process can take much time with a basic approach. For instance, with a cluster of 6 nodes, each with 32 CPU cores, it takes about 63 days using the baseline method (see details in Section 6.1) and the fitdistr function in R language to compute the PDFs of all the points in all the slices of a cube with a dataset of 2.4TB. Thus, computing PDFs is complex and takes time for big spatial datasets.</p><p>Third, the PDF computation should not have unacceptable error, i.e. under error bound. For instance, if the error is bigger than 50%, then the result will not be representative. This requirement implies choosing a distribution type that corresponds to the minimum error. The fitdistr function in the R <software>program</software> can only fit the parameters corresponding to a specific distribution type to calculate the PDF based on a set of observation values. But it cannot choose a distribution type.</p></div>
<div><head n="4.3">Problem Formulation</head><p>The main problem we address is to efficiently compute PDFs on big spatial datasets, such as the seismic simulation data discussed above. Since it takes much time to compute the PDFs of the points in the whole cube area, our approach is to divide it into multiple spatial regions and compute the PDFs of all the points in a chosen region within a reasonable time. The spatial region is chosen based on some statistical parameters of the region. Thus, the main problem is how to efficiently calculate the PDF of each point in a spatial region, e.g. a horizontal slice in the cube area, with a small average error between the PDF and the distribution of the observation values. A slice is a set of points that correspond to the same position in one of the three dimensions. In order to choose a region, the mean and standard deviation values and the distribution type of a part of the points in the spatial region should be computed. In addition, the average mean value, the average standard deviation value and the percentage of points corresponding to each distribution type in all the points of the region can also be computed. We denote these statistical parameters of a spatial region to compute by the features of a spatial region. A related subproblem is how to efficiently calculate the features of a spatial region. In this paper, we take a horizontal slice as a spatial region.</p><p>The input data is the set of spatial datasets, which are generated from the simulation as explained in Section 4.1. Let DS be a set of spatial datasets, d be a spatial dataset in DS and N be the number of points in a slice (the same for Equations 3, 4 and 6). d is generated using a set of V p of the 16th layers (each set of V p is different). As shown in Figure <ref type="figure" target="#fig_1">3</ref>, each d is composed of a matrix of observation values corresponding to the sampled points in the cube area. X, Y , I represent the first, second and third dimensions of the cube area and the index of observation values in the dataset. In the spatial data domain, the first dimension (x) is inline; the second dimension (y) is crossline and the third dimension (i) is time or depth. A dataset is composed of multiple slices, and each slice is composed of multiple lines. Each slice corresponds to the same position in the third dimension (I), and each line corresponds to the same position in the second (Y ) and third (I) dimensions. In this paper, we take a slice (i.e. <software ContextAttributes="used">Slice</software> 201 in Section 7) as the region to calculate PDFs. Each point (p x,y,i ) is represented as an observation value in the dataset and the indices are x, y and i in each dimension. Based on multiple datasets, each point p x,y in <software ContextAttributes="used">Slice</software> i corresponds to a set of values V = {v 1 , v 2 , ..., v n } while v k is the observation value corresponding to the point p x,y in <software ContextAttributes="used">Slice</software> i in d k ∈ DS and n is the number of observation values in the set (the same for Equations 1, 2 and 5). d k represents the kth dataset (d) in DS.</p><p>Based on these notations, we define Equations 1 -4 and 6, which are based on the formulas in <ref type="bibr" target="#b17">[18]</ref>. The mean (µ x,y ) and standard deviation (σ x,y ) values of a point can be calculated according to Equations 1 and 2, respectively. And the average mean (µ i ) and standard deviation (σ i ) values of <software ContextAttributes="used">Slice</software> i can be calculated according to Equations 3 and 4, respectively. The error e x,y,i between the PDF F and the set of observation values V can be calculated according to Equation <ref type="formula" target="#formula_4">5</ref>, which compares the probability of the values in different intervals in V and the probability computed according to the PDF. The intervals are obtained by evenly splitting the space between the maximum value and the minimum value in V . min is the minimum value in V , max is the maximum value in V and L represents the number of all considered intervals, which can be configured. F req k represents the number of values in V that are in the kth interval. The integral of P DF (x) computes the probability according to the PDF in the kth interval. Equation 5 is inspired by the Kolmogorov-Smirnov Test <ref type="bibr" target="#b29">[30]</ref>, which tests whether a PDF is adequate for a dataset. In addition, we assume that the probability of the values outside the space between the maximum value and the minimum value is negligible for this equation. Then, the average error E of <software ContextAttributes="used">Slice</software> i can be calculated according to Equation <ref type="formula" target="#formula_5">6</ref>. </p><formula xml:id="formula_0">µ x,y = n i=1 v i n<label>(1)</label></formula><formula xml:id="formula_1">σ x,y = n i=1 (v i -µ) 2 n -1<label>(2)</label></formula><formula xml:id="formula_2">µ i = px,y∈slicei µ x,y N<label>(3)</label></formula><formula xml:id="formula_3">σ i = px,y∈slicei σ x,y N<label>(4)</label></formula><formula xml:id="formula_4">e x,y,i = L k=1 | F req k n - min+(max-min) * k L min+(max-min) * k-1 L P DF (x)dx |<label>(5)</label></formula><formula xml:id="formula_5">E = px,y∈slicei e x,y,i N<label>(6)</label></formula><p>We can now express the main problem as follows: given a set of spatial datasets DS = {d 1 , d 2 , ..., d n } corresponding to the same spatial cube area C = {slice 1 , slice 2 , ..., slice j }, how to efficiently calculate the mean, standard deviation values and the PDF F at each point in slice i ∈ C with a small average error E not higher than a predefined average error ε. In addition, we also need to compute the statistical parameters of slices in order to choose <software>Slice</software> i mentioned in the first subproblem. Thus, the related subproblem can be expressed as: how to efficiently calculate the features of a slice when given the same datasets DS. The features are:</p><p>µ, σ and distribution type of some points in the slice µ i , σ i and the percentage of points for each distribution type in all the points of the slice</p></div>
<div><head n="5">Architecture for Computing PDFs</head><p>In this section, we describe the architecture for PDF computation (see Figure <ref type="figure" target="#fig_2">4</ref>). This architecture has four layers, i.e. infrastructure, basic process to compute PDFs, memory management and methods to compute PDFs. The higher layers take advantage of the lower layers' services to implement their functionality. The infrastructure layer provides the basic execution environment, including <software ContextAttributes="used">Spark</software>, HDFS and Network File System (NFS) in a cluster. The basic processing layer provides guiding principles to load the big spatial data and to compute PDFs. The memory management layer allows optimizing the execution of the basic process by caching data and managing tumbling windows on big data. The methods to compute PDFs are presented in Section 6.</p><p>The architecture has four layers and decomposes the problem in smaller problems, which can be solved independently and efficiently. In each layer, we use different optimization methods, e.g. data caching, window size ajustement, parallel data loading, to speed up execution. As a result, the baseline approach (see details in Section 6.1) also benefits from our architecture, which explains why it yields relatively good performance and good scalability.  </p></div>
<div><head n="5.1">Infrastructure with Spark</head><p>Figure <ref type="figure" target="#fig_3">5</ref> illustrates the infrastructure we deploy to process big spatial data. The big spatial data is produced by simulation application programs and stored in NFS <ref type="bibr" target="#b38">[39]</ref>, a shared-disk file system that is popular in scientific applications. <software ContextAttributes="used">Spark</software> and HDFS are deployed over the nodes of the cluster. The intermediate data produced during PDF computation and the output data are stored in HDFS, which provides persistence and fault-tolerance.</p><p>Keeping the input spatial data in NFS allows us to maximize the use of the cluster resources (disk, memory and CPU), which can be fully dedicated for PDF computation. With the input data stored in NFS, the NFS server is outside the <software>Spark</software>/HDFS cluster and takes care of file management services, including transferring the data that is read to the cluster nodes. An alternative solution would have been to store the input data in HDFS, which would lead to have HDFS tasks competing with <software ContextAttributes="used">Spark</software> tasks for resource usage on the same cluster nodes. We did try this solution and it is much less efficient in terms of data transfer between cluste nodes. This is because HDFS is more complex and does more work due to fault-tolerance and data replication. Note that we use NFS only for data loading (see Algorithm 2), while the data generated from execution is persisted in HDFS.</p><p>We use <software>Spark</software> as our execution environment for computing PDFs. We developed a <software ContextAttributes="used">program</software> written in <software ContextAttributes="used">Scala</software>, which realizes the functionality of different methods to compute PDFs. Once the method to compute PDFs is chosen, the <software ContextAttributes="used">Scala</software> <software ContextAttributes="used">program</software> is executed as a job within <software ContextAttributes="used">Spark</software>.</p></div>
<div><head n="5.2">Principles for the Basic Processing of PDFs</head><p>The basic processing of PDFs consists of data loading, from NFS to <software>Spark</software> RDDs, followed by PDF computation using <software ContextAttributes="used">Spark</software>. The data loading process treats the data corresponding to a slice and pre-processes it, i.e. calculates statistical parameters of observation values of each point and relates the observation values of each point to an identification of the point. The identification of each point is an integer value which represents the location of the point in the cube area. Then, the PDF computation process groups the data and calculates the PDFs and errors of all the points in a slice based on the pre-processed data. To make the basic processing of PDFs efficient, we use the following guiding principles.</p><p>1. Parallel data loading. To perform data loading in parallel, we store the identifications of points in an RDD, which is evenly distributed on multiple cluster nodes. For each point in a node, all the corresponding values in different spatial datasets are retrieved from NFS. At the same time, the mean and standard deviation values are calculated. Then, the identification of the point is stored as the key of the point. The mean and standard deviation values and the observations values are stored as the value of the point. The key and value of the point are stored as a key-value pair in the RDD. This loading process is realized by a Map operation in <software>Spark</software>, which is fully parallel. 2. Data grouping. In order to avoid repeating the PDF computation for the same or similar sets of observation values, we group the data of different points that shares similar statistical features, e.g. the mean and standard values. Then, a representative point of the group is chosen. The PDF of the representative point is taken to represent all the points in the group. Thus, the PDF computation of all the points in the group is reduced to the computation of the representative point. The grouping can be realized using an Aggregation operation in <software ContextAttributes="used">Spark</software>. However, the shuffling process in the Aggregation operation may take much time. When this occurs, then we can simply avoid data grouping. To decide whether to use data grouping, the ideal solution would be to have a cost model, but it is difficult to estimate the time to transfer data within <software ContextAttributes="used">Spark</software>. However, we can test if data grouping is effective on small workloads, e.g. a few lines of points in a slice as explained in Section 7.</p><p>After data grouping, the set of the identifications of the points in the group is stored as a part of the value in the key-value pair. For each representative point, the key represents the identification while the value contains the mean and standard deviation values and the observation values. 3. Parallel processing of PDFs. The PDF of each point (or representative point) is computed based on its observation values. This process is also realized in a Map operation, which distributes the key-value pairs in RDD of different points to different nodes. There are two methods to calculate the PDF of a point, i.e. with ML and without ML. With ML, the distribution type corresponding of a point is predicted (see Section 6.3 for more details), and then the PDF and error, i.e. PDF error, are computed. Without ML, the PDFs of different distribution types are computed, and the PDF of the minimum error is chosen as the PDF of the point. The PDF computation of different points is executed in parallel in different nodes of the cluster. After the parallel processing of PDFs, the key remains the identification of each point (representative point) while the PDF is stored as a part of the value in the key-value pair of the RDD. In addition, since the mean and standard deviation values and the observation values are no longer useful, the corresponding data is removed from the value in the key-value pair. Finally, the PDF of each point is persisted in a file or database system for future use. In addition, an average error of the PDF of the points in the slice is calculated and shown as the result of executing the <software>Scala</software> <software ContextAttributes="used">program</software>. 4. Tumbling window. Since a slice can have many points that won't fit in memory, we use a tumbling window over the slice during data loading, data grouping and parallel processing of PDFs. A window represents a set of points to process, which corresponds to a swath area of several continuous lines in the slice to process. Any two windows have no intersection. After the processing of one window, the process of the next window begins until the end of the slice.</p><p>Once the size of the window is configured, it stays the same during execution. The size of the window has strong impact on execution and must be chosen carefully (see details in Section 5.3.2). Note that the tumbling window is different from the data chunk in <software>Spark</software>. Data chunk is generated by splitting the data in RDD, which is stored in each node of the cluster node.</p><p>The window here is a unit of points to process by all the nodes in the cluster. 5. Use of external programs. In the parallel data loading and parallel processing of PDFs, we use external programs, which do the specific loading of the points. Since some Java functions, e.g. skipBytes (Skips and discards a specified number of bytes in the current file input stream), may not work correctly during the parallel execution of a Map operation in <software ContextAttributes="used">Spark</software> <ref type="bibr" target="#b22">[23]</ref>, we call an external Java <software ContextAttributes="used">program</software> in the Map operation to retrieve observation values of a point from different spatial datasets and pre-process values. Since the PDF computation is implemented by an external <software ContextAttributes="used">program</software> (in R), we call it within the Map operation for the parallel processing of PDFs. Finally, the output data of the external <software ContextAttributes="used">program</software> is transformed to key-value pairs and stored in RDDs by the same Map operation, which executes the external <software ContextAttributes="used">program</software>.</p><p>Figure <ref type="figure" target="#fig_4">6</ref> shows the data flow of the process. First, the data is loaded from NFS in parallel. The data is stored in an RDD; the key is the identification (id) of each point and the value is the set of the mean and standard deviation value and the observation values of each point. The id of each point is calculated based on its indices in the three dimensions. If we apply data grouping, the key value pairs are processed in three steps. First, we take the mean and standard deviation value as the key and put the id and the set of observation values of each point as value. Then, the RDD is grouped based on the key, i.e. the mean and standard deviation values. The set of id of the points of the same group is stored in the value. In addition, we store the first set of the observation values in the value. Then, we take the set of id as the key, and the mean value, the standard deviation value and the set of observation values as the value. If we do not apply data grouping, we have only one id in the key. We calculate the PDF based on the set of observation values. Finally, we have the set of id as key and the PDF type and the PDF parameters as value, which is stored in HDFS.</p></div>
<div><head n="5.3">Memory Management</head><p>In order to efficiently compute PDFs, we use two memory management techniques to optimize the calculation of PDFs over big data: data caching and window size adjustment.</p></div>
<div><head n="5.3.1">Data Caching</head><p>We use data caching, i.e. keeping data in main memory, to reduce disk accesses. To identify which data to cache, we distinguish between four kinds of data: input data, instruction data, intermediate data and output data. The input data is the original data to be processed, i.e. the big spatial datasets. The instruction data is the data corresponding to the external programs, e.g. Java or R programs used in data loading and PDF computation. The intermediate data is the data generated by the data loading process or the execution of external programs, and used by subsequent execution. The output data is the final data generated by the PDF computation.</p><p>We use a simple caching strategy. We do not cache input data because it can be very big and read only once. We only cache instruction data and intermediate data, which are accessed much during execution. However, intermediate data that is not used in subsequent operations is removed from main memory. The output data is written to memory first and then persisted.</p><p>During execution, some intermediate data that is stored in <software ContextAttributes="used">Spark</software> RDDs can be cached using the <software ContextAttributes="used">Spark</software> Cache operation, which stores RDD data in main memory. However, the instruction data of external programs and the intermediate data directly generated by executing these external programs are outside of RDDs. We cache this data in temporary files in memory, using a memory-based file system <ref type="bibr" target="#b42">[43]</ref>. Then, the information in the cached files is retrieved and stored as intermediate data in RDDs, which can be again cached using the Cache operation of <software ContextAttributes="used">Spark</software>.</p></div>
<div><head n="5.3.2">Window Size Adjustment</head><p>The size of the tumbling window is critical for efficient PDF computation. When it is too small, the degree of parallel execution among multiple cluster nodes is low. Increasing the window size increases the degree of parallelism, but may introduce some overhead in terms of data transfers among nodes or management of concurrent tasks. Thus, it is important to find an optimal window size in order to make a good trade-off between the degree of parallelism and overhead.</p><p>To find the optimal window size, we distinguish between the data loading process and the PDF computation process, which have different data access patterns. In data loading, the processing of each point can be done independently by a Map operation in parallel. Thus, the overhead of data transfers and concurrent task management with a big window size is small. Thus, we can choose a window size that ensures that there is enough work to do for each node and that multiple nodes can be used. For instance, consider a cluster with n nodes, each node with c CPU cores. Assuming that the data loading for each point can occupy a CPU core, we can choose a maximum window size corresponding to n*c points. If the number of points is less than n*c, we choose the maximum number of points as the window size.</p><p>During PDF computation, the overhead of having a big window can be high since the processing of different points are not independent, in particular when we use data grouping. For instance, when the window size is bigger, the data of more points are present at each node. In order to group data, the data of each point need to be compared with the data of more points, which takes more time. In addition, there is much more data transferred among different nodes in the data shuffling process of data grouping. Furthermore, since the PDF computation takes much time, the management of concurrent tasks within each node also increases execution time for a big window. As a result, the overhead of having a big window becomes high for PDF computation.</p><p>To find an optimal window size, we test the <software>Scala</software> <software ContextAttributes="used">program</software> on a small workload (with a small number of points) with different window sizes, and then use the optimal size for the PDF computation of all the points in the slice.</p></div>
<div><head n="6">Methods to Compute PDFs</head><p>In this section, we present our methods to compute PDFs efficiently. First, we introduce a baseline method, which will be useful for comparison. Then, we propose two methods, i.e. data grouping and ML prediction, to compute PDFs, which addresses the main problem defined in Section 4. Finally, we propose a sampling method to calculate the features of a slice, which addresses the related problem.</p></div>
<div><head n="6.1">Baseline Method</head><p>The baseline method computes the PDF of each point in a slice as follows (see Algorithm 1). Line 2 loads the spatial datasets and calculates the mean and standard deviation values of each point by using Algorithm 2. Lines 3 -13 compute the PDF for all the points in the ith slice. Line 5 gets a window in the slice. Line 6 selects all the points in the window to process. The implementation of the select function is different for data grouping (see details in Section 6.2). For each point in the window, Line 8 gets the set of corresponding observation values, which can be automatically realized by <software>Spark</software> as the <software ContextAttributes="used">RawData</software> is stored in RDD with the id of a point as key and the set of observation values as value. Line 9 computes the PDF based on the observation values and the error between the PDF and the observation values. This can be achieved by executing an R <software ContextAttributes="used">program</software>. The loop of Lines 8-10 can be executed in parallel using the Map operation in <software ContextAttributes="used">Spark</software>. The ComputeP DF &amp;Error function is realized by Algorithm 3. The data is persisted in the storage resources (Line 12) and the average error E is calculated (Line 15). Lines 3-15 correspond to the PDF computation process.</p><p>Algorithm 2 loads and preprocesses the spatial data. Line 4 chooses a window to load the data. Then, for each point in the window, the data in each dataset of DS is loaded (Lines 6-10). This process is realized in a Map function in <software>Spark</software>. A Java <software ContextAttributes="used">program</software> is called in the Map function to read the data at a specific position instead of loading all the data. Then, the mean and standard deviation values are calculated (Lines 11 and 12). Finally, the loaded data is cached in memory in a <software ContextAttributes="used">Spark</software> RDD (Line 16).</p><p>Algorithm 3 computes the PDF of a point with the smallest error in a set of candidate distribution types. Line 3 calculates the statistical parameters of PDFs based on different distribution types in a set of distribution candidates T ypes. For instance, the parameters for Normal are mean and standard deviation values while the parameter for exponential is rate. The more types are</p></div>
<div><head>Algorithm 1 PDF computation</head><p>Input: DS: a set of spatial datasets corresponding to a spatial cube area; i: the ith slice to analyze; T ypes: a set of distribution types Output: P DF : the PDF of all the points in the ith slice of the cube area; E: the average error between the PDF and the observation values of all the points in the ith slice 1: P DF ← ∅ 2: <software>RawData</software> ← loadData(DS, i) 3: while not all points in slice i are processed do 4:</p><p>pdf s ← ∅ 5:</p><p>window ← GetN extW indow(slice i ) 6:</p><p>P oints ← Select(window, <software>RawData</software>) 7:</p><p>for each p ∈ P oints do 8:</p><p>d ← getObservationV alues(p, <software>RawData</software>) 9:</p><p>(pdf, error) ← ComputeP DF &amp;Error(d, T ypes) 10:</p><p>pdf s ← pdf ∪ pdf s 11:</p><p>end for 12: persist(pdf s) 13:</p><p>P DF ← P DF ∪ pdf s 14: end while 15: E ← Average(error) end</p></div>
<div><head>Algorithm 2 Data loading (loadData)</head><p>Input: DS: a set of datasets corresponding to a spatial cube area; i: the ith slice to analyze Output: <software>RawData</software>: mean, standard deviation and the original dataset of each point in the cube area 1: <software ContextAttributes="used">RawData</software> ← ∅ 2: while <software ContextAttributes="used">RawData</software> does not contain all the points in slice i do 3: <software ContextAttributes="used">windowData</software> ← ∅ 4:</p><p>window ← GetN extW indow(slice i ) 5:</p><p>for each p ∈ window do 6:</p><p>rd ← ∅ 7:</p><p>for each ds ∈ DS do 8:</p><p>data ← GetData(ds, p, i) 9:</p><p>rd ← data ∪ rd 10:</p><p>end for 11:</p><p>µ ← ComputeM ean(rd) 12:</p><p>σ ← ComputeStd(rd) 13:</p><p>rd ← µ ∪ σ ∪ rd 14:</p><p><software>windowData</software> ← rd ∪ <software ContextAttributes="used">windowData</software> 15:</p><p>end for 16:</p><p>Cache(<software>windowData</software>) 17:</p><p><software ContextAttributes="used">RawData</software> ← <software ContextAttributes="used">windowData</software> ∪ <software ContextAttributes="used">RawData</software> 18: end while end considered in T ypes, the longer the execution time of Algorithm 3 is. Then, the corresponding error of the PDF based on type and parameters are calculated using Equation <ref type="formula" target="#formula_4">5</ref>. Finally, the PDF that incurs the smallest error is chosen (Line 7).</p><p>We exploit Algorithms 1 and 2 in both the data grouping and ML prediction methods. However, the Select and ComputeP DF &amp;Error functions are different in different methods.</p></div>
<div><head n="6.2">Data Grouping</head><p>As some positions in the cube area may share the same physical properties, the corresponding points may have the same distribution of observation values. Thus, using the data grouping results ← {(type, parameter, pError)} ∪ results 6: end for 7: (P DF, error) ← GetSmallestError(results) end method, we can execute the PDF computation process only once and use the result to represent all the corresponding points. In this method, the Select function in Algorithm 1 has two steps. First, the points with exactly the same mean and standard deviation values are aggregated into a group. The grouping can be realized by the aggregation operation in <software>Spark</software>. Then, one point in each group is selected to represent the group of points. Then, the data corresponding to selected points are processed to compute the PDFs.</p><p>We randomly select one point in an aggregated group. We assume that the points that share the same mean and standard deviation have the same PDF although their observation values can be different. With this assumption, the selection of the point in an aggregated group does not have an effect on the computed PDF of the group. There are scenarios where the mean and standard deviations share the same values, respectively, but the PDFs are different. In this situation, we can take into consideration other normalized moments 1 , e.g. 3rd, 4th etc. However, it may take additional time to calculate other normalized moments. Thus, we only consider the mean and standard deviations. We will experiment with datasets where the points of the same set of mean and standard deviation values have the same PDFs.</p><p>In some cases, although some points may share the same PDF, the observation values of the points are slightly different. And the mean and standard values may be different by a very small fluctuation. In this case, we can cluster the points that have similar mean and standard deviation values with an acceptable error. For instance, we can use k-means <ref type="bibr" target="#b32">[33]</ref>, which is already implemented in parallel within <software ContextAttributes="used">Spark</software> MLlib. However, the implementation of point clustering is out of the scope of this paper and we leave it as future work.</p><p>When the number of nodes in the cluster is small and the window size is small, there is few data to be transferred for the grouping. And the grouping method can avoid repeated execution on the same set of observation values. However, when the number of nodes is high or the window size is big, there is much more data to be transferred among different nodes, which may take much time. In some situations, the time to transfer the data may be longer than the time to run the repeated execution. In addition, if a point corresponds to big amounts of data (many observation values), even though the window size is small, the shuffling process of data grouping may also take much time. In this case, the data grouping method is not efficient. 1 The nth moment is defined as:</p><formula xml:id="formula_6">µn = m i=1 (x i -µ) n (<label>7</label></formula><formula xml:id="formula_7">)</formula><p>where m is the number of values in the dataset, x i is the ith value in the dataset and µ is the mean value of the dataset.</p><p>Fig. <ref type="figure">7</ref>: A decision tree to choose the distribution type for a set of data.</p></div>
<div><head n="6.2.1">Reuse Optimization</head><p>When there are points of the same mean and standard deviation values in different windows, we can reuse the existing calculated results in order to avoid new executions. Thus, we propose the reuse optimization method, which not only aggregates the data to groups but also checks if there are already existing results, i.e. the PDFs for the point of the same mean and standard deviation values, generated from the previous execution of processed windows. We expect this method to be better than data grouping only. However, it may take time to store all the calculated results and to search existing PDFs from a large list of previously generated results. This extra time may be longer than the reduced time, i.e. the time to compute the PDF on the dataset (the execution time of Algorithm 3).</p></div>
<div><head n="6.3">ML Prediction</head><p>In this section, we propose an ML prediction method based on a decision tree to compute PDFs. Decision tree classifier <ref type="bibr" target="#b37">[38]</ref> is a typical ML technique to classify an object into different categories. The basic idea is to break up a complex decision into a union of several simpler decisions, hoping the final solution obtained this way would resemble the intended desired solution. The decision tree can be described as a tree selected from a lattice <ref type="bibr" target="#b6">[7]</ref>. A tree G = (V, E) consists of a finite, nonempty set of nodes V and a set of edges E. A path in a tree is a sequence of edges. The depth of the graph in a tree is the length of the largest path from the root to a leaf. A leaf is the node that has no proper descendant. The data to be used to generate a decision tree is training data. In order to generate a decision tree, the training data can be split into different datasets (bins) according to different features and each dataset is a unit to be processed. The maximum number of bins is defined in order to reduce the time to generate a decision tree. Figure <ref type="figure">7</ref> shows a decision tree of depth 3 to choose a distribution type for a dataset. However, in this paper, we take some statistical features as parameters to choose a distribution as explained below.</p><p>In Algorithm 3, the execution of Lines 3-5 is repeated several times (the number of distribution type candidates), which is very inefficient. We assume that we can learn the correlation among statistical features, e.g. mean and standard deviation values, and the distribution type. Then, we can directly predict the distribution type based on the relationship and use the predicted distribution type to execute Lines 3-5 in Algorithm 3 once for each point.</p><p>We assume that we have some previously generated output data, which contains the results of several points, i.e. the mean and standard deviation values and the type of distribution. Before execution of our method, we can generate a ML model (that correlates between statistical features and distribution types), i.e. decision tree, based on the previous existing data. Then, we use Algorithm 4 to replace Algorithm 3.</p><p>When the points in the current slice have the same correlation between the statistical features and the distribution types as that in the previously generated output data, we can use ML prediction. Generally, the points in different slices but corresponding to the same spatial dataset have the same correlation. Thus, we can use the output data generated based on some points in one slice, e.g. <software>Slice</software> 0, to generate decision tree model and use the model to calculate PDFs of the points in other slices, e.g. <software ContextAttributes="used">Slice</software> 201. This correlation information is not based on the fact that various points present the same statistical features, e.g. PDF, although this fact is general since the positions in the cube area related to the points may share similar physical properties. The correlation is related to each set of datasets and corresponds to a cube area. In addition, we can find that the correlation information in <software ContextAttributes="used">Slice</software> 0 can always be used in our targeted slice with a few acceptable extra errors as shown in Section 7. According to Equation 5 end</p></div>
<div><head>Algorithm 4 PDF computing based on ML prediction</head></div>
<div><head n="6.3.1">ML Model Generation</head><p>In order to generate the decision tree model, there are some hyperparameters to determine, e.g. the depth of the tree (depth) and the maximum number of bins (maxBins). In order to tune the hyperparameters, we randomly split the previously generated output data into two datasets, i.e. training set and validation set. We use the training set to train the model with different combinations of hyperparameters. Then, we test the generated models on the validation set in order to choose an optimal combination of hyperparameters. We take the wrong prediction rate in the validation set as the model error while tuning the hyperparameters. The model error represents the preciseness of the decision tree model.</p><p>The model error decreases at the beginning and then increases, because of overfitting <ref type="bibr" target="#b47">[48]</ref>, as the values of depth and maxBins increase. While the time to train the model becomes longer when the values of depth and maxBins increase. We choose the minimum values of depth and maxBins, from which the error does not decrease when they (depth or maxBin) increase. The process to tune the hyperparameters can take much time. We assume that the points in different slices have the same correlation between the statistical features and distribution types. We also assume that the hyperparameters can be shared to train the models based on different previously generated output data in order to avoid tuning the hyperparameters and to reduce the time to train the decision tree model. Thus, the chosen values of depth and maxBins are used as fixed hyperparameters to generate the decision tree model for different previously generated output data.</p><p>Within the previously generated output data, each point has mean and standard deviation values and the distribution type. Since we have fixed hyperparameters, we do not need to use a validation dataset to tune the hyperparameters. Thus, in order to generate the model, we randomly partition the previously generated output dataset into two parts, i.e. training set and test set. In the training processing, we train the model using the training set. The input of the trained model is the mean and standard values and the is a predicted distribution type. After generating the model, we take the wrong prediction rate in the test set as the model error while training models. During the PDF computation process, the generated model is broadcast to all the nodes, which reduces communication cost.</p><p>There are scenarios in which the mean and standard deviation share the same values, respectively, but the distribution type is different. In this situation, we can take into consideration other normalized moments as explained in Section 6.2. We will experiment with datasets where the points of the same set of mean and standard deviation values have the same distribution types.</p></div>
<div><head n="6.4">Complexity Analysis</head><p>Since the ML approach optimizes the ComputeP DF &amp;Error function, it can be combined with other methods as shown in Table <ref type="table" target="#tab_0">1</ref>. Thus, we call the pure adoption of ML: ML or baseline + ML; the combination of data grouping and ML: data grouping + ML or grouping + ML; the combination of reuse and ML: reuse + ML. Grouping + ML first uses the data grouping methods to group points and then use ML prediction to calculate PDF and error for each representative point. Reuse + ML first groups the points using the data grouping method and then searches the PDF corresponding to the set of the mean and standard deviation values of each representative point in the results generated by previous execution. If there is no results for the set of mean and standard deviation values, it uses the ML method to calculate the PDF and the error. Table <ref type="table" target="#tab_1">2</ref> shows the computation complexity and the communication complexity of different combinations of methods. Let α (the same as that in Table <ref type="table" target="#tab_1">2</ref>) represent the average ratio between the number of points that have different sets of mean and standard values and the total number of points in a window. β (the same as that in Table <ref type="table" target="#tab_1">2</ref>) represents the average ratio between the number of points that have different sets of mean and standard values and the number of all points in a slice. The "average" in α and β is calculated in terms of the whole data set. Thus, β is smaller than α when there are points that have the same set of mean and standard deviation values 1 . W S represents the window size. When W S * α is bigger than 1, the computation complexity of Grouping is bigger than Baseline. In this case, we ignore data grouping. When N * W S * β is bigger than 1, the computation complexity of Reuse is bigger than that of Baseline. When N increases, the computation complexity of Reuse increases much faster than Baseline because of N 2 . The communication complexity of Grouping and Reuse is bigger than that of Baseline. When the number of nodes in a cluster g or the number of observation values for each point get big, the communication complexity of Grouping or Reuse increases linearly, which corresponds to longer execution time. When combining with ML, Grouping and Reuse have the same features, i.e. bigger computation complexity when W S * α (N * W S * β for Reuse) is bigger than 1 and bigger communication complexity. The computation complexity of ML is smaller than that of Baseline, which corresponds to smaller execution time as shown in Section 7. In addition, ML has the same communication complexity as that of Baseline. K represents the number of nodes in a cluster. α represents the ratio between the number of points that have different sets of mean and standard values and the total number of points in a window. β represents the ratio between the number of points that have different sets of mean and standard values and the number of all the points in a slice.</p></div>
<div><head>Name Computation complexity Communication complexity Baseline</head><formula xml:id="formula_8">O(N * M * L) O(N * M * L) Grouping O(N * W S * M * L * α) O(N * M * L + N * K * D * L) Reuse O(N 2 * W S * M * L * β) O(N * M * L + N * K * M * L) ML O(N * M ) O(N * M * L) Grouping + ML O(N * M * W S * α) O(N * M * L + N * K * M * L) Reuse + ML O(N 2 * M * W S * β) O(N * M * L + N * K * M * L)</formula></div>
<div><head n="6.5">Sampling</head><p>In order to choose a slice to compute PDFs, we need to compute the features of a slice very quickly. We propose a sampling method (see Algorithm 5) that samples the points and uses ML prediction to generate the distribution type of each point. This method does not run the statistical calculation of the each point in the ComputeP DF &amp;Error function in Algorithm 1 in order to reduce execution time.</p><p>In Algorithm 5, Line 1 samples the points in slice i based on a predefined sampling rate. A sampling rate represents the ratio between the selected points from the sampling process and all the original points. Lines 3-13 implement the process that loads the data from the datasets and calculates the mean and standard deviation values for each double sampled point. Line 14 groups the data as explained in Section 6.2. When the number of nodes in the cluster is high, we can 1 Let n window be the number of points that have different sets of mean and standard values in a window and N window be the total number of points in a window. Then, α = n window remove Line 14 in order to reduce the time of shuffling. Lines 16 -19 calculate the distribution type of each double sampled point based on a decision tree (see Section 6.3.1 for details). Lines 21 -25 calculate the final results, i.e. the average mean value, the average standard deviation and the percentage of different distribution types. Lines 16 -25 correspond to the PDF computation process.</p></div>
<div><head>Algorithm 5 Sampling process</head><p>Input: DS: a set of datasets produced by simulations; i: the ith slice to analyze; model: the decision tree corresponding to the relationship between the mean and standard value and the distribution type; T ypes: a set of distribution types; rate: the sampling rate Output: µ: the average mean value of the slice; σ: the average standard deviation of the slice; T ypesP ercentage:</p><p>the percentage of distribution types of the points in the slice 1: P oints ← Sample(slice i , rate) 2: <software>RawData</software> ← ∅ 3: for each p ∈ points do 4:</p><p>rd ← ∅ 5:</p><p>for each ds ∈ DS do 6:</p><p>data ← GetData(ds, p, i) 7:</p><p>rd ← data ∪ rd 8:</p><p>end for 9:</p><p>µ ← ComputeM ean(rd) 10:</p><formula xml:id="formula_9">σ ← ComputeStd(rd) 11: rd ← µ ∪ σ ∪ rd 12:</formula><p><software>RawData</software> ← rd ∪ <software ContextAttributes="used">RawData</software> 13: end for 14: P oints ← selectByGrouping(<software ContextAttributes="used">RawData</software>) 15: allT ypes ← ∅ 16: for each p ∈ P oints do 17:</p><p>type ← predict(model, <software>RawData</software>) 18:</p><p>allT ypes ← type ∪ allT ypes 19: end for 20: T ypesP ercentage ← ∅ 21: for each type ∈ T ypes do 22:</p><p>pct ← calculateP ercentage(type, allT ypes) 23:</p><p>T ypesP ercentage ← pct ∪ T ypesP ercentage 24: end for 25: (µ, σ) ← averageCalculation(<software>RawData</software>) end</p><p>We can randomly sample the points. The random sampling method takes much time while the selected points may contain little repeated information. We could also use a k-means clustering algorithm <ref type="bibr" target="#b32">[33]</ref> and choose the point that is the closest to the center of each cluster as double sampled points. The double sampled data generated by k-means contains diverse information, which does not help much to choose a slice (see details in Section 7.2.3). Furthermore, k-means may take much time to converge. Therefore, instead of k-means, we use random sampling in Algorithm 5.</p></div>
<div><head n="7">Experimental Evaluation</head><p>In this section, we evaluate and compare the different methods presented in Section 6 for different datasets and cluster sizes. To ease reading, we call each method by a name as: Baseline, Grouping, Reuse, ML and Sampling. First, we introduce the experimental setup, with two different clusters (a small one and a big one). Then, we perform experiments with a 235 GB dataset. Finally, we perform experiments with big datasets (1.9 TB and 2.4 TB).</p></div>
<div><head n="7.1">Experimental Setup</head><p>In this section, we present the different spatial datasets, the candidate distribution types (see Algorithms 3 and 5) and the cluster testbeds, which we use in our various experiments.</p><p>To generate the spatial data, we use the <software ContextAttributes="used">UQlab</software> framework <ref type="bibr" target="#b31">[32]</ref> to produce 16 values as the input parameters, i.e. V p, of the models from the seismic benchmark of the HPC4e project <ref type="bibr" target="#b1">[2]</ref>. The input parameters obey four distribution types, i.e. normal, exponential, uniform and lognormal. In each simulation, we generate a set of the input parameters according to the PDF of each layer and generate a spatial dataset by using the set of the input parameters and the models. We run the simulation multiple times and generate three sets of spatial datasets, denoted by Set1, Set2 and Set3. The simulation is repeated 1000 times to generate Set1. Set1 contains 1000 files, each of which is a spatial dataset and has 235 GB. In Set1, the dimension of the cube area is 251 * 501 * 501, i.e. 501 slices, each slice has 501 lines and each line is composed of 251 points. To generate Set2, we run the simulation 1000 times while the dimension is 501 * 1001 * 1001. Set2 has 1.9 TB. A point is associated to 1000 observation values in both Set1 and Set2. Finally, we run the simulation 10000 times with the dimension of 251 * 501 * 501 in order to generate Set3, which has 2.4 TB. In this dataset, a point is associated to 10000 observation values. We use the same slice (<software ContextAttributes="used">Slice</software> 201 because it has interesting information) in all the experiments. We consider two sets of candidate distribution types T ypes, introduced in Algorithms 3 and 5. The first set is the set of distribution types of the input parameters, i.e. normal, exponential, uniform and log-normal, since we assume that the distribution types of different points are within those of the input parameters of the simulation. In addition, we assume that the distribution types may belong to other types beyond the scope of the distribution types of the input parameters because of non-linear relationship between the input parameters and the values of each point of the spatial cube area. With this assumption, we have a second type of candidate distribution types, i.e. normal, exponential, uniform, log-normal, Cauchy, gamma, geometric, logistic, Student's t and Weibull. We call the first set 4 -types and the second 10 -types.</p><p>We use two cluster testbeds, each with NFS, HDFS and <software>Spark</software> deployed. The first one, which we call LNCC cluster, is a cluster located at LNCC with 6 nodes, each having 32 CPU cores and 94 GB memory. The second one is a cluster of Grid5000, which we call G5k cluster, with 64 nodes, each having 16 CPU cores and 131 GB of RAM storage.</p></div>
<div><head n="7.2">Experiments on a 235 GB Dataset</head><p>In this section, we compare the different methods based on the spatial dataset Set1 of 235 GB. First, we compare the performance of different methods using the LNCC cluster: Baseline, Grouping, Reuse and the combination of these methods with ML. Then, we study the scalability of different methods using the G5k cluster. Finally, we study the performance of Sampling.</p><p>We take the relationship between the set of mean and standard deviation values and the distribution types of 25000 points in <software>Slice</software> 0 in the previously generated output data of Set1 to tune the hyperparameters of the decision tree. The hyperparameters are tuned at the very beginning, which takes 18 minutes for 4 -types and 22 minutes for 10 -types using <software ContextAttributes="used">Spark</software> on a workstation with 8 CPU cores and 32 GB of RAM.</p><p>We take advantage of the previously generated output data of Set1 to generate the decision tree model for the experiments of this section. The model error is 0.03 for 4 -types, and 0. Fig. <ref type="figure">9</ref>: Error in PDF computation. NoML represents the error of different methods without adoption of ML, i.e. Baseline, Grouping and Reuse with 4 -types or 10 -types (see Figure <ref type="figure" target="#fig_8">8</ref>). WithML represents the combinations of methods with ML, i.e. Baseline + ML, Grouping + ML, Reuse + ML with 4 -types or 10 -types (see Figure <ref type="figure" target="#fig_8">8</ref>). 10 -types. Although the model error of 10 -types is bigger than that of 4 -types , the average error E in Algorithm 1 is smaller. The time to train the model ranges from 1 to 20 seconds, which is negligible compared with the execution time of the whole PDF computation process.</p></div>
<div><head n="7.2.1">Performance Comparisons</head><p>In this section, we compare the performance of different methods, i.e. Baseline, Grouping, Reuse, and the combination with ML, using the LNCC cluster. First, we execute the <software>Scala</software> <software ContextAttributes="used">program</software> (for computing PDFs with different methods) on a small workload (6 lines and 3006 points). Second, we compare the performance of different methods for different window sizes. Finally, we compare the performance of different methods with the tuned window size for the whole slice.</p><p>Experiments with Small Workload In this section, we use a small workload of 6 lines, i.e. 3006 points, to compare the performance of different methods. The execution time for PDF computation is shown in Figure <ref type="figure" target="#fig_8">8</ref> and the error is shown in Figure <ref type="figure">9</ref>.</p><p>We execute the <software>program</software> for the points of the first 6 lines in <software ContextAttributes="used">Slice</software> 201 using Baseline, Grouping, Reuse, ML and the combination of these methods with ML. We take 3 lines as a window for PDF computation and we process the data of the points in two windows. The execution time for data loading (see Algorithm 2) is 67s, which is the same for all the methods since we use the same algorithm.</p><p>Figure <ref type="figure" target="#fig_8">8</ref> shows the good performance of our methods: Grouping (without Reuse), Reuse and ML. The execution time of Baseline with 10-types is much longer than that with 4-types. This is expected since the complexity of Algorithm 3 is O(n), with n distribution types, and the execution time increases with n. However, with ML, the execution time is significantly reduced (46% for 4 -types and 78% for 10 -types). If we use Grouping without ML or Reuse, the execution time is also reduced a lot (69% for 4 -types and 72% for 10 -types). This shows that there are many points that obey the same distribution and have the same mean and standard deviation values. In addition, since the data size corresponding to a point is small, i.e. 1000 observation values, the shuffling time for computing the aggregation function is short. Thus, Grouping outperforms Baseline much. When we couple Grouping and ML, the advantage becomes more obvious. The combined method is 88% and 95% better compared with the Baseline for 4-types and 10-types, i.e. the combined method is up to more than 17 times better than Baseline. Reuse is slightly worse than Grouping, but it is still much better than Baseline. This is expected since it takes more time to search for the existing results than to compute PDFs. The difference between Grouping and Reuse is small when the workload is small but it can be significant for bigger workloads.</p><p>Figure <ref type="figure">9</ref> shows the average error (E in Equation <ref type="formula" target="#formula_5">6</ref>) corresponding to different methods. In Figure <ref type="figure">9</ref>, we distinguish between the methods that do not use ML, i.e. Baseline, Grouping and Reuse, which we call NoML, and those that do exploit ML, i.e. Baseline with ML, Grouping with ML and Reuse with ML, which we call WithML. The methods NoML and WithML have the same error for the same distribution type set.</p><p>The figure shows that 10 -types does not reduce much the average error for Baseline (up to 0.0013). However, PDF computation may take more time when we consider 10 -types as shown in Figure <ref type="figure" target="#fig_8">8</ref>. The average error is higher for WithML than NoML. The difference is small (up to 0.017 for 10 -types calculated based on the measured error shown in Figure <ref type="figure">9</ref>) but WithML can reduce much the execution time of the PDF computation. In addition, Figure <ref type="figure">9</ref> shows that 10 -types leads to a smaller average error, even though the model error is higher for WithML. This is reasonable since there are some types that are very difficult to distinguish in 10 -types but the wrong classification of the distribution types does not increase much the average error. In addition, with more distribution candidate types, the decision tree produces a better classification.</p><p>Window Size Adjustment The window size is critical for the PDF computation with Grouping. To adjust the window size, we conduct the following experiment. We execute the <software ContextAttributes="used">Scala</software> <software ContextAttributes="used">program</software> with Grouping (with 4 -types and without ML prediction) for two windows while each window is composed of different numbers of lines. Then, we choose a window that corresponds to the shortest average execution time of each line. Figure <ref type="figure" target="#fig_10">10</ref> shows the average execution time of PDF computation for different window sizes using Grouping. As shown in Figure <ref type="figure" target="#fig_10">10</ref>, the average execution time of each line decreases for larger window sizes. This is reasonable because when the number of lines increases, more points are aggregated to each group so that more redundant calculations are avoided. When the window size is 25 lines, the average execution time of each line is minimal. From that point on, when the window size increases, the execution time also increases. This is expected since when the number of lines in a window increases, the time to shuffle data among different nodes increases more than the reduced time by avoiding redundant calculations. As a result, the average execution time of each line becomes more significant. However, the   We measure the execution time of PDF computation for different window sizes using other methods. In Figure <ref type="figure" target="#fig_11">11</ref>, we can see that the window size of 25 is the optimal size for the other methods. In addition, the execution time of PDF computation is almost the same for different combinations of methods: Grouping plus ML and Reuse plus ML both with 4-types or 10-types. Baseline always corresponds to longer execution times of PDF computation. Compared with Baseline, Grouping, Reuse and ML can reduce the execution time up to 91% (more than 10 times faster) and 84% (more than 6 times faster). In addition, the combination of Grouping and ML can be up to 97% (more than 33 times) faster. This shows the obvious performance advantage of our methods in computing PDFs. Execution of One <software ContextAttributes="used">Slice</software> In this section, we compare the performance of different methods to execute the <software ContextAttributes="used">program</software> for the points of the whole slice, i.e. Slice 201. We take 25 lines as the window size for PDF computation and execute the <software ContextAttributes="used">program</software> with different methods for the whole slice, i.e. 11 windows of points in <software ContextAttributes="used">Slice</software> 201. The execution time of data loading is the same for the different methods, i.e. 4098s. The average execution time of each line is longer than that of the small workload since we cache all the data in memory during the execution of different methods and the time to store data increases as the amount of cached data grows. The execution time of PDF computation is shown in Figure <ref type="figure" target="#fig_12">12</ref> and the error is shown in 13.</p><p>Figure <ref type="figure" target="#fig_12">12</ref> shows that our proposed methods, i.e. Grouping, Reuse and ML, always outperform Baseline for both 4 -types and 10 -types. Grouping can reduce the execution time up to 92% (more than 10 times) and ML up to 78% (more than 3 times faster). The performance of Reuse is better than that of Baseline when we do not combine ML and the advantage can be up to 70% (more than 2 times faster). However, the performance of the combination of Reuse and ML can be worse than the combination of Baseline and ML. This is possible when it takes too much time to search for the existing results. The combination of Grouping and ML can reduce the execution time up to 97% (more than 27 times faster) compared with Baseline.</p><p>Figure <ref type="figure" target="#fig_5">13</ref> shows the error of PDF computation. The error is smaller than that of the small workload. The error of NoML is still smaller than that of WithML. The error for 4 -types is almost the same as that for 10 -types. But the error for 10 -types using ML is smaller that for 4 -types. This is similar to what was observed when executing the small workload. In addition, the error for 10-types with ML is even smaller than the error for 4-types without ML. Although there is very small difference (up to 0.0016) of error between Baseline and ML, the execution time of the PDF computation process is largely reduce by ML.</p></div>
<div><head n="7.2.2">Scalability Comparisons</head><p>In this section, we study the scalability of Baseline and the methods that have good performance, i.e. Grouping, ML and Grouping + ML. We use the G5k cluster with different numbers of nodes from 10 to 60.</p><p>The execution time of data loading with different methods remains the same for the same number of nodes. Figure <ref type="figure" target="#fig_2">14</ref> shows the execution time of data loading, which decreases rapidly as the number of nodes increases. This indicates the good scalability of our data loading process. The execution time of PDF computation for different methods and different numbers of nodes is shown in Figure <ref type="figure" target="#fig_3">15</ref>. We do not consider Reuse since it is less efficient than Grouping. We focus on ML for 10 -types because it has small error and the execution time is similar to that for 4 -types. The figure shows that the execution time of Grouping and ML is always shorter than that of Baseline. The advantage of Grouping can be up to 87% (more than 6 times faster). ML outperforms Baseline up to 89% (more than 8 times faster). Grouping + ML can be better than Baseline by up to 90% (more than 9 times faster). In addition, the execution time of each method decreases as the number of nodes increases, which indicates that all methods have good scalability. However, the advantage becomes less obvious from 50 nodes on.</p><p>Figure <ref type="figure" target="#fig_4">16</ref> gives a focus on our methods. We do not compare the error of PDF computation since it is similar to that given in Section 7.2.1. The figure shows that Grouping + ML is better than either Grouping or ML when the number of nodes is 10. However, ML starts outperforming Grouping + ML when the number of nodes exceeds 10. This is because the shuffling of data among different nodes takes much time. As the number of nodes increases, the time to transfer data among the nodes increases. Thus, the performance of the aggregation function becomes a bottleneck for Grouping + ML when the number of nodes is high. </p></div>
<div><head n="7.2.3">Performance of Sampling</head><p>We now study the efficiency of Sampling. We carried out the experiments in the LNCC cluster. We use two sampling methods, i.e. random sampling and k-means clustering (see Section 6.5). We compare the performance of the two sampling method with different sampling rates.</p><p>Figure <ref type="figure">17</ref> shows the execution time of random sampling with different sampling rates. The execution time for data loading decreases almost linearly as the sampling rate decreases (both the X and Y axis use a base-10 log scale). This is reasonable since when the sampling rate gets small, the data loading needs to process less points and thus loads less data. The execution time for PDF computation is very short (about 2 seconds). This is expected since we avoid calling the R <software ContextAttributes="used">program</software> to compute PDFs and use a decision tree model to predict the distribution type of each point. The execution time is almost the same for different sampling rates since it is already very short and data shuffling also takes some time. In addition, we only need to transfer the mean and standard deviation values instead of the whole dataset for the prediction, which also reduces time. This execution time is about 2 seconds, which is very short. However, this method cannot calculate the error of the PDF computation process. It can help to have a general view of the whole slice. Then a slice is chosen to compute the PDFs of the corresponding points. Figure <ref type="figure" target="#fig_8">18</ref> shows the performance of k-means clustering for sampling the points. The results are almost the same as that of random sampling, i.e. the execution time for data loading decreases linearly and the execution time for PDF computation is very short and almost the same for different sampling rates. The execution time of k-means is longer than that of random for the same sampling rate. We measure the sampling rate from 0.2 since the corresponding execution time of k-means for data loading is already longer than that of random sampling with the sampling rate of 1.</p><p>Figure <ref type="figure" target="#fig_16">19</ref> shows the Euclidean distance of the distribution type percentage between the double sampled points and all points in one slice using k-means and random sampling. When the sampling rate is small, the result of k-means is close to the percentage of overall points. This is because the double sampled points of k-means method contain diverse information. However, when the sampling rate is high, the results of random sampling are similar or better since enough points are selected with a high sampling rate. Since random sampling is faster than k-means, we choose it in the following experiments.  </p></div>
<div><head n="7.3">Experiments on Big Datasets</head><p>In this section, we compare the performance of different methods based on big spatial datasets of several TB, i.e. Set2 and Set3 (see details in Section 7.1). We use the G5k cluster with 30 and 60 nodes.</p></div>
<div><head n="7.3.1">Experiments with 1000 Simulations</head><p>In this section, we perform experiments using Set2 of 1.9 TB generated by 1000 simulations. We take the relationship between the combination of mean and standard deviation values and the distribution types of 25000 points in <software>Slice</software> 0 as previously generated data to build the decision tree model. The model error of 4 -types is 0.02 and the model error of 10 -types is 0.09. The time to load the model ranges between 1 and 20 seconds, which is negligible compared with the execution time of PDF computation. The time for data loading is 2671 seconds with 30 nodes and 1619 seconds with 60 nodes, which shows the good scalability of the data loading process.</p><p>Figure <ref type="figure" target="#fig_17">20</ref> shows the good performance of our methods, i.e. Grouping and ML. Grouping is better than Baseline (up to 79%) but not as good as ML, because of data shuffling with many nodes. And the scalability of Grouping is not good. ML largely outperforms Baseline (up to 89%) and has good scalability. Because of the shuffling the combination of Grouping and ML is worse than ML but still better (up to 77%) than Baseline. The error of ML with 10 -types is always smaller than with 4 -types.</p><p>Finally, we experimented with random sampling to process the data in two clusters of 30 and 60 nodes. The execution time of PDF computation ranges between 260s and 280s while the sampling rate ranges between 0.001 and 1. The average execution time (272s for 30 nodes and 266 for 60 nodes) is 82% and 71% shorter than the minimum execution time (ML with 4 -types for 30 and 60 nodes) as shown in Figure <ref type="figure" target="#fig_17">20</ref>. Note that doubling the number of nodes does not yield much improvement. This is because using more nodes increases data transfers to send the mean and standard deviation values and the distribution type from each node to the <software ContextAttributes="used">Spark</software> master node in <software ContextAttributes="used">Spark</software> cluster to compute the distribution percentage.</p></div>
<div><head n="7.3.2">Experiments with 10000 Simulations</head><p>In this section, we perform experiments using Set3 of 2.4 TB generated by 10000 simulations. Again, we take the relationship between the combination of mean and standard deviation values and the distribution types of 25000 points in <software ContextAttributes="used">Slice</software> 0 as the previously generated data for the decision tree model. The model error of 4 -types is 0.006 and the model error of 10 -types is 0.012. The time to load the model ranges between 1 and 20 seconds, which is negligible compared with the execution time of PDF computation. First, we execute the <software ContextAttributes="used">program</software> for the points of the first 2 lines in <software ContextAttributes="used">Slice</software> 201 in a cluster of 30 nodes. We take 1 line as a window, and we process the data of two windows. The execution time for data loading (Algorithm 2) is 28s, which is the same for all the methods since we use the same algorithm (Algorithm 2). The execution time for PDF computation is shown in Figure <ref type="figure" target="#fig_18">21</ref>.</p><p>Figure <ref type="figure" target="#fig_18">21</ref> shows the superior performance of ML. The execution time of Baseline with 10types is much longer than that of 4 -types as expected. Compared with Baseline, ML reduces execution time much (57% with 4 -types and 72% with 10 -types). However, the execution time of Grouping is much longer. This is because the data size of each point is 9 times bigger since a point corresponds to 10000 observation values instead of 1000. During Grouping, much more data is transferred among different nodes, which takes much time. Thus, in the next experiment, we do not use Grouping. We also measure the error (E defined in Equation <ref type="formula" target="#formula_5">6</ref>) during execution. 10 -types does not reduce much the average error for Baseline (up to 0.008). However, it may take much more time for PDF computation when we consider 10 -types as shown in Figure <ref type="figure" target="#fig_18">21</ref>. WithML, the average error is slightly bigger than that of NoML. The difference is very small (up to 0.007) but ML can reduce much the execution time of PDF computation. Furthermore, 10 -types leads to smaller average error even though the model error is bigger when using ML. Now, we execute the <software ContextAttributes="used">program</software> for all points in <software ContextAttributes="used">Slice</software> 201. As we only compare the execution with Baseline and ML, we take 126 lines as a window in order to parallelize the execution of different points in different nodes. The time for data loading is 4592 seconds with 30 nodes. Figure <ref type="figure" target="#fig_19">22</ref> shows the superior performance of ML (up to 88%, i.e. more than 7 times faster than Baseline ). Furthermore, the PDF computation process scales very well. We measured the average error and found that ML incurs small error while reducing execution time much. Finally, we use the random sampling to process the data in two clusters of 30 and 60 nodes. The execution time of PDF computation ranges between 128s and 200s while the sampling rate ranges between 0.001 and 1. The average execution time (172s for 30 nodes and 155s for 60 nodes) is 64% and 41% smaller than the minimum execution time (ML with 4 -types for 30 and 60 nodes) as shown in Figure <ref type="figure" target="#fig_19">22</ref>. Again (as for the experiment with 1000 simulations), doubling the number of nodes does not yield much improvement.</p></div>
<div><head n="8">Conclusion</head><p>Uncertainty quantification of spatial data requires computing a Probability Density Function (PDF) of each point in a spatial cube area. However, computing PDFs on big spatial data, as produced by applications in scientific areas such as geological or seismic interpretation, can be very time consuming.</p><p>In this paper, we addressed the problem of efficiently computing PDFs under bounded error constraints. We proposed a parallel solution using a <software>Spark</software> cluster with three new methods: Grouping, ML and Sampling. Grouping aggregates the points of the same statistical features together in order to reduce redundant calculation. This method is very efficient when the data to be transferred is not too big and the number of cluster nodes is small. ML generates a decision tree model based on previously generated data and predicts the distribution type of a point in order to avoid useless calculation based on wrong distribution types. Sampling enables to efficiently compute statistical parameters of a region by sampling a fraction of the total number of points to reduce the computation space.</p><p>To validate our solution, we implemented these methods in a <software>Spark</software> cluster and performed extensive experiments on two different clusters (with 6 and 64 nodes) using big spatial data ranging from hundreds of GB to several TB. This data was generated from simulations based on the models from a seismic benchmark for oil and gas exploration, which includes models for seismic wave propagation.</p><p>The experimental results show that our solution is efficient and scales up very well compared with Baseline. Grouping outperforms Baseline by up to 92% (more than 10 times) without introducing extra error. ML can be up to 91% (more than 9 times) better than Baseline with very slight acceptable error (up to 0.017). The combination of Grouping and ML can be up to 97% (more than 33 times) better than Baseline. As the number of nodes exceeds 10 nodes, ML outperforms the combination. Thus, in order to compute PDFs, the combination of Grouping and ML is the optimal method when each point corresponds to a small number of observation values, e.g. 1000, and when there is small number of nodes (less than 20). Otherwise, ML is the best option. We also showed that Sampling is very efficient to calculate general statistics information in order to choose a slice for calculating PDFs. Finally, Sampling should be used with the aforementioned best option in order to efficiently compute PDFs.</p></div><figure xml:id="fig_0"><head>Fig. 2 :</head><label>2</label><figDesc>Fig. 2: Data generation from simulation. This process is repeated multiple times in order to generate multiple datasets. The values of V p are different at different iterations. T ype represents the distribution type.</figDesc><graphic coords="7,117.07,101.03,396.84,163.35" type="bitmap" /></figure>
<figure xml:id="fig_1"><head>Fig. 3 :</head><label>3</label><figDesc>Fig. 3: Dataset structure. A dataset is composed of multiple slices. Each slice is composed of multiple lines. The red rectangle represents a line. Each circle represents a point, which contains an observation value of the point in the original cube area.</figDesc><graphic coords="10,138.05,101.03,283.47,76.13" type="bitmap" /></figure>
<figure xml:id="fig_2"><head>Fig. 4 :</head><label>4</label><figDesc>Fig. 4: Architecture for Computing PDFs.</figDesc><graphic coords="11,195.02,101.03,240.94,192.19" type="bitmap" /></figure>
<figure xml:id="fig_3"><head>Fig. 5 :</head><label>5</label><figDesc>Fig. 5: Infrastructure. d i represents the ith spatial dataset.</figDesc><graphic coords="11,195.02,328.44,240.95,116.88" type="bitmap" /></figure>
<figure xml:id="fig_4"><head>Fig. 6 :</head><label>6</label><figDesc>Fig. 6: Data flow and key value pairs. The blue boxes and arrows represent the data flow. @Spark represents that the data is stored in a Spark RDD. The green boxes and arrows represent key value pairs corresponding to the data flow. id represents the id of each point. A set of d 1 to d n represents the observation values. µ and σ represent the mean and standard derivation value of the set of observation values. The set of id 1 to id m represents the ids of all the points in the group after data grouping. If data grouping is not used, there is only one id as the key of the key value pair in the result of the PDF computation. pdf type represents the type of PDF. The set of p 1 to p l represents the parameters of the calculated PDF.</figDesc><graphic coords="14,147.70,101.03,264.15,113.38" type="bitmap" /></figure>
<figure xml:id="fig_5"><head>Algorithm 3</head><label>3</label><figDesc>PDF computing (ComputePDF&amp;Error)Input: d: a set of observation values for a point; T ypes: a set of distribution types Output: P DF : the PDF of the point; error: the error between the PDF defined by the distribution type and the statistical parameters and the observation values of the point 1: results ← ∅ 2: for each type ∈ T ypes do 3:parameters ← f itDistribution(d, type) fitDistribution implemented by MLE or NM 4:pError ← CalculateError(type, parameters, d) According to Equation 5 5:</figDesc></figure>
<figure xml:id="fig_6"><head /><label /><figDesc>Input: d: a vector of observation values for a point; model: the decision tree; µ: the mean value of d; σ: the standard deviation value of d Output: T : the distribution type of the point; P : the parameters of the distribution; error: the error between the PDF defined by the distribution type and the parameters of the distribution and the real distribution of data in d 1: T ← predict(model, µ, σ) 2: P ← f itDistribution(d, T ) 3: error ← CalculateError(T, P, d)</figDesc></figure>
<figure xml:id="fig_7"><head>.</head><label /><figDesc>We assume that there are l windows in a slice. When the points in different windows have different sets of mean and standard deviation values, β = l * n window l * N window = α. When k points have same sets of mean and standard deviation values in two different slices, β = l * n window -k l * N window &lt; α.</figDesc></figure>
<figure xml:id="fig_8"><head>Fig. 8 :</head><label>8</label><figDesc>Fig. 8: Execution time for PDF computation on a small workload (6 lines and 3006 points) with 235 GB input data. The time unit is second.</figDesc></figure>
<figure xml:id="fig_9"><head /><label /><figDesc>time (s) per line Number of lines in a window</figDesc></figure>
<figure xml:id="fig_10"><head>Fig. 10 :</head><label>10</label><figDesc>Fig. 10: Average execution time per line for PDF computation with two windows.</figDesc><graphic coords="26,114.42,281.65,330.72,198.48" type="bitmap" /></figure>
<figure xml:id="fig_11"><head>Fig. 11 :</head><label>11</label><figDesc>Fig. 11: Average execution time per line for PDF computation with two windows. With 4 -types (4) and 10 -types (10).</figDesc></figure>
<figure xml:id="fig_12"><head>Fig. 12 :</head><label>12</label><figDesc>Fig. 12: Execution time of PDF computation of Slice 201 with different methods (235 GB input data). The time unit is minute. The window size is 25 lines.</figDesc></figure>
<figure xml:id="fig_13"><head>Fig. 13 :Fig. 14 :</head><label>1314</label><figDesc>Fig. 13: Error in PDF computation. NoML stands for Baseline, Grouping and Reuse plus 4 -type and 10 -types. WithML stands for Baseline + Grouping + ML, Reuse + ML 4 -type and 10 -types.</figDesc></figure>
<figure xml:id="fig_14"><head>Fig. 15 :Fig. 16 :</head><label>1516</label><figDesc>Fig. 15: Execution time of PDF computation with different numbers of nodes.</figDesc></figure>
<figure xml:id="fig_15"><head>Fig. 17 :Fig. 18 :</head><label>1718</label><figDesc>Fig. 17: Execution time with different sampling rates using random sampling.</figDesc></figure>
<figure xml:id="fig_16"><head>Fig. 19 :</head><label>19</label><figDesc>Fig. 19: Distance of the distribution type percentage between the double sampled points and all points.</figDesc></figure>
<figure xml:id="fig_17"><head>Fig. 20 :</head><label>20</label><figDesc>Fig. 20: Execution time for all the points in Slice 201 (1.9 TB input data). The time unit is second.</figDesc></figure>
<figure xml:id="fig_18"><head>Fig. 21 :</head><label>21</label><figDesc>Fig. 21: Execution time for PDF computation on small workload (2 lines and 1002 points) with 2.4 TB input data. The time unit is second.</figDesc></figure>
<figure xml:id="fig_19"><head>Fig. 22 :</head><label>22</label><figDesc>Fig. 22: Execution time for all the points in Slice 201 (2.4 TB input data). The time unit is second.</figDesc></figure>
<figure type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Configuration of different methods. Reuse represents that the process tries to reuse the calculated PDFs corresponding to the same mean and standard deviation values in other windows (see details in Section 6.2.1).</figDesc><table><row><cell>Name</cell><cell>Configuration</cell><cell>Select Function in Algorithm 1</cell><cell>Algorithms</cell><cell>Reuse</cell></row><row><cell>Baseline</cell><cell>No data grouping or ML</cell><cell>Select all the points</cell><cell>1, 3</cell><cell>No</cell></row><row><cell>Grouping</cell><cell>Data grouping without ML</cell><cell>Select one point per group</cell><cell>1, 3</cell><cell>No</cell></row><row><cell>Reuse</cell><cell>Reuse without ML</cell><cell>Select one point per group</cell><cell>1, 3</cell><cell>Yes</cell></row><row><cell>ML</cell><cell>Baseline with ML</cell><cell>Select all the points</cell><cell>1, 4</cell><cell>No</cell></row><row><cell>Grouping + ML</cell><cell>Data grouping with ML</cell><cell>Select one point per group</cell><cell>1, 4</cell><cell>No</cell></row><row><cell>Reuse + ML</cell><cell>Reuse with ML</cell><cell>Select one point per group</cell><cell>1, 4</cell><cell>Yes</cell></row></table></figure>
<figure type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Computation</figDesc><table /><note><p>and communication complexity of different methods. N represents the number of points. M represents the number of observation values for each point. L represents the number of distribution types in Algorithm 3. W S represents the window size.</p></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgment</head><p>This work was partially funded by <rs type="funder">EU</rs> <rs type="grantNumber">H2020</rs> Project <rs type="projectName">HPC4e</rs> with <rs type="funder">MCTI/RNP-Brazil</rs>, <rs type="funder">CNPq</rs>, <rs type="funder">FAPERJ</rs>, and <rs type="funder">Inria Associated Team SciDISC</rs>. The experiments were carried out using a cluster at LNCC in Brazil and the Grid5000 testbed in France (https://www.grid5000.fr).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_aAQrnAN">
					<idno type="grant-number">H2020</idno>
					<orgName type="project" subtype="full">HPC4e</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<ptr target="https://www.rdocumentation.org/packages/MASS/versions/7.3-51.1/topics/fitdistr" />
		<title level="m">Fitdistr Function in R language</title>
		<imprint />
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<ptr target="https://hpc4e.eu/downloads/hpc-geophysical-simulation-test-suite" />
		<title level="m">Hpc geophysical simulation test suite</title>
		<imprint />
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title />
		<author>
			<persName><forename type="first">Spark</forename><surname>Mlib</surname></persName>
		</author>
		<ptr target="https://spark.apache.org/mllib/" />
		<imprint />
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Efficient machine learning for big data: A review</title>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">Y</forename><surname>Al-Jarrah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">D</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Muhaidat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">K</forename><surname>Karagiannidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Taha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Big Data Research</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="87" to="93" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">fisher and the making of maximum likelihood 1912-1922</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Aldrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistical Science</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="162" to="176" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A very efficient approach to compute the first-passage probability density function in a time-changed brownian model: Applications in finance</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">V</forename><surname>Ballestra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Pacellib</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Radi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physica A: Statistical Mechanics and its Applications</title>
		<imprint>
			<biblScope unit="volume">463</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="330" to="344" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Inducing decision trees via concept lattices</title>
		<author>
			<persName><forename type="first">R</forename><surname>Belohlávek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">D</forename><surname>Baets</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Outrata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vychodil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. Journal of General Systems</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="455" to="467" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Analysis of car crash simulation data with nonlinear machine learning methods</title>
		<author>
			<persName><forename type="first">B</forename><surname>Bohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Garcke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Iza-Teran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Paprotny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Peherstorfer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Schepsmeier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Thole</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Computational Science ICCS</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="621" to="630" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Discovering tight space-time sequences</title>
		<author>
			<persName><forename type="first">R</forename><surname>Campisano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Borges</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Porto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Perosi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Pacitti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Masseglia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">S</forename><surname>Ogasawara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Big Data Analytics and Knowledge Discovery</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="247" to="257" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Spatial sequential pattern mining for seismic data</title>
		<author>
			<persName><forename type="first">R</forename><surname>Campisano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Porto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Pacitti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Masseglia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">S</forename><surname>Ogasawara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Simpósio Brasileiro de Banco de Dados (SBBD)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="241" to="246" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Flexible distribution modeling with the generalized lambda distribution</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chalabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Würtz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="2012" to="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Big data: A survey</title>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mobile Networks and Applications</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="171" to="209" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Computationally efficient estimation of the probability density function for the load bearing capacity of concrete columns exposed to fire</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">V</forename><surname>Coile</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Balomenos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pandey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Caspeele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Criel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Alfred</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Symposium of the Int. Association for Life-Cycle Civil Engineering (IALCCE)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Machine learning on big data</title>
		<author>
			<persName><forename type="first">T</forename><surname>Condie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mineiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Polyzotis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Weimer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">29th IEEE Int. Conf. on Data Engineering, ICDE</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1242" to="1244" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Statistics for spatial data</title>
		<author>
			<persName><forename type="first">N</forename><surname>Cressie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>John Wiley &amp; Sons</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Mapreduce: Simplified data processing on large clusters</title>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Symp. on Operating System Design and Implementation (OSDI)</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="137" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">libstable: Fast, parallel, and high-precision computation of α-stable distributions in r, c/c++, and matlab</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Del Val</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Simmross-Wattenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Alberola-López</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Statistical Software</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="25" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Introduction to statistical analysis</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Dixon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">J</forename><surname>Massey</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1968">1968</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Quantitative Geography: Perspectives on Spatial Data Analysis</title>
		<author>
			<persName><forename type="first">S</forename><surname>Fotheringham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Brunsdon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Charlton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Decision tree classification of land cover from remotely sensed data</title>
		<author>
			<persName><forename type="first">M</forename><surname>Friedl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Brodley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sensing of Environment</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="399" to="409" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A survey on deep learning in big data</title>
		<author>
			<persName><forename type="first">M</forename><surname>Gheisari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Z A</forename><surname>Bhuiyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. on Computational Science and Engineering, CSE, and IEEE Int. Conf. on Embedded and Ubiquitous Computing</title>
		<meeting><address><addrLine>EUC</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="173" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The google file system</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Gobioff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Leung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Symp. on Operating Systems Principles (SOSP)</title>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="29" to="43" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Java I/O: Tips and Techniques for Putting I/O to Work</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">R</forename><surname>Harold</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="131" to="132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Reducing the dimensionality of data with neural networks</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">313</biblScope>
			<biblScope unit="issue">5786</biblScope>
			<biblScope unit="page" from="504" to="507" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Soil moisture mapping at regional scales using microwave radiometry: the southern great plains hydrology experiment</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Jackson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M L</forename><surname>Vine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Oldak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Starks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">T</forename><surname>Swift</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Isham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Haken</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2136" to="2151" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A bayesian framework for adaptive selection, calibration, and validation of coarse-grained models of atomistic systems</title>
		<author>
			<persName><forename type="first">F</forename><surname>Kathryn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Oden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Faghihi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computational Physics</title>
		<imprint>
			<biblScope unit="volume">295</biblScope>
			<biblScope unit="page" from="189" to="208" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The case for learned index structures</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kraska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Beutel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">H</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Polyzotis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Management of Data (SIGMOD)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="489" to="504" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A survey of open source tools for machine learning with big data in the hadoop ecosystem</title>
		<author>
			<persName><forename type="first">S</forename><surname>Landset</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Khoshgoftaar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Richter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hasanin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Big Data</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">24</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A survey of scheduling frameworks in big data systems</title>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Pacitti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Valduriez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Cloud Computing</title>
		<imprint>
			<biblScope unit="page">27</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Kolmogorov-smirnov test</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">H C</forename><surname>Lopes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Encyclopedia of Statistical Science</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="718" to="720" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">UQLab: A Framework for Uncertainty Quantification in MATLAB</title>
		<author>
			<persName><forename type="first">S</forename><surname>Marelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Sudret</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
		<respStmt>
			<orgName>ETH-Zürich</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Uqlab: A framework for uncertainty quantification in MATLAB</title>
		<author>
			<persName><forename type="first">S</forename><surname>Marelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Sudret</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Vulnerability, Risk Analysis and Management (ICVRAM)</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="2554" to="2563" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">MLlib: Machine Learning in Apache Spark</title>
		<author>
			<persName><forename type="first">X</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bradley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yavuz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Sparks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Venkataraman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Amde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Owen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Xin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Franklin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zaharia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Talwalkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">34</biblScope>
			<biblScope unit="page" from="1" to="7" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Sensitivity and uncertainty analysis in spatial modelling based on gis</title>
		<author>
			<persName><forename type="first">C</forename><surname>Michele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Stefano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Andrea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Agriculture, Ecosystems &amp; Environment</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="71" to="79" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A simplex method for function minimization</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Nelder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mead</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Journal</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="308" to="313" />
			<date type="published" when="1965">1965</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">The parallel C++ statistical library 'queso': Quantification of uncertainty for estimation, simulation and optimization</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">E</forename><surname>Prudencio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">W</forename><surname>Schulz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Euro-Par: Parallel Processing Workshops</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="398" to="407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">An approximate method for generating asymmetric random variables</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ramberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">W</forename><surname>Schmeiser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="78" to="82" />
			<date type="published" when="1974">1974</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A survey of decision tree classifier methodology</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Safavian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Landgrebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Systems, Man, and Cybernetics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="660" to="674" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Design and implementation of the sun network file system</title>
		<author>
			<persName><forename type="first">R</forename><surname>Sandberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kleiman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Walsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lyon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">the Summer USENIX conf</title>
		<imprint>
			<date type="published" when="1985">1985</date>
			<biblScope unit="page" from="119" to="130" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Understanding Machine Learning -From Theory to Algorithms</title>
		<author>
			<persName><forename type="first">S</forename><surname>Shalev-Shwatrz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ben-David</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">The hadoop distributed file system</title>
		<author>
			<persName><forename type="first">K</forename><surname>Shvachko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Radia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chansler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Symp. on Mass Storage Systems and Technologies (MSST)</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Complexity analysis of nelder-mead search iterations</title>
		<author>
			<persName><forename type="first">S</forename><surname>Singer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conf. on Applied Mathematics and Computation</title>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="185" to="196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">tmpfs: A virtual memory file system</title>
		<author>
			<persName><forename type="first">P</forename><surname>Snyder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European UNIX Users' Group Conf</title>
		<imprint>
			<date type="published" when="1990">1990</date>
			<biblScope unit="page" from="241" to="248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Big data classification: Problems and challenges in network intrusion prediction with machine learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Suthaharan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGMETRICS Performance Evaluation Review</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="70" to="73" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Uncertainty in spatial trajectories</title>
		<author>
			<persName><forename type="first">G</forename><surname>Trajcevski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computing with Spatial Trajectories</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="63" to="107" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Networked wireless sensor data collection: Issues, challenges, and approaches</title>
		<author>
			<persName><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Communications Surveys and Tutorials</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="673" to="687" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Fitting Statistical Distributions: The Generalized Lambda Distribution and Generalized Bootstrap Methods</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">D Z</forename><surname>Karian</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<publisher>Chapman and Hall/CRC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Obtaining calibrated probability estimates from decision trees and naive bayesian classifiers</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zadrozny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Elkan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="609" to="616" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Spark: Cluster computing with working sets</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zaharia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chowdhury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Franklin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shenker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Stoica</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX Workshop on Hot Topics in Cloud Computing</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>