<TEI xmlns="http://www.tei-c.org/ns/1.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xml:space="preserve" xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd">
	<teiHeader xml:lang="fr">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Convolutional Ladder Networks for Legal NERC and the Impact of Unsupervised Data in Better Generalizations</title>
				<funder ref="#_TDU4Qtc">
					<orgName type="full">European Union</orgName>
				</funder>
				<funder ref="#_KJRkThE">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher />
				<availability status="unknown"><licence /></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Cristian</forename><surname>Cardellino</surname></persName>
							<email>ccardellino@unc.edu.ar</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Natural Language Processing Group</orgName>
								<orgName type="institution">FAMAFyC-UNC</orgName>
								<address>
									<settlement>Córdoba</settlement>
									<country key="AR">Argentina</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Laura</forename><forename type="middle">Alonso</forename><surname>Alemany</surname></persName>
							<email>lauraalonsoalemany@unc.edu.ar</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Natural Language Processing Group</orgName>
								<orgName type="institution">FAMAFyC-UNC</orgName>
								<address>
									<settlement>Córdoba</settlement>
									<country key="AR">Argentina</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Milagro</forename><surname>Teruel</surname></persName>
							<email>mteruel@unc.edu.ar</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Natural Language Processing Group</orgName>
								<orgName type="institution">FAMAFyC-UNC</orgName>
								<address>
									<settlement>Córdoba</settlement>
									<country key="AR">Argentina</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Serena</forename><surname>Villata</surname></persName>
							<email>villata@i3s.unice.fr</email>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Université Côte d'Azur. CNRS</orgName>
								<orgName type="institution" key="instit2">Inria</orgName>
								<address>
									<region>I3S</region>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Santiago</forename><surname>Marro</surname></persName>
							<email>smarro@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Natural Language Processing Group</orgName>
								<orgName type="institution">FAMAFyC-UNC</orgName>
								<address>
									<settlement>Córdoba</settlement>
									<country key="AR">Argentina</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Convolutional Ladder Networks for Legal NERC and the Impact of Unsupervised Data in Better Generalizations</title>
					</analytic>
					<monogr>
						<imprint>
							<date />
						</imprint>
					</monogr>
					<idno type="MD5">540535C4B07FFBEEEE152DFCD7D31772</idno>
					<note type="submission">Submitted on 27 Nov 2019</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-04-12T14:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid" />
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div><p>des établissements d'enseignement et de recherche français ou étrangers, des laboratoires publics ou privés.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="fr">
		<body>
<div><head>Introduction</head><p>In recent years, deep learning methods have provided very powerful models for different kind of tasks, like computer vision and natural language processing. An important asset for many of these deep learning models is the existence of vasts amounts of labeled data to train them. Having the power of a neural network with small amounts of data comes with the burden of the model memorizing the inputs and overfitting the data, which renders useless models.</p><p>Obtaining labeled data to improve these kind of deep learning models is expensive and sometimes very difficult. Depending on the task, it may require domain experts for the annotation. Legal Named Entity Recognition and Classification (NERC) is an example of this case, where lawyers are needed to, if not annotate, at least supervise the process. On the other hand, however, we have large amounts of unlabeled data available that is cheap and fast to obtain in large pools. However, unlabeled data cannot be used directly to train a regular deep learning model. In this scenario, a semisupervised deep learning model like "Convolutional Ladder Networks" <ref type="bibr" target="#b12">(Rasmus et al. 2015)</ref> is of high value.</p><p>In this work, by adapting Convolutional Ladder Networks (CLadder) from image to text, we explore how the information given by unlabeled data sources improves on the generalization of the model, making it less prone to overfitting. We compare the use of unlabeled data by testing: (i) a purely supervised Convolutional Neural Network, (ii) the CLadder, and comparing it with (iii) the Stanford CRF-NER as a reference tool for the NERC task. Even if CLadder does not reach the same performance as the reference tool, results show how unlabeled data contributes to reduce overfitting, by not drastically dropping performance when changing from training to test data.</p><p>The rest of the paper is organized as follows. First we lay down previous work and compare it with our approach. Then we highlight the main idea behind Ladder Networks and explain how we adapted CLadder for text data. We continue with a brief description of the dataset used for the experimentation and the experimental work (with its corresponding evaluation) done in this paper. Next, we analyze the results and end with some conclusions and future work.</p></div>
<div><head>Related work</head><p>Named entity recognition and classification (NERC) in the legal domain have been recently explored in the works of <ref type="bibr">(Cardellino et al. 2017a</ref>) and <ref type="bibr">(Cardellino et al. 2017b</ref>). In their works, they explore Curriculum Learning <ref type="bibr" target="#b0">(Bengio et al. 2009)</ref> to train a supervised classifier for different hierarchies of the ontology. This work is based on their dataset, but as we are only in early stages of research we limited ourselves to the most abstract layer (i.e. the one with less classes).</p><p>Although convolutional neural networks (CNN) were originally associated to computer vision tasks, in recent years there have been a lot of different applications of convolutional networks in natural language processing. In particular, the network we base our architecture on is the one of <ref type="bibr" target="#b7">(Kim 2014)</ref>. This work evaluates a CNN architecture on various classification datasets, mostly comprised of Sentiment Analysis and Topic Categorization tasks, achieving good performance for the different datasets. The network is very simple, yet powerful, where the input is the sentence comprised of concatenated word2vec <ref type="bibr" target="#b11">(Mikolov et al. 2013)</ref> embeddings, followed by a convolutional layer with multiple filters, a max-pooling layer, and finally a SoftMax classifier.</p><p>In the area of NERC, one of the latest neural networks architectures being used, which has reached state-of-theart performances, is the Bidirectional LSTM-CNN network <ref type="bibr" target="#b3">(Chiu and Nichols 2016)</ref>. In this architecture, CNNs are used to extract character based features for the recurrent network. Unlike them, we explore the CNNs on a word level approach.</p></div>
<div><head>Ladder networks</head><p>This work adapts a semi-supervised deep learning technique from the domain of computer vision. Ladder networks were introduced in the work of <ref type="bibr" target="#b12">(Rasmus et al. 2015)</ref>, that extends the original work by Valpola <ref type="bibr" target="#b15">(Valpola 2015)</ref> which introduces the concept of ladder networks for unsupervised learning.</p><p>The idea of using unsupervised learning to help training a neural network was proposed by Suddarth and Kergosien <ref type="bibr" target="#b14">(Suddarth and Kergosien 1990)</ref>. Most of the methods that use an auxiliary task to help the supervised learning are only applied at pre-training, followed by normal supervised learning <ref type="bibr" target="#b5">(Hinton and Salakhutdinov 2006)</ref> (this is sometimes known in the literature as "disjoint semi-supervised learning"). In contrast, with ladder networks representations are learned jointly (this is sometimes called "joint semisupervised learning" in the relevant literature). Unsupervised learning is implemented through an auxiliary task, for example, reconstructing the input. In learning, the hidden representations among supervised and unsupervised tasks are shared, and thus the network generalizes better.</p><p>The key idea of ladder networks is to simultaneously train a feed-forward neural network (whether it is fully connected or convolutional) alongside an autoencoder, with shared weights. The network is trained by learning two different objectives: a supervised one, given by the prediction error of the labeled data; and an unsupervised one, given by the reconstruction error of the unlabeled data.</p><p>The model structure is an autoencoder with skip connections from the encoder to decoder and the learning task is similar to that in denoising autoencoders but applied to every layer, not just the inputs. The skip connections relieve the pressure to represent details in the higher layers of the model because, through the skip connections, the decoder can recover any details discarded by the encoder. For a more detailed description of the ladder network we refer the reader to <ref type="bibr" target="#b12">(Rasmus et al. 2015)</ref>.</p></div>
<div><head>Convolutional ladder networks for text data</head><p>Convolutional ladder networks (CLadder) are a variation of ladder networks that were also proposed by <ref type="bibr" target="#b12">(Rasmus et al. 2015)</ref>. In their work the convolutional layers and the max pooling layers are stacked forming a deep convolutional network. Each convolution window can have different sizes of width and length, this also applies for the max pooling layers.</p><p>As we follow the approach by <ref type="bibr" target="#b7">(Kim 2014)</ref>, our Convolutional Neural Network (CNN) network is not "deep" but "wide". It has a single convolution layer (that can be considered a wide convolution because of the different sizes), a layer of global max pooling per convolution filter, and finally one fully connected layer with a SoftMax classifier.</p><p>For text, the convolution moves through one dimension only, which represents the number of words that the window Figure <ref type="figure">1</ref>: Convolutional network with text input. There are multiple windows sizes (2, 3 and 4 words for the colors blue, red and green respectively) and for each there are multiple number of filters. Each filter is then max pooled to get a convolved feature. The convolved features are concatenated to form a feature vector that is later fed to the SoftMax classifier.</p><p>will take into account to apply the filters. Sometimes this is referred as "temporal convolution" in the literature. The max pooling layer, on the other hand, is global to the feature map (that is the convolved features obtained by the convolution operation), thus giving only one feature per filter instead of multiple features per filter region as is the case for CNNs used in computer vision.</p><p>In this architecture there are different window sizes (i.e. number of words covered by the convolution) applied directly to the word representation input (generally a word embedding). This gives multiple feature maps, one per each feature, which gives one convolved feature after applying global max pooling. This is visualized in Fig. <ref type="figure">1</ref>, where there are three sizes for the sliding windows (taking 2, 3 and 4 words, represented by colors blue, red and green respectively), as well as two filters per window (represented by different color intensity), we get a total of 6 convolved features map (the layers overlapped in the middle of the Figure.</p><p>Finally, the layer with global max pooling operation gives a total of 6 features (3 sliding window with 2 filters each), which is the "convolved features vector" that is then fed to the SoftMax classifier.</p><p>Following this structure we defined the CLadder represented in Fig. <ref type="figure" target="#fig_0">2</ref>. The ladder network has two encoder paths, one corrupted (the one on the left, marked by the red arrows) and one clean (the one on the right, marked by the green arrows), and one decoder path (in the center, with the blue arrows going from top to bottom). The corrupted encoder adds Gaussian noise in each layer as a method of regularization. The decoder, which works as an unsupervised learner, inverts the mappings of each layer of the encoder. It uses a denoising function to reconstruct the activations of each layer given the corrupted version of the mirror layer (denoted by the purple arrow going from left to right), and the previous layer output (the blue arrow). The target at each layer is the clean version of the activation (the dotted arrow going from right to left) and the difference between the reconstruction and the clean version serves as the denoising cost of that layer. The denoising function used in these experiments is the same one defined in the original paper of CLadders. In the encoder paths, the bottom layer represents the inputs and the top layer the outputs (predictions). In the decoder path, the bottom layer is the output of the autoencoder (the reconstruction of the original input).</p><p>The supervised cost between the predicted label and the ground truth label is calculated from the output of the corrupted encoder and the target label. The unsupervised cost is the sum of the denoising cost of all layers scaled by a hyperparameter that denotes the importance of each layer. For example, the first layers are more important than the last to reconstruct the input. The final cost is the sum of the supervised and the unsupervised cost. Batch normalization <ref type="bibr" target="#b6">(Ioffe and Szegedy 2015)</ref> is applied to each preactivation including the topmost layer to improve convergence (due to reduced covariate shift) and to prevent the denoising cost from encouraging the trivial solution (encoder outputs constant values as these are the easiest to denoise). Beside this we also decided to add regularization by L2 norm of the weights in the encoder and decoder path, which proved to be useful for better generalizations (we also tried dropout but without any improvements on the task).</p><p>In the encoder paths, the first layer (from bottom to top) represents the convolutions over the words, the different sizes of the layers mean different sizes of the sliding window, and the multiple filters are represented by the rectangles overlapped (in Fig. <ref type="figure" target="#fig_0">2</ref> there are 3 different sizes for convolution and 2 filters per convolution size, giving a total of 6 feature maps). The second layer represents the pooled features of each filter and each convolution size (by global max pooling). The final layer is a fully connected layer with a SoftMax classifier.</p><p>In the decoder path, the first layer (from top to bottom) is a "transpose" of the same layer of the encoder path (the fully connected layer). The second layer (that corresponds to the max pooling layer in the encoder) is an "upsampling" layer that simply takes the input and repeats it to map the dimensions of the convolutional layers. Finally, the last layer is a "transposed convolutional" layer (also know as "deconvolutional layer") that maps the convolutions to the original input matrix (the one in Fig. <ref type="figure">1</ref>). It is important to note that the mappings (and reconstruction objectives) of the decoder path to the encoder, in the case of the convolutional layer, are for each of the different window slide sizes (that is there is mapping between the convolution layer with 2-word windows, 3-word windows, etc.), not between convolutions of different window sizes.</p></div>
<div><head>Training dataset</head><p>As labeled data, we exploited Wikipedia links. To build our corpus, we downloaded a XML dump of the English Wikipedia<ref type="foot" target="#foot_1">1</ref> from March 2016, and we processed it via the <software ContextAttributes="used">WikiExtractor</software> (of Pisa 2015) to remove all the XML tags and Wikipedia markdown tags, but leaving the links. We extracted all those articles that contained a link to an entity of the WordNet-and Wikipedia-based YAGO ontology<ref type="foot" target="#foot_2">2</ref> (Suchanek, Kasneci, and Weikum 2007) that has been mapped to the legal ontology presented in <ref type="bibr">(Cardellino et al. 2017b</ref>). We considered as tagged entities the spans of text that are an anchor for a hyperlink whose URI is one of the mapped entities. We obtained a total of 4,5 million mentions, corresponding to 102,000 unique entities. Then, we extracted sentences that contained at least one mention of a named entity.</p><p>We consider the problem of Named Entity Recognition and Classification as a word-based representation, i.e., each word represents a training instance and is represented, in the network, by the concatenation of all the words that precede it. Then, words within the anchor span belong to one of the NE classes, others to the O class (Outside a Named Entity). The O class made more than 90% of the instances. This imbalance in the classes results largely biased the classifiers, so we randomly downsampled non-named entity words to make them at most 50% of the corpus. The resulting corpus consists of 10 million words, with words belonging to the O-class already downsampled. The data was split in three datasets: 80% for training, 10% for validation and 10% for test. The test dataset is the one used for evaluation and is held out until the final models were ready. The validation dataset was the one used to tune the hyperparameters of the different models.</p><p>As said before, Ladder Networks rely on a big quantity of unlabeled data to obtain a good representation of the universe. For these experiments, we use the same Wikipedia corpus as unlabeled data, because it is big enough. However, the autoencoder part of the ladder network captures other aspects of the same instances, obtaining a different representation that improves the representation obtained in the supervised task. In some of the experiments, the labeled data is limited to a percentage of the total labeled data, however the unlabeled data is still the same amount. That is a more realistic scenario, because CLadder can take advantage of much unlabeled data for its training objective.</p></div>
<div><head>Experimental setting</head><p>In this paper we want to assess how the unsupervised objective of the ladder network helps improve the generalization of the model. We measure this by as the reduction in overfitting of the CLadder as compared to supervised methods. Moreover, we want to explore how the amount of labeled training data affects the overfitting tendency of a model. In order to do that, we carry out two sets of experiments.</p><p>First we compare: (i) a purely supervised Convolutional Neural Network (CNN) for NERC, (ii) the same architecture in a semi-supervised environment given by the ladder networks, and, to have a point of comparison with a more standard algorithm, (iii) we use the reference given by the Stanford CRF-NER tool <ref type="bibr" target="#b4">(Finkel, Grenager, and Manning 2005)</ref>.</p><p>As we want to assess the impact provided by unlabeled data in the final task, we train two sets of classifiers, one using the whole corpus and one using only 10% of the labeled corpus.</p><p>Finally, for the case of the CNN and the CLadder, we inspect how the model's supervised cost progresses as different epochs occur while augmenting the number of unlabeled examples (one epoch is one pass of the mini-batch stochastic gradient descent algorithm through the whole corpus).</p><p>The neural networks were trained using backpropagation and the ADAM optimization <ref type="bibr" target="#b8">(Kingma and Ba 2014)</ref>. The sizes of the sliding windows were 2 to 5 words, with 32 filters each (which gives a total of 448 convolved features). The hyperparameters (learning rate, regularization rate, noise of the corrupted encoder, weights of the reconstruction costs, number of filter, sizes of the windows, etc.) were chosen by random hyperparameter optimization and evaluated on the validation data.</p><p>Each instance is the sequence of words up to the word that is being classified. The words are represented by a concatenation of their dense vector and the dense vector of the partof-speech tag of that word. Both the word and part-of-speech tags vectors were calculated previously with word2vec on the legal Wikipedia corpus. Originally we only used the word vector alone, but after some experiments we found out that the use of a PoS tag embedding improved the final overall performance. The PoS tags were obtained through the Stanford Parser <ref type="bibr" target="#b9">(Klein and Manning 2003)</ref>.</p></div>
<div><head>Evaluation</head><p>The evaluation of the experiments that compare the different networks and the Stanford NER were done on the training dataset, after the model finished training, as well as the heldout test set described previously.</p><p>Our idea is to measure the overfitting of different models when comparing the results of the training and test sets. An indicator of model overfitting is the difference in performance between training and test. The metrics we measure are three: accuracy, F1-score macro average (i.e. unweighted mean) and F1-score weighted average. The idea of showing both these averages is to also see how biased the models are toward the most frequent class, as a higher difference between these results (i.e. a very low value in macro average and very high value in weighted average) shows the algorithm is more biased towards the most frequent class.</p><p>In order to assess the evolution of the supervised cost (i.e. the prediction error) for training and validation data, we show the learning curves of 5 different experiments: (i) using only supervised CNN (i.e. no unlabeled data), and using CLadder with (ii) 25%, (iii) 50%, (iv) 75%, and (v) 100% of the unlabeled data (in all but (v), the unlabeled data is randomly sampled from the unannotated corpus). For these different experiments we split the supervised training data into 10-folds, train a model with 9 folds and evaluate it on the remaining fold (i.e. the "validation data", in this scenario, is really part of the training data). This is done for each fold.</p><p>We record the training and validation supervised cost on each epoch on a run of 10 epochs and use the information to calculate the mean and the standard deviation of the loss. The idea is to see how the proportion of unlabeled data in the ladder network affects the model generalization as well as the "error due to high variance" <ref type="bibr" target="#b10">(Manning, Raghavan, and Schütze 2008)</ref> the models have. This type of error is defined as the variation of the prediction of learned classifiers: it measures how inconsistent the predictions are from one another, over different datasets, not whether they are accurate or not.</p></div>
<div><head>Analysis of results</head></div>
<div><head>Evaluation of models' overfitting tendency</head><p>For the first set of experiments, described in the previous section, we show the results in Table <ref type="table" target="#tab_0">1</ref>. The table displays Accuracy, F1-score macro average, and F1-score weighted average on training and test data.</p><p>There are two sets of experiments, one with the full labeled dataset and one with only 10% (in the case of the CLadder, the unlabeled data remains total since it is a real case scenario with the availability of unlabeled information).</p><p>When using the full labeled dataset we see that, on training data, the two supervised approaches overcome the CLadder in all three metrics. However, on test data, accuracy and F1-score drop drastically for both supervised approaches, while performance is roughly maintained at the same level for CLadder. Moreover, CLadder's performance is much better than the performance of the supervised CNN. This tendency is even more visible if only 10% of the corpus is used for training, as in that case, the supervised CNN overfitting is even bigger, since the difference between training and test data performance increases.</p><p>Compared with the Stanford CRF, CLadder is not so far behind in performance on the test data. In future work, we will explore the impact of adding a Recurrent Layer or CRF to the top of the CLadder, as this kind of architecture has been successful for a variety of sequential NLP tasks, including NERC.</p><p>Regarding the F1-score averages, it is noticeable that, while in all cases drops significantly, for the macro average (the one that is the most susceptible to changes in classes with less instances), it is for CLadder that also drops the least, while maintaining the weighted average. This means that in general, the unlabeled data, besides helping in the model generalization, it makes it more robust to the less frequent classes, something that is very problematic in unbalanced datasets (very common in natural language processing tasks).</p><p>From these results it is clear that unsupervised data is indeed helpful to obtain a better generalization in the model, as the fact that the model cannot fit so well the training data (as the supervised models do) doesn't affect how well the model works with unseen examples of the test data.</p></div>
<div><head>Learning curves</head><p>Figure <ref type="figure" target="#fig_1">3</ref> shows the results of the second experiment we described in the previous section. The plot consists of 5 different plots, one for each experiment using different sizes of the unlabeled corpus to train the CLadder. The leftmost graphic shows the case for the supervised CNN, and as we move to the right we have more unlabeled data available for the CLadder. It is interesting to see how using only 25% of the unlabeled data to train the network affects the learning curve as a whole.</p><p>In all cases, for the CLadder, the training cost in the first epoch begins higher than for Supervised data (in which practically stays the same) and decreases after some epochs. Specially, the more unlabeled data there is for each epoch, the more the training cost decreases. This is a symptom of the unlabeled data affecting the weights of the encoder path (by trying to optimize the reconstruction) and thus making it more difficult the model overfitting training data. However, the most important result in this graphic is shown in the case of the validation data. For supervised CNN, there is a high gap between training and validation data throughout all the iterations, but for the case of CLadder this gap decreases as the model fits the training data better. Moreover, the more unlabeled data there is available, the more similar are the training and validation learning curves and the better results we obtain for validation data. Also, there is the error due to high variance, shown by the width of the shade (i.e. the standard deviation for the folds), which is higher for Supervised CNN than for CLadders. This means fully supervised models suffer from higher variance, thus making them more prone to overfitting when there is no presence of unlabeled data that helps the model generalize.</p></div>
<div><head>Conclusions and future work</head><p>In this paper we adapted a semi-supervised deep learning technique from computer vision into a natural language processing task of named entity recognition and classification in the legal domain. We wanted to assess how unlabeled data affects a model. By doing some experiments with this adapted technique we have shown the importance of unlabeled data and the impact it can have on better model generalization. The results we achieved, even if not state-of-theart, are promising in that they generalize well to unseen data, and we will continue to explore them.</p><p>We are currently working on applying this approach to smaller datasets, where the impact of a mechanism to counterbalance the overfitting tendency will is more valuable.</p><p>Further areas of study are to adapt different convolutional neural networks architectures used on other natural language processing tasks and see if that helps improving the performance even more. For example, something in the line of <ref type="bibr" target="#b3">(Chiu and Nichols 2016)</ref>, where the character level convolution could be included in the CLadder. Even more, there are some research for Recurrent Ladder Networks applied in image data which could also be adapted for natural language processing tasks. </p></div><figure xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Convolutional ladder network: the corrupted encoder is the one on the left, connecting the layers by red arrows. The clean encoder is the one on the right, connecting the layers with the green arrows. The decoder path is the one in the middle, connecting the layers with the blue arrows going from top to bottom. Each layer in the decoder is fed with information of the previous layer (blue arrow) and the same level layer of the corrupted encoder path (purple arrows on the left). It uses a denoising function with that information to try and reconstruct the input of each layer given by the clean encoder (purple dotted arrows on the right).</figDesc><graphic coords="4,59.96,54.00,226.57,108.49" type="bitmap" /></figure>
<figure xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Learning curve: mean and standard deviation of the supervised cost function for training (red) and validation (indigo) datasets across the successive epochs of the algorithm. The line represents the mean of the loss and the shaded area represents the standard deviation (error due to high variance) for the supervised cost for the different folds of training data. The left-most graphics represents the case of purely supervised CNN classification (no unsupervised data). The other represent the models using different portions of the unlabeled corpus in the ladder network.</figDesc><graphic coords="7,129.60,54.00,352.79,154.35" type="bitmap" /></figure>
<figure type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Table of results for Stanford CRF-NER, supervised CNN and ladder CNN. It shows the Accuracy (Acc), F1-score Macro Average (F1 M) and F1-score Weighted Average (F1 W) for both training and test data and for training data using the full training dataset and only 10% of the training dataset</figDesc><table><row><cell cols="2">Training Size Algorithm</cell><cell>Training Data</cell><cell /><cell>Test Data</cell></row><row><cell /><cell /><cell cols="4">Acc F1 M F1 W Acc F1 M F1 W</cell></row><row><cell /><cell>Stanford CRF</cell><cell>0.99 0.98</cell><cell>0.99</cell><cell>0.83 0.51</cell><cell>0.82</cell></row><row><cell>Full</cell><cell cols="2">Supervised CNN 0.80 0.60</cell><cell>0.79</cell><cell>0.72 0.45</cell><cell>0.71</cell></row><row><cell /><cell>CLadder</cell><cell>0.78 0.58</cell><cell>0.74</cell><cell>0.77 0.48</cell><cell>0.76</cell></row><row><cell /><cell>Stanford CRF</cell><cell>0.99 0.99</cell><cell>0.99</cell><cell>0.75 0.44</cell><cell>0.73</cell></row><row><cell>10%</cell><cell cols="2">Supervised CNN 0.84 0.63</cell><cell>0.82</cell><cell>0.61 0.40</cell><cell>0.60</cell></row><row><cell /><cell>CLadder</cell><cell>0.79 0.58</cell><cell>0.73</cell><cell>0.72 0.43</cell><cell>0.7</cell></row></table></figure>
			<note place="foot" xml:id="foot_0"><p>The </p></note>
			<note place="foot" n="1" xml:id="foot_1"><p>https://dumps.wikimedia.org/</p></note>
			<note place="foot" n="2" xml:id="foot_2"><p>https://www.yago-knowledge.org/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>The authors have received funding from the <rs type="funder">European Union</rs>'s <rs type="programName">Horizon 2020 research and innovation programme</rs> under the <rs type="grantName">Marie Skodowska-Curie</rs> grant agreement No <rs type="grantNumber">690974</rs> for the project <rs type="projectName">MIREL</rs>: <rs type="projectName">MIning</rs> and REasoning with Legal texts.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_TDU4Qtc">
					<idno type="grant-number">690974</idno>
					<orgName type="grant-name">Marie Skodowska-Curie</orgName>
					<orgName type="project" subtype="full">MIREL</orgName>
					<orgName type="program" subtype="full">Horizon 2020 research and innovation programme</orgName>
				</org>
				<org type="funded-project" xml:id="_KJRkThE">
					<orgName type="project" subtype="full">MIning</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Curriculum learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Louradour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th Annual International Conference on Machine Learning, ICML '09</title>
		<meeting>the 26th Annual International Conference on Machine Learning, ICML '09<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="41" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning Slowly To Learn Better: Curriculum Learning for Legal Ontology Population</title>
		<author>
			<persName><forename type="first">C</forename><surname>Cardellino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Teruel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Alemany</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Villata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirtieth International Florida Artificial Intelligence Research Society Conference (FLAIRS 2017)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A low-cost, high-coverage legal named entity recognizer, classifier and linker</title>
		<author>
			<persName><forename type="first">C</forename><surname>Cardellino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Teruel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Alemany</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Villata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th Edition of the International Conference on Articial Intelligence and Law, ICAIL '17</title>
		<meeting>the 16th Edition of the International Conference on Articial Intelligence and Law, ICAIL '17<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="9" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Named entity recognition with bidirectional lstm-cnns</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Nichols</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="357" to="370" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Incorporating non-local information into information extraction systems by gibbs sampling</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Grenager</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43nd Annual Meeting of the Association for Computational Linguistics (ACL 2005)</title>
		<editor>
			<persName><forename type="first">T</forename><forename type="middle">A</forename><surname>Computer Linguistics</surname></persName>
		</editor>
		<meeting>the 43nd Annual Meeting of the Association for Computational Linguistics (ACL 2005)</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="363" to="370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Reducing the dimensionality of data with neural networks</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">313</biblScope>
			<biblScope unit="issue">5786</biblScope>
			<biblScope unit="page" from="504" to="507" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32Nd International Conference on International Conference on Machine Learning</title>
		<meeting>the 32Nd International Conference on International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>JMLR</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1746" to="1751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno>CoRR abs/1412.6980</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Accurate unlexicalized parsing</title>
		<author>
			<persName><forename type="first">D</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 41st Annual Meeting on Association for Computational Linguistics</title>
		<meeting>the 41st Annual Meeting on Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="423" to="430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Raghavan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Schütze</surname></persName>
		</author>
		<title level="m">In-troduction to Information Retrieval</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<ptr target="http://medialab.di.unipi.it/wiki/WikipediaExtractor" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Conference on Neural Information Processing Systems -Volume 2, NIPS'13</title>
		<meeting>the 26th International Conference on Neural Information Processing Systems -Volume 2, NIPS'13<address><addrLine>USA; Pisa, M. U</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates Inc.</publisher>
			<date type="published" when="2013">2013. 2015</date>
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Semi-supervised learning with ladder networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Rasmus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Valpola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Honkala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Berglund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Raiko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Neural Information Processing Systems -Volume 2, NIPS'15</title>
		<meeting>the 28th International Conference on Neural Information Processing Systems -Volume 2, NIPS'15<address><addrLine>Cambridge, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3546" to="3554" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Yago: A core of semantic knowledge</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">M</forename><surname>Suchanek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kasneci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Weikum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th International Conference on World Wide Web, WWW '07</title>
		<meeting>the 16th International Conference on World Wide Web, WWW '07<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="697" to="706" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Rule-injection hints as a means of improving network performance and learning time</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Suddarth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">L</forename><surname>Kergosien</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Networks</title>
		<editor>
			<persName><forename type="first">L</forename><forename type="middle">B</forename><surname>Almeida</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Wellekens</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin, Heidelberg; Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1990">1990</date>
			<biblScope unit="page" from="120" to="129" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Chapter 8 -from neural pca to deep unsupervised learning</title>
		<author>
			<persName><forename type="first">H</forename><surname>Valpola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Independent Component Analysis and Learning Machines</title>
		<editor>
			<persName><forename type="first">E</forename><surname>Bingham</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Kaski</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Laaksonen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Lampinen</surname></persName>
		</editor>
		<imprint>
			<publisher>Academic Press</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="143" to="171" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>