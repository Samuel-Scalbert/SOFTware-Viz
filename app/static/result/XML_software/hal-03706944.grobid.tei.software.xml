<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="fr">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Extraction d&apos;informations liées au locuteur depuis un modèle acoustique personnalisé</title>
				<funder ref="#_MtpteKy #_DRTzNtn">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-06-17">17 Juin 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Salima</forename><surname>Mdhaffar</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">LIA</orgName>
								<orgName type="institution" key="instit2">Avignon Université</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jean-François</forename><surname>Bonastre</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">LIA</orgName>
								<orgName type="institution" key="instit2">Avignon Université</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Marc</forename><surname>Tommasi</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">UMR 9189</orgName>
								<orgName type="institution" key="instit1">Université de Lille</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<orgName type="institution" key="instit3">Inria</orgName>
								<orgName type="institution" key="instit4">Centrale Lille -CRIStAL</orgName>
								<address>
									<settlement>Lille</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Natalia</forename><surname>Tomashenko</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">LIA</orgName>
								<orgName type="institution" key="instit2">Avignon Université</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yannick</forename><surname>Estève</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">LIA</orgName>
								<orgName type="institution" key="instit2">Avignon Université</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Extraction d&apos;informations liées au locuteur depuis un modèle acoustique personnalisé</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-06-17">17 Juin 2022</date>
						</imprint>
					</monogr>
					<idno type="MD5">4D3B647DC8C5890548EC29C5964AE647</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-04-12T14:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Modèles acoustiques personnalisés</term>
					<term>apprentissage fédéré</term>
					<term>vie privée Personalized acoustic model</term>
					<term>federated learning</term>
					<term>privacy</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Plusieurs services intégrés dans notre vie quotidienne utilisent la reconnaissance automatique de la parole (Apple-Siri, Amazon-Alexa...). Ces services s'appuient sur des modèles entraînés sur une grande quantité de données pour assurer leur efficacité. Les données utilisées sont collectées via les applications, à partir des interactions des utilisateurs. Elles contiennent souvent des informations sensibles, ce qui peut créer d'importants problèmes de confidentialité. Dans ce contexte, de nouveaux paradigmes d'apprentissage automatique ont été proposés, comme l'apprentissage fédéré et distribué. Tous deux permettent de créer des modèles personnalisés à partir de données privées. Seuls les modèles sont alors partagés, sans exposer les données elles-mêmes. Une question cruciale est de savoir si la diffusion de ces modèles acoustiques (MA) personnalisés peut aussi entraîner une fuite d'informations personnelles. Les résultats montrent qu'il est possible de retrouver des informations liées au locuteur en s'appuyant sur les modifications de poids d'un MA induites par l'adaptation locale sur ce locuteur.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="fr">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Plusieurs services présents dans notre vie quotidienne utilisent la reconnaissance automatique de la parole (Apple Siri, Amazon Alexa, Google Home...). Pour être performants, ces services ont besoin de modèles entraînés sur une grande quantité de données et utilisent des données collectées en mode production, venant de leurs utilisateurs, créant un risque sérieux en termes de confidentialité des données des utilisateurs. Les nouvelles réglementations sur les données, comme le Règlement général sur la protection des données (RGPD) dans l'Union européenne, changent les règles afin de protéger la vie privée des citoyens <ref type="bibr" target="#b11">(Nautsch et al., 2019)</ref>. Afin d'améliorer les performances des modèles de la reconnaissance de la parole en exploitant l'expérience des utilisateurs sans accéder à leurs données, des solutions telles que l'apprentissage fédéré et l'apprentissage distribué ont été proposées. Elles consistent à échanger des modèles personnalisés, ou leurs gradients, au lieu des données brutes <ref type="bibr" target="#b6">(Leroy et al., 2019;</ref><ref type="bibr" target="#b4">Hard et al., 2020;</ref><ref type="bibr" target="#b3">Guliani et al., 2021;</ref><ref type="bibr" target="#b16">Yu et al., 2021;</ref><ref type="bibr" target="#b2">Cui et al., 2021)</ref> pour préserver la vie privée des utilisateurs. Dans le cadre de l'apprentissage distribué, un modèle personnalisé est un modèle qui a été adapté localement à un utilisateur <ref type="bibr" target="#b8">(Mansour et al., 2020)</ref>. Dans un travail récent <ref type="bibr" target="#b10">(Mdhaffar et al., 2021)</ref>, nous avons étudié une approche pour personnaliser un modèle acoustique hybride HMM/TDNN <ref type="bibr" target="#b12">(Peddinti et al., 2015)</ref> dans un contexte d'apprentissage collaboratif.</p><p>Dans cet article, nous étudions les informations contenues dans les modèles acoustiques personnalisés. En particulier, nous nous intéressons aux informations liées à l'identité et au sexe du locuteur qui peuvent être extraites à partir des modèles acoustiques personnalisés. Des travaux antérieurs ont étudié les représentations intermédiaires de la parole calculées dans des modèles neuronaux de bout en bout pour la reconnaissance de la parole. Ils ont illustré la manière dont ces modèles construisent des représentations phonétiques et graphémiques <ref type="bibr" target="#b1">(Belinkov &amp; Glass, 2017;</ref><ref type="bibr" target="#b0">Belinkov et al., 2019)</ref>, et ils ont montré comment la variabilité du locuteur et le bruit sont progressivement éliminés à mesure qu'on s'éloigne de la couche d'entrée d'un modèle neuronal profond <ref type="bibr" target="#b7">(Li et al., 2020)</ref>. À notre connaissance, il n'existe aucune étude dans la littérature sur l'information contenue dans les changements des poids des réseaux neuronaux dus à la personnalisation d'un modèle acoustique.</p><p>L'article est organisé comme suit : la section 2 introduit un modèle acoustique personnalisé. La section 3 présente la méthode utilisée dans cette étude pour retrouver l'information contenue dans les changements des poids des réseaux neuronaux d'un modèle acoustique personnalisé. La section 4 présente notre protocole expérimental. La section 5 présente les résultats. Enfin, nos conclusions et nos perspectives futures sont énoncées dans la section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Personnalisation d'un modèle acoustique</head><p>Dans notre scénario, un modèle acoustique générique est entraîné avec un jeu de données public. Il est ensuite déployé sur chaque terminal client où ce dernier est adapté par fine-tuning localement à partir des données utilisateur . Le fine-tuning consiste à poursuivre le processus d'apprentissage du modèle acoustique générique sur un petit jeu de données du locuteur, en veillant à éviter le sur-apprentissage. Le modèle résultant du fine-tuning est considéré comme un modèle personnalisé pour le locuteur local. La figure 1 illustre le processus de personnalisation d'un modèle acoustique.</p><p>Dans le cadre de l'apprentissage fédéré, les modèles personnalisés sont partagés pour être agrégé et améliorer un modèle global sans partager les données de l'utilisateur. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Sexe du locuteur</head><p>Dans la première partie de cette étude, nous souhaitons étudier le niveau d'information liée au sexe capturé dans les modèles acoustiques neuronaux personnalisés, ou plus exactement dans les matrices de poids neuronaux correspondantes. Nous considérons que si cette information est encore présente dans les modèles personnalisés, les différences liées au sexe seront prédominantes et peuvent émerger lors d'un processus de regroupement automatique en deux classes : une classe dédiée aux femmes et l'autre classe est dédiée aux hommes. Pour cela, nous appliquons un regroupement agglomératif sur les matrices de poids de l'ensemble des modèles acoustiques personnalisés afin de créer une structure hiérarchique, jusqu'à avoir deux classes. Cette approche est basée sur un algorithme de classification non supervisé qui construit une arbre en partant des "feuilles" (poids d'une couche d'un modèle acoustique personnalisé) et procède par fusions successives des plus proches regroupements jusqu'à obtenir un regroupement unique "racine". La distance entre deux modèles acoustiques neuronaux est calculée avec la distance euclidienne appliquée aux matrices de poids de la même couche cachée.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Identité du locuteur</head><p>Dans la deuxième partie de cette étude, nous voulons évaluer la capacité d'identifier les locuteurs, toujours en ne considérant que les modifications appliquées aux matrices de poids lors de la personnalisation d'un modèle acoustique. Cependant, ces matrices de poids et même leurs couches cachées, sont de très grande dimension. Les approches de réduction de la dimensionnalité comme l'Analyse en Composantes Principales (ACP) sont une solution potentielle mais le grand facteur de réduction visé, combiné à un nombre limité d'échantillons (un modèle par locuteur) pourrait résulter dans ce cas à une grande perte d'information discriminante.</p><p>Afin de résoudre ce problème, nous proposons d'appliquer une méthode inspirée de <ref type="bibr" target="#b14">(Snyder et al., 2018)</ref>, qui consiste à apprendre un extracteur des représentations du locuteur. Nous proposons de construire une extracteur à l'aide d'un réseau de neurones appris sur les matrices de poids d'une couche cachée donnée des modèles neuronaux personnalisés de reconnaissance automatique de la parole. L'objectif de l'apprentissage est une tâche de reconnaissance du locuteur. Vu le nombre de données limité (notre jeu de données d'apprentissage est très petit), nous proposons de modifier la tâche de discrimination du locuteur en utilisant des classes de locuteurs comme labels de classification, au lieu des locuteurs. Ceci permet d'augmenter le nombre d'exemples par classe pendant la phase d'apprentissage, et donc de réduire le risque de sur-apprentissage. Les classes de locuteurs utilisées pour l'entraînement de l'extracteur ont été construites à partir d'un regroupement hiérarchique des i-vecteurs présents dans les données d'entraînement du modèle générique.</p><p>Afin de réduire le problème de mémoire de l'extracteur (les matrices d'entrée sont très larges) et de surmonter cette difficulté, nous avons conçu une structure spécifique pour notre extracteur. En partant d'un classificateur classique de réseau neuronal profond (DNN), nous appliquons une approche d'entrée multi-blocs. La matrice de poids est divisée en petits blocs qui sont liés séparément à une couche cachée dédiée. Un petit bloc de la matrice de poids d'entrée est composé de tous les poids liés à quelques unités neuronale dans la couche cachée ciblée dans le modèle acoustique. Par exemple, si la couche cachée ciblée H t de l'architecture du modèle acoustique du système de reconnaissance de la parole contient n unités, la matrice des poids utilisée comme entrée de notre extracteur d'intégration du locuteur sera divisée en multiple de n blocs différents. Ensuite, les sorties de la couche cachée dédiée à chaque bloc sont concaténées pour créer la couche cachée suivante de l'extracteur basé sur un DNN, composé de couches entièrement connectées suivies de la couche finale softmax.</p><p>La structure de l'extracteur est illustrée dans la Figure <ref type="figure" target="#fig_1">2</ref>. La couche cachée juste en dessous de la couche softmax représente la couche de la représentation vectorielle du locuteur. Le modèle neuronal résultant est capable d'extraire des représentations de locuteurs à partir des modèles acoustiques, y compris pour des locuteurs qui n'étaient pas présents dans les données d'apprentissage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Protocole expérimental 4.1 Système de reconnaissance de la parole</head><p>Notre système de reconnaissance est fondé sur la boîte à outils de reconnaissance vocale Kaldi <ref type="bibr" target="#b13">(Povey et al., 2011)</ref>. Nous utilisons les modèles acoustiques hybrides de type HMM/TDNN (Time Delay Neural Network). Notre modèle acoustique neuronal est composé de 13 couches avec une dimension de 512. Il prend comme entrée des paramètres MFCC de dimension 40 et des i-vecteurs de dimension 100. Nous avons appliqué deux stratégies d'augmentation des données sur les données d'apprentissage : perturbation de la vitesse (avec des facteurs 0,9 ; 1,0 et 1,1), et perturbation du volume. Le nombre total de paramètres est 13,8M. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Modèles personnalisés</head><p>Un modèle acoustique générique est entrainé avec la partie generic. Les modèles personnalisés sont obtenus en fine-tunant le modèle générique sur les données du locuteur provenant de p1 et p2 : pour chaque locuteur, nous créons deux modèles personnalisés en utilisant séparément ses deux sessions de cinq minutes. Alors, pour la plupart des locuteurs (locuteurs avec une durée &gt; 10 minutes), deux modèles personnalisés différents sont obtenus.</p><p>Pour la reproductibilité des différentes expériences, les modèles acoustiques personnalisés ainsi que le modèle générique sont disponibles en ligne<ref type="foot" target="#foot_2">2</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Résultats</head><p>Dans cette section, nous présentons les résultats de deux analyses <ref type="bibr" target="#b9">(Mdhaffar et al., 2022)</ref> : celle concernant l'information sur le sexe du locuteur et celle concernant l'identité du locuteur.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Sexe du locuteur</head><p>Il existe plusieurs méthodes utilisées pour évaluer la performance du regroupement agglomératif. Dans notre étude, nous utilisons la pureté. La pureté se concentre uniquement sur la maximisation du nombre total de vraies réponses positives par regroupement. Les valeurs de pureté sont comprises entre 0 et 1 (regroupement parfait). La pureté est définie comme</p><formula xml:id="formula_0">P urity = 1 N k X i=1 max j |c i \ t j |<label>(1)</label></formula><p>où N est le nombre de locuteurs, k est le nombre de regroupements, c i est un regroupement et t j est le nombre de classifications pour le regroupement c i .</p><p>La figure 3 montre les résultats pour les différentes couches cachées du réseau neuronal des modèles acoustiques pour les données dans p2. Nous observons qu'il est possible d'obtenir deux regroupements basés sur le sexe avec une valeur de pureté de 0,96 pour la couche 5. Nous observons que l'information sur le genre peut être identifiée dans les poids du réseau neuronal des modèles acoustiques. Les résultats montrent que l'information de genre peut être plus facilement identifiée dans les cinq premières couches. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Vérification du locuteur</head><p>Tout d'abord, un extracteur de représentations des locuteurs est entraîné en utilisant chaque couche de nos modèles acoustiques comme entrée. Pour entraîner l'extracteur des représentations des locuteurs, nous utilisons 926 modèles de locuteurs personnalisés correspondant à 463+463 sous-ensembles uniques de p1. L'extracteur est entraîné avec les données p2 pour extraire les représentations pour les locuteurs p1 (sachant qu'il n'y a pas de chevauchement entre les locuteurs p1 et p2). Respectivement, une deuxième expérience est menée avec p1 comme ensemble de test et p2 comme ensemble d'entraînement pour l'extracteur.</p><p>Le nombre de classes cibles (issue du regroupement hiérarchique des i-vecteurs des locuteurs présents dans les données d'entraînement) utilisées pour entraîner notre extracteur est fixé à 20 et la dimension des vecteurs de sortie (les représentations de locuteurs) est fixée à 100.</p><p>Au moment du test, l'extracteur entraîné est utilisé pour extraire les représentations du locuteur à partir de chaque instance d'entrée. Les instances d'entrée sont composées de certains poids de modèles acoustiques de réseaux de neurones personnalisés en fonction du locuteur. Nous utilisons une tâche de vérification des locuteurs pour évaluer la capacité à reconnaître les locuteurs à partir d'un poids de couche donné. Une simple distance cosinus est utilisée pour calculer le score de vérification pour une paire (enrollment, test).</p><p>Les données de chaque locuteur (voir section 4.2) sont divisées en deux sessions, notées s1 et s2. On obtient une paire "target", (x s1 i , x s2 i ), par locuteur x i . Les paires "non target", (x s1 i , x s2 j ), sont formées en croisant la première session d'un locuteur donné avec toutes les secondes sessions des autres locuteurs. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>Dans cette étude, nous avons montré qu'il est possible de retrouver le sexe et l'identité d'un locuteur à partir de l'analyse des modifications appliquées aux poids de son modèle acoustique personnalisé.</p><p>Les expériences menées sur le jeu de données TED-LIUM3 montrent que l'information concernant le sexe du locuteur est apportée principalement par les mises à jour impactant les cinq premières couches d'un modèle acoustique HMM/TDNN composé de 13 couches cachées, alors que l'identité du locuteur est principalement intégrée dans les couches cachées intermédiaires (5 à 9). Afin d'analyser l'information liée à l'identité, nous avons également proposé une méthode originale pour construire un extracteur de représentations du locuteur à partir de matrices de poids personnalisées. Nous avons obtenu une pureté de 0,96 pour la reconnaissance du sexe sur les cinq premières couches et un EER de vérification du locuteur de 9% pour la couche 9. Ces résultats sont particulièrement intéressants pour de futurs travaux portant sur l'apprentissage distribué pour la préservation de la vie privée. Dans cette direction, nous avons proposé dans une étude en parallèle, deux modèles d'attaques pour les modèles personnalisés <ref type="bibr" target="#b15">(Tomashenko et al., 2022)</ref>. L'approche utilisée pour ces modèles consiste à construire des empreintes de ces modèles personnalisés à partir des traces de leur application sur un jeu de données fixe et indépendant. Nous envisageons dans nos futurs travaux d'approfondir le travail présenté dans cet article en étudiant l'information de l'identité du locuteur sur des modèles acoustiques personnalisés entraînés sans i-vecteurs. Nous souhaitons également effectuer une analyse similaire en étudiant plus de caractéristiques du locuteur (accent, nationalité, âge, émotion, etc).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>FIGURE 1 -</head><label>1</label><figDesc>FIGURE 1 -Processus de personnalisation d'un modèle acoustique</figDesc><graphic coords="3,190.19,152.73,214.62,134.14" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>FIGURE 2 -</head><label>2</label><figDesc>FIGURE 2 -Approche utilisée pour entraîner l'extracteur des représentations du locuteur à partir des poids d'une couche d'un modèle acoustique personnalisé</figDesc><graphic coords="5,161.19,152.73,272.61,154.36" type="bitmap" /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>Session Orale -Reconnaissance Automatique de la Parole</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_1"><p>https ://lium.univ-lemans.fr/ted-lium3/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_2"><p>https ://github.com/mdhaffar/Acoustic_model_personalisation</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_3"><p>Session Orale -Reconnaissance Automatique de la Parole</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_4"><p>13-17 Juin 2022 JEP2022Session Orale -Reconnaissance Automatique de la Parole</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="7">Remerciements</head><p>Ce travail a été financé par les projets <rs type="projectName">ANR DEEP-PRIVACY</rs> (<rs type="grantNumber">ANR18-CE23-0018</rs>) et <rs type="projectName">VoicePersonae</rs> (<rs type="grantNumber">ANR-18-JSTS-0001</rs>). Ces travaux ont bénéficié d'un <rs type="programName">accès aux moyens de calcul de l'IDRIS au travers de l</rs>'allocation de ressources 2021-AD011012551 attribuée par GENCI.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_MtpteKy">
					<idno type="grant-number">ANR18-CE23-0018</idno>
					<orgName type="project" subtype="full">ANR DEEP-PRIVACY</orgName>
				</org>
				<org type="funded-project" xml:id="_DRTzNtn">
					<idno type="grant-number">ANR-18-JSTS-0001</idno>
					<orgName type="project" subtype="full">VoicePersonae</orgName>
					<orgName type="program" subtype="full">accès aux moyens de calcul de l&apos;IDRIS au travers de l</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Analyzing Phonetic and Graphemic Representations in End-to-End Automatic Speech Recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Belinkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><forename type="middle">A</forename><surname>Glass</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page" from="81" to="85" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Analyzing hidden representations in end-to-end automatic speech recognition systems</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Belinkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Glass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Neural Information Processing Systems</title>
		<meeting>the 31st International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2438" to="2448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Federated acoustic modeling for automatic speech recognition</title>
		<author>
			<persName><forename type="first">X</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><forename type="middle">S</forename><surname>Kingsbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="6748" to="6752" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Training speech recognition models with federated learning : A quality/cost framework</title>
		<author>
			<persName><forename type="first">D</forename><surname>Guliani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Beaufays</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Motta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3080" to="3084" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Training keyword spotting models on non-iid data with federated learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Partridge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Subrahmanya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">L</forename><surname>Moreno</surname></persName>
		</author>
		<author>
			<persName><surname>Mathews R</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Interspeech 2020</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">TED-LIUM 3 : twice as much data and corpus repartition for experiments on speaker adaptation</title>
		<author>
			<persName><forename type="first">F</forename><surname>Hernandez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ghannay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Tomashenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Estève</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Speech and Computer</title>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="198" to="208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Federated learning for keyword spotting</title>
		<author>
			<persName><forename type="first">Leroy</forename><forename type="middle">D</forename><surname>Coucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lavril</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Gisselbrecht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Dureau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6341" to="6345" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">What does a network layer hear ? analyzing hidden representations of end-to-end asr through speech synthesis</title>
		<author>
			<persName><forename type="first">Li C.-Y</forename><surname>Yuan P.-C</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lee H.-Y</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="6434" to="6438" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Three approaches for personalization with applications to federated learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Mansour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mohri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">T</forename><surname>Suresh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Retrieving speaker information from personalized acoustic models for speech recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mdhaffar</surname></persName>
		</author>
		<author>
			<persName><surname>Bonastre J.-F</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tommasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Tomashenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Estève</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Study on acoustic model personalization in a context of collaborative learning constrained by privacy preservation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mdhaffar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tommasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Estève</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
		<respStmt>
			<orgName>SPECOM</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The GDPR &amp; Speech Data : Reflections of legal and technology communities, first steps towards a common understanding</title>
		<author>
			<persName><forename type="first">A</forename><surname>Nautsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Jasserand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Kindt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Todisco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Trancoso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Evans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Interspeech : ISCA</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A time delay neural network architecture for efficient modeling of long temporal contexts</title>
		<author>
			<persName><forename type="first">V</forename><surname>Peddinti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Interspeech 2015, 16th Annual Conference of the International Speech Communication Association</title>
		<meeting><address><addrLine>Dresden, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>ISCA</publisher>
			<date type="published" when="2015-09-06">2015. September 6-10, 2015</date>
			<biblScope unit="page" from="3214" to="3218" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The Kaldi speech recognition toolkit</title>
		<author>
			<persName><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ghoshal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Boulianne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Glembek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hannemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Motlicek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Schwarz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 2011 workshop on automatic speech recognition and understanding</title>
		<imprint>
			<publisher>IEEE Signal Processing Society</publisher>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">X-vectors : Robust dnn embeddings for speaker recognition</title>
		<author>
			<persName><forename type="first">D</forename><surname>Snyder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Garcia-Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5329" to="5333" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Privacy attacks for automatic speech recognition acoustic models in a federated learning framework</title>
		<author>
			<persName><forename type="first">N</forename><surname>Tomashenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mdhaffar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tommasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Estève</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bonastre J.-F</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2022</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Federated learning in ASR : Not as easy as you think</title>
		<author>
			<persName><forename type="first">Yu</forename><forename type="middle">W</forename><surname>Freiwald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tewes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Huennemeyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Kolossa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ITG Conference on Speech Communication</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
