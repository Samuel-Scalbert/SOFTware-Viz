<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="fr">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Speech enhancement with variational autoencoders and alpha-stable distributions</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Simon</forename><surname>Leglaive</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Inria Grenoble Rhône-Alpes</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Umut</forename><surname>Şimşekli</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Télécom ParisTech</orgName>
								<orgName type="laboratory">LTCI</orgName>
								<orgName type="institution">Université Paris-Saclay</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Antoine</forename><surname>Liutkus</surname></persName>
							<affiliation key="aff2">
								<orgName type="laboratory">Inria and LIRMM</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Laurent</forename><surname>Girin</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Inria Grenoble Rhône-Alpes</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department" key="dep1">INP</orgName>
								<orgName type="department" key="dep2">GIPSA-lab</orgName>
								<orgName type="institution">Univ. Grenoble Alpes</orgName>
								<address>
									<settlement>Grenoble</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Radu</forename><surname>Horaud</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Inria Grenoble Rhône-Alpes</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Speech enhancement with variational autoencoders and alpha-stable distributions</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">ABEE39E4B1FFED790172AC63042995F5</idno>
					<idno type="DOI">10.1109/ICASSP.2019.8682546</idno>
					<note type="submission">Submitted on 8 Feb 2019</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-04-12T14:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Speech enhancement</term>
					<term>variational autoencoders</term>
					<term>alpha-stable distribution</term>
					<term>Monte Carlo expectation-maximization</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>à la diffusion de documents scientifiques de niveau recherche, publiés ou non, émanant des établissements d'enseignement et de recherche français ou étrangers, des laboratoires publics ou privés.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="fr">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Speech enhancement is one of the central problems in audio signal processing <ref type="bibr" target="#b0">[1]</ref>. The goal is to recover a clean speech signal after observing a noisy mixture. In this work, we address single-channel speech enhancement, which can be seen as an under-determined source separation problem, where the sources are of different nature.</p><p>One popular statistical approach for source separation combines a local Gaussian model of the time-frequency signal coefficients with a variance model <ref type="bibr" target="#b1">[2]</ref>. In this framework, non-negative matrix factorization (NMF) techniques have been used to model the timefrequency-dependent signal variance <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>. Recently, discriminative approaches based on deep neural networks (DNNs) have also been investigated for speech enhancement, with the aim of estimating either clean spectrograms or time-frequency masks, given noisy spectrograms <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7</ref>]. As a representative example, a DNN is used in <ref type="bibr" target="#b5">[6]</ref> to map noisy speech log-power spectrograms into clean speech log-power spectrograms.</p><p>Even more recently, generative models based on deep learning, and in particular variational autoencoders (VAEs) <ref type="bibr" target="#b7">[8]</ref>, have been used for single-channel <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref> and multi-channel speech enhancement <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref>. These generative model-based approaches provide important advantages and justify the interest of semi-supervised methods for speech enhancement. Indeed, as shown in <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref>, fully-supervised methods such as <ref type="bibr" target="#b5">[6]</ref> may have issues for generalizing to unseen noise types. The method proposed in <ref type="bibr" target="#b9">[10]</ref> was shown to outperform both a semi-supervised NMF baseline and the fully-supervised deep learning approach <ref type="bibr" target="#b5">[6]</ref>.</p><p>In most cases, probabilistic models for source separation or speech enhancement rely on a Gaussianity assumption, which turns This work is supported by the ERC Advanced Grant VHIA #340113.</p><p>out to be restrictive for audio signals <ref type="bibr" target="#b12">[13]</ref>. As a result, heavytailed distributions have started receiving increasing attention in the audio processing community <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16]</ref>. In particular, α-stable distributions (cf. Section 3) are becoming popular heavy-tailed models for audio modeling due to their nice theoretical properties <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21]</ref>.</p><p>In this work, we investigate the combination of a deep learningbased generative speech model with a heavy-tailed α-stable noise model. The rationale for introducing a noise model based on heavytailed distributions as opposed to a structured NMF approach as in <ref type="bibr" target="#b9">[10]</ref> is to avoid relying on restricting assumptions regarding stationarity or temporal redundancy of the noisy environment, that may be violated in practice, leading to errors in the estimates. In addition, we let the noise model remain unsupervised in order to avoid the aforementioned generalization issues regarding the noisy recording environment. We develop a Monte Carlo expectation-maximization algorithm <ref type="bibr" target="#b21">[22]</ref> for performing maximum likelihood estimation at test time. Experiments performed under challenging conditions show that the proposed approach outperforms the competing approaches in terms of both perceptual quality and intelligibility.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">SPEECH MODEL</head><p>We work in the short-term Fourier transform (STFT) domain where B = {0, ..., F -1}×{0, ..., N -1} denotes the set of time-frequency bins. For (f, n) ∈ B, f denotes the frequency index and n the timeframe index. We use s f n , b f n , x f n ∈ C to denote the complex STFT coefficients of the speech, noise, and mixture signals, respectively.</p><p>As in <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref>, independently for all (f, n) ∈ B, we consider the following generative speech model involving a latent random vector hn ∈ R L , with L ≪ F : hn ∼ N (0, I);</p><p>(1)</p><formula xml:id="formula_0">s f n hn ∼ Nc(0, σ 2 s,f (hn)),<label>(2)</label></formula><p>where N (x; µ, Σ) denotes the multivariate Gaussian distribution for a real-valued random vector, I is the identity matrix of appropriate size, and Nc(x; µ, σ 2 ) denotes the univariate complex proper Gaussian distribution. As represented in Fig. <ref type="figure" target="#fig_1">1a</ref>,</p><formula xml:id="formula_1">{σ 2 s,f ∶ R L ↦ R+} F -1</formula><p>f =0 is a set of non-linear functions corresponding to a neural network which takes as input hn ∈ R L . This variance term can be understood as a model for the short-term power spectral density of speech <ref type="bibr" target="#b22">[23]</ref>. We denote by θs the parameters of this generative neural network.</p><p>An important contribution of VAEs <ref type="bibr" target="#b7">[8]</ref> is to provide an efficient way of learning the parameters of such a generative model. Let s = {sn ∈ C F } N -1 n=0 be a training dataset of clean-speech STFT time frames and h = {hn ∈ R L } N -1 n=0 the set of associated latent random  vectors. Taking ideas from variational inference, VAEs estimate the parameters θs by maximizing a lower bound of the log-likelihood ln p(s; θs) defined by:</p><formula xml:id="formula_2">L (θs, ψ)=E q(h s;ψ) [ln p (s h; θs)]-DKL (q (h s; ψ) ∥ p(h)) ,<label>(3)</label></formula><p>where q (h s; ψ) denotes an approximation of the intractable true posterior distribution p(h s; θs), and DKL(q ∥ p) = Eq[ln(q p)] is the Kullback-Leibler divergence. Independently for all the dimensions l ∈ {0, ..., L -1} and all the time frames n ∈ {0, ..., N -1}, q(h s; ψ) is defined by:</p><formula xml:id="formula_3">h l,n sn ∼ N μl sn ⊙2 , σ2 l sn ⊙2 ,<label>(4)</label></formula><p>where h l,n = (hn) l and (⋅) ⊙⋅ denotes element-wise exponentiation. As represented in Figure <ref type="figure" target="#fig_1">1b</ref>,</p><formula xml:id="formula_4">{μ l ∶ R F + ↦ R} L-1 l=0 and {σ 2 l ∶ R F + ↦ R+} L-1 l=0</formula><p>are non-linear functions corresponding to a neural network which takes as input the speech power spectrum at a given time frame. ψ denotes the parameters of this recognition network, which also have to be estimated by maximizing the variational lower bound defined in <ref type="bibr" target="#b2">(3)</ref>. Using (1), ( <ref type="formula" target="#formula_0">2</ref>) and ( <ref type="formula" target="#formula_3">4</ref>) we can develop this objective function as follows:</p><formula xml:id="formula_5">L (θs, ψ) c = - F -1 f =0 N -1 n=0 E q(hn sn;ψ) dIS s f n 2 ; σ 2 f (hn) + 1 2 L-1 l=0 N -1 n=0 ln σ2 l sn ⊙2 -μl sn ⊙2 2 -σ2 l sn ⊙2 ,<label>(5)</label></formula><p>where dIS(x; y) = x yln(x y) -1 is the Itakura-Saito divergence. Finally, using the so-called "reparametrization trick" <ref type="bibr" target="#b7">[8]</ref> to approximate the intractable expectation in (5), we obtain an objective function which is differentiable with respect to both θs and ψ and can be optimized using gradient-ascent-based algorithms. It is important to note that the only reason why the recognition network is introduced is to learn the parameters of the generative network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">NOISE AND MIXTURE MODELS</head><p>In the previous section we have seen how to learn the parameters of the generative model ( <ref type="formula">1</ref>)-( <ref type="formula" target="#formula_0">2</ref>). This model can then be used as a speech signal probabilistic prior for a variety of applications. In this paper we are interested in single-channel speech enhancement. We do not assume prior knowledge about the recording environment, so that the noise model remains unsupervised. Independently for all (f, n) ∈ B, the STFT coefficients of noise are modeled as complex circularly symmetric α-stable random variables <ref type="bibr" target="#b23">[24]</ref>:</p><formula xml:id="formula_6">b f n ∼ SαS(σ b,f ),<label>(6)</label></formula><p>where α ∈]0, 2] is the characteristic exponent and σ b,f ∈ R+ is the scale parameter. As proposed in <ref type="bibr" target="#b19">[20]</ref> for multichannel speech enhancement, this scale parameter is only frequency-dependent, it does not depend on time. For algorithmic purposes, the noise model ( <ref type="formula" target="#formula_6">6</ref>) can be conveniently rewritten in an equivalent scale mixture of Gaussians form <ref type="bibr" target="#b24">[25]</ref>, by making use of the product property of the symmetric α-stable distribution <ref type="bibr" target="#b23">[24]</ref>:</p><formula xml:id="formula_7">φ f n ∼ P α 2 S 2 cos(πα 4) 2 α ; (7) b f n φ f n ∼ Nc(0, φ f n σ 2 b,f ),<label>(8)</label></formula><p>where φ f n ∈ R+ is called the impulse variable. It locally modulates the variance of the conditional distribution of b f n given in <ref type="bibr" target="#b7">(8)</ref>. P α 2 S denotes a positive stable distribution of characteristic exponent α 2. It corresponds to a right-skewed heavy-tailed distribution defined for non-negative random variables <ref type="bibr" target="#b25">[26]</ref>. These impulse variables can be understood as carrying uncertainty about the stationary noise assumption made in the marginal model <ref type="bibr" target="#b5">(6)</ref>, where the scale parameter does not depend on the time-frame index.</p><p>The observed mixture signal is modeled as follows for all (f, n) ∈ B:</p><formula xml:id="formula_8">x f n = √ gns f n + b f n ,<label>(9)</label></formula><p>where gn ∈ R+ represents a frame-dependent but frequencyindependent gain. The importance of this parameter was experimentally shown in <ref type="bibr" target="#b9">[10]</ref>. We further consider the conditional independence of the speech and noise STFT coefficients so that:</p><formula xml:id="formula_9">x f n hn, φ f n ∼ Nc 0, gnσ 2 s,f (hn) + φ f n σ 2 b,f .<label>(10)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">INFERENCE</head><formula xml:id="formula_10">Let θu = g = {gn ∈ R+} N -1 n=0 , σ 2 b = {σ 2 b,f ∈ R+} F -1 f =0</formula><p>be the set of model parameters to be estimated. For maximum likelihood estimation, in this section we develop a Monte-Carlo expectation maximization (MCEM) algorithm <ref type="bibr" target="#b21">[22]</ref>, which iteratively applies the so-called E-and M-steps until convergence, which we detail below. Remember that the speech generative model parameters θs have been learned during a training phase (see Section 2). We denote by x = {x f n } (f,n)∈B the set of observed data while z = hn,</p><formula xml:id="formula_11">φ n = {φ f n } F -1 f =0 N -1</formula><p>n=0 is the set of latent variables. We will also use xn = {x f n } F -1 f =0 and zn = {hn, φ n } to respectively denote the set of observed and latent variables at a given time frame n.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Monte Carlo E-Step.</head><p>Let θ ⋆ u be the current (or the initial) value of the model parameters. At the E-step of a standard expectation-maximization algorithm, we would compute the following conditional expectation of the complete-data log-likelihood</p><formula xml:id="formula_12">Q(θu; θ ⋆ u ) = E p(z x;θs,θ ⋆ u ) [ln p(x, z; θs, θu)].</formula><p>However, this expectation cannot be here computed analytically. We therefore approximate Q(θu; θ ⋆ u ) using an empirical average:</p><formula xml:id="formula_13">Q(θu; θ ⋆ u ) c = - 1 R R r=1 (f,n)∈B ln gnσ 2 s,f h (r) n + φ (r) f n σ 2 b,f + x f n 2 gnσ 2 s,f h (r) n + φ (r) f n σ 2 b,f -1 ,<label>(11)</label></formula><p>where c = denotes equality up to an additive constant, and z</p><formula xml:id="formula_14">(r) n = h (r) n , φ (r) n = φ (r) f n F -1</formula><p>f =0 , r ∈ {1, ..., R}, is a sample drawn from the posterior p(zn xn; θs, θ ⋆ u ) using a Markov Chain Monte Carlo (MCMC) method. This approach forms the basis of the MCEM algorithm <ref type="bibr" target="#b21">[22]</ref>. Note that unlike the standard EM algorithm, it does not ensure an improvement in the likelihood at each iteration. Nevertheless, some convergence results in terms of stationary points of the likelihood can be obtained under suitable conditions <ref type="bibr" target="#b26">[27]</ref>.</p><p>In this work we use a (block) Gibbs sampling algorithm <ref type="bibr" target="#b27">[28]</ref>. From an initialization z (0) n , it consists in iteratively sampling from the so-called full conditionals. More precisely, at the m-th iteration of the algorithm and independently for all n ∈ {0, ..., N -1}, we first sample h</p><formula xml:id="formula_15">(m) n ∼ p hn xn, φ (m-1) n ; θs, θ ⋆ u . Then, we sample φ (m) f n ∼ p φ f n x f n , h (m) n ; θs, θ ⋆</formula><p>u independently for all f ∈ {0, ..., F -1}. Those two full conditionals are unfortunately analytically intractable, but we can use one iteration of the Metropolis-Hastings algorithm to sample from them. This approach corresponds to the Metropolis-within-Gibbs sampling algorithm <ref type="bibr">[28, p. 393</ref>]. One iteration of this method is detailed in Algorithm 1. The proposal distributions for hn and φ f n are respectively given in lines 2 and 6. The two acceptance probabilities required in lines 3 and 7 are computed as follows:</p><formula xml:id="formula_16">α (h) n = min ⎛ ⎜ ⎜ ⎜ ⎜ ⎝ 1, p hn F -1 ∏ f =0 p x f n hn, φ (m-1) f n ; θs, θ ⋆ u p h (m-1) n F -1 ∏ f =0 p x f n h (m-1) n , φ (m-1) f n ; θs, θ ⋆ u ⎞ ⎟ ⎟ ⎟ ⎟ ⎠ ;<label>(12)</label></formula><formula xml:id="formula_17">α (φ) n = min ⎛ ⎝ 1, p x f n h (m) n , φfn ; θs, θ ⋆ u p x f n h (m) n , φ (m-1) f n ; θs, θ ⋆ u ⎞ ⎠ .<label>(13)</label></formula><p>The two distributions involved in the computation of those acceptance probabilities are defined in ( <ref type="formula">1</ref>) and <ref type="bibr" target="#b9">(10)</ref>. Finally, we only keep the last R samples for computing Q(θu; θ ⋆ u ), i.e. we discard the samples drawn during a so-called burn-in period.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>M-</head><p>Step. At the M-step we want to minimize -Q(θu; θ ⋆ u ) with respect to θu. Let C(θu) denote the cost function associated with this non-convex optimization problem. Similar to <ref type="bibr" target="#b28">[29]</ref>, we adopt a majorization-minimization approach. Let us introduce the two following sets of auxiliary variables c = {c (r) f n ∈ R+} r,f,n and λ = {λ (r) k,f n ∈ R+} k,r,f,n . We can show using standard concave/convex inequalities (see e.g. <ref type="bibr" target="#b28">[29]</ref>) that C(θu) ≤ G(θu, c, λ), where:</p><formula xml:id="formula_18">G(θu, c, λ) = 1 R R r=1 (f,n)∈B ⎡ ⎢ ⎢ ⎢ ⎢ ⎣ ln c (r) f n + 1 c (r) f n gnσ 2 s,f h (r) n + φ (r) f n σ 2 b,f -c (r) f n + x f n 2 ⎛ ⎝ λ (r) 1,f n 2 gnσ 2 s,f h (r) n + λ (r) 2,f n 2 φ (r) f n σ 2 b,f ⎞ ⎠ ⎤ ⎥ ⎥ ⎥ ⎥ ⎦ . (<label>14</label></formula><formula xml:id="formula_19">)</formula><p>Moreover, this upper bound is tight, i.e. C(θu) = G(θu, c, λ), for c (r)</p><formula xml:id="formula_20">f n = gnσ 2 s,f h (r) n + φ (r) f n σ 2 b,f ;<label>(15)</label></formula><formula xml:id="formula_21">λ (r) 1,f n = gnσ 2 s,f h (r) n gnσ 2 s,f h (r) n + φ (r) f n σ 2 b,f -1 ;<label>(16)</label></formula><formula xml:id="formula_22">λ (r) 2,f n = φ (r) f n σ 2 b,f gnσ 2 s,f h (r) n + φ (r) f n σ 2 b,f -1 . (<label>17</label></formula><formula xml:id="formula_23">)</formula><p>Minimizing G(θu, c, λ) with respect to the model parameters θu is a convex optimization problem. By zeroing the partial derivatives of G with respect to each scalar in θu we obtain update rules that depend on the auxiliary variables. We then replace these auxiliary variables with the formulas given in ( <ref type="formula" target="#formula_20">15</ref>)-( <ref type="formula" target="#formula_22">17</ref>), which makes the upper Algorithm 1 m-th iteration of the Metropolis-within-Gibbs sampling algorithm 1: independently for all n ∈ {0, ..., N -1} do 2:</p><p>Sample hn ∼ N (h</p><formula xml:id="formula_24">(m-1) n , 2 I) 3:</formula><p>Compute acceptance probability α (h) n (equation ( <ref type="formula" target="#formula_16">12</ref>))</p><p>4:</p><p>Set h</p><formula xml:id="formula_25">(m) n = ⎧ ⎪ ⎪ ⎨ ⎪ ⎪ ⎩ hn if α (h) n &gt; u (h) n ∼ U([0, 1]) h (m-1) n otherwise 5:</formula><p>independently for all f ∈ {0, ..., F -1} do Compute acceptance probability α</p><formula xml:id="formula_26">(φ)</formula><p>n (equation ( <ref type="formula" target="#formula_17">13</ref>))</p><p>8:</p><p>Set φ (m)</p><formula xml:id="formula_27">f n = ⎧ ⎪ ⎪ ⎨ ⎪ ⎪ ⎩ φfn if α (φ) n &gt; u (φ) n ∼ U([0, 1]) φ (m-1) f n otherwise 9:</formula><p>end for 10: end for bound G tight. This procedure ensures that the cost C(θu) decreases <ref type="bibr" target="#b29">[30]</ref>. The resulting final updates are given as follows:</p><formula xml:id="formula_28">σ 2 b,f ← σ 2 b,f ⎡ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎣ N -1 ∑ n=0 x f n 2 R ∑ r=1 φ (r) f n v (r) x,f n -2 N -1 ∑ n=0 R ∑ r=1 φ (r) f n v (r) x,f n -1 ⎤ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎦ 1 2 ; (<label>18</label></formula><formula xml:id="formula_29">)</formula><formula xml:id="formula_30">gn ← gn ⎡ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎣ F -1 ∑ f =0 x f n 2 R ∑ r=1 σ 2 s,f h (r) n v (r) x,f n -2 F -1 ∑ f =0 R ∑ r=1 σ 2 s,f h (r) n v (r) x,f n -1 ⎤ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎦ 1 2 , (<label>19</label></formula><formula xml:id="formula_31">)</formula><p>where we introduced v (r)</p><formula xml:id="formula_32">x,f n = gnσ 2 s,f h (r) n + φ (r)</formula><p>f n σ 2 b,f in order to ease the notations. Non-negativity is ensured provided that the parameters are initialized with non-negative values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Speech reconstruction.</head><p>Once the unsupervised model parameters θu are estimated with the MCEM algorithm, we need to estimate the clean speech signal. For all (f, n) ∈ B, let sfn = √ gns f n be the scaled version of the speech STFT coefficients. We estimate these variables according to their posterior mean, given by:</p><formula xml:id="formula_33">ŝfn = E p(s f n x f n ;θu,θs) [s f n ] = E p(zn xn;θu,θs) E p(s f n zn,xn;θu,θs) [s f n ] = E p(zn xn;θu,θs) gnσ 2 s,f (hn) gnσ 2 s,f (hn) + φ f n σ 2 b,f x f n .<label>(20)</label></formula><p>As before, this expectation cannot be computed analytically so it is approximated using the Metropolis-within-Gibbs sampling algorithm detailed in Algorithm 1. This estimate corresponds to a probabilistic Wiener filtering averaged over all possible realizations of the latent variables according to their posterior distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">EXPERIMENTS</head><p>Reference method: The proposed approach is compared with the recent method <ref type="bibr" target="#b9">[10]</ref>. The speech signal in this paper is modeled in the exact same manner as in the current work, only the noise model differs. This latter is a Gaussian model with an NMF parametrization of the variance <ref type="bibr" target="#b2">[3]</ref>: 0 .8 5 0 .8 5 0 .8 6 0 .8 7 0 .8 7 0 .8 9 0 .8 9 0 .9 0 0 .9 0 0 .9 1 0 .9 1 Fig. <ref type="figure">2</ref>: Speech enhancement results obtained with the proposed method as a function of the characteristic exponent α in the noise model <ref type="bibr" target="#b5">(6)</ref>. α = 2.0 actually corresponds to α = 1.999. The value of the median is indicated within each boxplot. and H b are estimated from the noisy mixture signal. This method relies on an MCEM algorithm. By comparing the proposed method with <ref type="bibr" target="#b9">[10]</ref>, we fairly investigate which noise model leads to the best speech enhancement results. Note that the method proposed in <ref type="bibr" target="#b9">[10]</ref> was shown to outperform both a semi-supervised NMF baseline and the fully-supervised deep learning approach <ref type="bibr" target="#b5">[6]</ref>, the latter having difficulties for generalizing to unseen noise types. We do not include here the results obtained with these two other methods.</p><formula xml:id="formula_34">b f n ∼ Nc 0, (W b H b ) f,n , where W b ∈ R F ×K</formula><p>Database: The supervised speech model parameters in the proposed and the reference methods are learned from the training set of the TIMIT database <ref type="bibr" target="#b30">[31]</ref>. It contains almost 4 hours of 16-kHz speech signals, distributed over 462 speakers. For the evaluation of the speech enhancement algorithms, we mixed clean speech signals from the TIMIT test set and noise signals from the DEMAND database <ref type="bibr" target="#b31">[32]</ref>, corresponding to various noisy environments: domestic, nature, office, indoor public spaces, street and transportation. We created 168 mixtures at a 0 dB signal-to-noise ratio (one mixture per speaker in the TIMIT test set). Note that both speakers and sentences are different than in the training set.</p><p>Parameter setting: The STFT is computed using a 64-ms sine window (i.e. F = 513) with 75%-overlap. Based on <ref type="bibr" target="#b9">[10]</ref>, the latent dimension in the speech generative model ( <ref type="formula">1</ref>)-( <ref type="formula" target="#formula_0">2</ref>) is fixed to L = 64. In this reference method, the NMF rank of the noise model is fixed to K = 10. The NMF parameters are randomly initialized. For both the proposed and the reference method, the gain gn is initialized to one for all time frames n. For the proposed method, the noise scale parameter σ b,f is also initialized to one for all frequency bins. We run 200 iterations of the MCEM algorithm. At each Monte-Carlo E-Step, we run 40 iterations of the Metropolis-within-Gibbs algorithm and we discard the first 30 samples as the burn-in period. The parameter<ref type="foot" target="#foot_1">2</ref> in line 2 of Algorithm 1 is set to 0.01.</p><p>Neural network: The structure of the generative and recognition networks is the same as in <ref type="bibr" target="#b9">[10]</ref> and is represented in Fig. <ref type="figure" target="#fig_1">1</ref>. Hidden layers use hyperbolic tangent (tanh(⋅)) activation functions and output layers use identity activation functions (I d (⋅)). The output of these last layers is therefore real-valued, which is the reason why we consider logarithm of variances. For learning the parameters θs and φ, we use the Adam optimizer <ref type="bibr" target="#b32">[33]</ref> with a step size of 10 -3 , exponential decay rates for the first and second moment estimates of 0.9 and 0.999 respectively, and an epsilon of 10 -7 for preventing division by zero. 20% of the TIMIT training set is kept as a validation set, and early stopping with a patience of 10 epochs is used. Weights are initialized using the uniform initializer described in <ref type="bibr" target="#b33">[34]</ref>.</p><p>Results: The estimated speech signal quality is evaluated in terms of standard energy ratios expressed in decibels (dBs) <ref type="bibr" target="#b34">[35]</ref>: the signal-to-distortion (SDR), signal-to-interference (SIR) and signalto-artifact (SAR) ratios. We also consider the perceptual evaluation of speech quality (PESQ) <ref type="bibr" target="#b35">[36]</ref> measure (in [-0.5, 4.5]), and the short-time objective intelligibility (STOI) measure <ref type="bibr" target="#b36">[37]</ref> (in [0, 1]). For all measures, the higher the better. We first study the performance of the proposed method according to the choice of the characteristic exponent α in the noise model <ref type="bibr" target="#b5">(6)</ref>. Results presented in Fig. <ref type="figure">2</ref> indicate that according to the PESQ and STOI measures, the best performance is obtained for α = 2 (Gaussian case). <ref type="foot" target="#foot_0">1</ref> The SDR indicates that we should choose α = 1.8. Indeed, for greater values of α, the SIR starts to decrease (∼1dB difference between α = 1.8 and α = 2) while the SAR remains stable (results are not shown here due to space constraints). Therefore, in Fig. <ref type="figure" target="#fig_4">3</ref> we compare the results obtained with the reference <ref type="bibr" target="#b9">[10]</ref> and the proposed method using α = 1.8. With the proposed method, the estimated speech signal contains more interferences (SIR is lower) but less artifacts (SAR is higher). According to the SDR both methods are equivalent. But for intelligibility and perceptual quality, artifacts are actually more disturbing than interferences <ref type="bibr" target="#b37">[38]</ref>, which is the reason why the proposed method obtains better results in terms of both STOI and PESQ measures. For reproducibility, a Python implementation of our algorithm and audio examples are available online. 2   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">CONCLUSION</head><p>In this work, we proposed a speech enhancement method exploiting a speech model based on VAEs and a noise model based on alphastable distributions. At the expense of more interferences, the proposed α-stable noise model reduces the amount of artifacts in the estimated speech signal, compared to the use of a Gaussian NMFbased noise model as in <ref type="bibr" target="#b9">[10]</ref>. Overall, it is shown that the proposed approach improves the intelligibility and perceptual quality of the enhanced speech signal. Future works include extending the proposed approach to a multi-microphone setting using multivariate αstable distributions <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b19">20]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: Generative and recognition networks. Beside each layer is indicated its size and the activation function.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>+</head><label></label><figDesc>and H b ∈ R K×N + . It is also unsupervised in the sense that both W b</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 3 :</head><label>3</label><figDesc>Fig.3: Results obtained with the reference and the proposed methods (using α = 1.8). The median is indicated within each boxplot.</figDesc><graphic coords="5,54.43,219.28,243.88,83.01" type="bitmap" /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Actually α = 1.999 because for α =</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>the positive α-stable distribution in (6) is degenerate. 2 https://team.inria.fr/perception/icassp2019-asvae/</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Speech enhancement: theory and practice</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">C</forename><surname>Loizou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
			<publisher>CRC press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Probabilistic modeling paradigms for audio source separation</title>
		<author>
			<persName><forename type="first">E</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Jafari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Abdallah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Plumbley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Davies</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Audition: Principles, Algorithms and Systems</title>
		<editor>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</editor>
		<imprint>
			<publisher>IGI Global</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="162" to="185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Nonnegative matrix factorization with the Itakura-Saito divergence: With application to music analysis</title>
		<author>
			<persName><forename type="first">C</forename><surname>Févotte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Bertin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-L</forename><surname>Durrieu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="793" to="830" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Supervised and unsupervised speech enhancement using nonnegative matrix factorization</title>
		<author>
			<persName><forename type="first">N</forename><surname>Mohammadiha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Smaragdis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Leijon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Audio, Speech, Language Process</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2140" to="2151" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Supervised speech separation based on deep learning: An overview</title>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Audio, Speech, Language Process</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1702" to="1726" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A regression approach to speech enhancement based on deep neural networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-R</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Audio, Speech, Language Process</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="7" to="19" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Speech enhancement with LSTM recurrent neural networks and its application to noise-robust ASR</title>
		<author>
			<persName><forename type="first">F</forename><surname>Weninger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Erdogan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Le Roux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schuller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Latent Variable Analysis and Signal Separation (LVA/ICA)</title>
		<meeting>Int. Conf. Latent Variable Analysis and Signal Separation (LVA/ICA)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Auto-encoding variational Bayes</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Learning Representations (ICLR)</title>
		<meeting>Int. Conf. Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Statistical speech enhancement based on probabilistic integration of variational autoencoder and non-negative matrix factorization</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bando</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mimura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Itoyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yoshii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kawahara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Acoust., Speech, Signal Process</title>
		<meeting>IEEE Int. Conf. Acoust., Speech, Signal ess</meeting>
		<imprint>
			<publisher>ICASSP</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="716" to="720" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A variance modeling framework based on variational autoencoders for speech enhancement</title>
		<author>
			<persName><forename type="first">S</forename><surname>Leglaive</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Girin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Horaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Workshop Machine Learning Signal Process</title>
		<meeting>IEEE Int. Workshop Machine Learning Signal ess</meeting>
		<imprint>
			<publisher>MLSP)</publisher>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Bayesian multichannel speech enhancement with a deep speech prior</title>
		<author>
			<persName><forename type="first">K</forename><surname>Sekiguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bando</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yoshii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kawahara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC)</title>
		<meeting>Asia-Pacific Signal and Information essing Association Annual Summit and Conference (APSIPA ASC)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1233" to="1239" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Semi-supervised multichannel speech enhancement with variational autoencoders and non-negative matrix factorization</title>
		<author>
			<persName><forename type="first">S</forename><surname>Leglaive</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Girin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Horaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Acoust., Speech, Signal Process</title>
		<meeting>IEEE Int. Conf. Acoust., Speech, Signal ess</meeting>
		<imprint>
			<publisher>ICASSP)</publisher>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Generalized Wiener filtering with fractional power spectrograms</title>
		<author>
			<persName><forename type="first">A</forename><surname>Liutkus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Badeau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Acoust., Speech, Signal Process</title>
		<meeting>IEEE Int. Conf. Acoust., Speech, Signal ess</meeting>
		<imprint>
			<publisher>ICASSP</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="266" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Alpha-stable matrix factorization</title>
		<author>
			<persName><forename type="first">U</forename><surname>Şimşekli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Liutkus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">T</forename><surname>Cemgil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Lett</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2289" to="2293" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Cauchy nonnegative matrix factorization</title>
		<author>
			<persName><forename type="first">A</forename><surname>Liutkus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Fitzgerald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Badeau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Workshop Applicat. Signal Process. Audio Acoust. (WASPAA)</title>
		<meeting>IEEE Workshop Applicat. Signal ess. Audio Acoust. (WASPAA)<address><addrLine>New Paltz, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Student&apos;s t nonnegative matrix factorization and positive semidefinite tensor factorization for singlechannel audio source separation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Yoshii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Itoyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Goto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Acoust., Speech, Signal Process</title>
		<meeting>IEEE Int. Conf. Acoust., Speech, Signal ess</meeting>
		<imprint>
			<publisher>ICASSP</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="51" to="55" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">E</forename><surname>Kuruoglu</surname></persName>
		</author>
		<title level="m">Signal processing in alpha-stable noise environments: a least lp-norm approach</title>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
		<respStmt>
			<orgName>University of Cambridge</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Alpha-stable multichannel audio source separation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Leglaive</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Şimşekli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Liutkus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Badeau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Richard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Acoust., Speech, Signal Process</title>
		<meeting>IEEE Int. Conf. Acoust., Speech, Signal ess</meeting>
		<imprint>
			<publisher>ICASSP</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="576" to="580" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Explaining the parameterized Wiener filter with alpha-stable processes</title>
		<author>
			<persName><forename type="first">M</forename><surname>Fontaine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Liutkus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Girin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Badeau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Workshop Applicat. Signal Process. Audio Acoust</title>
		<meeting>IEEE Workshop Applicat. Signal ess. Audio Acoust<address><addrLine>WASPAA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Multichannel Audio Modeling with Elliptically Stable Tensor Decomposition</title>
		<author>
			<persName><forename type="first">M</forename><surname>Fontaine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F.-R</forename><surname>Stöter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Liutkus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Şimşekli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Serizel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Badeau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Latent Variable Analysis and Signal Separation (LVA/ICA)</title>
		<meeting>Int. Conf. Latent Variable Analysis and Signal Separation (LVA/ICA)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Alpha-stable low-rank plus residual decomposition for speech enhancement</title>
		<author>
			<persName><forename type="first">U</forename><surname>Şimşekli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Erdogan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Leglaive</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Liutkus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Badeau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Richard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Acoust., Speech, Signal Process</title>
		<meeting>IEEE Int. Conf. Acoust., Speech, Signal ess</meeting>
		<imprint>
			<publisher>ICASSP)</publisher>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A Monte Carlo implementation of the EM algorithm and the poor man&apos;s data augmentation algorithms</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">C</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Tanner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American statistical Association</title>
		<imprint>
			<biblScope unit="volume">85</biblScope>
			<biblScope unit="issue">411</biblScope>
			<biblScope unit="page" from="699" to="704" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Gaussian processes for underdetermined source separation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Liutkus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Badeau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Richard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Signal Process</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3155" to="3167" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Stable non-Gaussian random processes: stochastic models with infinite variance</title>
		<author>
			<persName><forename type="first">G</forename><surname>Samorodnitsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Taqqu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994">1994</date>
			<publisher>CRC press</publisher>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Scale mixtures of normal distributions</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">F</forename><surname>Andrews</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Mallows</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society. Series B (Methodological)</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="99" to="102" />
			<date type="published" when="1974">1974</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Lévy NMF for robust nonnegative source separation</title>
		<author>
			<persName><forename type="first">P</forename><surname>Magron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Badeau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Liutkus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Workshop Applicat. Signal Process. Audio Acoust. (WASPAA)</title>
		<meeting>IEEE Workshop Applicat. Signal ess. Audio Acoust. (WASPAA)<address><addrLine>New Paltz, NY, United States</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="259" to="263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Monte Carlo EM estimation for time series models involving counts</title>
		<author>
			<persName><forename type="first">K</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ledolter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="issue">429</biblScope>
			<biblScope unit="page" from="242" to="252" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">P</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Casella</surname></persName>
		</author>
		<title level="m">Monte Carlo Statistical Methods</title>
		<meeting><address><addrLine>Secaucus, NJ, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag New York, Inc</publisher>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Algorithms for nonnegative matrix factorization with the β-divergence</title>
		<author>
			<persName><forename type="first">C</forename><surname>Févotte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Idier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2421" to="2456" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A tutorial on MM algorithms</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Hunter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lange</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The American Statistician</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="30" to="37" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">TIMIT acoustic phonetic continuous speech corpus</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Garofolo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">F</forename><surname>Lamel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Fiscus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Pallett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">L</forename><surname>Dahlgren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Zue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Linguistic data consortium</title>
		<imprint>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">The Diverse Environments Multi-channel Acoustic Noise Database (DEMAND): A database of multichannel environmental noise recordings</title>
		<author>
			<persName><forename type="first">J</forename><surname>Thiemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Cong. on Acoust</title>
		<meeting>Int. Cong. on Acoust</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Learning Representations (ICLR)</title>
		<meeting>Int. Conf. Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Artif. Intelligence and Stat</title>
		<meeting>Int. Conf. Artif. Intelligence and Stat</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Performance measurement in blind audio source separation</title>
		<author>
			<persName><forename type="first">E</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gribonval</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Févotte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Audio, Speech, Language Process</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1462" to="1469" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Perceptual evaluation of speech quality (PESQ)-a new method for speech quality assessment of telephone networks and codecs</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Rix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Beerends</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Hollier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Hekstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Acoust., Speech, Signal Process</title>
		<meeting>IEEE Int. Conf. Acoust., Speech, Signal ess</meeting>
		<imprint>
			<publisher>ICASSP</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="749" to="752" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">An algorithm for intelligibility prediction of time-frequency weighted noisy speech</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Taal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Hendriks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Heusdens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Audio, Speech, Language Process</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2125" to="2136" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Performance based cost functions for end-to-end speech separation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Venkataramani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Higa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Smaragdis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC)</title>
		<meeting>Asia-Pacific Signal and Information essing Association Annual Summit and Conference (APSIPA ASC)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="350" to="355" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
