<TEI xmlns="http://www.tei-c.org/ns/1.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xml:space="preserve" xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Overview of LifeCLEF 2023: evaluation of AI models for the identification and prediction of birds, plants, snakes and fungi</title>
				<funder ref="#_jEzpTYa">
					<orgName type="full">European Commission</orgName>
				</funder>
				<funder ref="#_vTf27E6 #_x6XMpBB">
					<orgName type="full">European Union</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher />
				<availability status="unknown"><licence /></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Alexis</forename><surname>Joly</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Christophe</forename><surname>Botella</surname></persName>
							<idno type="ORCID">0000-0002-2161-9940</idno>
						</author>
						<author>
							<persName><forename type="first">Lukáš</forename><surname>Picek</surname></persName>
							<idno type="ORCID">0000-0002-5249-911X</idno>
						</author>
						<author>
							<persName><forename type="first">Stefan</forename><surname>Kahl</surname></persName>
							<idno type="ORCID">0000-0002-6041-9722</idno>
						</author>
						<author>
							<persName><forename type="first">Hervé</forename><surname>Goëau</surname></persName>
							<idno type="ORCID">0000-0002-2411-8877</idno>
						</author>
						<author>
							<persName><forename type="first">Benjamin</forename><surname>Deneu</surname></persName>
							<idno type="ORCID">0000-0003-3296-3795</idno>
						</author>
						<author>
							<persName><forename type="first">Diego</forename><surname>Marcos</surname></persName>
							<idno type="ORCID">0000-0001-5228-9238</idno>
						</author>
						<author>
							<persName><forename type="first">Joaquim</forename><surname>Estopinan</surname></persName>
							<idno type="ORCID">0000-0001-5607-4445</idno>
						</author>
						<author>
							<persName><forename type="first">César</forename><surname>Leblanc</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Théo</forename><surname>Larcher</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Cesar</forename><surname>Leblanc</surname></persName>
							<idno type="ORCID">0000-0001-5228-9238</idno>
						</author>
						<author>
							<persName><forename type="first">Rail</forename><surname>Chamidullin</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Milan</forename><surname>Šulc</surname></persName>
							<idno type="ORCID">0000-0003-1728-8939</idno>
						</author>
						<author>
							<persName><forename type="first">Marek</forename><surname>Hrúz</surname></persName>
							<idno type="ORCID">0000-0001-6800-9878</idno>
						</author>
						<author>
							<persName><forename type="first">Maximilien</forename><surname>Servajean</surname></persName>
							<idno type="ORCID">0000-0002-7851-9879</idno>
						</author>
						<author>
							<persName><forename type="first">Hervé</forename><surname>Glotin</surname></persName>
							<idno type="ORCID">0000-0002-9426-2583</idno>
						</author>
						<author>
							<persName><forename type="first">Robert</forename><surname>Planqué</surname></persName>
							<idno type="ORCID">0000-0001-7338-8518</idno>
						</author>
						<author>
							<persName><forename type="first">Willem-Pier</forename><surname>Vellinga</surname></persName>
							<idno type="ORCID">0000-0002-0489-5425</idno>
						</author>
						<author>
							<persName><forename type="first">Holger</forename><surname>Klinck</surname></persName>
							<idno type="ORCID">0000-0003-3886-5088</idno>
						</author>
						<author>
							<persName><forename type="first">Tom</forename><surname>Denton</surname></persName>
							<idno type="ORCID">0000-0003-1078-7268</idno>
						</author>
						<author>
							<persName><forename type="first">Ivan</forename><surname>Eggel</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Pierre</forename><surname>Bonnet</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Henning</forename><surname>Müller</surname></persName>
							<idno type="ORCID">0000-0001-6800-9878</idno>
						</author>
						<author>
							<affiliation>
								<orgName>1 Inria, LIRMM, Univ Montpellier, CNRS, </orgName>
								<address><addrLine>Montpellier, France Montpellier, Occitanie, France Marseille, France The Netherlands Sierre, Switzerland USA France Caltech, USA Czechia USA San Francisco, USA Prague, Czech Republic Chemnitz, Germany</addrLine></address>
							</affiliation>
						</author>
						<author>
							<affiliation>
								<orgName> 2 CIRAD, UMR AMAP, </orgName>
							</affiliation>
						</author>
						<author>
							<affiliation>
								<orgName> 3 Univ. Toulon, Aix Marseille Univ., CNRS, LIS, DYNI team, </orgName>
							</affiliation>
						</author>
						<author>
							<affiliation>
								<orgName> 4 Xeno-canto Foundation, </orgName>
							</affiliation>
						</author>
						<author>
							<affiliation>
								<orgName> 5 Informatics Insitute, HES-SO Valais , </orgName>
							</affiliation>
						</author>
						<author>
							<affiliation>
								<orgName> 6 K. Lisa Yang Center for Conservation Bioacoustics, Cornell Lab of Ornithology, Cornell University, </orgName>
							</affiliation>
						</author>
						<author>
							<affiliation>
								<orgName> 7 LIRMM, AMIS, Univ Paul Valéry Montpellier, Univ Montpellier, CNRS, </orgName>
							</affiliation>
						</author>
						<author>
							<affiliation>
								<orgName> 8 Department of Computing and Mathematical Sciences, </orgName>
							</affiliation>
						</author>
						<author>
							<affiliation>
								<orgName> 9 Department of Cybernetics, FAV, University of West Bohemia, </orgName>
							</affiliation>
						</author>
						<author>
							<affiliation>
								<orgName> 10 Department of Biological Sciences, Florida Gulf Coast University, </orgName>
							</affiliation>
						</author>
						<author>
							<affiliation>
								<orgName> 11 Google Research, </orgName>
							</affiliation>
						</author>
						<author>
							<affiliation>
								<orgName> 12 Second Foundation, </orgName>
							</affiliation>
						</author>
						<author>
							<affiliation>
								<orgName> 13 Chemnitz University of Technology,</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Overview of LifeCLEF 2023: evaluation of AI models for the identification and prediction of birds, plants, snakes and fungi</title>
					</analytic>
					<monogr>
						<imprint>
							<date />
						</imprint>
					</monogr>
					<idno type="MD5">47CA08E0976AC88BBAF772333015017C</idno>
					<idno type="DOI">10.1007/978-3-031-42448-9_27</idno>
					<note type="submission">Submitted on 4 Dec 2023</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-04-12T14:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid" />
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div><p>published or not. The documents may come from teaching and research institutions in France or abroad, or from public or private research centers.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div><head n="1">LifeCLEF Lab Overview</head><p>Accurately identifying organisms observed in the wild is an essential step in ecological studies. It forms the foundation for understanding species interactions, population dynamics, and ecological processes, allowing researchers to accurately assess biodiversity, track changes over time, and make informed management and conservation decisions. However, observing and identifying living organisms requires high levels of expertise. For instance, vascular plants alone account for more than 300,000 different species and the distinctions between them can be quite subtle. The worldwide shortage of trained taxonomists and curators capable of identifying organisms has come to be known as the taxonomic impediment. Since the Rio Conference of 1992, it has been recognized as one of the major obstacles to the global implementation of the Convention on Biological Diversity <ref type="foot" target="#foot_0">1</ref> . In 2004, Gaston and O'Neill <ref type="bibr" target="#b9">[10]</ref> discussed the potential of automated approaches for species identification. They suggested that if the scientific community were able to (i) produce large training datasets, (ii) precisely evaluate error rates, (iii) scale-up automated approaches, and (iv) detect novel species, then it would be possible to develop a generic automated species identification system that would open up new vistas for research in biology and related fields.</p><p>Since the publication of <ref type="bibr" target="#b9">[10]</ref>, automated species identification has been studied in many contexts <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b57">58]</ref>. This area continues to expand rapidly, particularly due to advances in deep learning <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b55">56]</ref>. Biodiversity monitoring through AI approaches is now recognized as a key solution to collect and analyze vast amounts of data from various sources, enabling us to gain a comprehensive understanding of species distribution, abundance, and ecosystem health <ref type="bibr" target="#b2">[3]</ref>. This information is essential for making informed conservation decisions and identifying areas in need of protection.</p><p>To measure progress in a sustainable and repeatable way, the LifeCLEF<ref type="foot" target="#foot_1">2</ref> virtual lab was created in 2014 as a continuation and extension of the plant identification task that had been run within the ImageCLEF lab <ref type="foot" target="#foot_2">3</ref> since 2011 <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17]</ref>. Since 2014, LifeCLEF has expanded the challenge by considering animals and fungi in addition to plants and including audio and video content in addition to images <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b27">28]</ref>. Nearly a thousand researchers and data scientists register yearly to LifeCLEF to download the data, subscribe to the mailing list, benefit from the shared evaluation tools, etc. The number of participants who finally crossed the finish line by submitting runs was respectively: 22 in 2014, 18 in 2015, 17 in 2016, 18 in 2017, 13 in 2018, 16 in 2019, 16 in 2020, 1, 022 in 2021 and 1146 in 2022. LifeCLEF 2023 consists of five challenges (BirdCLEF, SnakeCLEF, <software ContextAttributes="created">PlantCLEF</software>, <software ContextAttributes="created">FungiCLEF</software>, <software ContextAttributes="created">GeoLifeCLEF</software>) whose methodology and main outcomes are described in this paper. Table <ref type="table" target="#tab_0">1</ref> provides an overview of the data and tasks of the five challenges. The systems used to run the challenges (registration, submission, leaderboard, etc.) were the Kaggle platform for the BirdCLEF and <software ContextAttributes="created">GeoLifeCLEF</software> challenges, the Hugging Face competition platform for SnakeCLEF and Fung-iCLEF challenges, and the AICrowd platform for the <software ContextAttributes="created">PlantCLEF</software> challenge. Three of the challenges (<software ContextAttributes="created">GeoLifeCLEF</software>, SnakeCLEF, and <software ContextAttributes="created">FungiCLEF</software>) were organized jointly with FGVC 10, an annual workshop dedicated to Fine-Grained Visual Categorization organized in the context of the CVPR international conference on computer vision and pattern recognition. In total, 1, 226 people/teams participated to LifeCLEF 2023 edition by submitting runs to at least one of the five challenges (1, 189 only for the BirdCLEF challenge). Only some of them managed to get the results right, and 17 of them went all the way through the CLEF process by writing and submitting a working note describing their approach and results (for publication in CEUR-WS proceedings. In the following sections, we provide a synthesis of the methodology and main outcomes of each of the five challenges. More details can be found in the extended overview reports of each challenge and in the individual working notes of the participants (references provided below).</p></div>
<div><head n="2">BirdCLEF Challenge: Bird call identification in soundscapes</head><p>A detailed description of the challenge and a more complete discussion of the results can be found in the dedicated working note <ref type="bibr" target="#b29">[30]</ref>.</p></div>
<div><head n="2.1">Objective</head><p>Recognizing bird sounds in complex soundscapes is an important sampling tool that often helps reduce the limitations of point counts. In the future, archives of recorded soundscapes will become increasingly valuable as the habitats in which they were recorded will be lost. In the past few years, deep learning approaches have transformed the field of automated soundscape analysis. Yet, when training data is sparse, detection systems struggle to recognize bird species reliably. The goal of this competition was to establish training and test datasets that can serve as real-world applicable evaluation scenarios for endangered habitats and help the scientific community to advance their conservation efforts through automated bird sound recognition.</p></div>
<div><head n="2.2">Dataset</head><p>We built on the experience from previous editions and adjusted the overall task to encourage participants to focus on task-specific model designs. We selected training and test data to suit this demand. As in previous iterations, Xeno-canto was the primary source for training data, and expertly annotated soundscape recordings were used for testing. We focused on bird species which are usually underrepresented in large bird sound collections, but we also included common species so that participants were able to train good recognition systems. In search of suitable test data, we considered different data sources with varying complexity (call density, chorus, signal-to-noise ratio, man-made sounds, etc.) and quality (mono and stereo recordings). We also wanted to focus on very specific real-world use cases (e.g., conservation efforts in Africa) and framed the competition based on the demand of the particular use case.</p></div>
<div><head n="2.3">Evaluation Protocol</head><p>The challenge was held on Kaggle, and the evaluation mode resembled the test mode of previous iterations, i.e., hidden test data, code competition, etc. We used the class-wise mean average precision (cmAP) as a metric, which allowed organizers to assess system performance independent of fine-tuned confidence thresholds. Participants were asked to return a list of species for short audio segments extracted from labeled soundscape data. We used 5-second segments, which reflect a good compromise between typical signal length and sufficiently long context windows. Again, we kept the dataset size reasonably small (&lt;50 GB) and easy to process, and we also provided introductory code repositories and write-ups to lower the entry-level of the competition.</p></div>
<div><head n="2.4">Participants and Results</head><p>1,397 participants across 1,189 teams participated in the BirdCLEF 2023 challenge and submitted a total of 21,519 runs. In Figure <ref type="figure" target="#fig_0">1</ref> we report the performance achieved by the top 25 collected runs. The private leaderboard score is the primary metric and was revealed to participants after the submission deadline to avoid probing the hidden test data. Public leaderboard scores were visible to participants over the course of the entire challenge.</p><p>The baseline cmAP-score in this year's edition was 0.602 (public 0.717) with random confidence scores for all birds for all segments, and 1,165 teams managed to score above this threshold. The best submission achieved a cmAP-score of 0.7639 (public 0.8444) and the top 10 best performing systems were within only 1.5% difference in score. The vast majority of approaches were based on convolutional neural network ensembles and mostly differed in pre-and postprocessing and neural network backbone. Interestingly, few-shot learning techniques were vastly underrepresented despite the fact that some target species only had a handful of training samples. Some teams utilized embeddings of pretrained bird recognition models (such as <software>BirdNET</software> or <software ContextAttributes="created">Google Perch</software>, both were provided as supporting models) to train on high-level features, which somewhat mitigated the need for extensive training data. Due to the limited CPU runtime for submissions, participants focused on accelerating model inference and efficient architectures, with EfficientNet backbones being the most common choice. Interestingly, participants also experimented with <software ContextAttributes="created">ONNX</software> and openVINO to improve model inference speed.</p></div>
<div><head n="3">SnakeCLEF challenge: Snake Identification in Medically Important scenarios</head><p>A detailed description of the challenge and a more complete discussion of the results can be found in the dedicated overview paper <ref type="bibr" target="#b36">[37]</ref>.</p></div>
<div><head n="3.1">Motivation</head><p>Developing a robust system for identifying species of snakes from photographs is an important goal in biodiversity but also for human health. With over half a million victims of death &amp; disability from venomous snakebite annually, understanding the global distribution of the &gt;4,000 species of snakes and differentiating species from images (particularly images of low quality) will significantly improve epidemiology data and treatment outcomes. We have learned from previous editions that "machines" can accurately recognize (F C 1 ≈ 90% and Top1 Accuracy ≈ 90%) even in scenarios with long-tailed distributions and ≈ 1, 600 species. Thus, testing over real Medically Important Scenarios and specific countries (India and Central America) and integrating the medical importance of species is the next step that should provide a more reliable machine prediction.</p></div>
<div><head n="3.2">Objective</head><p>The main objective of this competition is to create a machine learning model that can accurately predict snake species for given observation data, i.e., images and location, and: (i) fits limits for memory footprint (max size of 1GB), (ii) minimizes the danger to human life, i.e., the venomous ←→ harmless confusion, (iii) generalize to all countries and geographic regions.</p></div>
<div><head n="3.3">Dataset</head><p>The dataset was constructed from observations submitted to the citizen science platforms -<software ContextAttributes="created">iNaturalist</software> and HerpMapper -and combined roughly 110,000 reals snake specimen observations with community-verified species labels. The number of species was extended up to ≈ 1, 800 snake species from around the world. Apart from image data, we have provided information about medical importance (i.e., how venomous the species is), and country-species relevance was provided for each species. We list the dataset statistics in Table <ref type="table" target="#tab_1">2</ref>. Geographical bias: There is a lack of data from remote parts of developing countries that tend to lack herpetological expertise and have high snake diversity, and snakebites are common (i.e., Asia, Africa, and Central/South America).</p></div>
<div><head n="3.4">Evaluation Protocol</head><p>To motivate research in recognition scenarios with uneven costs for different errors, such as mistaking a venomous snake for a harmless one, this year's challenge goes beyond the 0-1 loss common in classification. We make some assumptions to reduce the complexity of the evaluation. We consider that there exists a universal antivenom that is applicable to all venomous snake bites. Furthermore, such antivenom is not lethal or seriously harmful when applied to a healthy human. Hence, we will penalize the misclassification of a venomous species with a harmless one more than the other way around. Although this solution is not perfect, it is a first step into a more complex evaluation of snake bites. We specify two metrics (T 1 , T 2 ) reflecting these different scenarios.</p><formula xml:id="formula_0">T 1 = w 1 F 1 + w 2 C h )h + w 3 C h )v + w 4 C v )v + w 5 C v )h w 1 + w 2 + w 3 + w 4 + w 5 , (<label>1</label></formula><formula xml:id="formula_1">)</formula><p>where C is equal to 1-ratio of misclassified samples, confusing h-armless and v-enomous species. This metric has a lower bound of 0% and an upper bound of 100%. The lower bound is achieved when all species are misclassified, including misclassifications of harmless species as venomous and vice versa. On the other hand, if the F1-score reaches 100%, indicating the correct classification of all species, each C value must be zero, leading to an overall score of 100%.</p><formula xml:id="formula_2">T 2 = ∑ i L(y i , ŷi ), L(y, ŷ) =            0 if y = ŷ 1 if y ̸ = ŷ and p(y) = 0 and p(ŷ) = 0 2 if y ̸ = ŷ and p(y) = 0 and p(ŷ) = 1 2 if y ̸ = ŷ and p(y) = 1 and p(ŷ) = 1 5 if y ̸ = ŷ and p(y) = 1 and p(ŷ) = 0 , (2)</formula><p>where the function p returns 0 if y is a harmless species and 1 if it is venomous.</p></div>
<div><head n="3.5">Participants and Results</head><p>This year a total of 16 teams participated in the SnakeCLEF. However, just five teams submitted their models for private evaluation together with the working notes. Details of the best methods and systems used are synthesized in the competition overview paper <ref type="bibr" target="#b0">[1]</ref>.</p><p>In Figure <ref type="figure">2</ref> and Figure <ref type="figure" target="#fig_2">3</ref> we report the public and private leaderboard performance achieved by individual teams using: (i) Track 1 Metric (T 1 ), (ii) Track 2 Metric (T 2 ) and (iii) the macro F 1 score. The main outcomes we can derive from the achieved results are as follows:</p><p>NLP model encoded metadata might be the next big thing. Same as in previous years, most of the teams used the provided metadata and showed that by doing so the competition metric improves. CLIP <ref type="bibr" target="#b43">[44]</ref> -a strong multi-modal descriptor, was used for the first time in this competition to encode the metadata. This trend may lead to the utilization of bigger NLP models.</p><p>Transformers for the win. But do not rule out the CNNs yet. On the vision part, convolutional models (ResNet <ref type="bibr" target="#b17">[18]</ref>, EfficientNet <ref type="bibr" target="#b47">[48]</ref>, ConvNext <ref type="bibr" target="#b56">[57]</ref>) and Transformer models (<software ContextAttributes="created">MetaFormer</software> <ref type="bibr" target="#b7">[8]</ref>, Swin <ref type="bibr" target="#b32">[33]</ref>, VOLO <ref type="bibr" target="#b58">[59]</ref>) were used to extract the visual features. When teams compared the architectures side-by-side, most of the times the Transformer architecture performed better. However, the winning team used ConvNextv2. Due to the lack of a fair and exhaustive ablation study, it is not clear how a Transformer model would fare.</p><p>Task-tailored losses and self-supervision are the key to learning. Traditionally, Seesaw loss <ref type="bibr" target="#b54">[55]</ref> and SimCLR <ref type="bibr" target="#b6">[7]</ref> were used to cope with the long-tailed data. Some teams introduced a weighted version of the loss functions tackling the different penalization for different errors. Multi-Instance Learning <ref type="bibr" target="#b18">[19]</ref> was applied to make use of more images per observation.</p><p>Medically important scenarios might be on to something. The final results on the private dataset show an interesting behavior of the models. The best team (named word2vector) achieved macro F 1 score of 53.58% with the competition score of 91.31%. The runner-up (BBracke) actually achieved a much better F 1 score of 61.39% but had a lower competition score of 90.19%. We hypothesize that this was possible due to the post-processing step of team word2vector. When they observed that the top-5 results contained a venomous species, the observation was classified as such.   </p></div>
<div><head n="4">FungiCLEF Challenge: Fungi Recognition Beyond 0-1 Cost</head><p>A detailed description of the challenge and a more complete discussion of the results can be found in the dedicated working note <ref type="bibr" target="#b41">[42]</ref>.</p></div>
<div><head n="4.1">Objective</head><p>Automatic recognition of species at scale, such as in popular citizen-science projects <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b38">39]</ref>, requires efficient prediction on limited resources. In practice, species identification typically depends not solely on the visual observation of the specimen but also on other information available to the observer, e.g., habitat, substrate, location, and time. Thanks to rich metadata, precise annotations, and baselines available to all competitors, the challenge aims at providing a major benchmark for combining visual observations with other observed information. Additionally, the 2023 competition considers decision processes for different usage scenarios, which go beyond the commonly assumed 0/1 cost function -e.g., cost for misclassification of edible and poisonous mushrooms is an important practical aspect to be evaluated.</p></div>
<div><head n="4.2">Dataset</head><p>The challenge builds upon the Danish Fungi 2020 dataset <ref type="bibr" target="#b39">[40]</ref>, which comes from a citizen science project, the Atlas of Danish Fungi, where all samples went through an expert validation process, guaranteeing a high quality of labels. Rich metadata (Habitat, Substrate, Timestamp, GPS, EXIF etc.) are provided for most samples. The training set will be the union of the training and public-test set (without out-of-scope samples) from the 2022 challenge <ref type="bibr" target="#b40">[41]</ref> -i.e., 295,938 training images belonging to 1,604 species observed mostly in Denmark.</p><p>The validation and test sets include all the expert validates observations with species labels collected in 2021 and 2022. respectively. Both the validation and test set cover roughly 3,000 fungi species and include a high number of observations with "unknown" species. The test set was further split (50/50 ratio) to provide different data for a public and private evaluation. We list the dataset statistics in Table <ref type="table" target="#tab_3">3</ref>. </p></div>
<div><head n="4.3">Evaluation Protocol</head><p>Given the set of real fungi species observations and corresponding metadata, the goal of the task is to create a classification model that predicts a species for each given observation. The classification model must fit limits for memory footprint (max size of 1GB) and should have to consider and minimize the danger to human life, i.e., the confusion between poisonous and edible species. <software>FungiCLEF</software> 2023 considered five different decision scenarios, minimizing the empirical loss L = ∑ i W (y i , q(x i )) for decisions q(x) over observations x and true labels y, given a cost function W (y, q(x)). Five cost functions were given for the following scenarios:</p><p>-Track 1: Standard classification with "unknown" category; -Track 2: Cost for confusing edible species for poisonous and vice versa; -Track 3: An application user-focused loss composed of both the classification error (e.g., accuracy) and the poisonous ←→ edible confusion; -Track 4: Cost for missing "unknown" species is higher; misclassifying for "unknown" is cheaper than confusing species;</p><p>Baseline procedures of how metadata can help the classification, pre-trained baseline classifiers, and code submission example were provided to all participants as part of the task description.</p></div>
<div><head n="4.4">Participants and Results</head><p>Twelve teams participated in the <software ContextAttributes="created">FungiCLEF</software> 2023 challenge; four provided their models for a private evaluation, and three submitted working notes. Details of the best methods and systems used are synthesized in the overview working note paper of the task <ref type="bibr" target="#b37">[38]</ref> and further developed in the individual working notes of participants (see references in <ref type="bibr" target="#b37">[38]</ref>). In Figure <ref type="figure" target="#fig_4">4</ref> and Figure <ref type="figure" target="#fig_5">5</ref>, we report the performance achieved by the participants. Interestingly, none of the teams that submitted working notes optimized decision-making for each of the five tasks.</p><p>The best-performing team -meng18 -combined visual information with metadata using <software ContextAttributes="created">MetaFormer</software> <ref type="bibr" target="#b7">[8]</ref>, tackled class imbalance with the Seesaw loss <ref type="bibr" target="#b54">[55]</ref>, proposed an entropy-guided recognition of unknown species, and introduced an additional poisonous-classification loss.     </p></div>
<div><head n="5">PlantCLEF Challenge: Identify the World's Flora</head><p>A detailed description of the challenge and a more complete discussion of the results can be found in the dedicated working note <ref type="bibr" target="#b13">[14]</ref>.</p></div>
<div><head n="5.1">Objective</head><p>Advancements in deep learning and the growing abundance of field photographs have significantly enhanced the automated identification of plants. A notable milestone was achieved during LifeCLEF 2018, where a top-1 classification accuracy of up to 90% was attained for over 10k species. This demonstrated that automated systems had made remarkable progress and are approaching human expertise in this domain <ref type="bibr" target="#b20">[21]</ref>. However, it is crucial to recognize that such impressive performance levels are still a long way off from encompassing the vastness of the world's flora. Presently, science has identified approximately 391,000 vascular plant species, with new discoveries and descriptions being made each year. The significance of this plant diversity extends beyond the mere existence of species; it plays a pivotal role in ecosystem functioning and the advancement of human civilization. Regrettably, the majority of these species remain poorly understood, and there is an acute scarcity of training images available for the vast majority of them <ref type="bibr" target="#b42">[43]</ref>.</p><p>The objective of the <software ContextAttributes="created">PlantCLEF</software> challenges in 2022 and 2023 was to advance the field of plant identification on a global scale. To achieve this, a training dataset was curated, encompassing a remarkable 80,000 species and comprising 4 million images. This expansive dataset was made accessible to the community through a challenge hosted on the AIcrowd platform <ref type="foot" target="#foot_3">4</ref> , providing an opportunity for researchers and enthusiasts to contribute to the development of plant recognition.</p></div>
<div><head n="5.2">Dataset</head><p>The training set consists of two distinct subsets. The first subset, referred to as the trusted training dataset, is derived from the GBIF (Global Biodiversity Information Facility) portal 5 , which is the largest biodiversity data portal globally. This subset comprises over 2.9 million images encompassing 80,000 plant species. These images have been shared and collected primarily through GBIF, with some contributions from the Encyclopedia of Life 6 (EOL). The sources of these images include academic institutions such as museums, universities, and national institutions, as well as collaborative platforms like <software>iNaturalist</software> and <software ContextAttributes="created">Pl@ntNet</software>, implying a fairly high certainty of determination quality (collaborative platforms only share their highest quality data qualified as "research graded"). To maintain a manageable training set size and address class imbalance, the number of images per species was restricted to approximately 100. Additionally, the selection process favored specific views that are conducive to plant identification, such as close-ups of flowers, fruits, leaves, trunks, and other relevant features. This approach ensures that the training dataset comprises informative and relevant images for accurate plant recognition. In contrast, a second "web" training dataset comprises images obtained from commercial search engines like <software ContextAttributes="created">Google</software> and <software ContextAttributes="created">Bing</software>. This dataset comes with its own set of challenges. The raw downloaded data from these search engines contains a notable number of species identification errors and a substantial presence of (near)-duplicates and images that are not well-suited for plant identification purposes. For instance, the dataset includes images of herbarium sheets, landscapes, microscopic views, and various other non-relevant visuals. Moreover, the web dataset contains a significant amount of unrelated images, such as portraits of botanists, maps, graphs, images from other kingdoms of living organisms, and even manufactured objects. To address these issues, a semi-automatic filtering approach was adopted. This process involved multiple iterations of training Convolutional Neural Networks (CNNs), conducting inference, and human labeling. Through this iterative process, the raw data was as best as possible cleaned up, leading to a drastic reduction in the number of irrelevant pictures. Furthermore, the image quality was improved by prioritizing close-ups of flowers, fruits, leaves, trunks, and other relevant plant features. As a result of this filtering process, the web dataset consists of approximately 1.1 million images, covering approximately 57k plant species.</p><p>Participants were allowed to use complementary training data (e.g. for pretraining purposes) but at the condition that (i) the experiment is entirely reproducible, i.e. that the used external resource is clearly referenced and accessible to any other research group in the world, (ii) the use of external training data or not is mentioned for each run, and (iii) the additional resource does not contain any of the test observations. External training data was allowed but participants had to provide at least one submission that used only the provided data.</p><p>The test set used in the <software>PlantCLEF</software> challenge was constructed using multiimage plant observations obtained from the <software ContextAttributes="created">Pl@ntNet</software> platform during the year 2021. These observations had not been shared through GBIF, meaning they were not present in the training set. Only observations that received a very high confidence score in the <software ContextAttributes="created">Pl@ntNet</software> collaborative review process were selected for the challenge to ensure the highest possible quality of determination. This process involves people with a wide range of skills (from beginners to world-leading experts), but these have different weights in the decision algorithms. Finally, the test set contains about 27k plant observations related to about 55k images (a plant can be associated with several images) covering about 7.3k species.</p></div>
<div><head n="5.3">Evaluation Protocol</head><p>The evaluation of the task in the <software>PlantCLEF</software> challenge primarily relies on the Mean Reciprocal Rank (MRR) metric. MRR is a statistical measure used to assess processes that generate a list of potential responses to a set of queries, ordered by the probability of correctness. It quantifies the performance of a system by considering the reciprocal rank of the first correct answer for each query. The reciprocal rank of a query response is calculated as the multiplicative inverse of the rank of the first correct answer. In other words, if the correct answer is ranked first, the reciprocal rank is 1. If it is ranked second, the reciprocal rank is 1/2, and so on. To determine the MRR for the entire test set, the reciprocal ranks for all the queries are averaged together:</p><formula xml:id="formula_3">M RR = 1 Q Q ∑ q=1 1 rank q (<label>3</label></formula><formula xml:id="formula_4">)</formula><p>where Q is the total number of query occurrences (plant observations) in the test set. However, the macro-average version of the MRR (average MRR per species in the test set -MA-MRR) was used because of the long tail of the data distribution to re-balance the results between under-and over-represented species in the test set.</p></div>
<div><head n="5.4">Participants and Results</head><p>Although over a hundred participants signed up for the challenge, in the end only 3 participants from 3 countries participated to the <software ContextAttributes="created">PlantCLEF</software> 2023 challenge and submitted a total of 22 runs. Details of the best methods and systems used are synthesized in the overview working notes paper of the task <ref type="bibr" target="#b13">[14]</ref>. In Figure <ref type="figure" target="#fig_6">6</ref> we report the performance achieved by the different runs of the participants.</p><p>The main outcomes we can derive from that results are the following: -The most impressive outcomes were achieved by vision transformer-based approaches, particularly the vision-centric foundation model EVA <ref type="bibr" target="#b8">[9]</ref>, that was the state-of-the-art position during the challenge in the first quarter of 2023. While CNN-based approaches also produced respectable results, with a maximum MA-MRR of 0.618 (Neuon AI Run 9), they still fell notably short of the highest score attained by an EVA approach. The best EVA approach, Mingle Xu Run 8, achieved a remarkable MA-MRR of 0.674. -Utilizing the complete <software ContextAttributes="created">PlantCLEF</software> training dataset, comprising both the trusted and web datasets, proved advantageous, despite the added training time and the residual noise inherent in the web dataset. The inclusion of the web training dataset resulted in a noticeable improvement, with the MA-MRR reaching 0.674, compared to a maximum of 0.65 without it. -The reduction of the training set by removing the classes with the fewest images (Mingle Xu Run 1-4-2-6 vs Run 5) implies a significant drop in performance. This demonstrates that there might not always be a direct connection between the training data and the test data, emphasizing the importance of considering all classes, including those linked to uncommon species, when addressing the task of monitoring plant biodiversity </p></div>
<div><head n="6">GeoLifeCLEF Challenge: Species composition prediction with high spatial resolution at continental scale using remote sensing</head><p>A detailed description of the challenge and a more complete discussion of the results can be found in the dedicated working note <ref type="bibr" target="#b4">[5]</ref>. A graphical abstract of the challenge is provided in Figure <ref type="figure">7</ref>.</p></div>
<div><head n="6.1">Objective</head><p>Predicting which species are present in a given area through Species Distribution Models (SDM) is a central problem in ecology and a crucial issue for biodiversity conservation. Such predictions are a fundamental element of many decisionmaking processes, whether for land use planning, the definition of protected areas, or the implementation of more ecological agricultural practices. Classical SDMs are well-established but have the drawback of covering only a limited number of species at spatial resolutions often coarse in the order of kilometers, or hundreds of meters at best. In addition, while the use of the massive presenceonly data arising from large citizen science platforms has grown, the SDM built from such data are affected by many sampling biases, as, for instance, species detection bias or species set size bias. Developing scalable methods suited to account and correct for these biases is a necessary step to update regularly species distributions maps by capitalizing on the massive flow of citizen science data.</p><p>The objective of <software ContextAttributes="created">GeoLifeCLEF</software> is to evaluate models with orders of magnitude hitherto unseen, whether in terms of the number of species covered (thousands), Fig. <ref type="figure">7</ref>: <software ContextAttributes="created">GeoLifeCLEF</software> 2023 graphical abstract spatial resolution (on the order of 10 meters), or the number of occurrences used as training data (several million). These models have the potential to greatly improve biodiversity management processes, especially at the local level (e.g. municipalities), where the need for spatial and taxonomic precision is greatest.</p></div>
<div><head n="6.2">Training Dataset</head><p>A brand new dataset was built for the 2023 edition of <software ContextAttributes="created">GeoLifeCLEF</software> in the framework of a large-scale European project on biodiversity monitoring (MAMBO, Horizon EU program). It contains about 5 million plant species presence-only records (single positive labels, hereafter PO) covering 10 thousand species extracted from thirteen selected datasets of the Global Biodiversity Information Facility (GBIF) and covers the whole EU territory (38 countries including E.U. members). We also provided the participants with a validation set of 5 thousand standardized presence-absence (hereafter PA) surveys of small spatial plots (multi-label) to help calibrate the models, and specifically to correct for sampling biases. For the explanatory variables (to be used as inputs of the models), the dataset contains both high-resolution remote sensing data (10m resolution Sentinel-2 satellite images and Landsat multi-spectral time-series at each data location, along with elevation) and coarser resolution environmental raster data (land cover, human footprint, bioclimatic and soil variables). The geocoordinates and date of the species observations are also provided and can also be used as one modality. Participants are free to use one, several, or all available modalities in their models. The detailed description of the <software ContextAttributes="created">GeoLifeCLEF</software> 2023 dataset is provided in <ref type="bibr" target="#b3">[4]</ref>.</p></div>
<div><head n="6.3">Evaluation Protocol</head><p>The challenge is as a multi-label (/set) classification task. Given a test set of locations (i.e., geo-coordinates) and corresponding remote sensing data and environmental covariates, the goal of the task is to return for each location the set of plant species truly present in a small spatial plot (of area 10-400m²) as reported in a standardized presence-absence survey carried by botanical experts (same type as the validation PA data). This test set includes 22,404 PA surveys. Thus, one of the major difficulties of the challenge is to predict the presence or absence of all species from a dataset mostly made of PO data (i.e. the 5 million GIF records). As noted earlier, to enable participants to calibrate their models, and specifically to correct for sampling biases, we also provided a validation set of only 5 thousand PA surveys, spatially separated from the test set. Indeed, following the recommendations of <ref type="bibr" target="#b45">[46]</ref>, the split of the validation and test set was done using a spatial blocking strategy that enables a more robust estimation of the model's performance (based on a 50x50km spatial grid). Moreover, we have excluded the PO records located less than 500m from the test plots to avoid the risk that some may have originated from these plots. The detailed protocol is described in <ref type="bibr" target="#b3">[4]</ref>. The evaluation metric is the F1 score. It measures the precision and recall score for each test plot x and computes their harmonic mean:</p><formula xml:id="formula_5">F 1(x) = 2 1 P recision(x) + 1 Recall(x)</formula><p>It is equivalent to the Sørensen-Dice coefficient defined as the size of the intersection between the predicted and true set of species, divided by the mean of their respective size. The final global metric is calculated by averaging the F1 score of all plots in the test set.</p></div>
<div><head n="6.4">Participants and Results</head><p>Six participants from four countries participated in the <software ContextAttributes="created">GeoLifeCLEF</software> 2023 challenge and submitted a total of 121 entries (i.e. /textitruns). Details of the best methods and systems used are synthesized in the overview paper of the task <ref type="bibr" target="#b4">[5]</ref> and the winning team methodology is explained in details in their working note ( <ref type="bibr" target="#b50">[51]</ref>). In Table <ref type="table">4</ref> we report the performance achieved by the best performing methods of the participants as well as the baseline methods developed by the organizers. Hereafter, we briefly describe those different methods:</p></div>
<div><head>Participant's methods</head><p>-KDDI research: This team trained various convolutional neural networks, all based on the ResNet backbone (ResNet34 and 50). One of the CNN was trained solely on the 19 bioclimatic rasters while others were multi-modal networks with a late fusion layer to merge the different modalities used (see Table <ref type="table">4</ref>). The best performing run was an ensemble of the best models based on a simple average of their output. The best models were trained in three steps, firstly on the PA plots with a binary cross-entropy loss, then fine-tuned on the PO records with a cross-entropy loss, and finally fine-tuned again on the PA with the binary-cross-entropy loss. This team carried an ablation showing the importance of these three steps. -Jiexun Xu: This researcher focused on the tabular environmental data only, i.e he didn't use the spatial structure of the environmental co-variates nor the remotes sensing images and times series. The model used is <software ContextAttributes="created">XGBoost</software> and it was trained on the PA plots. He also added the one-hot encoded species presences in GBIF in a 1km radius of these plots as input variables. -Lucas Morin: This researcher optimized a K-Nearest Neighbor predictor using only the spatial coordinates and the PA plots. -QuantMetry: This team trained various models on the PA data, and their best scoring model was a ResNet50 using only the Sentinel2 satellite images (RGB+NIR) as input. The model was pre-trained on the satellite images in a prior work ( <ref type="bibr" target="#b59">[60]</ref>) and fine-tuned to the PA data in the challenge. -Nina van Tiel: This researcher used a small CNN, with two convolutional layers and two fully connected layers on the RGB images, along the bioclimatic, soil and land-cover rasters, trained on the PA plots. -Ousmane Youme: This researcher focused solely on the Landsat time series data at the location of the PA plots. He used a Conv1D neural network model with a binary-cross entropy loss. A common probability threshold was used to convert the predicted species-wise presence probabilities into a set of predicted species.</p><p>Organizer's baselines -MAXENT: the MAXENT method is a modeling approach widely used in ecology to predict the distribution of a given species based on tabular environmental variables. It is not adapted to handle complex input data such as the Sentinel images or Landsat time series. The model creates a predefined set of non-linear transformations of the input environmental variables consistent with the theoretical ecological response of species to environmental gradients (e.g. quadratic and threshold responses, see <ref type="bibr" target="#b35">[36]</ref>). The statistical model is equivalent to a Poisson regression modeling the count of a species per location ( <ref type="bibr" target="#b44">[45]</ref>). We fitted one Maxent model per species present in the PA plots. The species count was set to one when present or zero otherwise. The environmental input variables included were the climate, soil, land cover and human footprint variables, but only a subset of these variables were included for species with a smal number of observations. One random subset of the PA plots was used to train all species models while the other was used to assess the predictive accuracy of each species model. We thus determined that it was optimal to keep only the 391 most trustable species, in terms of validation score, for the final prediction, the left-out species being always predicted absent. A run including on all species models was also submitted, achieving a much lower performance due to an over-prediction of rare species in extrapolation (see <ref type="bibr" target="#b4">[5]</ref> for details). -Environmental Random Forest: Random forests are also widely used in ecology to predict the distribution of species based on a set of environmental variables. As for Maxent, the Env. Random Forest models were trained only on the environmental tabular variables at the location of the PA plots. One Random Forest was trained per species in the PA plots and its hyperparameters were optimized through a cross-validation grid search. -Spatial Random Forest: Contrary to the two previous baseline, this Random Forest were trained solely on the spatial coordinates of the PA plots, regardless the environmental variables. -Species co-occurrence: Conditionally to the presence of each species, we computed the proportion of presences of all other species among the PA plots. Then, for each test location, we combined the species probabilities conditionally to the species observed in the PO data in a 1km-radius into a predicted species set through a weighted average. Therefore, this method doesn't use any input variable except the spatial coordinates. -Constant predictor: this baseline always predict the same set of species, i.e. the ones that are the K most frequent in the PA plots, where K maximizes the F1-micro score over these PA plots (K = 25 species).</p></div>
<div><head>Outcomes</head><p>The main outcomes we can derive from the challenge are the following:</p><p>-The problem remains very difficult and the best model only achieves a F1score of 0.27 -The MAXENT method remains a strong baseline when considering only the tabular environmental data, regardless the spatial structure of the environment or the more complex data such as remote sensing images. -Training a model on the PO data (with a cross-entropy loss) and fine-tuning it on PA (with a binary cross-entropy loss) resulted in a considerable performance gain. This shows the wealth of information that can be mobilised in the PO data, provided that the learning strategy avoids sampling biases. -The best model was based on a Convolutional Neural Network which confirms that this kind of model is relevant for the task. It allows capturing complex patterns in the input data while allowing elaborated training strategy such as transfer learning. -Making use exclusively of PO data remains a major hurdle, and all the methods that did so had a very low performance. Most participants used only the PA validation data in the training of their models, and the best method succeeded by combining both. Much work lies ahead to extract the information from PO without complementary standardized data, if that is even possible.</p><p>The main outcome of this collaborative evaluation is a new snapshot of the performance of state-of-the-art computer vision, bioacoustic, and machine learning techniques toward building real-world biodiversity monitoring systems. Overall, this study shows that the field continues to progress year after year, and that, although the challenges that are most closely related to common tasks, such as multi-class classification based on images, are able to profit from the most recent advances in computer vision, certain problems are still wide open, such as the prediction of species as a function of location (as part of the <software ContextAttributes="created">GeoLifeCLEF</software> challenge). In terms of the methods used, the results show that convolutional neural networks are still a very powerful method for image and sound processing. In 4 of the 5 challenges, the best results were obtained using CNNs. Only the <software ContextAttributes="created">PlantCLEF</software> challenge obtained much better results (for the identification of plants from images) with the use of foundation vision transformer models such as EVA <ref type="bibr" target="#b8">[9]</ref>. The best submission to <software ContextAttributes="created">FungiCLEF</software> was based on <software ContextAttributes="created">MetaFormer</software> <ref type="bibr" target="#b7">[8]</ref>, utilizing both a convolutional backbone and a transformer to fuse visual and meta information. Complementary to vision-based models, NLP models were also used successfully, in particular hybrid models such as CLIP <ref type="bibr" target="#b43">[44]</ref> that efficiently learn visual concepts from natural language supervision. We believe that this principle of combining different modalities in the training of deep learning models will be a key to future progress in AI for biodiversity.</p></div><figure xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: BirdCLEF 2023 results of the top 25 teams.</figDesc><graphic coords="6,134.77,115.84,345.83,163.06" type="bitmap" /></figure>
<figure xml:id="fig_1"><head>B</head><label /><figDesc>B r a c k e w o r d 2 v e c t o r B A O f a n t i n g h e a d a c h e I s H e a d a c h e M R B a g h e r i f a r t r a n s f r o m s a m u e l i s e t i g e u 1 / Macro F1 [%]</figDesc></figure>
<figure xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>Fig. 3: Private Leaderboard -SnakeCLEF 2023 competition -5 teams.</figDesc></figure>
<figure xml:id="fig_4"><head>Fig. 4 :</head><label>4</label><figDesc>Fig. 4: Public Leaderboard -FungiCLEF 2023 competition -Top10 teams.</figDesc></figure>
<figure xml:id="fig_5"><head>Fig. 5 :</head><label>5</label><figDesc>Fig. 5: Private Leaderboard -FungiCLEF 2023 competition -4 teams.</figDesc></figure>
<figure xml:id="fig_6"><head>Fig. 6 :</head><label>6</label><figDesc>Fig. 6: PlantCLEF 2023 results</figDesc><graphic coords="16,152.06,115.84,311.24,201.80" type="bitmap" /></figure>
<figure><head /><label /><figDesc /><graphic coords="17,152.06,115.83,311.24,175.07" type="bitmap" /></figure>
<figure type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Overview of the data and tasks of the five LifeCLEF challenges</figDesc><table><row><cell /><cell cols="3">Modality #species #items</cell><cell>Task</cell><cell>Metric</cell></row><row><cell>BirdCLEF</cell><cell>audio</cell><cell>264</cell><cell>16,900</cell><cell>Multi-Label Classification</cell><cell>cmAP</cell></row><row><cell>SnakeCLEF</cell><cell>images metadata</cell><cell>1,500</cell><cell cols="3">150-200K Classification ad-hoc metric</cell></row><row><cell>FungiCLEF</cell><cell>images metadata</cell><cell>1,600</cell><cell>300K</cell><cell cols="2">Classification ad-hoc metric</cell></row><row><cell>PlantCLEF</cell><cell>images</cell><cell>80,000</cell><cell>4.0M</cell><cell>Classification</cell><cell>Macro-Average MRR</cell></row><row><cell>GeoLifeCLEF</cell><cell>images time-series tabular</cell><cell>10,040</cell><cell>5.3M</cell><cell>Multi-Label Classification</cell><cell>Micro-Average F1</cell></row></table></figure>
<figure type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>SnakeCLEF 2023 dataset statistics for each subset.</figDesc><table><row><cell>Subset</cell><cell cols="4">#Species #Countries #Images #Observations</cell></row><row><cell>Training</cell><cell>1,784</cell><cell>212</cell><cell>168,144</cell><cell>95,588</cell></row><row><cell>iNaturalist</cell><cell>1,784</cell><cell>210</cell><cell>154,301</cell><cell>85,843</cell></row><row><cell>HerpMapper</cell><cell>889</cell><cell>119</cell><cell>13,843</cell><cell>9,745</cell></row><row><cell>Validation</cell><cell>1,599</cell><cell>177</cell><cell>14,117</cell><cell>7,816</cell></row><row><cell>Public Test</cell><cell>1,784</cell><cell>191</cell><cell>28,274</cell><cell>15,632</cell></row><row><cell>Private Test</cell><cell>182</cell><cell>8</cell><cell>8,080</cell><cell>3,765</cell></row><row><cell>India</cell><cell>76</cell><cell>1</cell><cell>2,892</cell><cell>2,395</cell></row><row><cell>Central America</cell><cell>107</cell><cell>4</cell><cell>5,188</cell><cell>1,370</cell></row></table></figure>
<figure type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>FungiCLEF 2023 dataset statistics for each subset.</figDesc><table><row><cell>Subset</cell><cell cols="4">Species → Known/Unknown Images Observations</cell></row><row><cell>Training</cell><cell>1,604</cell><cell>1,604 / -</cell><cell>295,938</cell><cell>177,170</cell></row><row><cell>Validation</cell><cell>2,713</cell><cell>1,084 / 1,629</cell><cell>60,832</cell><cell>30,131</cell></row><row><cell>Public Test</cell><cell>2,650</cell><cell>1,085 / 1,565</cell><cell>60,225</cell><cell>30,130</cell></row><row><cell>Private Test</cell><cell>3,299</cell><cell>1,116 / 2,183</cell><cell>91,231</cell><cell>45,021</cell></row></table></figure>
			<note place="foot" n="1" xml:id="foot_0"><p>https://www.cbd.int/</p></note>
			<note place="foot" n="2" xml:id="foot_1"><p>http://www.lifeclef.org/</p></note>
			<note place="foot" n="3" xml:id="foot_2"><p>http://www.imageclef.org/</p></note>
			<note place="foot" n="4" xml:id="foot_3"><p>https://www.aicrowd.com/challenges/lifeclef-2022-23-plant/</p></note>
			<note place="foot" n="5" xml:id="foot_4"><p>https://gbif.org/</p></note>
			<note place="foot" n="6" xml:id="foot_5"><p>https://eol.org/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>The research described in this paper was partly funded by the <rs type="funder">European Commission</rs> via the <rs type="projectName">GUARDEN</rs> and <rs type="projectName">MAMBO</rs> projects, which have received funding from the <rs type="funder">European Union</rs>'s <rs type="programName">Horizon Europe research and innovation program</rs> under grant agreements <rs type="grantNumber">101060693</rs> and <rs type="grantNumber">101060639</rs>. The opinions expressed in this work are those of the authors and are not necessarily those of the GUARDEN or MAMBO partners or the <rs type="institution">European Commission</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_jEzpTYa">
					<orgName type="project" subtype="full">GUARDEN</orgName>
				</org>
				<org type="funded-project" xml:id="_vTf27E6">
					<idno type="grant-number">101060693</idno>
					<orgName type="project" subtype="full">MAMBO</orgName>
					<orgName type="program" subtype="full">Horizon Europe research and innovation program</orgName>
				</org>
				<org type="funding" xml:id="_x6XMpBB">
					<idno type="grant-number">101060639</idno>
				</org>
			</listOrg>
			<div type="annex">
<div />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Snake identification in medically important scenarios</title>
	</analytic>
	<monogr>
		<title level="m">Working Notes of CLEF 2023 -Conference and Labs of the Evaluation Forum</title>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
	<note>Overview of SnakeCLEF</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep learning-based appearance features extraction for automated carp species identification</title>
		<author>
			<persName><forename type="first">A</forename><surname>Banan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nasiri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Taheri-Garavand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Aquacultural Engineering</title>
		<imprint>
			<biblScope unit="volume">89</biblScope>
			<biblScope unit="page">102053</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Towards the fully automated monitoring of ecological communities</title>
		<author>
			<persName><forename type="first">M</forename><surname>Besson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Alison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Bjerge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">E</forename><surname>Gorochowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">T</forename><surname>Høye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Jucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">F</forename><surname>Clements</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ecology Letters</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2753" to="2775" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">The GeoLifeCLEF 2023 dataset to evaluate plant species distribution models at high spatial resolution across europe</title>
		<author>
			<persName><forename type="first">C</forename><surname>Botella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Deneu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcos</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Servajean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Larcher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Estopinan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leblanc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page">XXXX</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Overview of GeoLifeCLEF 2023: Species composition prediction with high spatial resolution at continental scale using remote sensing</title>
		<author>
			<persName><forename type="first">C</forename><surname>Botella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Deneu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcos</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Servajean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Larcher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Leblanc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Estopinan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Working Notes of CLEF 2023 -Conference and Labs of the Evaluation Forum</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Sensor network for the monitoring of ecosystem: Bird species recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Roe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1109/ISSNIP.2007.4496859</idno>
		<ptr target="https://doi.org/10.1109/ISSNIP.2007.4496859" />
	</analytic>
	<monogr>
		<title level="m">Intelligent Sensors, Sensor Networks and Information</title>
		<imprint>
			<date type="published" when="2007">2007. 2007</date>
		</imprint>
	</monogr>
	<note>ISSNIP 2007. 3rd International Conference on</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1597" to="1607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Metaformer: A unified meta framework for fine-grained recognition</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Diao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.02751</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Eva: Exploring the limits of masked visual representation learning at scale</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Automated species identification: why not?</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Gaston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>O'neill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Philosophical Transactions of the Royal Society of London B: Biological Sciences</title>
		<imprint>
			<biblScope unit="volume">359</biblScope>
			<biblScope unit="issue">1444</biblScope>
			<biblScope unit="page" from="655" to="667" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Plant identification using deep neural networks via optimization of transfer learning parameters</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Ghazi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yanikoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Aptoula</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">235</biblScope>
			<biblScope unit="page" from="228" to="235" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dugan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Halkias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sueur</surname></persName>
		</author>
		<ptr target="http://sabiod.org/ICML4B2013_book.pdf" />
		<title level="m">Proc. 1st workshop on Machine Learning for Bioacoustics -ICML4B. ICML</title>
		<meeting>1st workshop on Machine Learning for Bioacoustics -ICML4B. ICML<address><addrLine>Atlanta USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Artières</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mallat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Tchernichovski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Halkias</surname></persName>
		</author>
		<ptr target="http://sabiod.org/nips4b" />
		<title level="m">Proc. Neural Information Processing Scaled for Bioacoustics, from Neurons to Big Data</title>
		<meeting>Neural Information essing Scaled for Bioacoustics, from Neurons to Big Data</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note>NIPS Int. Conf.</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Overview of PlantCLEF 2023: Image-based plant identification at global scale</title>
		<author>
			<persName><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Working Notes of CLEF 2023 -Conference and Labs of the Evaluation Forum</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The imageclef 2013 plant identification task</title>
		<author>
			<persName><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Bakic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Barthélémy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Boujemaa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Molino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CLEF: Conference and Labs of the Evaluation Forum</title>
		<meeting><address><addrLine>Valencia, Spain. Valencia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-09">Sep. 2013. 2013</date>
		</imprint>
	</monogr>
	<note>CLEF task overview 2013</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The imageclef 2011 plant images classification task</title>
		<author>
			<persName><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Boujemaa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Barthélémy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Molino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Birnbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Mouysset</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Picard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CLEF: Conference and Labs of the Evaluation Forum</title>
		<meeting><address><addrLine>Amsterdam, Netherlands</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-09">Sep. 2011. 2011</date>
		</imprint>
	</monogr>
	<note>CLEF task overview 2011</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Imageclef2012 plant images identification task</title>
		<author>
			<persName><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Yahiaoui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Barthélémy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Boujemaa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Molino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CLEF: Conference and Labs of the Evaluation Forum</title>
		<meeting><address><addrLine>Rome, Italy; Rome</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-09">Sep. 2012. 2012</date>
		</imprint>
	</monogr>
	<note>CLEF task overview 2012</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Attention-based deep multiple instance learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ilse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tomczak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2127" to="2136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Interactive plant identification based on social image data</title>
		<author>
			<persName><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Bakić</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Barbe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Selmi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Yahiaoui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Carré</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Mouysset</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Molino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ecological Informatics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="22" to="34" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Overview of LifeCLEF 2018: a large-scale evaluation of species identification and recommendation algorithms in the era of ai</title>
		<author>
			<persName><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Botella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">P</forename><surname>Vellinga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CLEF: Cross-Language Evaluation Forum for European Languages. Experimental IR Meets Multilinguality, Multimodality, and Interaction</title>
		<editor>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Jones</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Lawless</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Gonzalo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Kelly</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Goeuriot</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Mandl</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Cappellato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<meeting><address><addrLine>Avigon, France</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018-09">Sep 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Overview of LifeCLEF 2019: Identification of Amazonian Plants, South &amp; North American Birds, and Niche Prediction</title>
		<author>
			<persName><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Botella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Servajean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Planqué</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">R</forename><surname>Stöter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">P</forename><surname>Vellinga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-28577-7_29</idno>
		<ptr target="https://hal.umontpellier.fr/hal-02281455" />
	</analytic>
	<monogr>
		<title level="m">Experimental IR Meets Multilinguality, Multimodality, and Interaction</title>
		<editor>
			<persName><forename type="first">F</forename><surname>Crestani</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Brascher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Savoy</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Rauber</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Müller</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Losada</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><forename type="middle">H</forename><surname>Bürki</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><forename type="middle">H</forename><surname>Bürki</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Cappellato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<meeting><address><addrLine>Lugano, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-09">Sep 2019</date>
			<biblScope unit="page" from="387" to="401" />
		</imprint>
	</monogr>
	<note>CLEF 2019 -Conference and Labs of the Evaluation Forum</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">LifeCLEF 2016: Multimedia Life Species Identification Challenges</title>
		<author>
			<persName><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Spampinato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">P</forename><surname>Vellinga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Champ</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Planqué</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Palazzo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-44564-9_26</idno>
		<ptr target="https://hal.archives-ouvertes.fr/hal-01373781" />
	</analytic>
	<monogr>
		<title level="m">CLEF: Cross-Language Evaluation Forum. Experimental IR Meets Multilinguality, Multimodality, and Interaction</title>
		<editor>
			<persName><forename type="first">N</forename><surname>Fuhr</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Quaresma</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Gonçalves</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Larsen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Balog</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Cappellato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<meeting><address><addrLine>Évora, Portugal</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016-09">Sep 2016</date>
			<biblScope unit="page" from="286" to="310" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">LifeCLEF 2017 Lab Overview: Multimedia Species Identification Challenges</title>
		<author>
			<persName><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Spampinato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">P</forename><surname>Vellinga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Lombardo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Planque</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Palazzo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-65813-1_24</idno>
		<ptr target="https://hal.archives-ouvertes.fr/hal-01629191" />
	</analytic>
	<monogr>
		<title level="m">CLEF: Cross-Language Evaluation Forum. Experimental IR Meets Multilinguality, Multimodality, and Interaction</title>
		<editor>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Jones</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Lawless</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Gonzalo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Kelly</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Goeuriot</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Mandl</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Cappellato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<meeting><address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017-09">Sep 2017</date>
			<biblScope unit="page" from="255" to="274" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">LifeCLEF 2014: Multimedia Life Species Identification Challenges</title>
		<author>
			<persName><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Spampinato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">P</forename><surname>Vellinga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Planque</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rauber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-11382-1_20</idno>
		<ptr target="https://hal.inria.fr/hal-01075770" />
	</analytic>
	<monogr>
		<title level="m">CLEF: Cross-Language Evaluation Forum. Information Access Evaluation. Multilinguality, Multimodality, and Interaction</title>
		<meeting><address><addrLine>Sheffield, United Kingdom</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2014-09">Sep 2014</date>
			<biblScope unit="page" from="229" to="249" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Lifeclef 2015: multimedia life species identification challenges</title>
		<author>
			<persName><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Spampinato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">P</forename><surname>Vellinga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Planqué</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rauber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Palazzo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Fisher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Experimental IR Meets Multilinguality, Multimodality, and Interaction</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="462" to="483" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Overview of lifeclef 2020: a systemoriented evaluation of automated species identification and species distribution prediction</title>
		<author>
			<persName><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Deneu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Servajean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Picek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>De Castaneda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Bolon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Durso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference of the Cross-Language Evaluation Forum for European Languages</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="342" to="363" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Overview of lifeclef 2022: an evaluation of machine-learning based species identification and species distribution prediction</title>
		<author>
			<persName><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Picek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lorieul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Deneu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Servajean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Durso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Planqué</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">P</forename><surname>Vellinga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Navine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Klinck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Eggel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Šulc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hruz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference of the Cross-Language Evaluation Forum for European Languages</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Overview of lifeclef 2021: An evaluation of machine-learning based species identification and species distribution prediction</title>
		<author>
			<persName><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Picek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lorieul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Deneu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Servajean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Durso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Bolon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference of the Cross-Language Evaluation Forum for European Languages</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="371" to="393" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Overview of BirdCLEF 2023: Automated bird species identification in eastern africa</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Klinck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Reers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Cherutich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">P</forename><surname>Vellinga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Planqué</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Working Notes of CLEF 2023 -Conference and Labs of the Evaluation Forum</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Contour matching for a fish recognition and migration-monitoring system</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Schoenberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shiozawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Optics East</title>
		<imprint>
			<publisher>International Society for Optics and Photonics</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="37" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Multi-organ plant classification based on convolutional and recurrent neural networks</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Remagnino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="4287" to="4301" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="10012" to="10022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A deep active learning system for species identification and counting in camera trap images</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Norouzzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Beery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jojic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Clune</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Methods in Ecology and Evolution</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="150" to="161" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">On the use of deep learning for fish species recognition and quantification on board fishing vessels</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Ovalle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Vilas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">T</forename><surname>Antelo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Marine Policy</title>
		<imprint>
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page">105015</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Modeling of species distributions with maxent: new extensions and a comprehensive evaluation</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dudík</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ecography</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="161" to="175" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Overview of snakeclef 2023: Snake identification in medically important scenarios</title>
		<author>
			<persName><forename type="first">L</forename><surname>Picek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Šulc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chamidullin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Durso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CLEF 2023-Conference and Labs of the Evaluation Forum</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Overview of fungiclef 2023: Fungi recognition beyond 1/0 cost</title>
		<author>
			<persName><forename type="first">L</forename><surname>Picek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Šulc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chamidullin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CLEF 2023-Conference and Labs of the Evaluation Forum</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Automatic fungi recognition: Deep learning meets mycology</title>
		<author>
			<persName><forename type="first">L</forename><surname>Picek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Šulc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Heilmann-Clausen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Jeppesen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Lind</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">633</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Danish fungi 2020-not just another image recognition dataset</title>
		<author>
			<persName><forename type="first">L</forename><surname>Picek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Šulc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Jeppesen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Heilmann-Clausen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Laessøe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Frøslev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1525" to="1535" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Overview of FungiCLEF 2022: Fungi recognition as an open set classification problem</title>
		<author>
			<persName><forename type="first">L</forename><surname>Picek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Šulc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Heilmann-Clausen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Working Notes of CLEF 2022 -Conference and Labs of the Evaluation Forum</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Overview of FungiCLEF 2023: Fungi recognition beyond 0-1 cost</title>
		<author>
			<persName><forename type="first">L</forename><surname>Picek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Šulc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Heilmann-Clausen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Working Notes of CLEF 2023 -Conference and Labs of the Evaluation Forum</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Identifying gaps in the photographic record of the vascular plant flora of the americas</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">C</forename><surname>Pitman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Suwa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ulloa Ulloa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Solomon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Philipp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">F</forename><surname>Vriesendorp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Derby</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Perk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature plants</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1010" to="1014" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="8748" to="8763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Equivalence of maxent and poisson point process models for species distribution modeling in ecology</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">W</forename><surname>Renner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">I</forename><surname>Warton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrics</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="274" to="281" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Crossvalidation strategies for data with temporal, spatial, hierarchical, or phylogenetic structure</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Bahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ciuti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Boyce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Elith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Guillera-Arroita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hauenstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Lahoz-Monfort</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schröder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Thuiller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ecography</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="913" to="929" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Fungi recognition: A practical use case</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sulc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Picek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Jeppesen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Heilmann-Clausen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2316" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6105" to="6114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">A toolbox for animal call recognition</title>
		<author>
			<persName><forename type="first">M</forename><surname>Towsey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Planitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nantes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wimmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Roe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioacoustics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="107" to="125" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Automated species recognition of antbirds in a mexican rainforest using hidden markov models</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">M</forename><surname>Trifa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Kirschel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">E</forename><surname>Vallejo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of the Acoustical Society of America</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="page">2424</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Leverage samples with single positive labels to train cnn-based models for multi-label plant species prediction</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">Q</forename><surname>Ung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kojima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Working Notes of CLEF 2023 -Conference and Labs of the Evaluation Forum</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">The inaturalist species classification and detection dataset</title>
		<author>
			<persName><forename type="first">G</forename><surname>Van Horn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Mac Aodha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shepard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">A new method to control error rates in automated species identification with deep learning algorithms</title>
		<author>
			<persName><forename type="first">S</forename><surname>Villon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mouillot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chaumont</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Subsol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Claverie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Villéger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific reports</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Automated plant species identification-trends and future directions</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wäldchen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rzanny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Seeland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mäder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS computational biology</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">1005993</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Seesaw loss for long-tailed instance segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="9695" to="9704" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Deep learning methods for animal counting in camera trap images</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 34th International Conference on Tools with Artificial Intelligence (ICTAI)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2022">2022. 2022</date>
			<biblScope unit="page" from="939" to="943" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Debnath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">S</forename><surname>Kweon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2301.00808</idno>
		<title level="m">Convnext v2: Co-designing and scaling convnets with masked autoencoders</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Automated identification of animal species in camera trap images</title>
		<author>
			<persName><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kays</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EURASIP Journal on Image and Video Processing</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Volo: Vision outlooker for visual recognition</title>
		<author>
			<persName><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Change is everywhere: Single-temporal supervised object change detection in remote sensing imagery</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF international conference on computer vision</title>
		<meeting>the IEEE/CVF international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="15193" to="15202" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>