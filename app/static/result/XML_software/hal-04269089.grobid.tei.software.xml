<TEI xmlns="http://www.tei-c.org/ns/1.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xml:space="preserve" xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Language Models as Controlled Natural Language Semantic Parsers for Knowledge Graph Question Answering</title>
				<funder ref="#_QhEjYMY">
					<orgName type="full">e-Vita</orgName>
				</funder>
				<funder ref="#_bn2FpEw">
					<orgName type="full">EU</orgName>
				</funder>
				<funder ref="#_udQFrnc">
					<orgName type="full">DAAD (German Academic Exchange Service)</orgName>
				</funder>
				<funder>
					<orgName type="full">BMBF (Federal Ministry of Education and Research)</orgName>
				</funder>
				<funder ref="#_w95nmAK">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher />
				<availability status="unknown"><licence /></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Jens</forename><surname>Lehmann</surname></persName>
							<email>jlehmnn@amazon.com.</email>
							<affiliation key="aff0">
								<orgName type="institution">Amazon</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Preetam</forename><surname>Gattogi</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Institute for Applied Informatics (InfAI)</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Dresden University of Technology d University Rennes</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Dhananjay</forename><surname>Bhandiwad</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Institute for Applied Informatics (InfAI)</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sébastien</forename><surname>Ferré</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Sahar</forename><surname>Vahdati</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Institute for Applied Informatics (InfAI)</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Language Models as Controlled Natural Language Semantic Parsers for Knowledge Graph Question Answering</title>
					</analytic>
					<monogr>
						<imprint>
							<date />
						</imprint>
					</monogr>
					<idno type="MD5">9F1082FC5D7DD583AAB8E8C0D5EABBC7</idno>
					<idno type="DOI">10.3233/FAIA230411</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-04-12T14:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid" />
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div><p>We propose the use of controlled natural language as a target for knowledge graph question answering (KGQA) semantic parsing via language models as opposed to using formal query languages directly. Controlled natural languages are close to (human) natural languages, but can be unambiguously translated into a formal language such as SPARQL. Our research hypothesis is that the pretraining of large language models (LLMs) on vast amounts of textual data leads to the ability to parse into controlled natural language for KGQA with limited training data requirements. We devise an LLMspecific approach for semantic parsing to study this hypothesis. To conduct our study, we created a dataset that allows the comparison of one formal and two different controlled natural languages. Our analysis shows that training data requirements are indeed substantially reduced when using controlled natural languages, which is relevant since collecting and maintaining high-quality KGQA semantic parsing training data is very expensive and time-consuming.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div><head n="1">Introduction</head><p>Over the past decades, large amounts of structured knowledge in the form of knowledge graphs have been published <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b31">32]</ref>. To access the information in such knowledge graphs, query languages like <software ContextAttributes="created">SPARQL 1</software> are used. The use of formal queries to access knowledge graph poses difficulties for non-expert users as they require the user to understand the syntax of the formal query language, as well as the underlying structure of entities and their relationships. KG question answering (KGQA) systems, therefore, aim to provide the users with an interface to ask questions in natural language, using their own terminology, to which they receive a concise answer generated by querying the KG. Recently, KGQA systems have also been used as an auxiliary system (commonly called a "tool" or "plugin") for LLMs in chatbots, since they allow LLMs to ground their responses in factual knowledge stored in a knowledge graph <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b33">34]</ref> to reduce LLM hallucinations.</p><p>KGQA systems typically use semantic parsing approaches in which natural language questions are transformed into formal language. Such approaches are often trained on large datasets of (natural language, formal query language) pairs. Creating such datasets can be very expensive since handwriting formal queries for natural language input queries is a time-consuming task. At the same time, machine learning techniques for semantic parsing are typically not sufficiently accurate when trained on smaller datasets. To enable the creation of large datasets, various semi-automated approaches have been used <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b30">31]</ref>. Although those datasets were helpful to advance the state-of-the-art, they are still largely template-based, do not generalise well outside of the scope of those templates and often do not reflect the traffic seen in QA systems in production. Therefore, we argue that it is important to investigate KGQA semantic parsing systems which require smaller amounts of training data.</p><p>As a step towards achieving this, we propose using a controlled natural language (CNL) as the target formal language. CNL expressions are close to (human) natural language while they can still be unambiguously translated into a formal language. Our research hypothesis is that using controlled natural language is more suitable than a formal query language for knowledge graphs, since CNLs are a) closer to the input query in natural language and b) closer to the high amounts of textual pre-training data of language models, that can be employed for semantic parsing. To the best of our knowledge, this research hypothesis has so far not been investigated for knowledge graph query languages. There are studies on controlled natural languages for simpler custom query languages, for example <ref type="bibr" target="#b19">[20]</ref>. However, these studies do not yet cover the more complex compositional patterns of graph query languages, aggregation functions, and common hyperrelational graph structures. We believe the research hypothesis is interesting because while controlled natural language expressions are close to the large amount of human-authored text that LLMs have been trained on, they typically do not occur frequently in LLM pre-training data directly. In contrast to this, formal query languages like SPARQL or SQL are frequent in pre-training data and in small-scale scenarios work to some extent already in zero-and few-shot settings. From this perspective, we were curiously interested which of those factors would have a bigger influence on semantic parsing performance.</p><p>To verify our research hypothesis, we conducted a study using the <software ContextAttributes="created">Mintaka</software> dataset <ref type="bibr" target="#b24">[25]</ref> based on Wikidata. Unlike most KGQA datasets, <software ContextAttributes="created">Mintaka</software> contains very natural questions, as they were collected manually in a high-quality data collection process. However, due to the significant effort that would be required to collect (natural language, logical form) pairs, <software ContextAttributes="created">Mintaka</software> only contains (natural language, response) pairs, i.e. a so called weak supervision setting. We prefer to investigate the strong supervision setting because of its widespread use in semantic parsing and its simplicity, which only requires LLM fine-tuning. To be able to investigate our research hypothesis, we labelled a subset of 550 questions of <software ContextAttributes="created">Mintaka</software> with their corresponding logical forms. We provide three variants of those logical forms: SPARQL <ref type="bibr" target="#b9">[10]</ref>, <software ContextAttributes="created">Sparklis</software> <ref type="bibr" target="#b6">[7]</ref> and <software ContextAttributes="created">SQUALL</software> <ref type="bibr" target="#b5">[6]</ref>. The latter two are controlled natural languages. We then used several language models, specifically BLOOM <ref type="bibr" target="#b23">[24]</ref>, GPT Neo<ref type="foot" target="#foot_0">2</ref> , GPT-2 <ref type="bibr" target="#b17">[18]</ref>, GPT-3 <ref type="bibr" target="#b2">[3]</ref>, T5 <ref type="bibr" target="#b18">[19]</ref> and Llama 2 <ref type="bibr" target="#b29">[30]</ref> on this dataset. We employ the state-of-the-art <software ContextAttributes="created">ReFinED</software> <ref type="bibr" target="#b0">[1]</ref> entity linker to enrich the LLM prompt with entity information. The specific prompting and fine-tuning techniques that we used are described in Section 4. We generally focused on comparing results across target languages rather than optimising the performance of the approach itself. Overall, we make the following contributions:</p><p>• Proposing controlled natural languages as target for KGQA semantic parsing, opposed to using formal query languages directly. • Provision of a dataset containing 550 (NL question, formal KG query) pairs to the community. <ref type="foot" target="#foot_1">3</ref> While larger (semi-)automatically created datasets exist, this is one of the largest publicly available strong supervision datasets using a manually collected set of questions (similar to, e.g. <ref type="bibr" target="#b16">[17]</ref>) and the largest for controlled natural language KGQA. • Development of an LLM-based semantic parsing approach that builds on an external entity linker and leverages the capabilities of LLMs to hallucinate for relation linking.</p><p>• An evaluation of semantic parsing accuracy for three target languages on six different language models using different metrics. • An analysis of the relationship between fine-tuning scale and semantic parsing accuracy across the three target languages. • A qualitative result analysis on our test set of 150 queries.</p></div>
<div><head n="2">Related Work</head><p>We first discuss related work in the area of knowledge graph question answering and then continue with controlled natural languages for semantic parsing.</p></div>
<div><head n="2.1">KGQA Semantic Parsing</head><p>A range of different approaches have been proposed for semantic parsing, such as Combinatory Categorial Grammars (CCG), rulebased systems or neural-network based methods. <ref type="bibr" target="#b8">[9]</ref> provides a general overview over such semantic parsing techniques. Since we focus more specifically on semantic parsing in the context of knowledge graphs, we refer to <ref type="bibr" target="#b3">[4]</ref> as an introduction and survey for neural network-based KGQA semantic parsing.</p><p>Modern KGQA semantic parsing methods are often based on sequence to sequence architectures, e.g. <ref type="bibr" target="#b20">[21]</ref> where an encoder translates the natural language utterance into an intermediate state and a decoder then translates this state into a logical form. Pointer generator networks allow to copy some part of the input, e.g. a data value, into the output. Those approaches have shown to be accurate when trained on large amounts of high quality training data. However, as described in the introduction obtaining training data is very challenging and updating the training set to incorporate new features or cover more domains requires substantial effort. For this reason, there was an increasing interest in training semantic parsers with less data. For example, <ref type="bibr" target="#b21">[22]</ref> explores an unsupervised approach for semantic parsing. While achieving promising results, such approaches do not achieve the performance levels required for production level systems and cannot easily be tuned to do so.</p><p>Large language models such as GPT-3 <ref type="bibr" target="#b2">[3]</ref>, BLOOM <ref type="bibr" target="#b23">[24]</ref> or Llama 2 <ref type="bibr" target="#b29">[30]</ref> also support semantic parsing with zero or few examples and are a promising avenue towards reducing the need for high amounts of training data, in particular after several improvements have been made to reduce the inference costs of such models. However, LLMs do not support KG-specific parsing directly without access to entities in the underlying knowledge graph. For this reason, they need to be augmented with a method to retrieve entities. We are not aware of any publication that has used an LLM-based generative approach for knowledge graph question answering at the point of writing but point to some related work with controlled natural language as a target in the subsection below.</p></div>
<div><head n="2.2">Controlled Natural Languages for Semantic Parsing</head><p>Controlled natural languages (CNLs) have been used as target for semantic parsing in applications outside of knowledge graph question answering. For example, <ref type="bibr" target="#b19">[20]</ref> has already investigated the idea of using a special-purpose CNL to reduce training data requirements and showed improvements on the Overnight dataset <ref type="bibr" target="#b32">[33]</ref> in several smaller domains. On the same (and other) datasets, <ref type="bibr" target="#b27">[28]</ref> used a constraint decoding procedure for semantic parsing that employs the GPT-3 and Codex models. They compared a controlled natural language and a standard logical form language and could show accuracy benefits of using a controlled natural language. Although these previous studies are an indicator and motivation for our investigation, they were performed on datasets with a smaller scale and not on knowledge graphs. For example, an Overnight domain has at most 45 relations compared to more than 10,000 relations (and more than 100 million entities) in the Wikidata knowledge graph <ref type="bibr" target="#b31">[32]</ref> that we use. Similarly, <ref type="bibr" target="#b26">[27]</ref> investigated the use of language models (BART, GPT-2, GPT-3) for constrained decoding of synchronous context-free grammars (SCFG) following an idea similar to ours by first converting an NL question into a canonical human-like utterance which is then translated into the target logical form. These are again evaluated on Overnight and related datasets. <ref type="bibr" target="#b13">[14]</ref> introduces a new language Graph-IR with the aim of unifying the semantic parsing of graph query languages with one intermediate representation. The principles of their intermediate representation language are similar to CNL but with less emphasis on naturalness compared to the <software ContextAttributes="created">SQUALL</software> language which we are investigating. Moreover, they use a BART-based encoder-decoder model, whereas we focus on generative approaches using LLMs.</p></div>
<div><head n="3.1">Semantic Parsing</head><p>Semantic parsing is the task of translating a natural language (NL) utterance into a machine-interpretable representation q (query) of its meaning in a given formal language. The generated logical form q is correct if it captures the meaning of the input NL utterance. Given that it is difficult to formalize the concept of meaning, there are several metrics for measuring the correctness of semantic parsing: One set of metrics relevant for semantic parsing are execution metrics, which measure whether the execution of the logical form, e.g. executing a query over a knowledge graph, yields the desired result. It should be noted that query execution is not sufficient for correctness because multiple queries can return the same result even if they do not correctly capture the meaning of the input question. We call such logical forms spurious. For example, translating "What is two plus two" to 2 * 2 yields the correct result but is not the correct query.</p><p>Another set of metrics are text similarity metrics such as BLEU <ref type="bibr" target="#b14">[15]</ref> or METEOR <ref type="bibr" target="#b1">[2]</ref>. Those are primarily used as proxy metrics to assess how close the semantic parse is to the gold standard query, but are less significant than execution metrics for KGQA.</p></div>
<div><head n="3.2">Knowledge Graphs</head><p>In this paper, we use hyperrelational knowledge graphs as a base. Hyperrelations are common, e.g., they are required to model the relationship properties in graph databases and the DBpedia <ref type="bibr" target="#b10">[11]</ref> or Wikidata <ref type="bibr" target="#b31">[32]</ref> knowledge graphs use variants of this data model. Using hyperrelations via so called qualifiers in Wikidata is indeed required for several questions in the <software ContextAttributes="created">Mintaka</software> dataset which we evaluate on. Let E = {e1 . . . en e } be a set of entities, L be the set of all literal values, and P = {p1 . . . pn r } be a set of properties (also called relations). A statement t = (s, p, o, Q) comprises a subject s ∈ E, a property p ∈ P, an object o ∈ (E ∪ L), and a set of qualifiers Q ⊆ P × (E ∪ L). The property p relates the subject s to the object o, and the set of qualifiers Q further describes the relationship. An example of statement in Wikidata is (The Twilight Saga: Breaking Dawn -Part 1, part of the series, The Twilight Saga, { (series ordinal, 4), (follows, The Twilight Saga: Eclipse)}), which describes the film "Breaking Dawn -Part 1" as the fourth in the Twilight Saga, following the film "Eclipse". A KG K is a set of such statements.</p></div>
<div><head n="3.3">Controlled Natural Languages for SPARQL</head><p>Generally, Controlled Natural Languages aim at bridging the highlevel and natural syntax of natural languages, and the lack of ambiguity of formal languages such as SPARQL <ref type="bibr" target="#b9">[10]</ref>. CNLs allow for abstracting from the low-level aspects of formal languages (such as bindings and relational algebra). <software ContextAttributes="created">Sparklis</software> <ref type="bibr" target="#b6">[7]</ref> and <software ContextAttributes="created">SQUALL</software> <ref type="bibr" target="#b5">[6]</ref> are examples of SPARQL-oriented CNLs for querying and updating knowledge graphs. <software ContextAttributes="created">SQUALL</software> has a very high coverage of SPARQL queries and updates. It therefore combines the expressivity of SPARQL, and at the same time the readability of natural languages. Its main limit is that the content words (e.g., proper nouns, verbs) must be non-ambiguous references to entities and relations. <software ContextAttributes="created">SQUALL</software> is based on Montague grammars <ref type="bibr" target="#b12">[13]</ref> that combine context-free grammars, first order logic, and λ-calculus. <software ContextAttributes="created">Sparklis</software> acts as an interactive SPARQL endpoint explorer that completely hides SPARQL behind a CNL. <software ContextAttributes="created">Sparklis</software>' CNL is slightly less expressive and less natural than <software ContextAttributes="created">SQUALL</software>, and it only covers SELECT queries. In counterpart, queries can be built incrementally through user interaction: at each step, <software ContextAttributes="created">Sparklis</software> suggests some query refinements, among which the user selects the refinement that fits her information needs. This allows the user to discover the data model on-the-fly, and this prevents ill-formed queries. The main limitation is that, in case of a complex data model (like in Wikidata), it may be difficult for the user to find a valid sequence of refinements. Figure <ref type="figure" target="#fig_0">1</ref> shows an example natural language query that is taken from the <software ContextAttributes="created">Mintaka</software> dataset. We showcase the corresponding SPARQL, <software ContextAttributes="created">Sparklis</software> and <software ContextAttributes="created">SQUALL</software> versions of this question.</p><p>4 Semantic Parsing Approach</p></div>
<div><head n="4.1">Overview</head><p>Our main goal is to compare different target languages for semantic parsing using LLMs. For this reason, we devised an approach that follows the same steps for all target languages. Upon receiving a question as input, we first perform an entity linking step. We then instruct the LLM to convert the input query into the target language.  </p></div>
<div><head n="4.2">Entity Linking</head><p>Entity linking is the NLP task that assigns a unique identifier -in our case a node in the knowledge graph -to entities mentioned in an input utterance. Our approach is not specific to a particular entity linker. In our experiments, we decided to use the <software ContextAttributes="created">ReFinED</software> <ref type="bibr" target="#b0">[1]</ref> entity linker, since it achieves state-of-the-art results on Wikidata and is runtime efficient. For a given input utterance, we retrieve all entities above a threshold probability value returned by <software ContextAttributes="created">ReFinED</software>. This threshold is a hyperparameter of our semantic parsing approach. For each returned entity, we obtain its label and description. For each entity the quadruple (entity ID, probability, label, description) is passed as input to the LLM prompting step. The additional information beyond the entity ID can be used by the LLM for the semantic parsing step. For example, the probability may lead to the LLM preferring highly probable entities over less probable entities. The label and description give the LLM additional context. It should be noted that the entity label alone is not unique, e.g. the label "The Hunger Games" exists multiples times in Wikidata (for the books, the movie series and the first movie in the series itself).</p><p>Relation linking is the NLP task that assigns a unique identifier to relations relevant to an input utterance. We do not employ a separate relation linker, but let the LLM generate the relation as part of the query generation. We do this for several reasons:</p><p>• First, relation linking is usually less accurate than entity linking, since relations are frequently implicit (i.e. not part of the question) or have a variety of surface forms. Given the difficulty of this task, the LLM itself may be the most suitable component for this task. • Second, knowledge graphs usually contain much fewer relations than entities. For example, as of May 2023, Wikidata contains approximately 10 thousand relations but more than 100 million entities 4 . So while the training data may contain at least a relevant portion of frequent relations, it will only contain a minuscule fraction of existing entities. • Third, we can use the hallucination of LLMs as a strength by letting it generate likely relations and then adjusting those in a postprocessing step if needed (described in more detail later in this section). This is useful since most of the infrequent relations will not be in the training data.</p></div>
<div><head n="4.3">Prompting</head><p>In our approach, the LLM prompt consists of three parts: the instruction, the input question, and the list of entities. As instruction, we use "Given the question and the entities generate a $language query!" where $language ∈ {SPARQL, <software>Sparklis</software>, SQUALL} is the target language. The question is passed to the LLM as is. For the entities, we pass the quadruples (entity ID, probability, label, description) as described in the previous section. An example of a prompt is given below. We did not perform prompt engineering since optimising performance is not the main scope of the experiments. While this may affect absolute ceiling performance, it is unlikely to substantially affect the performance difference between the different target languages that is our focus here.</p><p>Given the question and the entities generate a <software>SQUALL</software> query! Question: Which actor was the star of Titanic and born in Los Angeles? </p></div>
<div><head n="4.4">Posthoc Relation Adjustment</head><p>Given that the number of fine-tuning training examples which we use is relatively small compared to the number of relations, there is a high chance that a question contains an unseen relation. To overcome this problem, we can make use of LLM hallucinations. Although hallucinations are usually seen as a negative phenomenon in LLMs, we can exploit them as a strength in our case. Even for unseen (or infrequently seen) relations, the LLM causal modeling objective incentivizes it to output tokens that are plausible in the given context if we give it the opportunity to do so. In order to achieve this, we let the LLM output not only the identifier of a relation (e.g. P162) but also the label (e.g. producer) separated by a colon. For frequently seen relations, the LLM will likely output a correct identifier. If the relation is unseen or infrequently seen, it may hallucinate an incorrect identifier and a plausible relation name, which may, however, not exist in Wikidata. We can then use this relation name to post-process the relation. To do this, we use Cosine similarity to calculate within the 384-dim vector space using the (all-MiniLLM-L6-v23) Sentence transformer from Hugging face <ref type="foot" target="#foot_2">5</ref> . The model embeds each label(both the generated label and the Wikidata label) into 384 dim vectors. After this the generated label vector is compared against all label vectors of Wikidata. Then The hallucinated label is then replaced with the highest scoring relation label from Wikidata.</p></div>
<div><head n="5">The Mintaka-CNL Dataset</head><p><software ContextAttributes="created">Mintaka</software> <ref type="bibr" target="#b24">[25]</ref> is a natural and multilingual question answering dataset containing 20k manually written NL queries in English. We used <software ContextAttributes="created">Mintaka</software> as a base for our studies since it is the only KGQA dataset we are aware of that contains a high number of queries and does not contain questions that are, at least in part, automatically generated (e.g. "Is the WOEID of Tuscaloosa 14605?" in KQA Pro <ref type="bibr" target="#b25">[26]</ref> or "1520.0 is the minimum width for which size rail gauge?" in GrailQA <ref type="bibr" target="#b7">[8]</ref>). To create our dataset, we sampled questions from <software ContextAttributes="created">Mintaka</software> using the following criteria: a) diversity in question themes, b) diversity in question complexity, c) preference of questions with more than one entity, and d) having at least three examples of the same type of question in each category. After sampling relevant questions from <software ContextAttributes="created">Mintaka</software>, we constructed their equivalent CNL and SPARQL queries. For this, we first turned them into <software ContextAttributes="created">Sparklis</software> queries using its query construction interface <ref type="foot" target="#foot_3">6</ref> . On average, it takes several minutes to create one query using the tool which is still a substantial speedup compared to manually writing SPARQL queries. The majority of this work was done during internal workshops. Three researchers and two student assistants were involved in creating the <software ContextAttributes="created">Sparklis</software> queries.</p><p>The interface automatically converts queries into SPARQL queries. For <software ContextAttributes="created">SQUALL</software>, the queries are manually written based on the <software ContextAttributes="created">Sparklis</software> query. A semi-automatic conversion is subject to future work. Generally, <software ContextAttributes="created">SQUALL</software> is slightly more expressive than <software ContextAttributes="created">Sparklis</software>, where some features are not directly supported: e.g. selecting the nth element according to an order or comparisons of relation values (e.g. height, length). <software ContextAttributes="created">SQUALL</software> also supports a direct conversion into SPARQL such that we could verify that both queries return the same output modulo differences in language expressivity. <software ContextAttributes="created">Mintaka</software> has seven categories of questions including (1) count, (2) intersection, (3) superlative, (4) difference, (5) comparative, (6) generic and ( <ref type="formula">7</ref>) ordinal. Our questions are spread over the different categories as follows: 92, 85, 57, 49, 56, 74 and 137. The original <software ContextAttributes="created">Mintaka</software> dataset also has another category named of yes/no questions which we did not consider in this work, since <software ContextAttributes="created">Sparklis</software> does not support SPARQL ASK queries.</p><p>In total, we sampled 550 questions and created three different logical forms for each. This dataset is split into 400 train and 150 test questions.</p><p>6 Experimental Setup</p></div>
<div><head n="6.1">Models and Fine-Tuning</head><p>We used the language models shown in Table <ref type="table" target="#tab_2">1</ref>. The models were selected to cover a range of different sizes suitable for generative NLP tasks. For all models, we used a fine-tuning procedure using the prompt as described in Section 4 and the actual target query as the completion target. For models outside of the GPT-3 family, we updated the tokenizer to include special tokens and re-sized the model's token embeddings. During the fine-tuning phase, the models were trained following parameters of <ref type="bibr" target="#b22">[23]</ref> which states 25 epochs with a constant learning rate of 5e-05 and a batch size of 4. The maximum sequence length during training was set at 120. No extensive hyperparameter optimization was performed. The same settings are used for all target languages. For the decoding step, we use a greedy search strategy. Special tokens are excluded from the LLM output before evaluation. The threshold for entity linking was set to 0.1. The fine-tuning was done on 2 Nvidia A100 GPUs on <software ContextAttributes="created">Linux</software> and on Ope-nAI servers for GPT-3. <software ContextAttributes="created">Code</software> was written in Python 3.10. The fine tuning for Llama 2 <ref type="bibr" target="#b29">[30]</ref> was done for 20 epochs, with constant learning rate of 1e-04 and batch size of 1. No extensive hyper-parameter optimization was performed.</p></div>
<div><head n="6.2">Metrics</head><p>We employ a combination of translation quality metrics and execution metrics (see Section 3.1): BLEU Cumulative <ref type="bibr" target="#b14">[15]</ref>, ME-TEOR <ref type="bibr" target="#b1">[2]</ref>, ROUGE <ref type="bibr" target="#b11">[12]</ref> help in evaluating the translation quality, with BLEU cumulative focusing on n-gram precision, METEOR providing a balance between precision, recall and ROUGE-2 that focuses on bi-gram (2-gram) recall. These metrics help to assess the extent to which the generated queries preserve the structural and semantic information of the reference query at granular level. We also use Exact Match accuracy to evaluate the percentage of queries matching the ground truth. Additionally, we incorporate hits@1 as execution metric, which measures whether the first retrieved answer when actually querying the underlying knowledge graph matches the <software ContextAttributes="created">Mintaka</software> gold standard. The <software ContextAttributes="created">Mintaka</software> dataset reports exactly one correct result for each query, which is why we did not include further execution metrics such as micro precision and recall.</p></div>
<div><head n="7">Quantitative Evaluation Results</head><p>We investigate the following research questions:</p><p>1. Does a controlled natural language as target for KGQA semantic parsing using LLMs have a positive effect on accuracy? 2. Are smaller amounts of training data sufficient to teach an LLM a controlled natural language as opposed to a formal language? 3. Does the answer to the above two questions depend on the choice of language model? 4. Does the answer to the first question depend on the question type?</p><p>We will walk through our research questions and connect them to the quantitative evaluation results we obtained. The first research question is whether using CNL can result in improved accuracy. We report our results in Table <ref type="table">2</ref>. <software ContextAttributes="created">Mintaka</software> is generally a challenging dataset with hits@1 scores reported <ref type="bibr" target="#b24">[25]</ref> to be between 0.12 and 0.2 on its test set for 3 different approaches. (Note that the <software ContextAttributes="created">Mintaka</software> test set is a superset of our <software ContextAttributes="created">Mintaka</software>-CNL test set and the reported approaches use weak supervision rather than strong supervision. Therefore, the results are not directly comparable.)</p><p>The hits@1 results for the two controlled natural languages, <software ContextAttributes="created">Sparklis</software> and <software ContextAttributes="created">SQUALL</software>, are both higher than the SPARQL baseline. Using <software ContextAttributes="created">SQUALL</software> leads to higher accuracies for all models except T5, which could be due to it being even closer to natural language than <software ContextAttributes="created">Sparklis</software> and generally being more compact. For the top-performing model, the accuracy using <software ContextAttributes="created">SQUALL</software> is roughly twice as high compared to using SPARQL, so the choice of target language has a strong influence on the performance of the KGQA system. For the string similarity proxy metrics (BLEU, METEOR, ROUGE) <software ContextAttributes="created">SQUALL</software> and <software ContextAttributes="created">Sparklis</software> are similar, and consistently higher than SPARQL. For <software ContextAttributes="created">Sparklis</software>, we used the exact match score as an estimate for hits@1 execution accuracy. This can be an overestimate in some cases since Figure <ref type="figure">3</ref>. Illustration of hits@1 accuracy (y-axis) of the top performing models when using different amounts of fine-tuning examples (x-axis). Each line in the plot corresponds to a pair of language model and target language.</p><p>Table <ref type="table">2</ref>. Evaluation Results on the <software ContextAttributes="created">Mintaka</software>-CNL dataset for different combinations of language models and target languages. The last column shows the absolute difference between hits@1 score of the controlled natural language and SPARQL.</p></div>
<div><head>Model</head><p>Setting Language BLEU METEOR ROUGE Exact Match Hits@1 ∆ Hits@1 GPT-  <software>Sparklis</software> is less expressive than SPARQL and <software ContextAttributes="created">SQUALL</software>. This means that the ground truth queries may not capture all aspects of the input question (e.g. rather than selecting the "5th US president" the <software ContextAttributes="created">Sparklis</software> query returns "US presidents" in increasing "point in time"). It can also be an underestimate since different query surface forms can lead to the same execution results. For both SPARQL and <software ContextAttributes="created">SQUALL</software>, we observed that accuracy is usually much higher than the exact match score.</p><p>We also observed that the LLM sometimes generates <software ContextAttributes="created">SQUALL</software> queries that are substantially different from the ground truth, but are semantically equally good and often lead to the same execution results. If we take this ability to generate correct but syntactically different queries as a proxy for the LLM obtaining a deeper understand- query not matching KG structure, which uses a specific entity type &lt;Star Trek films&gt; ing of the target language (as opposed to simpler expression matching) then the pattern we see is: more natural target languages (see Figure <ref type="figure" target="#fig_0">1</ref>) lead to a deeper understanding of the language by the large language model. To answer the second research question, we varied the number of fine-tuning examples and plotted execution accuracies in Figure <ref type="figure">3</ref> for the two best performing LLMs. We observe that <software ContextAttributes="created">SQUALL</software> is generally more accurate across all data sizes. For lower amounts of fine-tuning data SPARQL is more accurate than <software ContextAttributes="created">Sparklis</software> for GPT-3 Davinci -probably because it was seen more in pre-training. Increasing the number of fine-tuning examples leads to <software ContextAttributes="created">Sparklis</software> performing slightly better than SPARQL for GPT-3 Davinci with a constant margin. From our data, we cannot generally conclude that using a controlled natural language always reduces training data requirements -it is the case for <software ContextAttributes="created">SQUALL</software> but to a lesser degree for <software ContextAttributes="created">Sparklis</software>. So whether or not training data requirements for a CNL are substantially lower appears to be language-specific. <software ContextAttributes="created">SQUALL</software> is both more natural and more compact than <software ContextAttributes="created">Sparklis</software>, i.e. it can express a query in less tokens, which could explain the performance gap.</p><p>For the third research question, we computed the relative improvement for each language model in the execution accuracy of <software ContextAttributes="used">SQUALL</software> compared to SPARQL in the last column of Table <ref type="table">2</ref>. It is interesting and surprising that the improvement from a formal language to a controlled natural language does not seem to depend much on the choice of the language model. Even for a small model like the GPT-2 124 million parameter model, which could not generate a single accurate SPARQL query, we observed substantial absolute improvements when using a controlled natural language.</p><p>For the fourth research question, we analyzed the different types of questions in <software ContextAttributes="used">Mintaka</software> in terms of absolute performance and performance improvement when using <software ContextAttributes="used">SQUALL</software> instead of SPARQL. The results are shown in Table <ref type="table" target="#tab_5">4</ref>. They indicate that the performance improvements are particularly high for difference, comparative, generic and ordinal questions. Some of those, e.g. comparative questions, are rather complex to phrase in SPARQL, which could explain the bigger performance differences for those types of questions. Generally, performance varies greatly depending on question type which could be explained by the types implying different query complexity.</p></div>
<div><head n="8">Qualitative Evaluation and Failure Analysis</head><p>To analyse failure cases in more depth, we grouped them into categories as shown in Table <ref type="table" target="#tab_4">3</ref>. While generally a single generated query can have multiple failures, we followed a hierarchical approach to identify one root cause per query: We first checked whether the entity linker provides the entities required to build a correct query. Please note that we feed all entities above a threshold to the LLM, so in case of ambiguous cases the LLM needs to decide which one to pick and we do not require the entity linker to resolve the ambiguity completely. However, we do require (by the formulation of the prompt) that the correct entities are among those provided by the entity linker. If this is not the case, then we classify this as entity linking error. If this is the case, but the LLM does not use the correct entity, we classify this as entity selection error. If entities were selected correctly, but a relation is incorrect, we classified this as a relation generation error. The last column of Table <ref type="table" target="#tab_4">3</ref> shows the frequency of those er-rors on the test set for the best performing models. We can observe that entity linking and relation generation are major sources of errors. While the entity linking approach scales to millions of entities, it limits the ceiling performance of the LLM to roughly 88% hits@1 with the selected threshold. For larger training data sizes, the entity linking threshold should be moved further down.</p><p>Assuming that entities and relations were selected correctly, we then analyse whether there are syntactic or semantic errors. Syntactic errors should rarely happen with LLMs, but we observed several of them -most likely because <software>SQUALL</software> permits a variable structure allowing multiple expressions that map to the same query. With limited training data, the LLM has not fully captured which of those variations are allowed. Semantic errors are those where the generated query does not capture the intent of the question -in one case we could observe that the LLM hallucinated an additional irrelevant part of a query that was not correlated to the input question. The last 3 categories are queries that are not judged to be correct, because of a) Wikidata opting for a different KG structure than generated by the LLM, b) the <software ContextAttributes="used">Mintaka</software> benchmark result not being correct or c) Wikidata not returning a result due to (never or no longer) having the data required for answering the query.</p><p>Table <ref type="table" target="#tab_6">5</ref> shows some examples of correct and incorrect parses. We could observe that the most powerful LLM was able to perform correct semantic parses even for entities that were not top-ranked by the entity linker and for cases where the relations are unseen in the training data. However, we also observe that the limited training data set size makes it difficult for the LLM to adapt to unseen topics. While semantic parsers are often trained on orders of magnitude more training data than in our study, we would generally still recommend (as suggested by Figure <ref type="figure">3</ref>) to include training data covering a wide range of topics allowing the LLM to observe KG modelling choices and relations. An interesting observation that we made is that the LLMs more frequently tend to generate repeated unnecessary patterns in SPARQL queries whereas this does not happen for <software ContextAttributes="used">SQUALL</software>. This might be due to the model having a very good understanding of natural language and therefore also of controlled natural language whereas for SPARQL even GPT-3 can generate queries that are not meaningful, e.g. contain too many or repeated triple patterns.</p></div>
<div><head n="9">Conclusions and Future Work</head><p>We verified our research hypothesis that using a controlled natural language as target for LLM-based KGQA semantic parsing provides advantages compared to translating into a logical form directly. This appears to hold despite the logical form being more prevalent in use and therefore in pre-training data. Our study with two different CNL shows that the more compact and natural <software>SQUALL</software> language leads to much better results compared to a more regular, verbose <software ContextAttributes="used">Sparklis</software> language. Using the most powerful LLM in our test, the semantic parsing accuracy could be doubled by switching from SPARQL to <software ContextAttributes="used">SQUALL</software>. We could also observe that the LLM likely obtains a better understanding of <software ContextAttributes="used">SQUALL</software>, e.g. it can generate correct syntactic variations of the ground truth and very rarely generates meaningless or duplicated patterns. Interestingly, the benefits of CNLs appear to be relatively independent of Language Model size as similar improvements were observed for LM sizes ranging from 124 million to 175 billion parameters.</p><p>In future work, the CNL dataset could be extended and further target languages can be investigated. Moreover, we plan to investigate the usage of CNL beyond knowledge graph semantic parsing as interface for the LLM to external knowledge and services.</p></div><figure xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. An example natural language query from the Mintaka dataset, and its equivalent queries written in the SQUALL and Sparklis controlled languages as well as SPARQL query language.</figDesc><graphic coords="4,361.97,176.31,193.59,83.77" type="bitmap" /></figure>
<figure xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Runtime workflow: First, entity linking is performed. The linked entities with additional information are fed into the prompt construction. The LLM generates an intermediate query after which relations can be adjusted.</figDesc></figure>
<figure type="table" xml:id="tab_2"><head>Table 1 .</head><label>1</label><figDesc>Used models including parameter count.</figDesc><table><row><cell>Model Name</cell><cell cols="2">Developed by Number of Parameters</cell></row><row><cell>GPT-2</cell><cell>OpenAI</cell><cell>0.12 B</cell></row><row><cell>T5-Large</cell><cell>Google</cell><cell>0.77 B</cell></row><row><cell>GPT Neo</cell><cell>EleutherAI</cell><cell>1.3 B</cell></row><row><cell>GPT-2 XL</cell><cell>OpenAI</cell><cell>1.5 B</cell></row><row><cell>Bloom</cell><cell>BigScience</cell><cell>1.7 B</cell></row><row><cell>GPT-3 Curie</cell><cell>OpenAI</cell><cell>6.7 B</cell></row><row><cell>Llama2 7B</cell><cell>Meta</cell><cell>7 B</cell></row><row><cell cols="2">GPT-3 Da Vinci OpenAI</cell><cell>175B</cell></row></table></figure>
<figure type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc>Error categories we observed on the test set for GPT-3 (Da Vinci) fine-tuned on 400 examples.</figDesc><table><row><cell>Error Category</cell><cell>Category Definition</cell></row></table></figure>
<figure type="table" xml:id="tab_5"><head>Table 4 .</head><label>4</label><figDesc>Hits@1 scores for SQUALL and SPARQL query generation using GPT-3 Davinci fine-tuned on 400 examples.</figDesc><table><row><cell cols="5">Question Type Count SPARQL SQUALL Improvement</cell></row><row><cell /><cell>(test)</cell><cell>hits@1</cell><cell>hits@1</cell><cell /></row><row><cell>ordinal</cell><cell>38</cell><cell>0.23</cell><cell>0.47</cell><cell>+ 0.24</cell></row><row><cell>difference</cell><cell>11</cell><cell>0.09</cell><cell>0.45</cell><cell>+ 0.36</cell></row><row><cell>generic</cell><cell>23</cell><cell>0.21</cell><cell>0.43</cell><cell>+ 0.22</cell></row><row><cell>intersection</cell><cell>21</cell><cell>0.28</cell><cell>0.33</cell><cell>+ 0.05</cell></row><row><cell>comparative</cell><cell>10</cell><cell>0.00</cell><cell>0.30</cell><cell>+ 0.30</cell></row><row><cell>count</cell><cell>28</cell><cell>0.25</cell><cell>0.21</cell><cell>-0.04</cell></row><row><cell>superlative</cell><cell>18</cell><cell>0.00</cell><cell>0.11</cell><cell>+ 0.11</cell></row></table></figure>
<figure type="table" xml:id="tab_6"><head>Table 5 .</head><label>5</label><figDesc>Examples for correct (top) and incorrect (bottom) parses of GPT-3 Davinci. For the incorrect parses, we describe one example per failure category.</figDesc><table><row><cell>Input</cell><cell>Generated SQUALL Query</cell><cell>Comment</cell></row><row><cell>What was the last Fast a Furious</cell><cell>Which &lt;film&gt; is-&lt;part of the series&gt; &lt;Fast &amp; Furious&gt;</cell><cell>successfully detects that "Fast &amp; Furious"</cell></row><row><cell>movie that Paul Walker starred</cell><cell>and has a &lt;cast member&gt; &lt;Paul Walker&gt; and has the</cell><cell>is a series and "latest" refers to publication</cell></row><row><cell>in?</cell><cell>latest &lt;publication date&gt;?</cell><cell>date</cell></row><row><cell>Who is a current senator of</cell><cell>Which &lt;human&gt; has &lt;position held&gt; &lt;United States</cell><cell>successfully builds a compositional query</cell></row><row><cell>Utah and a former governor of</cell><cell>senator&gt; at &lt;electoral district&gt; &lt;Utah Class 1 sen-</cell><cell>combining two criteria</cell></row><row><cell>Massachusetts?</cell><cell>ate seat&gt; and has &lt;position held&gt; &lt;Governor of Mas-</cell><cell /></row><row><cell /><cell>sachusetts&gt; at an &lt;end time&gt;?</cell><cell /></row><row><cell>What is Iron Man's real name?</cell><cell>What is the &lt;birth name&gt; of &lt;Iron Man&gt; ?</cell><cell>entity linking error: linked the movie in-</cell></row><row><cell /><cell /><cell>stead of the fictional character</cell></row><row><cell>Which Indiana Jones movie</cell><cell>Which &lt;film&gt; that is-&lt;part of the series&gt; &lt;Indiana</cell><cell>entity selection error: picked the figure In-</cell></row><row><cell>made the most money?</cell><cell>Jones&gt; has the highest &lt;box office&gt;?</cell><cell>diana Jones instead of the series</cell></row><row><cell>Which actors in Black Panther</cell><cell>Which &lt;human&gt; that is-&lt;part of the series&gt; &lt;Black</cell><cell>relation generation error: generated &lt;cast</cell></row><row><cell>are no longer alive?</cell><cell>Panther&gt; has not a &lt;cast member&gt; &lt;Black Panther&gt;?</cell><cell>member&gt; instead of &lt;cause of death&gt;</cell></row><row><cell>Which Iron Man movie did Jeff</cell><cell>Which &lt;film&gt; has &lt;part of the series&gt; &lt;Iron Man&gt; and</cell><cell>query correct, Mintaka lists the character in-</cell></row><row><cell>Bridges not appear in?</cell><cell>has not &lt;cast member&gt; &lt;Jeff Bridge&gt;?</cell><cell>stead of the movie as ground truth</cell></row><row><cell>Which member of The Police</cell><cell>Which &lt;member&gt; of &lt;The Police&gt; has not &lt;country of</cell><cell>query correct, Wikidata has no results (any-</cell></row><row><cell>was not British?</cell><cell>citizenship&gt; &lt;United Kingdom&gt;?</cell><cell>more)</cell></row><row><cell>How many movies has Quentin</cell><cell>How many &lt;film&gt;-s has-&lt;director&gt; &lt;Quentin</cell><cell>syntactically incorrect since there is a dash</cell></row><row><cell>Tarantino directed?</cell><cell>Tarantino&gt;?</cell><cell>after "has"</cell></row><row><cell>What basketball player became</cell><cell>Which &lt;human&gt; has &lt;position held&gt; &lt;United States</cell><cell>semantically incorrect query asking for 5th</cell></row><row><cell>a senator?</cell><cell>senator&gt; at &lt;series ordinal&gt; 5?</cell><cell>term rather than for basketball occupation</cell></row><row><cell>How many Star Trek movies</cell><cell>How many &lt;film&gt;-s is-&lt;part of the series&gt; &lt;Star Trek&gt;</cell><cell /></row><row><cell>have Chris Pine in them?</cell><cell>and has &lt;cast member&gt; &lt;Chris Pine&gt;?</cell><cell /></row></table></figure>
			<note place="foot" n="2" xml:id="foot_0"><p>https://github.com/EleutherAI/gpt-neo</p></note>
			<note place="foot" n="3" xml:id="foot_1"><p>https://github.com/NIMI-research/CNL_KGQA</p></note>
			<note place="foot" n="5" xml:id="foot_2"><p>See https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2.</p></note>
			<note place="foot" n="6" xml:id="foot_3"><p>http://www.irisa.fr/LIS/ferre/sparklis/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>This work is partly supported by the <rs type="funder">BMBF (Federal Ministry of Education and Research)</rs> and <rs type="funder">DAAD (German Academic Exchange Service)</rs> in project <rs type="projectName">SECAI</rs> (grant <rs type="grantNumber">57616814</rs>). It is also supported by <rs type="funder">EU</rs> projects <rs type="projectName">CALLISTO</rs> (<rs type="grantName">EU</rs> grant <rs type="grantNumber">101004152</rs>) and <rs type="funder">e-Vita</rs> (<rs type="grantName">EU grant</rs> <rs type="grantNumber">101016453</rs>) as well as the ScaDS.AI (<rs type="grantNumber">IS18026A-F</rs>) center for providing High Performance Computing infrastructure access at <rs type="institution">TU Dresden, Germany</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_udQFrnc">
					<idno type="grant-number">57616814</idno>
					<orgName type="project" subtype="full">SECAI</orgName>
				</org>
				<org type="funded-project" xml:id="_bn2FpEw">
					<idno type="grant-number">101004152</idno>
					<orgName type="grant-name">EU</orgName>
					<orgName type="project" subtype="full">CALLISTO</orgName>
				</org>
				<org type="funding" xml:id="_QhEjYMY">
					<idno type="grant-number">101016453</idno>
					<orgName type="grant-name">EU grant</orgName>
				</org>
				<org type="funding" xml:id="_w95nmAK">
					<idno type="grant-number">IS18026A-F</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Refined: An efficient zero-shot-capable approach to end-to-end entity linking</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Ayoola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shubhi</forename><surname>Tyagi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christos</forename><surname>Christodoulopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Pierleoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amazon</forename><surname>Alexa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">I</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT 2022</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">209</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Meteor: An automatic metric for mt evaluation with improved correlation with human judgments</title>
		<author>
			<persName><forename type="first">Satanjeev</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alon</forename><surname>Lavie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization</title>
		<meeting>the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="65" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1877" to="1901" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Introduction to neural network-based question answering over knowledge graphs</title>
		<author>
			<persName><forename type="first">Nilesh</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denis</forename><surname>Lukovnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaurav</forename><surname>Maheshwari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Priyansh</forename><surname>Trivedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jens</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Asja</forename><surname>Fischer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">1389</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Lc-quad 2.0: A large dataset for complex question answering over wikidata and dbpedia</title>
		<author>
			<persName><forename type="first">Mohnish</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Debayan</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abdelrahman</forename><surname>Abdelkawi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jens</forename><surname>Lehmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Semantic Web-ISWC 2019: 18th International Semantic Web Conference</title>
		<meeting><address><addrLine>Auckland, New Zealand</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">October 26-30, 2019. 2019</date>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="69" to="78" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">SQUALL: The expressiveness of SPARQL 1.1 made available as a controlled natural language</title>
		<author>
			<persName><forename type="first">Sébastien</forename><surname>Ferré</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data &amp; Knowledge Engineering</title>
		<imprint>
			<biblScope unit="volume">94</biblScope>
			<biblScope unit="page" from="163" to="188" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Sparklis: An expressive query builder for sparql endpoints with guidance in natural language</title>
		<author>
			<persName><forename type="first">Sébastien</forename><surname>Ferré</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Semantic Web</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="405" to="418" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Beyond iid: three levels of generalization for question answering on knowledge bases</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sue</forename><surname>Kase</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michelle</forename><surname>Vanni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Sadler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xifeng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Web Conference 2021</title>
		<meeting>the Web Conference 2021</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3477" to="3488" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">A survey on semantic parsing</title>
		<author>
			<persName><forename type="first">Aishwarya</forename><surname>Kamath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajarshi</forename><surname>Das</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.00978</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A survey and classification of controlled natural languages</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kuhn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Dbpedia-a large-scale, multilingual knowledge base extracted from wikipedia</title>
		<author>
			<persName><forename type="first">Jens</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Isele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Jakob</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anja</forename><surname>Jentzsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitris</forename><surname>Kontokostas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pablo</forename><forename type="middle">N</forename><surname>Mendes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Hellmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohamed</forename><surname>Morsey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Van Kleef</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sören</forename><surname>Auer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Semantic web</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="167" to="195" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Automatic evaluation of summaries using n-gram co-occurrence statistics</title>
		<author>
			<persName><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2003 human language technology conference of the North American chapter of the association for computational linguistics</title>
		<meeting>the 2003 human language technology conference of the North American chapter of the association for computational linguistics</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="150" to="157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Universal grammar</title>
		<author>
			<persName><forename type="first">R</forename><surname>Montague</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Theoria</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="373" to="398" />
			<date type="published" when="1970">1970</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Graphq ir: Unifying the semantic parsing of graph query languages with one intermediate representation</title>
		<author>
			<persName><forename type="first">Lunyiu</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shulin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaxin</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiuding</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juanzi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jidong</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2022 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="5848" to="5865" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Jing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th annual meeting of the Association for Computational Linguistics</title>
		<meeting>the 40th annual meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Check your facts and try again: Improving large language mod-els with external knowledge and automated feedback</title>
		<author>
			<persName><forename type="first">Baolin</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengcheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujia</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiuyuan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lars</forename><surname>Liden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhou</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.12813</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Andreas Both, 'Qald-9-plus: A multilingual dataset for question answering over dbpedia and wikidata translated by native speakers</title>
		<author>
			<persName><forename type="first">Aleksandr</forename><surname>Perevalov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dennis</forename><surname>Diefenbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ricardo</forename><surname>Usbeck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 16th International Conference on Semantic Computing (ICSC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2022">2022. 2022</date>
			<biblScope unit="page" from="229" to="234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI blog</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="5485" to="5551" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Training naturalized semantic parsers with very little data</title>
		<author>
			<persName><forename type="first">Subendhu</forename><surname>Rongali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Konstantine</forename><surname>Arkoudas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Rubino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wael</forename><surname>Hamza</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.14243</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Don't parse, generate! a sequence to sequence architecture for taskoriented semantic parsing</title>
		<author>
			<persName><forename type="first">Subendhu</forename><surname>Rongali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Soldaini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emilio</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wael</forename><surname>Hamza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The Web Conference 2020</title>
		<meeting>The Web Conference 2020</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2962" to="2968" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Tree-kgqa: an unsupervised approach for question answering over knowledge graphs</title>
		<author>
			<persName><forename type="first">Rashad</forename><forename type="middle">Al</forename><surname>Md</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Debanjan</forename><surname>Hasan Rony</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ricardo</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jens</forename><surname>Usbeck</surname></persName>
		</author>
		<author>
			<persName><surname>Lehmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="50467" to="50478" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Sgpt: A generative approach for sparql query generation from natural language questions</title>
		<author>
			<persName><forename type="first">Rashad</forename><forename type="middle">Al</forename><surname>Md</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Uttam</forename><surname>Hasan Rony</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roman</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liubov</forename><surname>Teucher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jens</forename><surname>Kovriguina</surname></persName>
		</author>
		<author>
			<persName><surname>Lehmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="70712" to="70723" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Bloom: A 176b-parameter openaccess multilingual language model</title>
		<author>
			<persName><forename type="first">Le</forename><surname>Teven</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angela</forename><surname>Scao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ellie</forename><surname>Akiki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suzana</forename><surname>Pavlick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Ilić</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roman</forename><surname>Hesslow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandra</forename><surname>Castagné</surname></persName>
		</author>
		<author>
			<persName><forename type="first">François</forename><surname>Sasha Luccioni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Yvon</surname></persName>
		</author>
		<author>
			<persName><surname>Gallé</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2211.05100</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Mintaka: A complex, natural, and multilingual dataset for end-to-end question answering</title>
		<author>
			<persName><forename type="first">Priyanka</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alham</forename><surname>Fikri Aji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Saffari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th International Conference on Computational Linguistics</title>
		<meeting>the 29th International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1604" to="1619" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Kqa pro: A large-scale dataset with interpretable programs and accurate sparqls for complex question answering over knowledge base</title>
		<author>
			<persName><forename type="first">Jiaxin</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shulin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liangming</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yutong</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juanzi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.03875</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Constrained language models yield few-shot semantic parsers</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Chen</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName><forename type="first">Subhro</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emmanouil</forename><surname>Antonios Platanios</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Pauls</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Eisner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="7699" to="7715" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Few-shot semantic parsing with language models trained on code</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.08696</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Linkedgeodata: A core for a web of spatial open data</title>
		<author>
			<persName><forename type="first">Claus</forename><surname>Stadler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jens</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Konrad</forename><surname>Höffner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sören</forename><surname>Auer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Semantic Web</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="333" to="354" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Llama 2: Open foundation and finetuned chat models</title>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Louis</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Stone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Albert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amjad</forename><surname>Almahairi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yasmine</forename><surname>Babaei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikolay</forename><surname>Bashlykov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumya</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prajjwal</forename><surname>Bhargava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shruti</forename><surname>Bhosale</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2307.09288</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Lc-quad: A corpus for complex question answering over knowledge graphs</title>
		<author>
			<persName><forename type="first">Priyansh</forename><surname>Trivedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaurav</forename><surname>Maheshwari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohnish</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jens</forename><surname>Lehmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Semantic Web-ISWC 2017: 16th International Semantic Web Conference</title>
		<meeting><address><addrLine>Vienna, Austria</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017">October 21-25, 2017. 2017</date>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="210" to="218" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Wikidata: a free collaborative knowledgebase</title>
		<author>
			<persName><forename type="first">Denny</forename><surname>Vrandečić</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Krötzsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="78" to="85" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Building a semantic parser overnight</title>
		<author>
			<persName><forename type="first">Yushi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1332" to="1342" />
		</imprint>
	</monogr>
	<note>Long Papers</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Qa-gnn: Reasoning with language models and knowl-edge graphs for question answering</title>
		<author>
			<persName><forename type="first">Michihiro</forename><surname>Yasunaga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Bosselut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="535" to="546" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>