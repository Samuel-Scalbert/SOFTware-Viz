<TEI xmlns="http://www.tei-c.org/ns/1.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xml:space="preserve" xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CAWET: Context-Aware Worst-Case Execution Time Estimation Using Transformers</title>
			</titleStmt>
			<publicationStmt>
				<publisher />
				<availability status="unknown"><licence /></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Abderaouf</forename><forename type="middle">N</forename><surname>Amalou</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Elisa</forename><surname>Fromont</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Isabelle</forename><surname>Puaut</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Univ. Rennes</orgName>
								<orgName type="institution" key="instit2">INRIA</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<orgName type="institution" key="instit4">IRISA</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Univ. Rennes</orgName>
								<orgName type="institution" key="instit2">IUF</orgName>
								<orgName type="institution" key="instit3">INRIA</orgName>
								<orgName type="institution" key="instit4">CNRS</orgName>
								<orgName type="institution" key="instit5">IRISA</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution" key="instit1">Univ. Rennes</orgName>
								<orgName type="institution" key="instit2">INRIA</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<orgName type="institution" key="instit4">IRISA</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department" key="dep1">Leibniz International Proceedings</orgName>
								<orgName type="department" key="dep2">Informatics Schloss Dagstuhl -Leibniz-Zentrum für Informatik</orgName>
								<orgName type="department" key="dep3">Dagstuhl Publishing</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">CAWET: Context-Aware Worst-Case Execution Time Estimation Using Transformers</title>
					</analytic>
					<monogr>
						<imprint>
							<date />
						</imprint>
					</monogr>
					<idno type="MD5">F25024FF7643866EB8EB621245905C7E</idno>
					<idno type="DOI">10.4230/LIPIcs.ECRTS.2023.7</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-04-12T14:51+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid" />
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>2012 ACM Subject Classification Computer systems organization → Real-time system architecture Worst-case execution time</term>
					<term>machine learning</term>
					<term>transformers</term>
					<term>hybrid technique</term>
				</keywords>
			</textClass>
			<abstract>
<div><p>This paper presents <software>CAWET</software>, a hybrid worst-case program timing estimation technique. <software ContextAttributes="used">CAWET</software> identifies the longest execution path using static techniques, whereas the worst-case execution time (WCET) of basic blocks is predicted using an advanced language processing technique called Transformer-XL. By employing Transformers-XL in <software ContextAttributes="used">CAWET</software>, the execution context formed by previously executed basic blocks is taken into account, allowing for consideration of the microarchitecture of the processor pipeline without explicit modeling. Through a series of experiments on the <software ContextAttributes="used">TacleBench</software> benchmarks, using different target processors (Arm Cortex M4, M7, and A53), our method is demonstrated to never underestimate WCETs and is shown to be less pessimistic than its competitors.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div><head>Introduction</head><p>The Worst-case execution time (WCET) of a task is its maximum execution time when varying its input data and hardware state. Knowledge of the WCET of all tasks in a system allows schedulability analysis techniques to demonstrate that all tasks will meet their timing requirements in real-time systems. The challenge addressed in this paper is to estimate WCETs for Commercial Off The Shelf (COTS) processors, for which the micro-architecture details are not fully known.</p><p>WCET estimation techniques can be divided into three broad categories <ref type="bibr" target="#b34">[36]</ref>: static, measurement-based, and hybrid techniques.</p><p>Static techniques (ST, e.g., <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b2">3]</ref>) operate on the Control Flow Graph (CFG) of the task, extracted from its binary code. The nodes in the CFG are Basic Blocks<ref type="foot" target="#foot_0">1</ref> (BB), and the edges represent the control flow between the BB. Static techniques proceed in two phases: in the first phase, the WCET of each BB is estimated using abstractions of the hardware state; in the second phase, the whole program's WCET is calculated by finding the worst path inside the CFG (e.g., this is achieved by employing the commonly used implicit path enumeration technique -IPET - <ref type="bibr" target="#b34">[36]</ref>). Although the static techniques produce safe WCET estimates, using hardware abstractions on complex micro-architectures will inevitably lead to state explosion. Moreover, each new architecture demands the design of a new hardware abstraction, which is time-consuming and error-prone (especially without the processor's micro-architectural details).</p><p>Measurement-based techniques (MBT) (e.g., <ref type="bibr" target="#b8">[9]</ref>) are empirical techniques that run the program end-to-end with varied input data and hardware states to gather measurements. The WCET is then estimated from the measurements by either returning the largest observed timing (with a configurable safety margin) or using statistical techniques such as extreme value theory to infer a probabilistic WCET from the observed values <ref type="bibr" target="#b27">[29,</ref><ref type="bibr" target="#b28">30]</ref>. Unless the worst input and hardware state are found, techniques in this category may produce unsafe results.</p><p>Hybrid techniques (HT) <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b30">32,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21]</ref> combine the benefits of ST and MBT: the longest path is safely identified using techniques from ST, like IPET; measurements are used at the BB level, avoiding the costly and error-prone design of hardware abstractions. However, using measurements at the BB level in hybrid methods raises code coverage issues: each BB has to be executed at least once, and each BB's worst-case scenario must be covered.</p><p>In recent works, machine learning (ML) techniques are used in HT instead of measurements to predict the WCET of BBs <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b1">2]</ref>. These techniques, named HT-ML in the following, train an ML model (e.g., neural network) on a large dataset of BB whose WCET is known. The ML model is then used to predict the WCET of previously unseen BB. HT-ML techniques have the following benefits:</p><p>(i) The time-consuming phase of HT-ML (training) is executed only once (per architecture) and does not need any design of a hardware abstraction like in ST. (ii) Although the training phase may be long, prediction is fast and does not require thousands of measurements per BB. (iii) HT-ML can process large amounts of execution scenarios for BB and identify patterns, allowing more accurate predictions.</p><p>Nevertheless, the current HT-ML methods use oversimplified code characterization. The features used for learning and prediction abstract too much information from the machine code, causing information that impacts timing to be lost. For example, not considering the ordering of machine instructions in a BB will make the technique unable to accurately learn the impact of pipelines on timing.</p><p>In this paper, we propose a novel HT-ML WCET estimation technique called <software ContextAttributes="used">CAWET</software>, for Context-Aware Worst-case execution time Estimation using Transformers. This technique uses the advanced machine learning algorithm Transformers-XL <ref type="bibr" target="#b7">[8]</ref>. Unlike other HT-ML methods, which only consider static features, <software ContextAttributes="used">CAWET</software> considers the internal dependencies within each BB and the context surrounding it when estimating its WCET. This is performed by treating the sequence of instructions in a BB as a natural language, where the timing of a BB depends not only on its own sequence of instructions but also on the sequence of BBs executed before it.</p><p><software>CAWET</software> consists of two main stages: training and deployment (or estimation). As in all systems using Transformers, the Transformer model is first pre-trained in the learning phase to comprehend the vocabulary (in our context, assembly language). Then, the model is fine-tuned using extensive measurements on various basic blocks extracted from real codes. In this fine-tuning stage, the model learns how to calculate the WCET of each basic block by considering the context surrounding the block (previously executed BBs). During the estimation stage, the WCET of each BB is determined for all bounded-length contexts leading to the BB, extracted from the program's CFG. The maximum timing estimate for these contexts is then selected as the WCET of the basic block and used by IPET to calculate the WCET of the overall program.</p><p><software>CAWET</software> is easy to deploy, as the training has to be done only once. Consideration of pipeline effects is performed automatically because of the consideration of the execution context of all basic blocks.</p></div>
<div><head>7:3</head><p><software ContextAttributes="used">CAWET</software> is evaluated on processors of varied complexity, including the basic pipelineonly cortex-M4, the more advanced cortex-M7 that features a cache, and the even more sophisticated cortex-A53. The quality of WCET estimates produced by <software ContextAttributes="used">CAWET</software> is compared to those produced by WE-HML, the HT-ML technique closest to <software ContextAttributes="used">CAWET</software> <ref type="bibr" target="#b1">[2]</ref>, on 13 programs from the <software ContextAttributes="used">TACLeBench</software> benchmark suite <ref type="bibr" target="#b12">[13]</ref>. Our results show that <software ContextAttributes="used">CAWET</software> produces better estimates than its competitors on more diverse architectures.</p><p>Our contributions are: A new hybrid timing ML-based WCET analysis technique that uses Transformers-XL to estimate the WCET of basic blocks and considers dependencies between instructions. We take into account the execution context that surrounds the BB under analysis by automatically exploring all bounded-length paths that leads to it.</p><p>We provide an empirical study on different targets and techniques. Our results show that this complex ML method is well-suited for timing estimation, with an average error of 23.8%, 102.2%, and 62.4%, on the Cortex M4, Cortex M7, and Cortex A53 processors, respectively.</p><p>The rest of this paper is organized as follows. Section 2 presents the <software>CAWET</software> HT-ML technique. The experimental methodology for evaluating it is detailed in Section 3, and experimental results are given in Section 4. Section 5 compares our approach to related techniques. We conclude in Section 6.</p></div>
<div><head n="2">CAWET: Context-Aware WCET estimation using Transformers</head><p><software>CAWET</software> is a hybrid context-aware WCET estimation technique that predicts an in-context WCET of individual basic blocks and then uses the predictions to calculate the overall program's WCET. A high-level overview of <software ContextAttributes="used">CAWET</software> is given in Section 2.1. The two main phases of <software ContextAttributes="used">CAWET</software>: training (using Transformers-XL) and prediction (i.e., deployment), are then respectively presented in Sections 2.2 and 2.3.</p></div>
<div><head n="2.1">Overview of CAWET</head><p>CAWET consists of two main stages: training and deployment (or estimation). Both stages operate on individual basic blocks (BB) and account for the execution context of the BB under study (i.e., the sequence of BBs executed before it). <software ContextAttributes="used">CAWET</software> relies on Transformers-XL, originally used in natural language processing, for their ability to learn long-term dependencies between words. In <software ContextAttributes="used">CAWET</software>, the language under study is a sequence of BBs, each composed of a sequence of assembly instructions. The overall structure of <software ContextAttributes="used">CAWET</software> is depicted in Figure <ref type="figure">1</ref>.</p><p>In the training phase (left block of Figure <ref type="figure">1</ref>), the Transformer model is first pre-trained on real programs to learn the vocabulary of the language it will process (in our context, assembly language) as it is usually done for large language models <ref type="bibr" target="#b9">[10]</ref>. Then, the model is fine-tuned using extensive measurements on a large set of BBs extracted from real code. In this fine-tuning stage, the model learns how to calculate the WCET of each BB by considering the context surrounding it (i.e., previously executed BBs).</p><p>During the estimation stage (right block of Figure <ref type="figure">1</ref>), the WCET of each BB is determined. Since there might be different execution paths leading to the BB under study, prediction operates on the set of contexts corresponding to these paths, with care taken to avoid combinatorial explosion, as further explained in Section 2. in the example is a list of 4 contexts, made of the sequence of BBs executed before BB 8:</p><p>(1, 4, 5), (1, 2, 3), <ref type="bibr" target="#b2">(3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5)</ref>, and <ref type="bibr" target="#b2">(3,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7)</ref>. The timing of BB 8 is estimated for each context. The maximum timing estimate is then selected as the WCET of the BB and used by IPET to calculate the WCET of the overall program.</p></div>
<div><head n="2.2">Training phase using Transformers-XL</head><p>A transformer is a neural network architecture originally designed for natural language processing, which can perform tasks such as language translation, text summarizing, and text-to-speech. It was first proposed in <ref type="bibr" target="#b33">[35]</ref>, and one of its main advantages is using selfattention mechanisms that enable the model to weigh different parts of the input data when making predictions. However, as defined in <ref type="bibr" target="#b33">[35]</ref>, the original transformer architectures have a fixed-length context window and may struggle to handle sequential data with long-term dependencies. To address this limitation, Transformers-XL (TXL) <ref type="bibr" target="#b7">[8]</ref> were introduced. A TXL is a variation of the transformer architecture that uses a so-called memory-augmented attention to better remember and utilize information from earlier in the sequence. We use a TXL architecture in <software ContextAttributes="used">CAWET</software> because it improves the ability of the transformer to handle long-term dependencies, which is necessary for handling long sequences of code.</p><p>Estimating the WCET of a given BB given its context is performed by first processing the context (formed by the BB executed before the analyzed BB as well as the analyzed BB), followed by processing the BB under analysis. This results in two embedding matrix representations (a global attention matrix for the context and a local attention matrix for the BB under analysis) that are then concatenated. The resulting embedding representation is given as input to a fully connected layer, producing a single scalar value (the timing estimate for the analyzed BB). Figure <ref type="figure" target="#fig_3">4</ref> provided in the Appendix illustrates this process.</p><p>The training of a TXL consists of two stages (pre-training and fine-tuning). During the pre-training stage, the TXL is trained to learn the structure of assembly instructions in text format using self-supervised learning. This classical self-supervised learning phase <ref type="bibr" target="#b9">[10]</ref> is achieved by masking random operations or operands in the sequence and (pre)training the model to reconstitute (i.e., predict) them as output. To perform this pre-training phase, thousands of disassembled binary programs are used without needing labeled information. Details about the hyper-parameters of the TXL architecture are provided in Table <ref type="table" target="#tab_0">10</ref> in the Appendix.</p><p>In the fine-tuning stage, a set of programs, the target processor, and a measurement tool are required. BBs execution time is measured using the measurement tool. Then, the instruction sequences are tokenized using sentence piece <ref type="bibr" target="#b22">[23]</ref>, a well-known tokenization technique trained in our work on the target assembly instructions. The training dataset for the fine-tuning stage is then built using the maximum observed timing of each BB, the tokenized BB, and its context. Contexts have a maximum size; the context size, expressed as a number of basic blocks, is a hyperparameter of the Tranformer-XL.</p></div>
<div><head n="2.3">Prediction phase</head><p><software>CAWET</software> predicts the WCET of BBs by considering their execution context. The results from <software ContextAttributes="used">CAWET</software> can then be used by a static WCET estimation tool. Section 2.3.1 presents the concepts and notations <software ContextAttributes="used">CAWET</software> relies on. Section 2.3.2 then details the context generation. Section 2.3.3 describes how the WCET of a BB is obtained from the predictions and the overall WCET of the program is finally calculated.</p></div>
<div><head n="2.3.1">Concepts and notations</head><p>The concepts and notations used in <software ContextAttributes="used">CAWET</software> are standard concepts used in compilers. They are illustrated in Figure <ref type="figure">3</ref>, which will be reused later to illustrate how <software ContextAttributes="used">CAWET</software> works. <ref type="bibr" target="#b18">[19]</ref>, is a sub-graph of a CFG that can only be entered by one edge and exited by one edge. A property of SESE regions is that they can be arranged into a tree, constructed in linear time.</p></div>
<div><head>▶ Definition 1 (Control Flow Graph). A Control flow graph (CFG) is a directed graph where each node represents a BB, and each edge represents the control flow from one BB to another.</head></div>
<div><head>▶ Definition 2 (SESE regions, SESE trees). A Single Entry Single Exit (SESE) region, as defined in</head><p>An example of CFG (with 7 BBs numbered from 1 to 7), and its SESE regions is depicted in Figure <ref type="figure" target="#fig_1">2 (A)</ref>. The dotted arrow in the figure represents the back edge of the loop composed of BB 5 and 6. The SESE tree that corresponds to the CFG is depicted in Figure <ref type="figure" target="#fig_1">2 (B)</ref>. The rationale behind using SESE regions is to have subsets of the CFG that are simple enough to explore all paths exhaustively, with the overall objective of avoiding combinatorial explosion when generating the possible contexts of a BB. <ref type="bibr" target="#b11">[12]</ref>. It can be thought of as the number of unique paths that can be taken through the code. It is calculated using the following formula:</p></div>
<div><head>▶ Definition 3 (Cyclomatic complexity). Cyclomatic complexity is a software metric that measures the number of independent paths through a program or a CFG</head><formula xml:id="formula_0">Cyclomatic_complexity(CF G) = edges -nodes + 2</formula><p>The cyclomatic complexity will be used during the prediction phase to decide which paths leading to a BB are worth exploring. The cyclomatic complexity of the SESE regions in our example is displayed in Figure <ref type="figure" target="#fig_1">2 (B)</ref>. </p></div>
<div><head n="2.3.2">Context generation</head><p>The task of finding all the possible paths in a graph may be computationally expensive.</p><p>To address this issue, we use a divide-and-conquer strategy based on the SESE tree of the program. In the example of Figure <ref type="figure" target="#fig_1">2</ref>, the root SESE region (SESE 1) represents the entire CFG. Each tree level represents a sub-SESE region (e.g., SESE 2 and SESE 3 are the children of SESE 1), with smaller and thus simpler sub-graphs.</p><p>To limit the complexity, <software ContextAttributes="used">CAWET</software> performs an exhaustive path exploration only for the SESE regions that are simple enough (based on their Cyclomatic Complexity, CC) to allow a full path exploration. SESE selection is performed using a top-bottom traversal of the SESE tree, and the SESE regions with a value of CC strictly higher than a threshold are filtered out. Path exploration for the selected regions uses Depth-First Search <ref type="bibr" target="#b32">[34]</ref> (DFS) to enumerate all possible paths<ref type="foot" target="#foot_1">2</ref> . We ensure, by construction, that the chosen SESE covers the entire input code. i.e., in situations where a SESE node cannot be analyzed due to its high CC value, we analyze all its children. Additionally, basic blocks that do not belong to any region in the tree are included to ensure complete code coverage.</p><p>This process is illustrated in Figure <ref type="figure">3</ref> step 1 using the CFG and SESE in Figure <ref type="figure" target="#fig_1">2</ref> as an example, with a CC threshold of 2. In this example, the SESE regions 2 and 3 are selected, and their paths are fully explored (step 2 in Figure <ref type="figure">3</ref>).</p></div>
<div><head>Management of loops</head><p>As explained above, the enumeration of paths in SESE regions ignores the back edges of loops. Therefore, all paths in a given loop are explored only for one iteration. Obtaining the execution context of any BB to be executed after a loop requires considering several loop Step 1 : Build the SESE tree from CFG, and choose the SESEs to list all the paths inside according to their Cyclomatic Complexity (CC)</p><p>Step 3 : Unroll loops using all available loop analysis results (loop tree, max iteration, head node, back edge, loop body)</p><p>Step 2 : Enumerate all paths within chosen SESE region</p></div>
<div><head>Figure 3</head><p>Example of the different steps for context generation, where the cyclomatic complexity limit is set to 2 and the context size is set to 3 BBs.</p><p>iterations. This is achieved in <software>CAWET</software> using (virtual) unrolling: the context of a loop is composed of several iterations of the loop body (from zero to the loop's maximum number of iterations).</p><p>As the path followed may differ across iterations, generating all possible contexts may lead to a combinatorial explosion. This issue is addressed by restricting the number of BBs added by the unrolling process for the loop body to a fixed value, the hyperparameter context size of <software ContextAttributes="used">CAWET</software>. In the presence of nested loops, the context of the inner loops is generated first, to be further used to generate the context for outer loops. This is performed using a bottom-up traversal of the loop nesting tree of every CFG <ref type="foot" target="#foot_2">3</ref> .</p><p>The result of the loop unrolling process on our example is given in Figure <ref type="figure">3</ref> step 3, for SESE 3. Three contexts are generated, corresponding respectively to 1, 2, and 3 executions of the loop. Note that, at this step, the size of the contexts of SESE regions may be longer than the context size hyperparameter.</p></div>
<div><head>Per BB context generation</head><p>The execution traces for the different SESE regions, after loop unrolling, are used to generate the context list of every basic block, as depicted in Figure <ref type="figure">3</ref>  In some cases, the initial nodes of some SESE sub-regions are smaller than the context size hyperparameter. To address this issue, we look for the preceding SESE region or BB to access the end of its traces. The peeked-on edges are shown in Figure <ref type="figure">3</ref>; they can easily be found by looking at the end of the traces of all the BB that occur before this trace. The obtained information can then be used as context for the start nodes of the current SESE region, provided we can find a region before the current one.</p><p>As an example, Figure <ref type="figure">3</ref> step 5 shows that the context of BB 5 can be augmented by peeking at the execution trace of SESE 2.</p></div>
<div><head n="2.3.3">Basic Block WCET estimation and program WCET calculation</head><p>After generating all possible limited-size contexts for each BB, we move on to estimating its WCET. This involves predicting the execution time of the BB under study for all its contexts. In an architecture without a cache, the maximum estimated time is selected as the worst-case scenario. If the target architecture includes a cache, we keep track of the two highest estimated execution values to account for cache effects. The largest value represents the first execution of the basic block within a loop, which is typically long, while the other value represents subsequent executions of the same basic block, which may be shorter <ref type="foot" target="#foot_3">4</ref> . The WCET of BBs is then fed into a static WCET estimation tool to calculate the WCET of the overall program using standard techniques such as IPET <ref type="bibr" target="#b34">[36]</ref>.</p></div>
<div><head n="3">Experimental setup</head><p>This Section provides a comprehensive description of the experimental setup used to evaluate <software>CAWET</software> on multiple ARM Cortex targets, specifically M4, M7, and A53. The programs used to train <software ContextAttributes="used">CAWET</software> and evaluate the quality of predictions are first described in Section 3.1.</p><p>The context-agnostic baselines <software>CAWET</software> is compared to are presented in Section 3.2, followed by an introduction to the software and hardware environments in Section 3.3. The setups for the learning and prediction phases of <software ContextAttributes="used">CAWET</software> are presented respectively in Section 3.4 and 3.5.</p></div>
<div><head n="3.1">Dataset and benchmarks</head><p><software ContextAttributes="used">CAWET</software> training consists of two steps: (self-supervised) pre-training and fine-tuning. We have pre-trained <software ContextAttributes="used">CAWET</software> on a large number of BBs in order for the Transformer to learn the assembly language under study, using <software ContextAttributes="used">CodeNet</software> <ref type="bibr" target="#b26">[28]</ref>. <software ContextAttributes="used">CodeNet</software> is a collection of solutions submitted by the public to competitive programming websites. It contains approximately 900,000 C programs, which we cross-compile to the target architecture and disassemble using GNU binary utilities using <software ContextAttributes="used">objdump</software>. The textual format produced by <software ContextAttributes="used">objdump</software>, after some basic parsing (e.g., extraction of addresses, separation of BBs) allows the creation of a large pre-training set. This pre-training set is used to build a vocabulary model with sentence piece <ref type="bibr" target="#b22">[23]</ref>. Once the model (sentence piece model) has been trained, it is then used to tokenize any binary programs written with the target instruction set. To fine-tune <software ContextAttributes="used">CAWET</software> on basic blocks with their context, we have used a diverse and publicly available set of programs:</p><p>The Algorithms<ref type="foot" target="#foot_4">5</ref> , <software ContextAttributes="used">MiBench</software> <ref type="bibr" target="#b14">[15]</ref> and <software ContextAttributes="used">Polybench</software> <ref type="bibr" target="#b35">[37]</ref>. Table <ref type="table" target="#tab_0">1</ref> gives a short description of each benchmark suite, the number of programs it contains, and the total number of BBs encountered when executing the programs. To validate the quality of the WCET predictions provided by <software ContextAttributes="used">CAWET</software>, we use a subset of the codes from the <software ContextAttributes="used">TacleBench</software> benchmark suite <ref type="bibr" target="#b12">[13]</ref> whose characteristics are given in Table <ref type="table" target="#tab_1">2</ref>. We chose these codes because: (i) the programs are analyzable by static WCET estimation tools, and in particular, they contain loop-bound annotations; (ii) they come with input data known to trigger the worst-case execution paths; (iii) they are used in our closest competitor WE-HML <ref type="bibr" target="#b1">[2]</ref>, allowing us to compare <software ContextAttributes="used">CAWET</software> with this work. Note that the selected <software ContextAttributes="used">TacleBench</software> programs were not used during any of the two steps of the training phase.</p></div>
<div><head n="3.2">Context-agnostic baselines</head><p><software ContextAttributes="used">CAWET</software> is evaluated by comparing it to two context-agnostic WCET predictors. The first one is a Multi-Layer Perceptron regressor (loosely called a neural network (NN)). Although not a naive approach, the neural network is a feed-forward architecture that does not incorporate sequential information and requires a fixed-size input. Our implementation of the NN employs a total of 233 static features of the basic blocks as input, including the proportion of different machine instruction types (e.g., MOV, ADD, LDR). We used a greedy search algorithm to determine optimal hyperparameters for the NN, including the number of hidden layers, optimizer, learning rate, and loss function. Based on the validation dataset, the ideal parameters were determined to be hidden layer sizes=(512, 256, 128), learning rate='adaptive', learning rate init=0.001, solver='adam'. The other baseline <software ContextAttributes="used">CAWET</software> is compared with is WE-HML, a hybrid ML-based WCET estimation technique presented in <ref type="bibr" target="#b1">[2]</ref>. The best performing ML algorithm of <ref type="bibr" target="#b1">[2]</ref> (Neural Network trained to account for cache effects) is used. <software ContextAttributes="used">CAWET</software> is compared to WE-HML for the Cortex A53 processor only, a processor for which the results of WE-HML were available.</p></div>
<div><head n="3.3">Hardware and software setups</head><p>Accurate timing values must be employed whenever possible when training and validating <software ContextAttributes="used">CAWET</software>, and the method used to obtain the timing values should not interfere with the execution of the code, a phenomenon commonly known as the probe effect. <software ContextAttributes="used">CAWET</software> either uses a hardware-based approach or a software solution when the hardware-based solution is not accessible. The hardware solution leverages the Joint Test Action Group (JTAG) interface. The <software ContextAttributes="used">J-Trace Pro</software> trace solution from <ref type="bibr">Segger [31]</ref> is used to connect to the JTAG interface of the target processor (in our case Cortex-M4 and Cortex-M7), in conjunction with <software ContextAttributes="used">Ozone</software> <ref type="bibr" target="#b13">[14]</ref>, a cross-platform debugger and performance analyzer. <software ContextAttributes="used">Ozone</software> generates execution traces that provide the value of the cycle counter, the instruction's address, opcode, and operands, as well as the corresponding assembly code for each instruction. The software solution involves adding code instrumentation to measure the execution time of individual basic blocks (BB) in a program. To provide context and assembly code for the timed BB, we retrieve the execution trace using <software ContextAttributes="used">GDB</software> (the GNU <software ContextAttributes="used">Debugger</software>). The software solution is only used when no JTAG interface is available since it is prone to probe effects and requires significant human effort to implement.</p><p>Our experiments are performed on various Arm processors, whose characteristics are summarized in Table <ref type="table" target="#tab_2">3</ref>. We initially focus on the Cortex-M4 processor, which has a simple in-order pipeline with three stages and no cache. This processor allows us to validate our method on a deterministic processor with precise timing measurements through the JTAG interface. Then, we evaluate our approach to the more advanced Cortex-M7 processor. This processor features a 6-stage in-order pipeline, data and instruction caches, and a branch predictor. Finally, we use a more complex processor, Cortex-A53, which is hosted in a Raspberry Pi 3. This superscalar processor has two data and instruction cache levels: an 8-stage in-order pipeline and a branch predictor. The Cortex-A53 has no JTAG interface; the reading of the cycle counter is used for the timing measurements. Using this commercial off-the-shelf (COTS) hardware is part of the experiments in the WE-HML approach <ref type="bibr" target="#b1">[2]</ref>. </p></div>
<div><head n="3.4">Setup for the learning phase</head><p><software ContextAttributes="used">PyTorch</software> was used to implement the learning models, which were then trained on a Tesla V100. Each setting (processor) required two days for <software ContextAttributes="used">CAWET</software> training: 1,5 days for pre-training and 0,5 days to fine-tune the model. To avoid underestimating execution times, we employed the Root Mean Squared Logarithmic Error (RMSLE) loss function provided in Equation <ref type="formula">1</ref>, which tends to penalize underestimations more heavily than overestimations. We also incorporated an additional penalty for predictions that underestimated the execution time, according to Equation <ref type="formula">2</ref>. We artificially modify the target value in the loss when the prediction is too low. When computing the loss, this is done by increasing the target with the predicting error (target -prediction).</p><p>RMSLE(target, predict) = (log (target + 1 ) -log (predict + 1 )) 2  (1)</p><formula xml:id="formula_1">UsedTarget = target if target ≤ prediction target + (target -prediction) if target &gt; prediction (2)</formula></div>
<div><head n="3.5">Setup for the prediction phase</head><p>The CFG, the SESE tree, and the loop tree is generated by the <software ContextAttributes="used">Heptane</software> WCET estimation tool <ref type="bibr" target="#b15">[16]</ref>. These structures are used to construct the list of contexts for each BB. Then, we predict the WCET for each BB using <software ContextAttributes="used">CAWET</software>. Finally, we employ <software ContextAttributes="used">Heptane</software>'s IPET to determine the overall WCET of the program.</p><p>To create the contexts, we opted for a cyclomatic complexity of 5, as this value has been shown empirically to generate paths within a reasonable amount of time (less than five minutes to generate traces for each basic block in the 13 programs previously described). Since the best context size varies across different architectures, we only considered a fixed number N of consecutive basic blocks, where N corresponds to the number of pipeline stages.</p></div>
<div><head n="4">Results</head><p>The quality of WCET predictions for the Cortex M4 and Cortex M7 architectures is evaluated in Sections 4.1 and 4.2. The effect of the different features of <software ContextAttributes="used">CAWET</software> on the quality of the predictions is studied in Section 4.3. Finally, <software ContextAttributes="used">CAWET</software> is evaluated in Subsection 4.4 on a more complex processor, the Cortex-A53, using a software measurement method and an operating system, allowing us to compare the WCET predictions of <software ContextAttributes="used">CAWET</software> with those of WE-HML <ref type="bibr" target="#b1">[2]</ref>. The results show that <software ContextAttributes="used">CAWET</software> is twice less pessimistic than the NN baseline on average, using the Mean Absolute Error 6 on the RPE (i.e., Error = RPE). This can be explained by the fact that: (i) neural networks do not consider the ordering of instructions in BBs (ii) neural networks are context-agnostic. We also observe that neither <software ContextAttributes="used">CAWET</software> nor the NN baseline underestimates the WCET since all RPE are positive.  Impact of the context size. Table <ref type="table" target="#tab_5">5</ref> shows the considered context size's impact on the prediction quality. Four values are considered: 0 (no context), 1 BB as context, 3 BBs as context, and 20 BBs as context. The results show that, on average, the error is minimal when the context size is 3 BBs. Accounting for the execution context of BBs is beneficial to the quality of the predictions up to a context size of 3. Taking into account larger context sizes results in much higher error values. One possible explanation for these higher error values is that the context vector is being disrupted by extensive information that cannot be processed efficiently with the current TXL architecture. In future works, we plan to examine this phenomenon more closely, which will require substantial computing resources.</p></div>
<div><head n="4.1">Quality of WCET predictions for the Cortex M4</head><p>A. N. Amalou, E. Fromont, and I. Puaut 7:13</p></div>
<div><head n="4.2">Quality of WCET predictions for the Cortex M7</head><p>The Cortex M7 processor is more complex than the Cortex M4. It features a 6-stage in-order pipeline, data, and instruction caches with random cache replacement and a branch predictor. Table <ref type="table" target="#tab_6">6</ref> evaluates WCET predictions produced by <software ContextAttributes="used">CAWET</software> and the baseline NN for the Cortex M7, using a context size of 6 for <software ContextAttributes="used">CAWET</software>. The results show that even with no explicit support for caches, <software ContextAttributes="used">CAWET</software> never underestimates compared to the Maximum observed execution time (the max of 1000 executions) and is again more precise than the NN baseline. It should also be noted that the average MAE, both for <software ContextAttributes="used">CAWET</software> and NN, is, as one would expect, higher for the more complex Cortex M7 than for the very simple Cortex M4, showing that the tight timing analysis of complex processors is harder to achieve than the analysis of simpler ones.</p><p>Since the context size in <software ContextAttributes="used">CAWET</software> is limited, the reuse of code/data (with instruction/data caches) may not be fully taken into account by the model. We thus modified <software ContextAttributes="used">CAWET</software> to add a cache miss penalty to the WCET of a BB when the static cache analysis of <software ContextAttributes="used">Heptane</software> cannot guarantee a cache hit. The same procedure is applied to the NN baseline, and the results are reported in Table <ref type="table" target="#tab_7">7</ref>.</p><p>The integration of cache analysis results into <software>CAWET</software> and NN leads to more pessimistic WCETs for both techniques. Two factors explain this additional pessimism: (i) the static cache analysis for random cache replacement is inherently pessimistic; (ii) <software ContextAttributes="used">CAWET</software> already captures parts of the cache behavior due to its use of the execution contexts for BBs. Thus the impact of some cache misses may be counted twice.</p></div>
<div><head n="4.3">Impact of CAWET features (Cortex M4 and M7)</head><p>In this section, we analyze the effect of different features of <software ContextAttributes="used">CAWET</software> on the Relative Percentage Error (RPE): context accounting, peek-on mechanism, loop management, and using <software ContextAttributes="used">Heptane</software>'s cache analysis. Our study involves a comparison of the impact of each feature, starting with context accounting (A), followed by the peek-on mechanism (B), loop  unrolling (C), and finally, applying cache analysis (D). The results in Table <ref type="table" target="#tab_8">8</ref> show that incorporating the context (A) provides the most significant improvement to <software ContextAttributes="used">CAWET</software>, while the effects of peeking (B) and loop enrolling (C) are less substantial. Additionally, we can see that adding the cache analysis (D) in Cortex M7 has a considerable impact on the predictions, with a significant increase in pessimism. </p></div>
<div><head n="4.4">Quality of WCET predictions for the Cortex A53</head><p>The objectives of these experiments are twofold: (i) evaluate the WCET predictions produced by <software ContextAttributes="used">CAWET</software> for a more complex processor than the Cortex M7; (ii) be able to compare <software ContextAttributes="used">CAWET</software> to WE-HML <ref type="bibr" target="#b1">[2]</ref>, the related work closest to <software ContextAttributes="used">CAWET</software>, that targets this architecture. We re-use the very same experimental conditions as in WE-HML: software measurements of execution times, and execution on top of an operating system. The maximum measured BB execution time is used alongside its context to train <software ContextAttributes="used">CAWET</software>. We have collected 1000 measurements for each studied benchmark and kept the maximum execution time observed as a reference value to calculate the RPE. On the thousand measurements collected, we have also applied the probabilistic WCET technique as described in <ref type="bibr" target="#b28">[30]</ref>, where we set the probability to 10 -3 to provide another reference point than the MOET. Table <ref type="table" target="#tab_9">9</ref> shows the Maximum Observed Execution Times (MOET) and Relative Percentage Error (RPE) for all considered techniques: probabilistic WCET estimation, WE-HML, Vanilla <software ContextAttributes="used">CAWET</software>, and <software ContextAttributes="used">CAWET</software> modified with the results of static cache analysis. On all benchmarks but one (matrix1), <software ContextAttributes="used">CAWET</software> is much less pessimistic than WE-HML (even for the modified <software ContextAttributes="used">CAWET</software>). This is due to the significant pessimism introduced by WE-HML to account for caches (WE-HML evaluates cache effects by generating the worst possible cache pollution in loops regardless of the actual accesses performed in the loop).</p><p>Compared to the probabilistic technique, we observe that the pWCET is sometimes unsafe. This may come from rare outliers (due, for example, to the presence of an operating system) that are considered as WCET and that pWCET (smartly) ignores because they are sufficiently rare. It may also happen when pWCET is less pessimistic than <software>CAWET</software>. However, in general, pWCET techniques may miss the worst-case execution path in programs, whereas <software ContextAttributes="created">CAWET</software>, a hybrid technique, will not.</p></div>
<div><head n="5">Related works</head><p>The challenge of accurately estimating the WCET of programs has led to the development of various hybrid timing analysis techniques that are compared with <software>CAWET</software> below. These techniques can be broadly categorized into two types: those that use measurements to estimate the WCET of individual basic blocks and those that incorporate machine learning to learn the BB's timing patterns.</p></div>
<div><head n="1.">Hybrid WCET estimation techniques using measurements</head><p>AbsInt <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21]</ref> and Rapita <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b30">32]</ref>  </p></div>
<div><head n="2.">Hybrid WCET estimation techniques using ML</head><p>Several methods for estimating Worst-Case Execution Time (WCET) using Machine Learning (ML) have been proposed <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25]</ref>. Bonenfant et al. <ref type="bibr" target="#b4">[5]</ref> use worst-case event counts for training a neural network, that will be subsequently used to calculate the WCET of a program at an early stage. Similarly, the approaches proposed by Kumar <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25]</ref> estimate WCET using features extracted from the source code. These approaches disregard valuable information about the code flow and hide the compilation effects by operating at the source code or intermediate code level, which can bias the timing prediction. The research works presented in <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b1">2]</ref>, similarly to <software ContextAttributes="created">CAWET</software>, propose to extract features from the binary code and to use ML techniques to predict the WCET of individual basic However, contrary to <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b1">2]</ref>, <software ContextAttributes="created">CAWET</software> takes a more fine-grained approach, considering the context surrounding each basic block, and the dependencies between instructions within it to better consider hardware components such as the pipeline. <ref type="bibr" target="#b1">[2]</ref> accounts for data caches by simulating the worst possible data access pattern for basic blocks within loops, whereas <software ContextAttributes="created">CAWET</software> relies on static analysis through the <software ContextAttributes="created">Heptane</software> tool <ref type="bibr" target="#b15">[16]</ref> to obtain less pessimistic estimations of data cache behavior. <ref type="bibr" target="#b25">[26]</ref> propose a technique similar to linear regression to estimate the WCET from a set of end-to-end measurements. Unlike <software ContextAttributes="created">CAWET</software>, this approach uses static features and is thus not able to accurately predict pipeline effects. All approaches described in this section oversimplify the code characterization, either by using high-level abstractions of the source code or by relying on static features of basic blocks at the binary code level. In contrast, <software ContextAttributes="created">CAWET</software> operates on the flow of instructions using state-of-the-art ML techniques (Transformers-XL).</p></div>
<div><head n="3.">Machine Learning for contention prediction and throughput prediction</head><p>Brando et al. <ref type="bibr" target="#b5">[6]</ref> use neural networks to estimate the worst contention factor of programs using hardware event counters. Similarly, Courtaud et al. <ref type="bibr" target="#b6">[7]</ref> introduce a profiling tool that produces high-resolution profiles of the memory behavior of applications. They train a regressor using microbenchmarks to finally calculate contention. Even though these two studies rely heavily on ML, they focus on contention prediction on multi-core targets and not on WCET prediction for single-cores like <software ContextAttributes="used">CAWET</software>.</p><p>Deep PM <ref type="bibr" target="#b31">[33]</ref> and Ithemal [27] employ transformers and LSTMs, respectively, to predict the throughput of isolated basic blocks. However, <software ContextAttributes="used">CAWET</software> takes a different approach by incorporating the execution context to predict WCETs. Similarly, <software ContextAttributes="used">CATREEN</software> <ref type="bibr" target="#b0">[1]</ref> uses stacked LSTMs to forecast the average execution time of basic blocks in a contextualized manner, but it differs from <software ContextAttributes="used">CAWET</software> in its focus on average execution time rather than worst-case execution time.</p></div>
<div><head n="6">Conclusion</head><p>In this paper, we presented <software>CAWET</software>: a hybrid approach that estimates the worst-case program timing for individual basic blocks in a program. Our approach uses static techniques to identify the longest execution path and an advanced machine learning architecture called transformer-XL to predict the worst-case execution time of each basic block. By considering the execution context formed by previously executed basic blocks, <software ContextAttributes="used">CAWET</software> is able to account for the micro-architecture of the processor pipeline without explicit modeling. The technique is demonstrated to be empirically reliable and less pessimistic than its competitors in experiments on the <software ContextAttributes="used">TacleBench</software> benchmarks for different target processors. While there are still challenges to be addressed, such as the need for more accurate context for less pessimistic predictions, <software ContextAttributes="used">CAWET</software> offers a promising solution for predicting worst-case execution times for Commercial off-the-shelf processors. In future work, the technique will be further explored for processors with out-of-order pipelines, such as Cortex A9 or A72.</p></div><figure xml:id="fig_0"><head>3 .Figure 1</head><label>31</label><figDesc>Figure 1 Overview of CAWET.</figDesc></figure>
<figure xml:id="fig_1"><head>Figure 2 A</head><label>2</label><figDesc>Figure 2 A CFG example transformed into a SESE tree and annotated with cyclomatic complexity.</figDesc></figure>
<figure xml:id="fig_2"><head>Step 4 : 3 Step 5 :{</head><label>435</label><figDesc>Iterate over all BBs to build context Context size = Size of pipeline = If BB without context =&gt; Peek on and update context with respect to Context size BB under analysis: Different possible contexts } 5 7</figDesc></figure>
<figure xml:id="fig_3"><head>step 4 .</head><label>4</label><figDesc>The size of each context is limited to the context size hyperparameter of CAWET.</figDesc></figure>
<figure xml:id="fig_4"><head>6</head><label /><figDesc>Mean Absolute Error: MAE = 1 n * n |Error|</figDesc></figure>
<figure type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc>The benchmarks used for training CAWET.</figDesc><table><row><cell>Dataset name</cell><cell>Description</cell><cell cols="2">Nb. of programs Nb. of BB</cell></row><row><cell>The Algorithms</cell><cell>Collection of open-source implementations of a variety of algorithms implemented in C</cell><cell>200</cell><cell>12123</cell></row><row><cell>PolyBench</cell><cell>A collection of benchmarks containing static control parts. The purpose is to uniformize the execution and monitoring of kernels</cell><cell>30</cell><cell>11224</cell></row><row><cell>MiBench</cell><cell>A free, commercially representative embedded benchmark suite</cell><cell>14</cell><cell>8324</cell></row><row><cell>Total</cell><cell /><cell>244</cell><cell>31671</cell></row></table></figure>
<figure type="table" xml:id="tab_1"><head>Table 2</head><label>2</label><figDesc>Selected TacleBench codes used to evaluate the quality of the predictions.</figDesc><table><row><cell>Name</cell><cell>Description</cell></row><row><cell>bs</cell><cell>Binary search in an array</cell></row><row><cell>bsort</cell><cell>Bubble sort algorithm</cell></row><row><cell>countnegative</cell><cell>Basic counting on arrays</cell></row><row><cell>crc</cell><cell>Cyclic redundancy codes</cell></row><row><cell>expint</cell><cell>Exponential integral function</cell></row><row><cell>fdct</cell><cell>Fast discrete cosine transform.</cell></row><row><cell>fir</cell><cell>Finite impulse response filter</cell></row><row><cell>h264 dec</cell><cell>H.264 block decoding functions</cell></row><row><cell>insertsort</cell><cell>Insertion sort</cell></row><row><cell>jfdctint</cell><cell>Discrete-cosine transformation</cell></row><row><cell>matrix1</cell><cell>Generic matrix multiplication</cell></row><row><cell>ns</cell><cell>Search in 4-dimension array</cell></row><row><cell>petrinet</cell><cell>Petri net simulation</cell></row></table></figure>
<figure type="table" xml:id="tab_2"><head>Table 3</head><label>3</label><figDesc>Summary of the processors used and their micro-architectural features.</figDesc><table><row><cell>Target</cell><cell cols="2">Measurement solution OS?</cell><cell cols="3">Pipeline/#stages Branch predictor Cache memory and proprieties</cell></row><row><cell>Cortex-M4</cell><cell>Hardware (JTAG)</cell><cell cols="2">Baremetal In-order/3</cell><cell>No</cell><cell>No</cell></row><row><cell>Cortex-M7</cell><cell>Hardware (JTAG)</cell><cell cols="2">Baremetal In-order/6</cell><cell>Yes</cell><cell>Yes data and instruction cache, L1, random replacement policy</cell></row><row><cell>Cortex-A53 (also used in [2])</cell><cell>Software</cell><cell>Linux</cell><cell>In-order/8</cell><cell>Yes</cell><cell>Yes data and instruction cache, L2, random replacement policy</cell></row></table></figure>
<figure type="table" xml:id="tab_3"><head>Table 4</head><label>4</label><figDesc>compares the WCET predictions of the selected TacleBench programs on the deterministic cache-less architecture Cortex M4. WCET predictions of BBs are either obtained by CAWET or by the context-agnostic Neural Network (NN) baseline described in Section 3.2. The table gives for the two techniques both the WCET prediction in cycles and the Relative Percentage Error RPE defined as RP E =(P redict-Actual)   </figDesc><table><row><cell>Actual</cell><cell>*  100. A context size</cell></row></table></figure>
<figure type="table" xml:id="tab_4"><head>Table 4</head><label>4</label><figDesc>Comparison of WCET predictions for CAWET and a Neural Network (NN) baseline on TacleBench programs for Cortex-M4.</figDesc><table><row><cell>Benchmark</cell><cell>Maximum observed execution time (Cycles)</cell><cell>NN estimations (Cycles)</cell><cell>NN RPE (%)</cell><cell>CAWET estimations (Cycles)</cell><cell>CAWET RPE (%)</cell></row><row><cell>bs</cell><cell>140</cell><cell>307</cell><cell>119.2</cell><cell>272</cell><cell>94.3</cell></row><row><cell>bsort</cell><cell>317279</cell><cell>414882</cell><cell>30.7</cell><cell>374712</cell><cell>18.1</cell></row><row><cell>countnegative</cell><cell>9638</cell><cell>14047</cell><cell>45.7</cell><cell>12858</cell><cell>33.4</cell></row><row><cell>crc</cell><cell>78496</cell><cell>102005</cell><cell>29.9</cell><cell>92872</cell><cell>18.3</cell></row><row><cell>expint</cell><cell>5683</cell><cell>7758</cell><cell>36.5</cell><cell>5727</cell><cell>0.7</cell></row><row><cell>fdct</cell><cell>7308</cell><cell>10557</cell><cell>44.4</cell><cell>8606</cell><cell>17.7</cell></row><row><cell>fir</cell><cell>6882</cell><cell>10844</cell><cell>57.5</cell><cell>7490</cell><cell>8.8</cell></row><row><cell>h264_dec</cell><cell>573752</cell><cell>661037</cell><cell>15.2</cell><cell>607918</cell><cell>5.9</cell></row><row><cell>insertsort</cell><cell>3125</cell><cell>3964</cell><cell>26.8</cell><cell>3898</cell><cell>24.7</cell></row><row><cell>jfdctint</cell><cell>7761</cell><cell>11454</cell><cell>47.5</cell><cell>9968</cell><cell>28.4</cell></row><row><cell>matrix1</cell><cell>440243</cell><cell>577831</cell><cell>31.2</cell><cell>564921</cell><cell>28.3</cell></row><row><cell>ns</cell><cell>28444</cell><cell>45026</cell><cell>58.2</cell><cell>34367</cell><cell>20.8</cell></row><row><cell>petrinet</cell><cell>3283</cell><cell>4159</cell><cell>26.7</cell><cell>3592</cell><cell>9.4</cell></row><row><cell>Avg. MAE</cell><cell>-</cell><cell>-</cell><cell>43.80</cell><cell>-</cell><cell>23.8</cell></row></table></figure>
<figure type="table" xml:id="tab_5"><head>Table 5</head><label>5</label><figDesc>Impact of the context size on the Mean Absolute Error (MAE) on TacleBench programs for Cortex-M4.</figDesc><table><row><cell>Benchmark</cell><cell cols="4">Context 0 Context 1 BB Context Pipeline size (3) Context 20 BB</cell></row><row><cell>bs</cell><cell>104,2%</cell><cell>97,6%</cell><cell>94,3%</cell><cell>117,9%</cell></row><row><cell>bsort</cell><cell>22,4%</cell><cell>27,6%</cell><cell>18,1%</cell><cell>34,2%</cell></row><row><cell>countnegative</cell><cell>47,3%</cell><cell>38,9%</cell><cell>33,4%</cell><cell>46,2%</cell></row><row><cell>crc</cell><cell>19,6%</cell><cell>11,1%</cell><cell>18,3%</cell><cell>19,3%</cell></row><row><cell>expint</cell><cell>21%</cell><cell>15,9%</cell><cell>0,7%</cell><cell>21,6%</cell></row><row><cell>fdct</cell><cell>39,2%</cell><cell>28,4%</cell><cell>17,7%</cell><cell>38,2%</cell></row><row><cell>fir</cell><cell>34,5%</cell><cell>31,6%</cell><cell>8,8%</cell><cell>39%</cell></row><row><cell>h264_dec</cell><cell>30,2%</cell><cell>22,1%</cell><cell>5,9%</cell><cell>30,9%</cell></row><row><cell>insertsort</cell><cell>15,5%</cell><cell>25,6%</cell><cell>24,7%</cell><cell>27,4%</cell></row><row><cell>jfdctint</cell><cell>34,6%</cell><cell>31,9%</cell><cell>28,4%</cell><cell>41,9%</cell></row><row><cell>matrix1</cell><cell>36,1%</cell><cell>33,3%</cell><cell>28,3%</cell><cell>53,4%</cell></row><row><cell>ns</cell><cell>45,7%</cell><cell>33,8%</cell><cell>20,8%</cell><cell>41,3%</cell></row><row><cell>petrinet</cell><cell>11%</cell><cell>17,2%</cell><cell>9,4%</cell><cell>16%</cell></row><row><cell>Avg. MAE</cell><cell>35,5%</cell><cell>31,9%</cell><cell>23,8%</cell><cell>40,6%</cell></row></table></figure>
<figure type="table" xml:id="tab_6"><head>Table 6</head><label>6</label><figDesc>Comparison of WCET predictions for CAWET (vanilla) and a Neural Network (NN) baseline for Cortex-M7.</figDesc><table><row><cell>Benchmark</cell><cell>Maximum observed execution time (Cycles)</cell><cell>NN estimations (Cycles)</cell><cell>NN RPE (%)</cell><cell>CAWET estimations (Cycles)</cell><cell>CAWET RPE (%)</cell></row><row><cell>bs</cell><cell>140</cell><cell>307</cell><cell>119.3</cell><cell>280</cell><cell>100.0</cell></row><row><cell>bsort</cell><cell>191406</cell><cell>464616</cell><cell>142.7</cell><cell>376784</cell><cell>96.9</cell></row><row><cell>countnegative</cell><cell>6956</cell><cell>15874</cell><cell>128.2</cell><cell>13904</cell><cell>99.9</cell></row><row><cell>crc</cell><cell>47476</cell><cell>98473</cell><cell>107.4</cell><cell>88668</cell><cell>86.8</cell></row><row><cell>expint</cell><cell>3592</cell><cell>8260</cell><cell>130.0</cell><cell>7140</cell><cell>98.8</cell></row><row><cell>fdct</cell><cell>4957</cell><cell>12044</cell><cell>143.0</cell><cell>9341</cell><cell>88.4</cell></row><row><cell>fir</cell><cell>4625</cell><cell>10856</cell><cell>134.7</cell><cell>9132</cell><cell>97.4</cell></row><row><cell>h264_dec</cell><cell>362349</cell><cell>779905</cell><cell>115.2</cell><cell>706162</cell><cell>94.9</cell></row><row><cell>insertsort</cell><cell>1760</cell><cell>4188</cell><cell>138.0</cell><cell>3414</cell><cell>94.0</cell></row><row><cell>jfdctint</cell><cell>4011</cell><cell>11877</cell><cell>196.1</cell><cell>10215</cell><cell>154.7</cell></row><row><cell>matrix1</cell><cell>301866</cell><cell>660739</cell><cell>118.9</cell><cell>644668</cell><cell>113.6</cell></row><row><cell>ns</cell><cell>21253</cell><cell>46004</cell><cell>116.5</cell><cell>41167</cell><cell>93.7</cell></row><row><cell>petrinet</cell><cell>1595</cell><cell>3741</cell><cell>134.5</cell><cell>3342</cell><cell>109.5</cell></row><row><cell>Avg. MAE</cell><cell>-</cell><cell>-</cell><cell>132.7</cell><cell>-</cell><cell>102.2</cell></row></table></figure>
<figure type="table" xml:id="tab_7"><head>Table 7</head><label>7</label><figDesc>Comparison of WCET predictions for CAWET and a Neural Network (NN) baseline for Cortex-M7 when accounting for the static cache analysis results.</figDesc><table><row><cell>Benchmark</cell><cell>Maximum observed execution time (Cycles)</cell><cell>NN estimations (Cycles)</cell><cell>NN RPE (%)</cell><cell>CAWET estimations (Cycles)</cell><cell>CAWET RPE (%)</cell></row><row><cell>bs</cell><cell>140</cell><cell>537</cell><cell>283.6</cell><cell>516</cell><cell>268.6</cell></row><row><cell>bsort</cell><cell>191406</cell><cell>840959</cell><cell>339.4</cell><cell>699961</cell><cell>265.7</cell></row><row><cell>countnegative</cell><cell>6956</cell><cell>33552</cell><cell>382.3</cell><cell>26997</cell><cell>288.1</cell></row><row><cell>crc</cell><cell>47476</cell><cell>184152</cell><cell>287.9</cell><cell>166025</cell><cell>249.7</cell></row><row><cell>expint</cell><cell>3592</cell><cell>14528</cell><cell>304.5</cell><cell>12764</cell><cell>255.3</cell></row><row><cell>fdct</cell><cell>4957</cell><cell>34861</cell><cell>603.3</cell><cell>20076</cell><cell>305.0</cell></row><row><cell>fir</cell><cell>4625</cell><cell>18088</cell><cell>291.1</cell><cell>16554</cell><cell>257.9</cell></row><row><cell>h264_dec</cell><cell>362349</cell><cell>1281479</cell><cell>253.7</cell><cell>1403042</cell><cell>287.2</cell></row><row><cell>insertsort</cell><cell>1760</cell><cell>6040</cell><cell>243.2</cell><cell>7105</cell><cell>303.7</cell></row><row><cell>jfdctint</cell><cell>4011</cell><cell>34044</cell><cell>748.8</cell><cell>19663</cell><cell>390.2</cell></row><row><cell>matrix1</cell><cell>301866</cell><cell>2021791</cell><cell>569.8</cell><cell>1249975</cell><cell>314.1</cell></row><row><cell>ns</cell><cell>21253</cell><cell>97870</cell><cell>360.5</cell><cell>76205</cell><cell>258.6</cell></row><row><cell>petrinet</cell><cell>1595</cell><cell>5813</cell><cell>264.5</cell><cell>6372</cell><cell>299.5</cell></row><row><cell>Avg. MAE</cell><cell>-</cell><cell>-</cell><cell>398.1</cell><cell>-</cell><cell>289.6</cell></row></table></figure>
<figure type="table" xml:id="tab_8"><head>Table 8</head><label>8</label><figDesc /><table><row><cell cols="3">RPE measures of CAWET predictions for Cortex-M4 and Cortex-M7 when adding</cell></row><row><cell cols="3">different features of CAWET: context accounting (A), peek-on mechanism (B), loop unrolling (C),</cell></row><row><cell>and cache analysis (D).</cell><cell /><cell /></row><row><cell cols="3">Feature(s) \Optimization Cortex-M4 RPE (%) Cortex-M7 RPE (%)</cell></row><row><cell>None</cell><cell>35.5</cell><cell>142.5</cell></row><row><cell>A</cell><cell>25.2</cell><cell>130.2</cell></row><row><cell>A+B</cell><cell>24.9</cell><cell>126.1</cell></row><row><cell>A+B+C</cell><cell>23.8</cell><cell>102.2</cell></row><row><cell>A+B+C+D</cell><cell>NA</cell><cell>288.0</cell></row></table></figure>
<figure type="table" xml:id="tab_9"><head>Table 9</head><label>9</label><figDesc>Comparison of WCET predictions on Cortex A53 for: CAWET, a probabilistic WCET solution, WE-HML, CAWET (vanilla), and a modified CAWET to account for static cache analysis results.</figDesc><table><row><cell>Benchmark</cell><cell>MOET (Cycles)</cell><cell>pWCET10 -3 RPE (%)</cell><cell>WE-HML RPE (%)</cell><cell>Vanilla CAWET RPE (%)</cell><cell>CAWET with cache analysis RPE (%)</cell></row><row><cell>bs</cell><cell>2568</cell><cell>43.8</cell><cell>177.1</cell><cell>97.0</cell><cell>122.8</cell></row><row><cell>bsort</cell><cell>358380</cell><cell>60.4</cell><cell>838.3</cell><cell>18.6</cell><cell>21.3</cell></row><row><cell>countnegative</cell><cell>29720</cell><cell>6.3</cell><cell>168.5</cell><cell>70.2</cell><cell>169.6</cell></row><row><cell>crc</cell><cell>66867</cell><cell>64.2</cell><cell>315.2</cell><cell>53.8</cell><cell>86.5</cell></row><row><cell>expint</cell><cell>6122</cell><cell>1.0</cell><cell>352.5</cell><cell>29.0</cell><cell>80.3</cell></row><row><cell>fdct</cell><cell>8877</cell><cell>1.2</cell><cell>195.0</cell><cell>25.5</cell><cell>52.2</cell></row><row><cell>fir</cell><cell>7646</cell><cell>-13.6</cell><cell>391.4</cell><cell>31.1</cell><cell>114.9</cell></row><row><cell>h264_dec</cell><cell>426327</cell><cell>120.4</cell><cell>590.0</cell><cell>76.5</cell><cell>88.4</cell></row><row><cell>insertsort</cell><cell>3042</cell><cell>75.8</cell><cell>297.6</cell><cell>29.6</cell><cell>40.2</cell></row><row><cell>jfdctint</cell><cell>8070</cell><cell>51.1</cell><cell>296.1</cell><cell>44.4</cell><cell>57.5</cell></row><row><cell>matrixl</cell><cell>21380</cell><cell>5.8</cell><cell>207.1</cell><cell>223.9</cell><cell>236.6</cell></row><row><cell>ns</cell><cell>22018</cell><cell>-0.3</cell><cell>731.1</cell><cell>108.6</cell><cell>119.5</cell></row><row><cell>petrinet</cell><cell>3920</cell><cell>30.7</cell><cell>1865.3</cell><cell>2.3</cell><cell>30.8</cell></row><row><cell>Avg. MAE</cell><cell>-</cell><cell>36.5</cell><cell>494.2</cell><cell>62.4</cell><cell>93</cell></row></table></figure>
<figure type="table" xml:id="tab_10"><head>0 2 3 7:16 CAWET: Context-Aware Worst-Case Execution Time Estimation their</head><label /><figDesc>have developed hybrid WCET estimation solutions, namely Timeweaver and Rapitime, which rely on hardware-assisted measurements (e.g., static tool.Kirner et al.  propose in<ref type="bibr" target="#b21">[22]</ref> to perform measurements on code segments larger than a basic block and propose techniques to enforce coverage of the measured segments. In contrast to these research works, CAWET does not use measurements to estimate the WCET of code snippets. Instead, it utilizes a timing model learned through Machine Learning (ML) techniques.</figDesc><table /><note><p><p>JTAG) and manual annotations (way/trace points and interest points, respectively) to measure the WCET on code snippets, and then estimate the WCET of the program with</p>E C R T S 2</p></note></figure>
			<note place="foot" n="1" xml:id="foot_0"><p>A basic block is defined as a sequence of instructions with a single entry point at the beginning and a single exit point at the end, without any branching or jumping to other instructions within the block.</p></note>
			<note place="foot" n="2" xml:id="foot_1"><p>DFS traversal ignores loop back-edges. Loop management is described later in this Section.</p></note>
			<note place="foot" n="3" xml:id="foot_2"><p>A loop nesting tree is a tree data structure used to represent nested loops. Each node in the tree represents a loop, and the edges between the nodes represent the nesting relationship between the loops.</p></note>
			<note place="foot" n="4" xml:id="foot_3"><p>Since the context size is limited, the predicted timing values may be too optimistic. We, therefore, analyze in Section 4.2 and 4.4 a technique that applies static cache analysis, and we add the overhead obtained by this analysis to the timing values produced by <software>CAWET</software>.</p></note>
			<note place="foot" n="5" xml:id="foot_4"><p>Available here: https://github.com/TheAlgorithms/C</p></note>
			<note place="foot" n="27" xml:id="foot_5"><p>Charith Mendis, Alex Renda, Saman Amarasinghe, and Michael Carbin. Ithemal: Accurate, portable and fast basic block throughput estimation using deep neural networks. In International Conference on machine learning, pages 4505-4515. PMLR, 2019.</p></note>
		</body>
		<back>
			<div type="annex">
<div><head>A Appendix</head></div>
<div><head>Sentence piece tokenizer</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Catreen: Context-aware code timing estimation with stacked recurrent networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Abderaouf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elisa</forename><surname>Amalou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Isabelle</forename><surname>Fromont</surname></persName>
		</author>
		<author>
			<persName><surname>Puaut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">34rd IEEE International Conference on Tools with Artificial Intelligence</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note>ICTAI)</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">We-hml: hybrid wcet estimation using machine learning for architectures with caches</title>
		<author>
			<persName><forename type="first">Isabelle</forename><surname>Abderaouf N Amalou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gilles</forename><surname>Puaut</surname></persName>
		</author>
		<author>
			<persName><surname>Muller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 27th International Conference on Embedded and Real-Time Computing Systems and Applications (RTCSA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">2021. 2021</date>
			<biblScope unit="page" from="31" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Otawa: An open toolbox for adaptive wcet analysis</title>
		<author>
			<persName><forename type="first">Clément</forename><surname>Ballabriga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hugues</forename><surname>Cassé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christine</forename><surname>Rochange</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Sainrat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Software Technologies for Embedded and Ubiquitous Systems: 8th IFIP WG 10.2 International Workshop, SEUS 2010</title>
		<meeting><address><addrLine>Waidhofen/Ybbs, Austria</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010">October 13-15, 2010. 2010</date>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="35" to="46" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Hybrid measurement-based wcet analysis at the source level using object-level traces</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Betts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Merriam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Bernat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">10th International Workshop on Worst-Case Execution Time Analysis (WCET 2010)</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
		<respStmt>
			<orgName>Schloss Dagstuhl -Leibniz-Zentrum fuer Informatik</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Early wcet prediction using machine learning</title>
		<author>
			<persName><forename type="first">Armelle</forename><surname>Bonenfant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denis</forename><surname>Claraz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marianne</forename><forename type="middle">De</forename><surname>Michiel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Sotin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">17th International Workshop on Worst-Case Execution Time Analysis (WCET 2017)</title>
		<imprint>
			<publisher>OASICs, Dagstuhl Publishing</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Using quantile regression in neural networks for contention prediction in multicore processors</title>
		<author>
			<persName><forename type="first">Axel</forename><surname>Brando</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Isabel</forename><surname>Serra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Enrico</forename><surname>Mezzetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaume</forename><surname>Abella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><forename type="middle">J</forename><surname>Cazorla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">34th Euromicro Conference on Real-Time Systems (ECRTS 2022)</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
		<respStmt>
			<orgName>Schloss Dagstuhl -Leibniz-Zentrum für Informatik</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Improving prediction accuracy of memory interferences for multicore platforms</title>
		<author>
			<persName><forename type="first">Cédric</forename><surname>Courtaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Sopena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gilles</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Gracia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pérez</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE Real-Time Systems Symposium (RTSS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="246" to="259" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Transformer-xl: Attentive language models beyond a fixed-length context</title>
		<author>
			<persName><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Quoc V Le</surname></persName>
		</author>
		<author>
			<persName><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.02860</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Safe measurement-based wcet estimation</title>
		<author>
			<persName><forename type="first">Jean-François</forename><surname>Deverge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Isabelle</forename><surname>Puaut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th International Workshop on Worst-Case Execution Time Analysis (WCET'05)</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
		<respStmt>
			<orgName>Schloss Dagstuhl -Leibniz-Zentrum für Informatik</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">BERT: pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Continuous non-intrusive hybrid wcet estimation using waypoint graphs</title>
		<author>
			<persName><forename type="first">Boris</forename><surname>Dreyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Hochberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Lange</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Wegener</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">16th International Workshop on Worst-Case Execution Time Analysis (WCET 2016)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
		<respStmt>
			<orgName>Schloss Dagstuhl -Leibniz-Zentrum fuer Informatik</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Cyclomatic complexity</title>
		<author>
			<persName><forename type="first">Christof</forename><surname>Ebert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Cain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giuliano</forename><surname>Antoniol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steve</forename><surname>Counsell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Laplante</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE software</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="27" to="29" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Taclebench: A benchmark collection to support worst-case execution time research</title>
		<author>
			<persName><forename type="first">Heiko</forename><surname>Falk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Altmeyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Hellinckx</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Björn</forename><surname>Lisper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wolfgang</forename><surname>Puffitsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christine</forename><surname>Rochange</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Schoeberl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Rasmus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Sørensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Wägemann</surname></persName>
		</author>
		<author>
			<persName><surname>Wegener</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">16th International Workshop on Worst-Case Execution Time Analysis</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Ozone User Guide &amp; Reference Manual</title>
		<ptr target="https://www.segger.com/" />
		<imprint>
			<publisher>SEGGER Microcontroller GmbH</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Mibench: A free, commercially representative embedded benchmark suite</title>
		<author>
			<persName><forename type="first">Jeffrey</forename><forename type="middle">S</forename><surname>Matthew R Guthaus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Ringenberg</surname></persName>
		</author>
		<author>
			<persName><surname>Ernst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Todd M Austin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">B</forename><surname>Mudge</surname></persName>
		</author>
		<author>
			<persName><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">4th IEEE international workshop on workload characterization</title>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The heptane static worst-case execution time estimation tool</title>
		<author>
			<persName><forename type="first">Damien</forename><surname>Hardy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Rouxel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Isabelle</forename><surname>Puaut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">17th International Workshop on Worst-Case Execution Time Analysis (WCET 2017)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
		<respStmt>
			<orgName>Schloss Dagstuhl -Leibniz-Zentrum fuer Informatik</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Introduction of deep neural network in hybrid wcet analysis</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Huybrechts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Cassimon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siegfried</forename><surname>Mercelis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Hellinckx</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances on P2P, Parallel, Grid, Cloud and Internet Computing: Proceedings of the 13th International Conference on P2P, Parallel, Grid, Cloud and Internet Computing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">3PGCIC-2018. 2019</date>
			<biblScope unit="page" from="415" to="425" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A new hybrid approach on wcet analysis for real-time systems using machine learning</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Huybrechts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siegfried</forename><surname>Mercelis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Hellinckx</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">18th International Workshop on Worst-Case Execution Time Analysis (WCET 2018)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
		<respStmt>
			<orgName>Schloss Dagstuhl -Leibniz-Zentrum fuer Informatik</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The program structure tree: Computing control regions in linear time</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Pearson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keshav</forename><surname>Pingali</surname></persName>
		</author>
		<idno type="DOI">10.1145/178243.178258</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGPLAN'94 Conference on Programming Language Design and Implementation (PLDI)</title>
		<editor>
			<persName><forename type="first">Vivek</forename><surname>Sarkar</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Barbara</forename><forename type="middle">G</forename><surname>Ryder</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Mary</forename><forename type="middle">Lou</forename><surname>Soffa</surname></persName>
		</editor>
		<meeting>the ACM SIGPLAN'94 Conference on Programming Language Design and Implementation (PLDI)<address><addrLine>Orlando, Florida, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1994">June 20-24, 1994. 1994</date>
			<biblScope unit="page" from="171" to="185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Obtaining worstcase execution time bounds on modern microprocessors</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Kästner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Pister</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Wegener</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Ferdinand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Embedded World Conference</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Timeweaver: A tool for hybrid worst-case execution time analysis</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Kästner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Pister</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Wegener</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Ferdinand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">19th International Workshop on Worst-Case Execution Time Analysis (WCET 2019)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
		<respStmt>
			<orgName>Schloss Dagstuhl -Leibniz-Zentrum fuer Informatik</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Using measurements as a complement to static worst-case execution time analysis</title>
		<author>
			<persName><forename type="first">Raimund</forename><surname>Kirner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ingomar</forename><surname>Wenzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Rieder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Puschner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Intelligent Systems at the Service of Mankind</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">20</biblScope>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Sentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing</title>
		<author>
			<persName><forename type="first">Taku</forename><surname>Kudo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Richardson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.06226</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep neural network approach to estimate early worst-case execution time</title>
		<author>
			<persName><forename type="first">Vikash</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/AIAA 40th Digital Avionics Systems Conference (DASC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">2021. 2021</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Estimation of an early wcet using different machine learning approaches</title>
		<author>
			<persName><forename type="first">Vikash</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances on P2P, Parallel, Grid, Cloud and Internet Computing: Proceedings of the 17th International Conference on P2P, Parallel, Grid, Cloud and Internet Computing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">3PGCIC-2022. 2022</date>
			<biblScope unit="page" from="297" to="307" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Model identification for wcet analysis</title>
		<author>
			<persName><forename type="first">Björn</forename><surname>Lisper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcelo</forename><surname>Santos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 15th IEEE Real-Time and Embedded Technology and Applications Symposium</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="55" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Codenet: A large-scale ai for code dataset for learning a diversity of coding tasks</title>
		<author>
			<persName><forename type="first">Ruchir</forename><surname>Puri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">S</forename><surname>Kung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geert</forename><surname>Janssen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giacomo</forename><surname>Domeniconi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Zolotov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Dolby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mihir</forename><surname>Choudhury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lindsey</forename><surname>Decker</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.12655</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Probabilistic-wcet reliability: On the experimental validation of evt hypotheses</title>
		<author>
			<persName><forename type="first">Federico</forename><surname>Reghenzani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giuseppe</forename><surname>Massari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Fornaciari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Galimberti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Omni-Layer Intelligent Systems</title>
		<meeting>the International Conference on Omni-Layer Intelligent Systems</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="229" to="234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Dealing with uncertainty in pwcet estimations</title>
		<author>
			<persName><forename type="first">Federico</forename><surname>Reghenzani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Santinelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Fornaciari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Embedded Computing Systems (TECS)</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><surname>Segger</surname></persName>
		</author>
		<ptr target="https://www.segger.com/products/debug-probes/j-trace/" />
		<title level="m">J-Trace PRO -The Leading Trace Solution</title>
		<imprint />
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Measurement based wcet analysis for multi-core architectures</title>
		<author>
			<persName><forename type="first">Hardik</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Coombes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Raabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alois</forename><surname>Knoll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22Nd International Conference on Real-Time Networks and Systems</title>
		<meeting>the 22Nd International Conference on Real-Time Networks and Systems</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="257" to="266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deeppm: transformer-based power and performance prediction for energy-aware software</title>
		<author>
			<persName><forename type="first">Bogyeong</forename><surname>Jun S Shim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yeseong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jihong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2022 Design, Automation &amp; Test in Europe Conference &amp; Exhibition (DATE)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1491" to="1496" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Depth-first search and linear graph algorithms</title>
		<author>
			<persName><forename type="first">Robert</forename><surname>Tarjan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM journal on computing</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="146" to="160" />
			<date type="published" when="1972">1972</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">The worst-case execution-time problem-overview of methods and survey of tools</title>
		<author>
			<persName><forename type="first">Reinhard</forename><surname>Wilhelm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Engblom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Ermedahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niklas</forename><surname>Holsti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Thesing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Whalley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Bernat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Ferdinand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reinhold</forename><surname>Heckmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tulika</forename><surname>Mitra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Embedded Computing Systems (TECS)</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="53" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Understanding polybench/c 3.2 kernels</title>
		<author>
			<persName><forename type="first">Yuki</forename><surname>Tomofumi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International workshop on polyhedral compilation techniques (IMPACT)</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>