<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Probabilistic End-to-End Graph-based Semi-Supervised Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Mariana</forename><surname>Vargas</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Inria Lille Nord Europe</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Aurélien</forename><surname>Bellet</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Inria Lille Nord Europe</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Pascal</forename><surname>Denis</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Inria Lille Nord Europe</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Probabilistic End-to-End Graph-based Semi-Supervised Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">6ECD3F4E574781108A1D87DB3096CC36</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-04-12T14:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper we address the problem of graph-based semi-supervised learning in tasks where a graph describing the relationships between data points is not available. We propose a method to jointly learn the graph and the parameters of a semi-supervised model using a probabilistic framework. We empirically show our proposal achieves competitive results in a variety of datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Graph-based semi-supervised models are good at leveraging unannotated data when small amounts of labels are available: they take as input a graph whose nodes are data points (both labeled and unlabeled) and edges describe how points are related to each other. There exists a variety of such models which propagate labels based on a smoothness criterion <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5]</ref>. A more recent approach uses Graph Convolutional Networks (GCN) to learn node representations based on all the input data while backpropagating the error on the labeled data <ref type="bibr" target="#b5">[6]</ref>. Unfortunately, in many applications the graph structure is not readily available.</p><p>A standard solution is to compute graphs using classical heuristics such as k-nn or -graphs <ref type="bibr" target="#b6">[7]</ref>, but those choices poorly adapt to the underlying data manifold and disregard label information, thus yielding suboptimal results. Dhillon et al. <ref type="bibr" target="#b7">[8]</ref> propose a metric learning based framework in which a graph is constructed via a metric that maximizes the confidence of label assignments. Following a different route, Alexandrescu et al. <ref type="bibr" target="#b8">[9]</ref> use a supervised model on the labeled subset to transform the data into a new space consisting in soft label predictions where the graph is constructed. These two approaches still rely on the classic heuristics for graph construction, and are not able to learn complex data representations. The recent work of Franceschi et al. <ref type="bibr" target="#b9">[10]</ref> represents edges as Bernoulli random variables and uses a bilevel programming framework to fit the parameters of the graph and a GCN for semi-supervised classification.</p><p>In this paper, we present a probabilistic model to learn the parameters of a semi-supervised classification model and the graph jointly. We model edges as latent variables, and we learn by minimizing a reconstruction error over the predicted labels. Our choice of a probabilistic framework allow us to explicitly define a prior over the graph. This enables the model to account for prior knowledge and provides a principled mechanism to impose specific structures (such as sparsity) upon the graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Model</head><p>Let us assume a training set of the form D = D L ∪ D U where D L = {(x i , y i )} l i=1 is the set of labeled data points with labels in some discrete set Y, and D U = {x i } l+u i=l+1 the set of unlabeled points. Let X = [x 1 ; . . . ; x l+u ] T be the design matrix and y L = [y 1 , . . . , y l ] the label vector, W ij a random variable representing an edge between x i and x j . Our goal is to find labels {y l+1 , . . . , y l+u }. We start by assuming labels y are generated by a random process that depends on the data X and unknown parameters W that encode how data points are connected. That is, W ij represents an edge between x i and x j . We can describe this process through the conditional probability p θ (y|X, W ) parameterized by θ that gives the likelihood of labels y provided the dataset X and the graph W . In a variational Bayesian context parameters W are latent variables with prior distribution p θ (W ) and approximate posterior q φ (W ij |X, y) parameterized by φ, on which we can make inference.</p><p>We therefore aim to find the parameters [θ, φ] so as to maximize the likelihood of the labels while keeping the distribution q φ close to the prior. To do this, we maximize the evidence lower bound (ELBO) given by:</p><formula xml:id="formula_0">L(θ, φ) = E q φ (W |X,y) [log p θ (y|W, X)] -KL[q φ (W |X, y)||p θ (W )]<label>(1)</label></formula><p>This model is similar to a variational autoencoder <ref type="bibr" target="#b10">[11]</ref> where q φ is the encoder and p θ the decoder. Maximizing this bound is equivalent to minimizing the reconstruction error of the estimations the decoder produces while the encoder remains as close to the prior as possible.</p><p>To instantiate our model, we choose p θ (y|W, X) to be a categorical distribution over the labels Y parametrized by a GCN <ref type="bibr" target="#b5">[6]</ref>, and q φ (W |X, y) to model edges as a collection of Bernoulli distributions parameterized by a Graph Neural Network (GNN) <ref type="bibr" target="#b11">[12]</ref>. Each Bernoulli distribution represents the probability of an edge connecting two nodes i and j, hence W ij is a binary variable.</p><p>We took inspiration from <ref type="bibr" target="#b12">[13]</ref> to use a message passing Graph Neural Network (GNN) <ref type="bibr" target="#b13">[14]</ref> that computes node and edge embeddings as follows:</p><formula xml:id="formula_1">h (1) i = MLP (1) node (x i ),<label>(2) h (1)</label></formula><formula xml:id="formula_2">ij = MLP<label>(1) edge ([h (1)</label></formula><p>i , h</p><formula xml:id="formula_3">j ]),<label>(1)</label></formula><formula xml:id="formula_4">i = MLP<label>(3) h (2)</label></formula><formula xml:id="formula_5">ij = MLP (2) edge ([h<label>(2) node h (1) i , j∈N (i) h (1) ij , (4) h (2)</label></formula><p>i , h</p><formula xml:id="formula_7">j ]),<label>(2)</label></formula><p>and finally g φ (X) = Softmax(h</p><formula xml:id="formula_9">(2) ij )</formula><p>. N (i) denotes the neighbors of point i in the GNN (in practice we simply take them to be the closest points to x i , or even all other points). Finally, we define g φ (X) = Softmax(h</p><formula xml:id="formula_10">(2) ij ).</formula><p>Here [x i , x j ] denotes concatenation and MLP is short for multilayer perceptron.</p><p>We also pick the prior distribution p θ (W ) to be a collection of Bernoulli distributions.</p><p>Training details. We train our model by backpropagation. We first run the encoder q φ , which is a distribution taking discrete values. This prevent us from being able to directly backpropagate the error through its reparametrized samples. We then use the concrete distribution <ref type="bibr" target="#b14">[15]</ref> to get a continuous approximation of q φ and apply the reparametrization trick to compute the gradients. More specifically, we draw samples W as follows: we draw a vector ξ from a Gumbel(0, 1) distribution and then we compute</p><formula xml:id="formula_11">W ij = Softmax((h (2) ij + ξ)/τ ),</formula><p>where τ is a parameter controlling how smooth the resulting distribution is (the bigger τ is, the more it will resemble a uniform distribution). To control the variability we take several samples this way, W (1) , . . . , W (r) and feed the decoder to get ŷ(1) , . . . , ŷ(r) where ŷi = GCN (X, W (i) ). We then backpropagate the error with respect to y L through the mean ŷ = 1 r (ŷ (1) </p><formula xml:id="formula_12">+ • • • + ŷ(r)</formula><p>). The reconstruction error that corresponds to the first term of Equation 1 is the average cross-entropy over the labeled examples:</p><formula xml:id="formula_13">L reconstruction = 1 l CrossEnt y L , ŷ D L . (<label>6</label></formula><formula xml:id="formula_14">)</formula><p>The KL-divergence of q φ that corresponds to the second term of Equation <ref type="formula" target="#formula_0">1</ref>, given a Bernoulli prior ρ, is given by: </p><formula xml:id="formula_15">L KL-divergence = l+u i=1 j:ρij =0 W ij log W ij ρ ij + (1 -W ij ) log 1 -W ij 1 -ρ ij .<label>(7)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments and Results</head><p>We carried out experiments to compare our probabilistic approach (denoted by PSSL) with supervised algorithms such as logistic regression (LogReg), support vector machines with a radial kernel (SVM) and feed-forward neural networks (FFNN), <ref type="foot" target="#foot_0">1</ref> and with the state-of-the-art semi-supervised method based on GCNs, which we fed with different types of heuristically computed graphs. We use three strategies for building a graph: k nearest neighbors (knn), radial kernel (RBF), and a random variant of the k-NN graph constructed as follows: denoting by K the regular k-nn graph, an edge e ij between x i and x j is sampled according to a Bernoulli distribution with some high probability α if K ij = 1, or with probability 1 -α otherwise (Sknn). The prior distribution for our model PSSL is constructed in the same way as Sknn. We also specified different sparsity patterns over the prior.</p><p>We evaluate the baselines and our method on three datasets: Cora <ref type="bibr" target="#b15">[16]</ref>, a subset of 20 Newsgroups with three classes and a TFIDF feature space, and Wine, a benchmark dataset available in scikit-learn <ref type="bibr" target="#b16">[17]</ref>.</p><p>We used Adam to optimize our objective function. For all methods we tune the main hyperparameters over five random splits with train, test, and validation sizes as described in Table <ref type="table" target="#tab_0">1</ref>.</p><p>Results are shown in Table <ref type="table" target="#tab_1">2</ref>. We can observe that we achieve competitive results in Wine, and outperform the baselines by a considerable margin in 20news3 and Cora.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion and Discussion</head><p>We presented preliminary work on a framework based on autoencoding variational bayes that learns the parameters of a semi-supervised model and the underlying graph structure of the data simultaneously. We empirically showed that our model can achieve considerable gains over different baselines in different semi-supervised datasets.</p><p>We plan to run experiments on other semi-supervised datasets, and to compare this method empirically with that of Franceschi et al. <ref type="bibr" target="#b9">[10]</ref>. We believe our proposal exhibits two advantages. First, we can explicitly specify a prior over the graph, which allow us to bias towards specific structures and sparsity patterns. Second, <ref type="bibr" target="#b9">[10]</ref> requires two separate validation sets while we require only one: we therefore have access to more training data.</p><p>An interesting future research line is to extend this work to an inductive setting in order to be able to elegantly handle unseen test examples.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>33rd</head><label></label><figDesc>Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Statistics of datasets. Dataset Size Dimension Nb. of classes Train/Val/Test</figDesc><table><row><cell>Wine</cell><cell>178</cell><cell>13</cell><cell>3</cell><cell>10/20/158</cell></row><row><cell cols="3">20news3 2756 229</cell><cell>3</cell><cell>20/40/2696</cell></row><row><cell>Cora</cell><cell cols="2">2708 1433</cell><cell>7</cell><cell>140/300/1000</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Mean and standard deviation of test accuracy over five random splits.</figDesc><table><row><cell></cell><cell>Wine</cell><cell>20news3</cell><cell>Cora</cell></row><row><cell>LogReg</cell><cell cols="3">.95 ± .02 .77 ± .01 .53 ± .00</cell></row><row><cell>SVM</cell><cell>.94 ± .03</cell><cell cols="2">.76 ± .00 .50 ± .00</cell></row><row><cell>FFNN</cell><cell>.93 ± .01</cell><cell cols="2">.77 ± .01 .55 ± .01</cell></row><row><cell>GCN+knn</cell><cell cols="3">.95 ± .03 .77 ± .02 .61 ± .01</cell></row><row><cell cols="2">GCN+Sknn .93 ± .03</cell><cell cols="2">.66 ± .05 .31 ± .00</cell></row><row><cell cols="2">GCN+RBF .94 ± .03</cell><cell cols="2">.76 ± .01 .51 ± .01</cell></row><row><cell>PSSL</cell><cell cols="3">.95 ± .01 .83 ± .01 .65 ± .04</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>This model is equivalent to a GCN with no graph.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Learning from Labeled and Unlabeled Data with Label Propagation</title>
		<author>
			<persName><forename type="first">Xiaojin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
		<idno>CMU-CALD-02-107</idno>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Semi-supervised Learning Using Gaussian Fields and Harmonic Functions</title>
		<author>
			<persName><forename type="first">Xiaojin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twentieth International Conference on International Conference on Machine Learning, ICML&apos;03</title>
		<meeting>the Twentieth International Conference on International Conference on Machine Learning, ICML&apos;03</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="912" to="919" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning with local and global consistency</title>
		<author>
			<persName><forename type="first">Dengyong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Bousquet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Navin Lal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="321" to="328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Transductive Learning via Spectral Graph Partitioning</title>
		<author>
			<persName><forename type="first">Thorsten</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twentieth International Conference on International Conference on Machine Learning, ICML&apos;03</title>
		<meeting>the Twentieth International Conference on International Conference on Machine Learning, ICML&apos;03</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="290" to="297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Manifold Regularization: A Geometric Framework for Learning from Labeled and Unlabeled Examples</title>
		<author>
			<persName><forename type="first">Mikhail</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Partha</forename><surname>Niyogi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vikas</forename><surname>Sindhwani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="2399" to="2434" />
			<date type="published" when="2006-12">December 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Semi-Supervised Classification with Graph Convolutional Networks</title>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations, ICLR 2017</title>
		<title level="s">Conference Track Proceedings</title>
		<meeting><address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">April 24-26, 2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Graph-Based Semi-Supervised Learning</title>
		<author>
			<persName><forename type="first">Amarnag</forename><surname>Subramanya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Partha</forename><surname>Pratim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Talukdar</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>Morgan &amp; Claypool Publishers</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Inference Driven Metric Learning (IDML) for Graph Construction</title>
		<author>
			<persName><forename type="first">Paramveer</forename><surname>Dhillon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Partha</forename><surname>Talukdar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koby</forename><surname>Crammer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Technical Reports (CIS)</title>
		<imprint>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Data-Driven Graph Construction for Semi-Supervised Graph-Based Learning in NLP</title>
		<author>
			<persName><forename type="first">Andrei</forename><surname>Alexandrescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katrin</forename><surname>Kirchhoff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Conference of the North American Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference</title>
		<meeting><address><addrLine>Rochester, New York</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2007-04">2007. April 2007</date>
			<biblScope unit="page" from="204" to="211" />
		</imprint>
	</monogr>
	<note>Human Language Technologies</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning Discrete Structures for Graph Neural Networks</title>
		<author>
			<persName><forename type="first">Luca</forename><surname>Franceschi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mathias</forename><surname>Niepert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Massimiliano</forename><surname>Pontil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning, ICML 2019</title>
		<meeting>the 36th International Conference on Machine Learning, ICML 2019<address><addrLine>Long Beach, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-06-15">9-15 June 2019. 2019</date>
			<biblScope unit="page" from="1972" to="1982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Auto-Encoding Variational Bayes</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2nd International Conference on Learning Representations, ICLR 2014</title>
		<title level="s">Conference Track Proceedings</title>
		<meeting><address><addrLine>Banff, AB, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">April 14-16, 2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The Graph Neural Network Model</title>
		<author>
			<persName><forename type="first">Franco</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ah</forename><surname>Chung Tsoi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriele</forename><surname>Monfardini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. Neur. Netw</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="80" />
			<date type="published" when="2009-01">January 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Neural Relational Inference for Interacting Systems</title>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ethan</forename><surname>Fetaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuan-Chieh</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning, ICML 2018</title>
		<meeting>the 35th International Conference on Machine Learning, ICML 2018<address><addrLine>Stockholmsmässan, Stockholm, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">July 10-15, 2018. 2018</date>
			<biblScope unit="page" from="2693" to="2702" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Neural Message Passing for Quantum Chemistry</title>
		<author>
			<persName><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><forename type="middle">S</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><forename type="middle">F</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<idno>CoRR, abs/1704.01212</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables</title>
		<author>
			<persName><forename type="first">Chris</forename><forename type="middle">J</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yee</forename><forename type="middle">Whye</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations, ICLR 2017</title>
		<title level="s">Conference Track Proceedings</title>
		<meeting><address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">April 24-26, 2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Collective Classification in Network Data</title>
		<author>
			<persName><forename type="first">Prithviraj</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Galileo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mustafa</forename><surname>Namata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lise</forename><surname>Bilgic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tina</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName><surname>Eliassi-Rad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI Magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="93" to="106" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Scikit-learn: Machine Learning in Python</title>
		<author>
			<persName><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Dubourg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cournapeau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brucher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Perrot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Duchesnay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
