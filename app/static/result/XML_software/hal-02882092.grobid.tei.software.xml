<TEI xmlns="http://www.tei-c.org/ns/1.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xml:space="preserve" xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Robust Kernel Density Estimation with Median-of-Means principle</title>
			</titleStmt>
			<publicationStmt>
				<publisher />
				<availability status="unknown"><licence /></availability>
				<date type="published" when="2020-06-05">June 5, 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Pierre</forename><surname>Humbert</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Université Paris-Saclay</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<orgName type="institution" key="instit3">ENS Paris-Saclay</orgName>
								<address>
									<addrLine>Centre Borelli</addrLine>
									<postCode>F-91190</postCode>
									<settlement>Gif-sur-Yvette</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Batiste</forename><surname>Le Bars</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Université Paris-Saclay</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<orgName type="institution" key="instit3">ENS Paris-Saclay</orgName>
								<address>
									<addrLine>Centre Borelli</addrLine>
									<postCode>F-91190</postCode>
									<settlement>Gif-sur-Yvette</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ludovic</forename><surname>Minvielle</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Université Paris-Saclay</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<orgName type="institution" key="instit3">ENS Paris-Saclay</orgName>
								<address>
									<addrLine>Centre Borelli</addrLine>
									<postCode>F-91190</postCode>
									<settlement>Gif-sur-Yvette</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Nicolas</forename><surname>Vayatis</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Université Paris-Saclay</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<orgName type="institution" key="instit3">ENS Paris-Saclay</orgName>
								<address>
									<addrLine>Centre Borelli</addrLine>
									<postCode>F-91190</postCode>
									<settlement>Gif-sur-Yvette</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Robust Kernel Density Estimation with Median-of-Means principle</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-06-05">June 5, 2020</date>
						</imprint>
					</monogr>
					<idno type="MD5">408F7FFB06F2B2AD87FFDE933839E00B</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-04-12T14:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid" />
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div><p>In this paper, we introduce a robust nonparametric density estimator combining the popular Kernel Density Estimation method and the Median-of-Means principle (MoM-KDE). This estimator is shown to achieve robustness to any kind of anomalous data, even in the case of adversarial contamination. In particular, while previous works only prove consistency results under known contamination model, this work provides finite-sample high-probability error-bounds without a priori knowledge on the outliers. Finally, when compared with other robust kernel estimators, we show that MoM-KDE achieves competitive results while having significant lower computational complexity.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div><head n="1">Introduction</head><p>Over the past years, the task of learning in the presence of outliers has become an increasingly important objective in both statistics and machine learning. Indeed, in many situations, training data can be contaminated by undesired samples, which may badly affect the resulting learning task, especially in adversarial settings. Building robust estimators and algorithms that are resilient to outliers is therefore becoming crucial in many learning procedures. In particular, the inference of a probability density function from a contaminated random sample is of major concerns.</p><p>Density estimation methods are mostly divided into parametric and nonparametric techniques. Among the nonparametric family, the Kernel Density Estimator (KDE) is probably the most known and used for both univariate and multivariate densities <ref type="bibr" target="#b25">[Parzen, 1962;</ref><ref type="bibr" target="#b30">Silverman, 1986;</ref><ref type="bibr" target="#b28">Scott, 2015]</ref>, but it also known to be sensitive to dataset contaminated by outliers <ref type="bibr">[Kim and</ref><ref type="bibr">Scott, 2011, 2012;</ref><ref type="bibr" target="#b35">Vandermeulen and Scott, 2014]</ref>. The construction of robust KDE is therefore an important area of research, that can have useful applications such as anomaly detection and resilience to adversarial data corruption. Yet, only few works have proposed such robust estimators. <ref type="bibr" target="#b16">Kim and Scott [2012]</ref> proposed to combine KDE with ideas from M-estimation to construct the so-called Robust Kernel Density Estimator (RKDE). However, no consistency results were provided and robustness was rather shown experimentally. Later, RKDE was proven to converge to the true density, however at the condition that the dataset remains uncorrupted <ref type="bibr" target="#b34">[Vandermeulen and Scott, 2013]</ref>. More recently, <ref type="bibr" target="#b35">Vandermeulen and Scott [2014]</ref> proposed another robust estimator, called Scaled and Projected KDE (<software ContextAttributes="used">SPKDE</software>). Authors proved the L 1 -consistency of <software ContextAttributes="used">SPKDE</software> under a variant of the Huber's ε-contamination model where two strong assumptions are made <ref type="bibr" target="#b11">[Huber, 1992]</ref>. First, the contamination parameter ε is known, and second, the outliers are drawn from an uniform distribution when outside the support of the true density. Unfortunately, as they did not provided rates of convergence, it still remains unclear at which speed <software ContextAttributes="used">SPKDE</software> converges to the true density. Finally, both RKDE and <software ContextAttributes="used">SPKDE</software> require iterative algorithms to compute their estimators, increasing the overall complexity of their construction.</p><p>In statistical analysis, another idea to construct robust estimators is to use the Median-of-Means principle (MoM). Introduced by <ref type="bibr" target="#b24">Nemirovsky and Yudin [1983]</ref>, <ref type="bibr" target="#b12">Jerrum et al. [1986]</ref>, and <ref type="bibr" target="#b0">Alon et al. [1999]</ref>, the MoM was first designed to estimate the mean of a real random variable. It relies on the simple idea that rather than taking the average of all the observations, the sample is split in several non-overlapping blocks over which the mean is computed. The MoM estimator is then defined as the median of these means. Easy to compute, the MoM properties have been studied by <ref type="bibr" target="#b23">Minsker et al. [2015]</ref> and <ref type="bibr" target="#b5">Devroye et al. [2016]</ref> to estimate the means of heavy-tailed distributions. Furthermore, due to its robustness to outliers, MoM-based estimators have recently gained a renewed of interest in the machine learning community <ref type="bibr" target="#b19">[Lecué et al., 2020;</ref><ref type="bibr" target="#b18">Lecué and Lerasle, 2019]</ref>.</p><p>Contributions. In this paper, we propose a new robust nonparametric density estimator based on the combination of the Kernel Density Estimation method and the Median-of-Means principle (MoM-KDE). We place ourselves in a more general framework than the classical Huber contamination model, called O ∪ I, which gets rid of any assumption on the outliers. We demonstrate the statistical performance of the estimator through finite-sample high-confidence error bounds in the L ∞ -norm and show that MoM-KDE's convergence rate is the same as KDE without outliers. Additionally, we prove the consistency in the L 1 -norm, which is known to reflect the global performance of the estimate. To the best of our knowledge, this is the first work that presents such results in the context of robust kernel density estimation, especially under the O ∪ I framework. Finally, we demonstrate the empirical performance of MoM-KDE on both synthetic and real data and show the practical interest of such estimator as it has a lower complexity than the baseline RKDE and <software>SPKDE</software>.</p></div>
<div><head n="2">Median-of-Means Kernel Density Estimation</head><p>We first recall the classical kernel density estimator. Let X 1 , • • • , X n be independent and identically distributed (i.i.d.) random variables that have a probability density function (pdf) f (•) with respect to the Lebesgue measure on R d . The Kernel Density Estimate of f (KDE), also called the Parzen-Rosenblatt estimator, is a nonparametric estimator given by</p><formula xml:id="formula_0">fn (x) = 1 nh d n i=1 K X i -x h ,<label>(1)</label></formula><p>where h &gt; 0 and K : R d -→ R + is an integrable function satisfying K(u)du = 1 <ref type="bibr" target="#b32">[Tsybakov, 2008]</ref>.</p><p>Such a function K(•) is called a kernel and the parameter h is called the bandwidth of the estimator. The bandwidth is a smoothing parameter that controls the bias-variance tradeoff of fn (•) with respect to the input data.</p><p>While this estimator is central in statistic, a major drawback is its weakness against outliers <ref type="bibr" target="#b14">[Kim and Scott, 2008</ref><ref type="bibr">, 2011</ref><ref type="bibr">, 2012;</ref><ref type="bibr" target="#b35">Vandermeulen and Scott, 2014]</ref>. Indeed, as it assigns uniform weights 1/n to every points regardless of whether X i is an outlier or not, inliers and outliers contribute equally in the construction of the KDE, which results in undesired "bumps" over outlier locations in the final estimated density (see Figure <ref type="figure" target="#fig_0">1</ref>). In the following, we propose a KDE-based density estimator robust to the presence of outliers in the sample set. These outliers are considered in a general framework described in the next section.</p></div>
<div><head n="2.1">Outlier setup</head><p>Throughout the paper, we consider the O ∪ I framework introduced by <ref type="bibr" target="#b18">Lecué and Lerasle [2019]</ref>. This very general framework allows the presence of outliers in the dataset and relax the standard i.i.d. assumption on each observation. We therefore assume that the n random variables are partitioned into two (unknown) groups: a subset {X i | i ∈ I} made of inliers, and another subset {X i | i ∈ O} made of outliers such that O ∩ I = ∅ and O ∪ I = {1, . . . , n}. While we suppose the X i∈I are i.i.d. from a distribution that admits a density f with respect to the Lebesgue measure, no assumption is made on the outliers X i∈O .</p><p>Hence, these outlying points can be dependent, adversarial, or not even drawn from a proper probability distribution.</p><p>The O ∪ I framework is related to the well-known Huber's ε-contamination model <ref type="bibr" target="#b11">[Huber, 1992]</ref> where it is assumed that data are i.i.d. with distribution g = εf I + (1 -ε)f O , and ε ∈ [0, 1); the distribution f I being related to the inliers and f O to the outliers. However, there are several important differences. First, in the O ∪ I the proportion of outliers is fixed and equals to |O|/n, whereas it is random in the Huber's ε-contamination model <ref type="bibr" target="#b20">[Lerasle, 2019]</ref>. Second, the O ∪ I is less restrictive. Indeed, contrary to Huber's model which considers that inliers and outliers are respectively i.i.d from the same distributions, O ∪ I does not make a single assumption on the outliers.</p></div>
<div><head n="2.2">MoM-KDE</head><p>We now present our main contribution, a robust kernel density estimator based on the MoM. This estimator is essentially motivated by the fact that the classical kernel density estimation at one point corresponds to an empirical average (see Equation ( <ref type="formula" target="#formula_0">1</ref>)). Therefore, the MoM principle appears to be an intuitive solution to build a robust version of the KDE. A formal definition of MoM-KDE is given below. </p><formula xml:id="formula_1">(x 0 ) ∝ Median fn1 (x 0 ), • • • , fn S (x 0 ) ,<label>(2)</label></formula><p>where fns (x 0 ) is the value of the standard kernel density estimator at x 0 obtained via the samples of the s-th block B s . Note that fMoM (•) is not necessarily a density as its integral may not be equal to 1. When needed, we thus normalize it by its integral the same way it is proposed by <ref type="bibr" target="#b4">Devroye and Lugosi [2012]</ref>.</p><p>Broadly speaking, MoM estimators appear to be a good tradeoff between the unbiased but non robust empirical mean and the robust but biased median <ref type="bibr" target="#b19">[Lecué et al., 2020]</ref>. A visual example of the robustness of MoM-KDE is displayed in Figure <ref type="figure" target="#fig_0">1</ref>. We now give a simple example highlighting the robustness of MoM-KDE.</p><p>Example 1. (MoM-KDE v.s. Uniform KDE) Let the inliers be i.i.d. samples from a uniform distribution on the interval [-1, 1] and the outliers be i.i.d. samples from another uniform distribution on <ref type="bibr">[-3, 3]</ref>. Let the kernel function be the uniform kernel, x 0 = 2 and h ∈ (0, 1). Then if S &gt; 2|O|, we obtain</p><formula xml:id="formula_2">| fMoM (x 0 ) -f (x 0 )| = 0 a.s. and P | fn (x 0 ) -f (x 0 )| = 0 = (1 -h/3) |O| = 1 .</formula><p>This result shows that the MoM-KDE makes (almost surely) no error at the point x 0 . On the contrary, the KDE here has a non-negligible probability to make an error.</p></div>
<div><head n="2.3">Time complexity</head><p>The complexity of MoM-KDE to evaluate one point is the same as the standard KDE, O(n); O(S • n S ) for the block-wise evaluation and O(n) to compute the median with the median-of-medians algorithm <ref type="bibr" target="#b2">[Blum et al., 1973]</ref>. Since RKDE and <software ContextAttributes="used">SPKDE</software> are KDEs with modified weights, they also perform the evaluation step in O(n) time. However, these weights need to be learnt, thus requiring an additional non-negligible computing capacity. Indeed, each one of them rely on an iterative method -the iteratively reweighted least squares algorithm and the projected gradient descent algorithm, that both have a complexity of O(n iter • n 2 ), where n iter is the number of needed iterations to reach a reasonable accuracy. MoM-KDE on the other hand does not require any learning procedure. Note that the evaluation step can be accelerated  <ref type="bibr" target="#b35">Vandermeulen and Scott, 2014]</ref> </p><formula xml:id="formula_3">O(n iter • n 2 ) O(n) yes SPKDE [</formula><formula xml:id="formula_4">O(n iter • n 2 ) O(n) yes MoM-KDE - O(n) no</formula><p>through several ways, hence potentially reducing computational time of all these competing methods <ref type="bibr">[Gray and Moore, 2003a,b;</ref><ref type="bibr" target="#b37">Wang and Scott, 2019;</ref><ref type="bibr" target="#b1">Backurs et al., 2019]</ref>. Theoretical time complexities are gathered in Table <ref type="table" target="#tab_1">1</ref>.</p></div>
<div><head n="3">Theoretical analysis</head><p>In this section, we give a finite-sample high-probability error bound in the L ∞ -norm for MoM-KDE under the O ∪ I framework. To our knowledge, we are the first to provide such error bounds in robust kernel density estimation under this framework. In particular, our objective is to prove that even with a contaminated dataset, MoM-KDE achieves a similar convergence rate than KDE without outliers <ref type="bibr" target="#b31">[Sriperumbudur and Steinwart, 2012;</ref><ref type="bibr" target="#b13">Jiang, 2017;</ref><ref type="bibr">Wang et al., 2019]</ref>. In order to build this high-probability error bound, it is assumed, among other standard hypotheses, that the true density is Hölder-continuous, a smoothness property usually considered in KDE analysis <ref type="bibr" target="#b32">[Tsybakov, 2008;</ref><ref type="bibr" target="#b13">Jiang, 2017;</ref><ref type="bibr">Wang et al., 2019]</ref>. In addition, we show the consistency in the L 1 -norm. In this last result, we will see that the aforementioned assumptions are not necessary to obtain the consistency. In the following, we give the necessary definitions and assumptions to perform our non-asymptotic analysis.</p></div>
<div><head n="3.1">Setup and assumptions</head><p>Let us first list the usual assumptions, notably on the considered kernel function, that will allow us to derive our results. They are standard in KDE analysis, and are chosen for their simplicity of comprehension <ref type="bibr" target="#b32">[Tsybakov, 2008;</ref><ref type="bibr" target="#b13">Jiang, 2017]</ref>. More general hypotheses could be made in order to obtain the same results, notably assuming kernel of order (see for example the works of <ref type="bibr" target="#b32">Tsybakov [2008]</ref> and <ref type="bibr">Wang et al. [2019]</ref>).</p><formula xml:id="formula_5">Assumption 1. (Bounded density) f ∞ &lt; ∞.</formula><p>We make the following assumptions on the kernel K.</p><p>Assumption 2. (Density kernel) ∀u ∈ R d , K(u) ≥ 0, and K(u)du = 1.</p><p>Assumption 3. (Spherically symmetric and non-increasing) There exists a non-increasing function</p><formula xml:id="formula_6">k : R + -→ R + such that K(u) = k( u ) for all u ∈ R d , where • is any norm of R d .</formula><p>Assumption 4. (Exponentially decaying tail) There exists positive constants ρ, C ρ , t 0 &gt; 0 such that for all</p><formula xml:id="formula_7">t &gt; t 0 k(t) ≤ C ρ • exp(-t ρ ) .</formula><p>All the above assumptions are respected by most of the popular kernels, in particular the Gaussian, Exponential, Uniform, Triangular, Cosine kernel, etc. Furthermore, the last assumption implies that for any m &gt; 0, we have u m K(u)du &lt; ∞ (finite norm moment) <ref type="bibr" target="#b13">[Jiang, 2017]</ref>. Finally, when taken together, these assumptions imply that the kernel satisfies the VC property <ref type="bibr">[Wang et al., 2019]</ref>. Theses are key properties to provide the bounds presented in the next section.</p><p>Before stating our main results, we recall the definition of the Hölder class of functions.</p><p>Definition 2. (Hölder class) Let T be an interval of R d , and 0 &lt; α ≤ 1 and L &gt; 0 be two constants. We say that a function f :</p><formula xml:id="formula_8">T → R belongs to the Hölder class Σ(L, α) if it satisfies ∀x, x ∈ T, |f (x) -f (x )| ≤ L x -x α .</formula><p>(3)</p><p>This definition implies a smoothness regularization on the function f , and is a convenient property to bound the bias of KDE-based estimators.</p></div>
<div><head n="3.2">L ∞ and L 1 consistencies of MoM-KDE</head><p>This section states our central finding, a L ∞ finite-sample error bound for MoM-KDE that proves its consistency and yields the same convergence rate as KDE with uncontaminated data. The latter is given by the following Lemma partly proven by <ref type="bibr" target="#b31">Sriperumbudur and Steinwart [2012]</ref> and verified several times in the literature <ref type="bibr" target="#b7">[Giné and Guillou, 2002;</ref><ref type="bibr" target="#b13">Jiang, 2017;</ref><ref type="bibr">Wang et al., 2019]</ref>.</p><p>Lemma 1. (L ∞ error-bound of the KDE without anomalies) Suppose that f belongs to the class of densities P(α, L) defined as</p><formula xml:id="formula_9">P(α, L) f | f ≥ 0, f (x)dx = 1, and f ∈ Σ(α, L) ,<label>(4)</label></formula><p>where Σ(α, L) is the Hölder class of function on R d (Definition 2). Grant assumptions 1 to 4 and let n &gt; 1, h ∈ (0, 1) and S ≥ 1 such that nh d ≥ S and nh d ≥ |log(h)|. Then with probability at least 1 -exp(-S), we have</p><formula xml:id="formula_10">fn -f ∞ ≤ C 1 S| log(h)| nh d + C 2 h α ,<label>(5)</label></formula><p>where</p><formula xml:id="formula_11">C 2 = L u α K(u)du &lt; ∞ and C 1 is a constant that only depends on f ∞ , the dimension d,</formula><p>and the kernel properties.</p><p>This Lemma comes from the well-known bias-variance decomposition, where we separately bound the variance (see Theorem 3.1 of <ref type="bibr" target="#b31">Sriperumbudur and Steinwart [2012]</ref>) and the bias (see e.g. <ref type="bibr" target="#b32">[Tsybakov, 2008]</ref> or <ref type="bibr" target="#b27">[Rigollet et al., 2009]</ref>). It shows the consistency of KDE without anomalies, as soon as h → 0 and nh d → ∞.</p><p>We now present our main result. Its objective is to show that even under the O ∪ I framework, we do not need any additional hypothesis -besides the ones of the previous lemma -to show that MoM-KDE achieves the same convergence rate as KDE when used with uncontaminated data.</p><p>Proposition 1. (L ∞ error-bound of the MoM-KDE under the O ∪ I) Suppose that f belongs to the class of densities P(α, L) and grant assumptions 1 to 4. Let S be the number of blocks, δ &gt; 0 such that S &gt; (2 + δ)|O|, and ∆ = (1/(2 + δ) -|O|/S). Then, for any h ∈ (0, 1), δ sufficiently small, and n ≥ 1 such that nh d ≥ S log(2(2 + δ)/δ), and nh d ≥ S| log(h)|, we have with probability at least 1 -exp(-2∆ 2 S),</p><formula xml:id="formula_12">fMoM -f ∞ ≤ C 1 S log 2(2+δ) δ | log(h)| nh d + C 2 h α ,<label>(6)</label></formula><p>where</p><formula xml:id="formula_13">C 2 = L u α K(u)du &lt; ∞ and C 1 is a constant that only depends on f ∞ , the dimension d,</formula><p>and the kernel properties.</p><p>The proof is given in the supplementary material. From equation ( <ref type="formula" target="#formula_12">6</ref>), the optimal choice of the</p><formula xml:id="formula_14">bandwidth is h S log(n) n 1/(2α+d)</formula><p>leading to the final rate of</p><formula xml:id="formula_15">S log(n) n α/(2α+d)</formula><p>. This convergence rate is the same (up to a constant) to the one of KDE without anomalies, with the same exponential control (Lemma 1). Note that when there is no outlier, i.e. |O| = 0, the bound holds for S = 1, and we recover the classical KDE minimax optimal rate <ref type="bibr">[Wang et al., 2019]</ref>. In addition, the previous proposition states that the convergence of the MoM-KDE only depends on the number of outliers in the dataset, and not on their "type". This estimator is therefore robust in a wide range of scenarios, including the adversarial one.</p><p>We now give a L 1 -consistency result under mild hypotheses, which is known to reflect the global performance of the estimate. Indeed, small L 1 error leads to accurate probability estimation <ref type="bibr" target="#b3">[Devroye and Gyorfi, 1985]</ref>.</p><formula xml:id="formula_16">Proposition 2. (L 1 -consistency in probability) If n/S → ∞, h → 0, nh d → ∞, and S &gt; 2|O|, then fMoM -f 1 P -→ n→∞ 0 . (<label>7</label></formula><formula xml:id="formula_17">)</formula><p>This result is obtained by bounding the left-hand part by the errors in the healthy blocks only, i.e. those without anomalies. Under the hypothesis of the proposition, these errors are known to converge to 0 in probability <ref type="bibr">[Wang et al., 2019]</ref>. The complete proof is given in supplementary material. Contrary to <software ContextAttributes="used">SPKDE</software> <ref type="bibr" target="#b35">[Vandermeulen and Scott, 2014]</ref>, no assumption on the outliers generation process is necessary to obtain this consistency result. Moreover, while we need to assume that the proportion of outliers is perfectly known to prove the convergence of <software ContextAttributes="used">SPKDE</software>, the MoM-KDE converges whenever the number of outliers is overestimated.</p></div>
<div><head n="4">Numerical experiments</head><p>In this section, we display numerical results supporting the relevance of MoM-KDE. All experiences were run over a personal laptop computer using Python. The code of MoM-KDE is made available online<ref type="foot" target="#foot_0">1</ref> .</p><p>Comparative methods. In the following experiments, we propose to compare MoM-KDE to the classical KDE and two robust versions of KDE, called RKDE <ref type="bibr" target="#b16">[Kim and Scott, 2012]</ref> and <software ContextAttributes="used">SPKDE</software> <ref type="bibr" target="#b35">[Vandermeulen and Scott, 2014]</ref>.</p><p>As previously explained, RKDE takes the ideas of robust M-estimation and translate it to kernel density estimation. Authors point out that classical KDE estimator can be seen as the minimizer of a squared error loss in the Reproducing Kernel Hilbert Space H corresponding to the chosen kernel. Instead of minimizing this loss, they propose to minimize a robust version of it, i ρ( φ(X i ) -g H ), with respect to g ∈ H. Here φ is the canonical feature map and ρ(•) is either the robust Huber or Hampel function. The solution of the newly expressed problem is then found using the iteratively reweighted least squares algorithm.</p><p><software>SPKDE</software> proposes to scale the standard KDE in a way that it decontaminates the dataset. This is done by minimizing the function β fn -g 2 with respect to g, belonging to the convex hull of {k h (•, X i )} n i=1 . Here, β is an hyperparameter that controls the robustness and fn is the KDE estimator. The minimization is shown to be equivalent to a quadratic program over the simplex, solved via projected gradient descent.</p><p>Metrics. The performance of the MoM-KDE is measured through three metrics, two are used to measure the similarity between the estimated and the true density, and one describes performances of an anomaly detector based on the estimated density. The first one is the Kullback-Leibler divergence <ref type="bibr" target="#b17">[Kullback and Leibler, 1951]</ref> which is the most used in robust KDE <ref type="bibr" target="#b14">[Kim and Scott, 2008</ref><ref type="bibr">, 2011</ref><ref type="bibr">, 2012;</ref><ref type="bibr" target="#b35">Vandermeulen and Scott, 2014]</ref>. Used to measure the similarity between distributions, it is defined as</p><formula xml:id="formula_18">D KL ( f f ) = f (x) log f (x) f (x) dx .</formula><p>As the Kullback-Leibler divergence is non-symmetric and may have infinite values when distributions do not share the same support, we also consider the Jensen-Shannon divergence <ref type="bibr" target="#b6">[Endres and Schindelin, 2003;</ref><ref type="bibr" target="#b21">Liese and Vajda, 2006]</ref>. It is a symmetrized version of D KL , with positive values, bounded by 1 (when the logarithm is used in base 2), and has found applications in many fields, such as deep learning <ref type="bibr" target="#b8">[Goodfellow et al., 2014]</ref> or transfer learning <ref type="bibr" target="#b29">[Segev et al., 2017]</ref>. It is defined as</p><formula xml:id="formula_19">D JS ( f f ) = 1 2 D KL ( f g) + D KL (f g) , with g = 1 2 ( f + f ) .</formula><p>Motivated by real-world application, the third metric is not related to the true density, which is usually not available in practical cases. Instead, we quantify the capacity of the learnt density to detect anomalies using the well-known Area Under the ROC Curve criterion (AUC). An input point x 0 is considered abnormal whenever f (x 0 ) is below a given threshold.</p><p>Hyperparameters. All estimators are built using the Gaussian kernel. The number of blocks in MoM-KDE is selected on a regular grid of 20 values between 1 and 2|O| + 1 in order to obtain the lowest D JS . The bandwidth h is chosen for KDE via the pseudo-likelihood k-cross-validation method <ref type="bibr" target="#b33">[Turlach, 1993]</ref>, and used for all estimators. The construction of RKDE follows exactly the indications of its authors <ref type="bibr" target="#b16">[Kim and Scott, 2012]</ref> and ρ(•) is taken to be the Hampel function as they empirically showed that it is the most robust. For <software ContextAttributes="used">SPKDE</software>, the true ratio of anomalies is given as an input parameter. A lower score means a better estimation of the true density.</p></div>
<div><head n="4.1">Results on synthetic data.</head><p>To evaluate the efficiency of the MoM-KDE against KDE and its robust competitors, we set up several outlier situations. In all theses situations, we draw N = 1000 inliers from an equally distributed mixture of two normal distribution N (µ 1 , σ 1 ) and N (µ 2 , σ 2 ) with µ 1 = 0, µ 2 = 6, and σ 1 = σ 2 = 0.5. The outliers however are sampled through various schemes:</p><p>(a) Uniform. A uniform distribution U ([µ 1 -3, µ 2 + 3]) which is the classical setting used for outlier simulation.</p><p>(b) Regular Gaussian. A similar -variance normal distribution N (3, 0.5) located between the two inlier clusters.</p><p>(c) Thin Gaussian. A low -variance normal distribution N (3, 0.01) located between the two inliers clusters.</p><p>(d) Adversarial Thin Gaussian. A low variance normal distribution N (0, 0.01) located on one of the inliers' Gaussian mode. This scenario can be seen as adversarial as an ill-intentioned agent may hide wrong points in region of high density. It is the most challenging setting for standard robust estimators as they are in general robust to outliers located outside the support of the density we wish to estimate.</p><p>For all situations, we consider several ratios of contamination and set the number of outliers |O| in order to obtain a ratio |O|/n ranging from 0.05 to 0.5 with 0.05-wide steps. Finally, to evaluate the pertinence of our results, for each set of parameters, data are generated 10 times.</p><p>We display in Figure <ref type="figure" target="#fig_1">2</ref> the results over synthetic data using the D JS score. The average scores and standard deviations over the 10 experiments are represented for each outlier scheme and ratio. Overall, the results show the good performance of MoM-KDE in all the considered situations. Furthermore, they highlight the dependency of the two competitors to the type of outliers. Indeed, as <software ContextAttributes="used">SPKDE</software> is designed to handle uniformly distributed outliers, the algorithm struggles when confronted with differently distributed outliers (see Figure <ref type="figure" target="#fig_1">2 (b,</ref><ref type="figure">c,</ref><ref type="figure">d</ref>)). RKDE performs generally better, but fails against adversarial contamination, which may be explained by its tendency to down-weight points located in low-density regions, which for this particular case correspond to the inliers. Results over D KL and AUC are reported in the supplementary materials. Generally, they show similar results and the same conclusions on the good performance of MoM-KDE can be made.</p></div>
<div><head n="4.2">Results on real data.</head><p>Experiments are also conducted over six classification datasets: Banana, German, Titanic, Breast-cancer, Iris, and Digits. They contain respectively n = 5300, 1000, 2201, 569, 150 and 1797 data points having d = 2, 20, 3, 30, 4 and 64 input dimensions. They are all publicly available either from open repositories<ref type="foot" target="#foot_1">2</ref> (for the first three) or directly from <software ContextAttributes="used">Scikit-learn</software> package (for the last three) <ref type="bibr" target="#b26">[Pedregosa et al., 2011]</ref>. We follow the approach of <ref type="bibr" target="#b16">Kim and Scott [2012]</ref> that consists in setting the class labeled 0 as outliers and the rest as inliers. To artificially control the outlier proportion, we randomly downsample the abnormal class to reach a ratio |O|/n ranging from 0.05 to 0.5 with 0.05-wide steps. When a dataset does not contain enough outliers to reach a given ratio, we similarly downsample the inliers. For each dataset and each ratio, the experiments are performed 50 times, the random downsampling resulting in different learning datasets. The empirical performance is evaluated through the capacity of each estimator to detect anomalies, which we measure with the AUC.</p><p>Results are displayed in Figure <ref type="figure" target="#fig_2">3</ref>. With the Digits dataset, we also explore additional scenarios with changing inlier and outlier classes (specified in figure titles). Overall, results are in line with performances observed over synthetic experiments, achieving good results in comparison to its competitors. Note that even in the highest dimensional scenarios, i.e. Digits and Breast cancer (d = 64 and d = 30), MoM-KDE still behaves well, outperforming its competitors. Additional results are reported in the supplementary materials.</p></div>
<div><head n="5">Conclusion</head><p>The present paper introduced MoM-KDE, a new efficient way to perform robust kernel density estimation. The method has been shown to be consistent in both L ∞ and L 1 error-norm in presence of very generic outliers, enjoying a similar rate of convergence than the KDE without outliers. MoM-KDE achieved good empirical results in various situations while having a lower computational complexity than its competitors.</p><p>This work proposed to use the coordinate-wise median to construct its robust estimator. Future works will investigate the use of other generalization of the median in high dimension, e.g. the geometric median. In addition, further investigation will include a deeper statistical analysis under the hurdle contamination model in order to analyse the minimax optimality <ref type="bibr" target="#b22">[Liu et al., 2019]</ref> of MoM-KDE.</p></div>
<div><head>APPENDIX</head></div>
<div><head>A Technical proofs</head><p>Lemma 1. (L ∞ error-bound of the KDE without anomalies) Suppose that f belongs to the class of densities P(α, L) defined as</p><formula xml:id="formula_20">P(α, L) f | f ≥ 0, f (x)dx = 1, and f ∈ Σ(α, L) ,</formula><p>where Σ(α, L) is the Hölder class of function on R d . Grant assumptions 1 to 4 and let n &gt; 1, h ∈ (0, 1) and S ≥ 1 such that nh d ≥ S and nh d ≥ |log(h)|. Then with probability at least 1 -exp(-S), we have</p><formula xml:id="formula_21">fn -f ∞ ≤ C 1 S| log(h)| nh d + C 2 h α ,</formula><p>where </p><formula xml:id="formula_22">C 2 = L u α K(u)du &lt; ∞</formula><formula xml:id="formula_23">fMoM -f ∞ ≤ C 1 S log 2(2+δ) δ | log(h)| nh d + C 2 h α ,</formula><p>where C 2 = L u α K(u)du &lt; ∞ and C 1 is a constant that only depends on f ∞ , the dimension d, and the kernel properties.</p><p>Proof. From the definition of the MoM-KDE, we have the following implication <ref type="bibr" target="#b19">[Lecué et al., 2020</ref>]</p><formula xml:id="formula_24">sup x fMoM (x) -f (x) ≥ ε =⇒ sup x S k=1 I fns (x) -f (x) &gt; ε ≥ S/2 .</formula><p>Thus to upper-bound the probability of the left-hand event, it suffices to upper-bound the probability of the right-hand event. Moreover, we have  </p><formula xml:id="formula_25">fns (x) -f (x) ≤ sup x fns (x) -f (x) =⇒ I fns (x) -f (x) &gt; ε ≤ I sup x fns (x) -f (x) &gt; ε =⇒ S k=1 I fns (x) -f (x) &gt; ε ≤ S s=1 I sup x fns (x) -f (x) &gt; ε =⇒ sup x S s=1 I fns (x) -f (x) &gt; ε ≤ S s=1 I sup x fns (x) -f (x) &gt; ε ,</formula><formula xml:id="formula_26">s=1 I sup x fns (x) -f (x) &gt; ε = S s=1 Z s = s∈S Z s + s∈S C Z s ≤ s∈S Z s + |O| = s∈S [Z k -E (Z s ) + E (Z s )] + |O| = s∈S [Z s -E (Z s )] + s∈S E (Z s ) + |O| ≤ S s=1 [Z s -E (Z s )] + S • E (Z 1 ) + |O| ≤ S s=1 [Z s -E (Z s )] + S • P sup x fn1 (x) -f (x) &gt; ε + |O| ,<label>(8)</label></formula><p>where Z 1 is assumed, without loss of generality, to be associated to a block not containing outliers. This block always exists thanks to the hypothesis S &gt; (2 + δ)|O|.</p><formula xml:id="formula_27">Let ε = C 1 S log( 2(2+δ) δ )| log(h)| nh d + C 2 h α , then using Lemma 1 with S = log( 2(2+δ) δ ), we have P sup x fn1 (x) -f (x) &gt; ε ≤ δ 2(2 + δ) .</formula><p>Combining this last inequality with equation ( <ref type="formula" target="#formula_26">8</ref>) leads to where A s = x | fMoM (x) = fns (x) . Without loss of generality, we assume that</p><formula xml:id="formula_28">P S s=1 I sup x fns (x) -f (x) &gt; ε ≥ S/2 ≤ P S s=1 [Z s -E (Z s )] + S • δ 2(2 + δ + |O| ≥ S/2 ≤ P S s=1 [Z s -E (Z s )] ≥ S 1 2 - δ 2(2 + δ) - |O| S ≤ P S s=1 [Z s -E (Z s )] ≥ S 1 2 + δ - |O| S Tacking ∆ = 1 2+δ -|O| S &gt;<label>0</label></formula><formula xml:id="formula_29">A k S ∩ s = A = ∅, S ∪ s=1 A s = R d , and S s=1 I As (x) = 1 . fMoM (x) -f (x) dx = S s=1 fns (x)I As (x) -f (x) dx = S s=1 fns (x) -f (x) I As (x) dx ≤ S s=1 fns (x) -f (x) I As (x)dx = S s=1 As fns (x) -f (x) dx = s∈S As fns (x) -f (x) dx + s∈S C As fns (x) -f (x) dx .<label>(9)</label></formula><p>From the L 1 -consistency of the KDE in probability, if the number of anomalies grows at a small enough speed <ref type="bibr" target="#b3">[Devroye and Gyorfi, 1985]</ref>, the left part is bounded, i.e.</p><p>s∈S As</p><formula xml:id="formula_30">fns (x) -f (x) dx ≤ s∈S fns (x) -f (x) dx P -→ n→∞ 0 .<label>(10)</label></formula><p>We now upper-bound the right part of equation ( <ref type="formula" target="#formula_29">9</ref>). Let consider a particular block A s where s ∈ S C . In this block, the estimator f ns is selected and is calculated with samples containing anomalies. As ∀x ∈ A s , f ns (x) is the median (by definition), if S &gt; 2|O|, we can always find a s ∈ S such that f ns (x) ≤ f n s (x) or f ns (x) ≥ f n s (x). </p></div>
<div><head>Now let denote by A</head><formula xml:id="formula_31">+ s = x ∈ A s | fns (x) ≥ f (x) and A - s = x ∈ A s | fns (x) &lt; f (x) . We have A + s ∪ A - s = A</formula><formula xml:id="formula_32">= x ∈ As | fn s (x) ≥ fns (x) ≥ f (x) and A s ,- s = x ∈ As | fn s (x) ≤ fns (x) &lt; f (x) .</formula><p>Finally, the right-hand term of equation ( <ref type="formula" target="#formula_29">9</ref>) can be upperbounded by</p><formula xml:id="formula_33">s∈S C As fns (x) -f (x) dx ≤ s∈S C A + s fns (x) -f (x) dx + A - s fns (x) -f (x) dx ≤ s∈S C s ∈S A s ,+ s fns (x) -f (x) dx + A s ,- s fns (x) -f (x) dx ≤ s∈S C s ∈S A s ,+ s fn s (x) -f (x) dx + A s ,- s fn s (x) -f (x) dx ≤ s∈S C s ∈S fn s (x) -f (x) dx + fn s (x) -f (x) dx .</formula><p>Since ∀s ∈ S we have | fn s (x) -f (x)|dx </p></div>
<div><head>B Additional results</head><p>As stated in the main paper, we display here the additional results containing:</p><p>• For synthetic data, the Kullback-Leibler divergence in both directions, i.e. D KL ( f , f ) and D KL (f, f ), and the ROC AUC measuring the performance of an anomaly detector based on f . Results are displayed on Figure <ref type="figure" target="#fig_8">4</ref>.</p><p>• For Digits data, the ROC AUC measuring the performance of an anomaly detector based on f . As stated in the main paper, this is done under multiple scenarios, where outliers and inliers can be chosen among the nine available classes. Here we show the AUC when the outliers are set as one class (class 2 to class 9), and inliers are set as "the rest" of all classes. Results are displayed on Figure <ref type="figure" target="#fig_10">5</ref>.</p><p>Synthetic data. When considering the Kullback-Leibler divergence, results lead to a very similar conclusion as previously stated, that is, an overall good performance of MoM-KDE while its competitors, notably <software>SPKDE</software>, are more data-dependent. When the density estimate f is used in a simple anomaly detector, results are quite different. Indeed, when outliers are uniformly distributed, even if MoM-KDE seems to better estimate the true density (according to D JS and D KL ), this doesn't make fMoM a better anomaly detector. It seems that in this case, the outliers are either easily detected because distant from the density estimate, or located in dense regions, thus making them impossible to identify, and this for all density estimates provided by competitors. In the case of adversarial contamination, the conclusion is quite similar. Although MoM-KDE better fits the true density, the situation is extremely difficult for anomaly detection, hence making all competitors yield very poor results. In the two other cases -Gaussian outlier, anomaly detection results follow the density estimation.</p><p>Digits data. Results over Digits scenarios are inline with main conclusions over real data. Although from one scenario to another, all methods have varied results, the overall observation is that MoM-KDE is either similar or better than its competitors.    </p></div><figure xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: True density, outliers, KDE, and MoM-KDE. (a) Estimates from a 1-D true density and outliers from a normal density centered in µ O = 10 with variance σ 2 O = 0.1. (b) Estimates from a 2-D true density and outliers from a normal density centered in µ O = (3, 3) with variance σ 2 O = 0.5I 2 .</figDesc></figure>
<figure xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Density estimation with synthetic data. The displayed metric is the Jensen-Shannon divergence. A lower score means a better estimation of the true density.</figDesc></figure>
<figure xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure3: Anomaly detection with real datasets, measured with AUC over varying outlier proportion. A higher score means a better detection of the outliers. For Digits, we specify which classes are chosen to be inliers (I) and outliers (O).</figDesc></figure>
<figure xml:id="fig_3"><head /><label /><figDesc>-f (x) &gt; ε ≥ S/2 . Let Z s = I sup x fns (x) -f (x) &gt; ε and let S = s ∈ {1, • • • , S} | B s ∩ O = ∅ i.e. the set of indices s such that the block B s does not contain any outliers. Since s∈S C I(•) is bounded by |O|, almost surely, the following holds.</figDesc></figure>
<figure xml:id="fig_4"><head>S</head><label /><figDesc /></figure>
<figure xml:id="fig_5"><head /><label /><figDesc>and applying Hoeffding's inequality to the right-hand side of the previous equation givesP ) -f (x) &gt; ε ≥ S/2 ≤ e -2S∆ 2 ,which concludes the proof. Proposition 2. (L 1 -consistency in probability) If n/S → ∞, h → 0, nh d → ∞, and S &gt; 2|O|, then fMoM -We first rewrite the MoM-KDE as fMoM (x) = S s=1 fns (x)I As (x) ,</figDesc></figure>
<figure xml:id="fig_6"><head /><label /><figDesc>can conclude using similar arguments as those used for (10) that s∈S C As | fns (x) -f (x)|dx P -→ n→∞ 0, which concludes the proof.</figDesc></figure>
<figure xml:id="fig_8"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Density estimation with synthetic data. The displayed metrics are the Kullback-Leibler divergence (a lower score means a better estimation of the true density) and the AUC (a higher score means a better detection of the outliers).</figDesc></figure>
<figure xml:id="fig_9"><head /><label /><figDesc>(a) O: 2, I: all (b) O: 3, I: all (c) O: 4, I: all (d) O: 5, I: all (e) O: 6, I: all (f) O: 7, I: all (g) O: 8, I: all (h) O: 9, I: all</figDesc></figure>
<figure xml:id="fig_10"><head>Figure 5 :</head><label>5</label><figDesc>Figure5: Anomaly detection with Digits data, measured with AUC over varying outlier proportion. A higher score means a better detection of the outliers. We specify which classes are chosen to be inliers (I) and outliers (O).</figDesc></figure>
<figure type="table" xml:id="tab_0"><head /><label /><figDesc>Definition 1. (MoM Kernel Density Estimator) Let 1 ≤ S ≤ n, and let B 1 , • • • , B S be a random partition of {1, • • • , n} into S non-overlapping blocks B s of equal size n s n/S. The MoM Kernel Density Estimator (MoM-KDE) of f at x 0 is given by fMoM</figDesc><table /></figure>
<figure type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Computational complexity</figDesc><table><row><cell>Method</cell><cell>Learning</cell><cell cols="2">Evaluation Iterative method</cell></row><row><cell>KDE [Parzen, 1962]</cell><cell>-</cell><cell>O(n)</cell><cell>no</cell></row><row><cell>RKDE [Kim and Scott, 2012]</cell><cell /><cell /><cell /></row></table></figure>
<figure type="table" xml:id="tab_2"><head /><label /><figDesc>and C 1 is a constant that only depends on f ∞ , the dimension d, and the kernel properties.Proposition 1. (L ∞ error-bound of the MoM-KDE under the O ∪ I) Suppose that f belongs to the class of densities P(α, L) and grant assumptions 1 to 4. Let S be the number of blocks, δ &gt; 0 such that S &gt; (2 + δ)|O|, and ∆ = (1/(2 + δ) -|O|/S). Then, for any h ∈ (0, 1), δ sufficiently small, and n ≥ 1 such that nh d ≥ S log(2(2 + δ)/δ), and nh d ≥ S| log(h)|, we have with probability at least 1 -exp(-2∆ 2 S),</figDesc><table /></figure>
			<note place="foot" n="1" xml:id="foot_0"><p>https://github.com/lminvielle/mom-kde For the sake of comparison, we also implemented RKDE and <software>SPKDE</software>.</p></note>
			<note place="foot" n="2" xml:id="foot_1"><p>http://www.raetschlab.org/Members/raetsch/benchmark/</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The space complexity of approximating the frequency moments</title>
		<author>
			<persName><forename type="first">N</forename><surname>Alon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Matias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computer and System Sciences</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="137" to="147" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Space and time efficient kernel density estimation in high dimensions</title>
		<author>
			<persName><forename type="first">A</forename><surname>Backurs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Indyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wagner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="15773" to="15782" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Time bounds for selection</title>
		<author>
			<persName><forename type="first">M</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>Floyd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">R</forename><surname>Pratt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Rivest</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Tarjan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computer and System Sciences</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="448" to="461" />
			<date type="published" when="1973">1973</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Devroye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Gyorfi</surname></persName>
		</author>
		<title level="m">Nonparametric Density Estimation: The L1 View</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>John Wiley &amp; Sons</publisher>
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Combinatorial methods in density estimation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Devroye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lugosi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Sub-gaussian mean estimators</title>
		<author>
			<persName><forename type="first">L</forename><surname>Devroye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lerasle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lugosi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">I</forename><surname>Oliveira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2695" to="2725" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A new metric for probability distributions</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Endres</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Schindelin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information theory</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1858" to="1860" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Rates of strong uniform consistency for multivariate kernel density estimators</title>
		<author>
			<persName><forename type="first">E</forename><surname>Giné</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Guillou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annales de l'Institut Henri Poincare (B) Probability and Statistics</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="907" to="921" />
			<date type="published" when="2002">2002</date>
			<publisher>Elsevier</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Nonparametric density estimation: Toward computational tractability</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Moore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2003 SIAM International Conference on Data Mining</title>
		<meeting>the 2003 SIAM International Conference on Data Mining</meeting>
		<imprint>
			<biblScope unit="page" from="203" to="211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Rapid evaluation of multiple density models</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Moore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Robust estimation of a location parameter</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Huber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Breakthroughs in statistics</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page" from="492" to="518" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Random generation of combinatorial structures from a uniform distribution</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Jerrum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">G</forename><surname>Valiant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">V</forename><surname>Vazirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Theoretical Computer Science</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="169" to="188" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Uniform convergence rates for kernel density estimation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>JMLR. org</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1694" to="1703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Robust kernel density estimation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Scott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2008 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="3381" to="3384" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">On the robustness of kernel density m-estimators</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Scott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Machine Learning (ICML-11)</title>
		<meeting>the 28th International Conference on Machine Learning (ICML-11)</meeting>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="697" to="704" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">kernel density estimation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Scott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="2529" to="2565" />
			<date type="published" when="2012-09">Sep. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">On information and sufficiency</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kullback</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Leibler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The annals of mathematical statistics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="79" to="86" />
			<date type="published" when="1951">1951</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning from mom's principles: Le cam's approach</title>
		<author>
			<persName><forename type="first">G</forename><surname>Lecué</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lerasle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Stochastic Processes and their applications</title>
		<imprint>
			<biblScope unit="volume">129</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="4385" to="4410" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Robust classification via mom minimization</title>
		<author>
			<persName><forename type="first">G</forename><surname>Lecué</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lerasle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mathieu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Lerasle</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.10761</idno>
		<title level="m">Lecture notes: Selected topics on robust statistical learning theory</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">On divergences and informations in statistics and information theory</title>
		<author>
			<persName><forename type="first">F</forename><surname>Liese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Vajda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="4394" to="4412" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Density estimation with contamination: minimax rates and theory of adaptation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Electronic Journal of Statistics</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="3613" to="3653" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Geometric median and robust estimation in banach spaces</title>
		<author>
			<persName><forename type="first">S</forename><surname>Minsker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bernoulli</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="2308" to="2335" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Problem complexity and method efficiency in optimization</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Nemirovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Yudin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1983">1983</date>
			<publisher>Wiley Interscience</publisher>
			<pubPlace>New-York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">On estimation of a probability density function and mode</title>
		<author>
			<persName><forename type="first">E</forename><surname>Parzen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The annals of mathematical statistics</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1065" to="1076" />
			<date type="published" when="1962">1962</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Scikit-learn: Machine learning in Python</title>
		<author>
			<persName><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Dubourg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cournapeau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brucher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Perrot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Duchesnay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Optimal rates for plug-in estimators of density level sets</title>
		<author>
			<persName><forename type="first">P</forename><surname>Rigollet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bernoulli</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1154" to="1178" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Multivariate density estimation: theory, practice, and visualization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Scott</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>John Wiley &amp; Sons</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learn on source, refine on target: A model transfer learning framework with random forests</title>
		<author>
			<persName><forename type="first">N</forename><surname>Segev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Harel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mannor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>El-Yaniv</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1811" to="1824" />
			<date type="published" when="2017-09">Sep. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Density estimation for statistics and data analysis</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">W</forename><surname>Silverman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1986">1986</date>
			<publisher>CRC press</publisher>
			<biblScope unit="volume">26</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Consistency and rates for clustering with dbscan</title>
		<author>
			<persName><forename type="first">B</forename><surname>Sriperumbudur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Steinwart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1090" to="1098" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Introduction to nonparametric estimation</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Tsybakov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Bandwidth selection in kernel density estimation: A review</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Turlach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">CORE and Institut de Statistique</title>
		<imprint>
			<date type="published" when="1993">1993</date>
			<publisher>Citeseer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Consistency of robust kernel density estimators</title>
		<author>
			<persName><forename type="first">R</forename><surname>Vandermeulen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Scott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Learning Theory</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="568" to="591" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Robust kernel density estimation by scaling and projection in hilbert space</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Vandermeulen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Scott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="433" to="441" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Dbscan: Optimal rates for density-based cluster estimation</title>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rinaldo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">170</biblScope>
			<biblScope unit="page" from="1" to="50" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Nonparametric density estimation for high-dimensional data-algorithms and applications</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Scott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Wiley Interdisciplinary Reviews: Computational Statistics</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">e1461</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>