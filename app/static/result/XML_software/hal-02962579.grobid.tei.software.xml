<TEI xmlns="http://www.tei-c.org/ns/1.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xml:space="preserve" xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Distributed Caching of Scientific Workflows in Multisite Cloud</title>
				<funder>
					<orgName type="full">France Grille Scientific Interest Group</orgName>
				</funder>
				<funder ref="#_8JRs6fG">
					<orgName type="full">Agence Nationale de la Recherche</orgName>
					<orgName type="abbreviated">ANR</orgName>
				</funder>
				<funder ref="#_z5DFHtC">
					<orgName type="full">DigitAg French initiative</orgName>
				</funder>
				<funder ref="#_FDruKP7">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher />
				<availability status="unknown"><licence /></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Gaëtan</forename><surname>Heidsieck</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Inria &amp; LIRMM</orgName>
								<orgName type="institution">Univ. Montpellier</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Daniel</forename><surname>De Oliveira</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">UFF</orgName>
								<address>
									<settlement>Niteroi</settlement>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Esther</forename><surname>Pacitti</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Inria &amp; LIRMM</orgName>
								<orgName type="institution">Univ. Montpellier</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Christophe</forename><surname>Pradal</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Inria &amp; LIRMM</orgName>
								<orgName type="institution">Univ. Montpellier</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">CIRAD &amp; AGAP</orgName>
								<orgName type="institution">Univ. Montpellier</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">François</forename><surname>Tardieu</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">INRAE &amp; LEPSE</orgName>
								<orgName type="institution">Univ. Montpellier</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Patrick</forename><surname>Valduriez</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Inria &amp; LIRMM</orgName>
								<orgName type="institution">Univ. Montpellier</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Distributed Caching of Scientific Workflows in Multisite Cloud</title>
					</analytic>
					<monogr>
						<imprint>
							<date />
						</imprint>
					</monogr>
					<idno type="MD5">633C107D3ED90E2A1C9F69858E25927E</idno>
					<idno type="DOI">10.1007/978-3-030-59051-2_4</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-04-12T14:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid" />
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Multisite cloud</term>
					<term>Distributed Caching</term>
					<term>Scientific Workflow</term>
					<term>Workflow System</term>
					<term>Workflow Scheduling</term>
				</keywords>
			</textClass>
			<abstract>
<div><p>Many scientific experiments are performed using scientific workflows, which are becoming more and more data-intensive. We consider the efficient execution of such workflows in the cloud, leveraging the heterogeneous resources available at multiple cloud sites (geo-distributed data centers). Since it is common for workflow users to reuse code or data from other workflows, a promising approach for efficient workflow execution is to cache intermediate data in order to avoid re-executing entire workflows. In this paper, we propose a solution for distributed caching of scientific workflows in a multisite cloud. We implemented our solution in the <software>OpenAlea</software> workflow system, together with cache-aware distributed scheduling algorithms. Our experimental evaluation on a three-site cloud with a data-intensive application in plant phenotyping shows that our solution can yield major performance gains, reducing total time up to 42% with 60% of same input data for each new execution.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div><head n="1">Introduction</head><p>In many scientific domains, e.g., bio-science <ref type="bibr" target="#b6">[7]</ref>, complex numerical experiments typically require many processing or analysis steps over huge datasets. They can be represented as scientific workflows, or workflows, for short, which facilitate the modeling, management and execution of computational activities linked by data dependencies. As the size of the data processed and the complexity of the computation keep increasing, these workflows become data-intensive <ref type="bibr" target="#b6">[7]</ref>, thus requiring high-performance computing resources.</p><p>The cloud is a convenient infrastructure for handling workflows, as it allows leasing resources at a very large scale and relatively low cost. In this paper, we consider the execution of a large workflow in a multisite cloud, i.e., a cloud with geo-distributed cloud data centers (sites). Note that a multisite option is now well supported by all popular public clouds, e.g., Microsoft <software ContextAttributes="used">Azure</software>, Amazon <software ContextAttributes="used">EC2</software>, and <software ContextAttributes="used">Google Cloud</software>, which provide the capability of using multiple sites with a single cloud account, thus avoiding the burden of using multiple accounts. The main reasons for using multiple cloud sites for data-intensive workflows is that they often exceed the capabilities of a single site, either because the site imposes usage limits for fairness and security, or simply because the datasets are too large. In scientific applications, there can be much heterogeneity in the storage and computing capabilities of the different sites, e.g., on premise servers, HPC platforms from research organizations or federated cloud sites at the national level <ref type="bibr" target="#b3">[4]</ref>. As an example in plant phenotyping, greenhouse platforms generate terabytes of raw data from plants, which are typically stored at data centers geographically close to the greenhouse to minimize data transfers. However, the computation power of those data centers may be limited and fail to scale when the analyses become more complex, such as in plant modeling or 3D reconstruction. Other computation sites are then required.</p><p>Most Scientific Workflow Management Systems (workflow systems) can execute workflows in the cloud <ref type="bibr" target="#b11">[12]</ref>. Some examples are Swift/T, Pegasus, <software ContextAttributes="used">SciCumulus</software>, Kepler and <software ContextAttributes="used">OpenAlea</software> <ref type="bibr" target="#b8">[9]</ref>. Our work is based on <software ContextAttributes="used">OpenAlea</software> <ref type="bibr" target="#b13">[14]</ref>, which is widely used in plant science for simulation and analysis. Most existing systems use naive or user-based approaches to distribute the tasks across sites. The problem of scheduling a workflow execution over a multisite cloud has started to be addressed in <ref type="bibr" target="#b10">[11]</ref>, using performance models to predict the execution time on different resources. In <ref type="bibr" target="#b9">[10]</ref>, we proposed a solution based on multi-objective scheduling and a single site virtual machine provisioning approach, assuming homogeneous sites, as in public cloud.</p><p>Since it is common for workflow users to reuse code or data from other workflows <ref type="bibr" target="#b4">[5]</ref>, a promising approach for efficient workflow execution is to cache intermediate data in order to avoid re-executing entire workflows. Furthermore, a user may need to re-execute a workflow many times with different sets of parameters and input data depending on the previous results generated. Fragments of the workflow, i.e. a subset of the workflow activities and dependencies, can often be reused. Another important benefit of caching intermediate data is to make it easy to share with other research teams, thus fostering new analyses at low cost.</p><p>Caching has been supported by some workflow systems, e.g., Kepler, <software ContextAttributes="used">Vis-Trails</software> and <software ContextAttributes="used">OpenAlea</software>. Kepler <ref type="bibr" target="#b0">[1]</ref> provides a persistent cache on the cloud, but at a single site, and does not support multisite. <software ContextAttributes="used">VisTrails</software> <ref type="bibr" target="#b2">[3]</ref> provides a persistent cache, but only for local execution on a personal desktop. In <ref type="bibr" target="#b5">[6]</ref>, we proposed an adaptive caching method for <software ContextAttributes="used">OpenAlea</software> that automatically determines the most suited intermediate data to cache, taking into account workflow fragments, but only in the case of a single cloud site. Another interesting single site method, also exploiting workflow fragments, is to compute the ratio between re-computation cost and storage cost to determine what intermediate data should be stored <ref type="bibr" target="#b15">[16]</ref>. All these methods are single site (centralized). The only distributed caching method for workflow execution in a multisite cloud we are aware of is restricted to hot metadata (frequently accessed metadata) <ref type="bibr" target="#b7">[8]</ref>, ignoring intermediate data.</p><p>Caching data in a multisite cloud with heterogeneous sites is much more complex. In addition to the trade-off between re-computation and storage cost at single sites, there is the problem of site selection for placing cached data. The problem is more difficult than data allocation in distributed databases <ref type="bibr" target="#b12">[13]</ref>, which deals only with well-defined base data, not intermediate data produced by tasks. Furthermore, the scheduling of workflow executions must be cacheaware, i.e., exploit the knowledge of cached data to decide between reusing and transferring cached data versus re-executing the workflow fragments.</p><p>In this paper, we propose a distributed solution for caching of scientific workflows in a multisite cloud. Based on a distributed and parallel architecture <ref type="bibr" target="#b12">[13]</ref>, composed of heterogeneous sites (including on premise servers and shared-nothing clusters), we propose algorithms for adaptive caching, cache site selection and dynamic workflow scheduling. We implemented our caching solution in <software ContextAttributes="used">OpenAlea</software>, together with a multisite scheduling algorithm. Based on a real data-intensive application in plant phenotyping, we provide an extensive experimental evaluation using a cloud with three heterogeneous sites.</p><p>This paper is organized as follows. Section 2 presents our real use case in plant phenotyping. Section 3 introduces our workflow system architecture in multisite cloud. Section 4 describes our cache management solution. Section 5 gives our experimental evaluation. Finally, Section 6 concludes.</p></div>
<div><head n="2">Use Case in Plant Phenotyping</head><p>In this section, we introduce a real use case in plant phenotyping that will serve as motivation for the work and basis for the experimental evaluation. In the last decade, high-throughput phenotyping platforms have emerged to allow for the acquisition of quantitative data on thousands of plants in well-controlled environmental conditions. For instance, the seven facilities of the French Phenome project <ref type="foot" target="#foot_0">5</ref> produce each year 200 Terabytes of data, which are various (images, environmental conditions and sensor outputs), multiscale and originate from different sites. Analyzing such massive datasets is an open, yet important, problem for biologists <ref type="bibr" target="#b14">[15]</ref>.</p><p>The Phenomenal workflow ( <ref type="bibr" target="#b1">[2]</ref>) has been developed in <software ContextAttributes="used">OpenAlea</software> to analyze and reconstruct the geometry and topology of thousands of plants through time in various conditions. It is composed of nine fragments such as image binarization, 3D volume reconstruction, organ segmentation or intercepted light simulation. Different users can conduct different biological analyses by reusing some workflow fragments on the same dataset to test different hypotheses <ref type="bibr" target="#b5">[6]</ref>. To save both time and resources, they want to reuse the intermediate results that have already been computed rather than recompute them from scratch.</p><p>The raw data comes from the <software>Phenoarch</software> platform, which has a capacity of 1,680 plants within a controlled environment (e.g., temperature, humidity, irrigation) and automatic imaging through time. The total size of the raw image dataset for one experiment is 11 Terabytes. To limit data movement, the raw data is stored at a server near to the experimental platform, with both data storage and computing resources. However, these computing resources are not enough to process a full experiment in a relatively short time. Thus, scientists who need to do a full experiment will execute the Phenomenal workflow at a more powerful site by transferring the raw data for each new analysis.</p><p>In this Phenomenal use case, the cloud is composed of heterogeneous sites, with both on premise servers close to the experimental platform and other more powerful cloud sites. The on premise server has high storage capacity and hosts the raw data. Other sites are used to computational intensive executions, with high-performance computing resources. On premise servers are used locally to execute some Phenomenal fragments that do not require powerful resources. In this case, one has to choose between transferring the raw data or some intermediate data to a powerful site or re-executing some fragments locally before transferring intermediate data. The trade-off between data re-computation and data transfer is complex in a multisite cloud with much heterogeneity. In particular, one needs to pay attention to cached data placement, so as to avoid bottlenecks on the most used intermediate data.</p></div>
<div><head n="3">Multisite Cloud Workflow System Architecture</head><p>In this section, we present our workflow system architecture that integrates caching and reuse of intermediate data in a multisite cloud. We motivate our design decisions and describe our architecture in terms of nodes and components (see Figure <ref type="figure" target="#fig_0">1</ref>), which are involved in the processing of workflows.</p><p>Our architecture capitalizes on the latest advances in distributed and parallel data management to offer performance and scalability <ref type="bibr" target="#b12">[13]</ref>. We consider a distributed cloud architecture with on premise servers, where raw data is produced, e.g., by a phenotyping experimental platform in our use case, and remote sites, where the workflow is executed. The remote sites (data centers) are sharednothing clusters, i.e., clusters of server machines, each with processor, memory and disk. We adopt shared-nothing as it is the most scalable and cost-effective architecture for big data analysis.</p><p>In the cloud, metadata management has a critical impact on the efficiency of workflow scheduling as it provides a global view of data location, e.g., at which nodes some raw data is stored, and enables task tracking during execution <ref type="bibr" target="#b7">[8]</ref>. We organize the metadata in three repositories: catalog, provenance database and cache index. The catalog contains all information about users (access rights, etc.), raw data location and workflows (code libraries, application code). The provenance database captures all information about workflow execution. The cache index contains information about tasks and cache data produced, as well as the location of files that store the cache data. Thus, the cache index itself is small (only file references) and the cached data can be managed using the underlying file system. A good solution for implementing these metadata repositories is a key-value store, such as <software ContextAttributes="used">Cassandra</software><ref type="foot" target="#foot_1">6</ref> , which provides efficient key-based access, scalability and fault-tolerance through replication in a shared-nothing cluster.</p><p>The raw data (files) are initially produced and stored at some cloud sites, e.g., in our use case, at the phenotyping platform. During workflow execution, the intermediate data is generated and consumed at one site's node in memory. It gets written to disk when it must be transferred to another node (potentially at the same site), or when explicitly added to the cache. The cached data (files) can later be replicated at other sites to minimize data transfers.</p><p>We extend the workflow system architecture proposed in <ref type="bibr" target="#b8">[9]</ref> for single site. It is composed of six modules: workflow manager, global scheduler, local scheduler, task manager, data manager and metadata manager, to support both execution and intermediate data caching in a multisite cloud. The workflow manager provides a user interface for workflow definition and processing. Before workflow execution, the user selects a number of virtual machines (VMs), given a set of possible instance formats, i.e., the technical characteristics of the VMs, deployed on each site's nodes. When a workflow execution is started, the workflow manager simplifies the workflow by removing some workflow fragments and partitions depending on the raw input data and the cached data (see Section 4). The global scheduler uses the metadata (catalog, provenance database, and cache index) to schedule the workflow fragments of the simplified workflow. The VMs on each site are then initialized, i.e., the programs required for the execution of the tasks are installed and all parameters are configured. The local scheduler schedules the workflow fragments received on its VMs. The data manager module handles data transfers between sites during execution (for both newly generated intermediate data and cached data) and manages cache storage and replication. At a single site, data storage is distributed between nodes. Finally, the task manager (on each VM) manages the execution of fragments on the VMs at each site. It exploits the provenance metadata to decide whether or not the task's output data should be placed in the cache, based on the cache provisioning algorithm described in Section 4. Local scheduling and execution can be performed as in <ref type="bibr" target="#b5">[6]</ref>.</p><p>Figure <ref type="figure" target="#fig_0">1</ref> shows how these components are involved in workflow processing, using the traditional master-worker model. In this architecture, we consider two types of cloud sites, i.e., coordinator and participant. The relationship between the site is also based on the master-worker model, the coordinator site, managing the participant sites. The workflow manager and the global scheduler modules are implemented on the coordinator site. The remaining modules are implemented on all sites.</p><p>At each site, there are three kinds of nodes: master, compute and data nodes, which are mapped to cluster nodes at configuration time, e.g. using a cluster manager like <software>Yarn</software> ( http://hadoop.apache.org). There is one active master node per site. There is also a standby node to deal with master node failure. The master nodes are the only ones to communicate across sites. The local scheduler and metadata management modules are implemented on the master node, which manages communication, metadata and scheduling. The master nodes are responsible for transferring data between sites during execution.</p></div>
<div><head n="4">Multisite Cache-aware Workflow Execution</head><p>In this section, we present in more details how the global scheduler performs multisite cache-aware workflow execution. In particular, the global scheduler must decide which data to cache (cache data selection) and where (cache site selection), and where to execute workflow fragments (execution site selection). Since these decisions are not independent, we propose a cost function to make a global decision, based on the cost components for individual decisions. We start by giving an overview of distributed workflow execution. Then, we present the methods and cost functions for cache data selection, cache site selection and execution site selection. Finally, we introduce our cost function for the global decision.</p></div>
<div><head n="4.1">Distributed Workflow Execution Overview</head><p>We consider a multisite cloud with a set of sites S={s 1 , ..., s n }. A workflow W (A, D) is a a directed acyclic graph (DAG) of computational activities A and their data dependencies D. A task t is the instantiation of an activity during execution with specific associated input data. A fragment f of an instantiated workflow is a subset of tasks and their dependencies.</p><p>The execution of a workflow W (A, D) in S starts at a coordinator site s c and proceeds in three main steps:</p><p>1. The global scheduler at s c simplifies and partitions the workflow into fragments. Simplification uses metadata to decide whether a task can be replaced by corresponding cached data references. Partitioning uses the dependencies in D to produce fragments.</p><p>2. For each fragment, the global scheduler at s c computes a cost function to make a global decision on which data to cache where, and on which site to execute. Then, it triggers fragment execution and cache placement at the selected sites. 3. At each selected site, the local scheduler performs the execution of its received fragments using its task manager (to execute tasks) and data manager (to transfer the required input data). It also applies the decision of the global scheduler on storing new intermediate data into the cache.</p><p>We introduce basic cost functions to reflect data transfer and distributed execution. The time to transfer some data d from site s i to site s j , noted T tr (d, s i , s j ), is defined by</p><formula xml:id="formula_0">T tr (d, s i , s j ) = Size(d) T rRate(s i , s j )<label>(1)</label></formula><p>where T rRate(s i , s j ) is the transfer rate between s i and s j .</p><p>The time to transfer input and cached data, In(f ) and Cached(f ) respectively, to execute a fragment f at site s i is T input (f, s i ):</p><formula xml:id="formula_1">T input (f, s i ) = S sj (T tr (In(f ), s j , s i ) + T tr (Cached(f ), s j , s i ))<label>(2)</label></formula><p>The time to compute a fragment f at site s, noted T compute (f, s), can be estimated using Amdahl's law <ref type="bibr" target="#b16">[17]</ref>:</p><formula xml:id="formula_2">T compute (f, s) = ( α n + (1 -α)) * W (f ) CP U perf (s)<label>(3)</label></formula><p>where W (f ) is the workload for the execution of f , CP U perf (s) is the average computing performance of the CPUs at site s and n is the number of CPUs at site s. We suppose that the local scheduler may parallelize task executions. Therefore, α represents the percentage of the workload that can be executed in parallel.</p><p>The expected waiting time to be able to execute a fragment at site s is noted T wait (s), which is the minimum expected time for s to finish executing the fragments in its queue.</p><p>The time to transfer the intermediate data generated by fragment f at site s i to site s j , noted T write (Output(f ), s i , s j ), is defined by:</p><formula xml:id="formula_3">T write (Output(f ), s i , s j ) = T tr (Output(f ), s i , s j )<label>(4)</label></formula><p>where Output(f ) is the data generated by the execution of f .</p></div>
<div><head n="4.2">Cache Data Selection</head><p>To determine what new intermediate data to cache, we consider two different methods: greedy and adaptive. Greedy data selection simply adds all new data to the cache. Adaptive data selection extends our method proposed in <ref type="bibr" target="#b5">[6]</ref> to multisite cloud. It achieves a good trade-off between the cost saved by reusing cached data and the cost incurred to feed the cache.</p><p>To determine if it is worth adding some intermediate data Output(f ) at site s j , we consider the trade-off between the cost of adding this data to the cache and the potential benefit if this data was reused. The cost of adding the data to site s j is the time to transfer the data from the site where it was generated. The potential benefit is the time saved from loading the data from s j to the site of computation instead of re-executing the fragment. We model this trade-off with the ratio between the cost and benefit of the cache, noted p(f, s i , s j ), which can be computed from equations 2, 3 and 4,</p><formula xml:id="formula_4">p(f, s i , s j ) = T write (Output(f ), s i , s j ) T input (f, s i ) + T compute (f, s i ) -T tr (Output(f ), s j , s i )<label>(5)</label></formula><p>In the case of multiple users, the probability that Output(f ) will be reused or the number of times fragment f will be re-executed is not known when the workflow is executed. Thus, we introduce a threshold T hreshold (computed by the user) as the limit value to decide whether a fragment output will be added to the cache. The decision on whether Output(f ) generated at site s i is stored at site s j can be expressed by</p><formula xml:id="formula_5">i,j = 1, if p(f, s i , s j ) &lt; T hreshold. 0, otherwise.<label>(6)</label></formula></div>
<div><head n="4.3">Cache Site Selection</head><p>Cache site selection must take into account the data transfer cost and the heterogeneity of computing and storage resources. We propose two methods to balance either storage load (bStorage) or computation load (bCompute) between sites. The bStorage method allows preventing bottlenecks when loading cached data.</p><p>To assess this method at any site s, we use a load indicator, noted L bStorage (s), which represents the relative storage load as the ratio between the storage used for the cached data (Storage used (s)) and the total storage (Storage total (s)).</p><formula xml:id="formula_6">L bStorage (s) = Storage used (s) Storage total (s)<label>(7)</label></formula><p>The bCompute method balances the cached data between the most powerful sites, i.e., with more CPUs, to prevent computing bottlenecks during execution. Using the knowledge on the sites' computing resources and usage, we use a load indicator for each site s, noted L bCompute (s), based on CPUs idleness (CP U idle (s)) versus total CPU capacity (CP U total (s)).</p><formula xml:id="formula_7">L bCompute (s) = 1 -CP U idle (s) CP U total (s)<label>(8)</label></formula><p>The load of a site s, depending on the method used, is represented by L(s), ranging between 0 (empty load) and 1 (full). Given a fragment f executed at site s i , and a set of sites s j with enough storage for Output(f ), the best site s * to add Output(f ) to its cache can be obtained using Equation <ref type="formula" target="#formula_0">1</ref>(to include transfer time) and Equation <ref type="formula" target="#formula_5">6</ref>(to consider multiple users),</p><formula xml:id="formula_8">s * (f ) si = argmax sj ( i,j * (1 -L(s j )) T write (Output(f ), s i , s j ) ) (9)</formula></div>
<div><head n="4.4">Execution Site Selection</head><p>To select an execution site s for a fragment f , we need to estimate the execution time for f as well as the time to feed the cache with the result of f . The execution time f at site s (T execute (f, s)) is the sum of the time to transfer input and cached data to s, the time to get computing resources and the time to compute the fragment. It is obtained using Equations 2 and 3.</p><formula xml:id="formula_9">T execute (f, s) = T input (f, s) + T compute (f, s) + T wait (s) (<label>10</label></formula><formula xml:id="formula_10">)</formula><p>Given a fragment f executed at site s i and its intermediate data Output(f ), the time to write Output(f ) to the cache (T f eed cache (f, s i , s j )) can be defined as:</p><formula xml:id="formula_11">T f eed cache (f, s i , s j , i,j ) = i,j * T write (Output(f ), s i , s j ) (<label>11</label></formula><formula xml:id="formula_12">)</formula><p>where s j is given by Equation <ref type="formula">9</ref>.</p></div>
<div><head n="4.5">Global Decision</head><p>At Step 2 of workflow execution, for each fragment f , the global scheduler must decide on the best combination of individual decisions regarding cache data, cache site, and execution site. These individual decisions depend on each other. The decision on cache data depends on the site where the data is generated and the site where it will be stored. The decision on cache site depends on the site where the data is generated and the decision of whether or not the data will be cached. Finally, the decision on execution site depends on what data will be added to the cache and at which site. Using Equations 10 and 11, we can estimate the total time (T total ) for executing a fragment f at site s i and adding its intermediate data to the cache at another site s j :</p><formula xml:id="formula_13">T total (f, s i , s j , i,j ) = T execute (f, s i ) + T f eed cache (f, s i , s j , i,j )<label>(12)</label></formula><p>Then, the global decision for cache data ( (f )), cache site (s * cache ) and execution site (s * exec ) is based on minimizing the following equation for the n 2 pairs of sites s i and s j</p><formula xml:id="formula_14">(s * exec , s * cache , (f )) = argmin si,sj (T total (f, s i , s j , i,j )) (<label>13</label></formula><formula xml:id="formula_15">)</formula><p>This decision is done by the coordinator site at before each fragment execution and only takes into account the cloud site's status at that time. Note that s * exec , s * cache can be the coordinator site and can be the same site.</p></div>
<div><head n="5">Experimental Evaluation</head><p>In this section, we first present our experimental setup, which features a heterogeneous multisite cloud with multiple users who re-execute part of the workflow. Then, we compare the performance of our multisite cache scheduling method against two baseline methods. We end the section with concluding remarks.</p></div>
<div><head n="5.1">Experimental Setup</head><p>Our experimental setup includes a multisite cloud, with three sites in France, a workflow implementation and an experimental dataset. Site 1 in Montpellier is a server close to the <software>Phenoarch</software> phenotyping platform. It has the smallest number of CPUs and largest amount of storage among the sites. The raw data is stored at this site. Site 2 is the coordinator site, located in Lille. Site 3, located in Lyon, has the largest number of CPUs and the smallest amount of storage.</p><p>To model site heterogeneity in terms of storage and CPU resources, we use heterogeneity factor H in three configurations: H = 0, H = 0.3 and H = 0.7. For the three sites altogether, the total number of CPUs is 96 and the total storage on disk for intermediate data is 180 GB (The raw data is stored on an additional node at Site 1). On each site, several nodes are instantiated for the executions, they have a determined number of CPUs from 1, 2, 4, 8 or 16 CPUs. The available disk size for each node is limited by implementation. With H = 0 (homogeneous configuration), each site has 32 CPUs (two 16 CPUs nodes) and 60 GB (30 GB each). With H = 0.3, we have 22 CPUs and 83 GB for Site 1, 30 CPUs and 57 GB for Site 2 and 44 CPUs and 40 GB for Site 3. With H = 0.7 (most heterogeneous configuration), we have 6 CPUs and 135 GB for Site 1, 23 CPUs and 35 GB for Site 2 and 67 CPUs and 10 GB for Site 3.</p><p>The input dataset for the Phenomenal workflow is produced by the <software>Phenoarch</software> platform (see Section 2). Each execution of the workflow is performed on a subset of the input dataset, i.e. 200 GB of raw data, which represents the execution of 15,000 tasks. For each user, 60% of the raw data is reused from previous executions. Thus each execution requires only 40% of new raw data. For the first execution, no data is available in the cache.</p><p>We implemented our cache-aware scheduling method, which we call <software ContextAttributes="used">cacheA</software>, in <software ContextAttributes="used">OpenAlea</software> and deployed it at each site using the <software ContextAttributes="used">Conda</software> multi-OS package manager. The metadata distributed database is implemented using <software ContextAttributes="used">Cassandra</software>. Communication between the sites is done using the protocol library <software ContextAttributes="used">ZeroMQ</software>. Data transfer between sites is done through SSH. We have also implemented two baseline methods, <software ContextAttributes="used">Sgreedy</software> and <software ContextAttributes="used">Agreedy</software>, based on the <software ContextAttributes="used">SiteGreedy</software> and <software ContextAttributes="used">Act-Greedy</software> methods described in <ref type="bibr" target="#b9">[10]</ref>, respectively. The <software ContextAttributes="used">Sgreedy</software> method extends <software ContextAttributes="used">SiteGreedy</software>, which schedules each workflow fragment at a site that is available for execution, with our cache data/site selection methods. Similarly, the <software ContextAttributes="used">Agreedy</software> method extends <software ContextAttributes="used">ActGreedy</software>, which schedules each workflow fragment at a site that minimizes a cost function based on execution time and input data transfer time, with our cache data/site selection methods. These baseline methods perform execution site selection followed by cache data/site selection while <software ContextAttributes="used">CacheA</software> makes a global decision.</p></div>
<div><head n="5.2">Experiments</head><p>We compare <software>CacheA</software> with the two baseline methods in terms of execution time and amount of data transferred. We define total time as execution time plus transfer time. In experiment 1, we consider a workflow execution with caching or without. In Experiment 2, we consider multiple users who execute the same workflow on similar input data, where 60% of the data is the same. In Experiment 3, we consider different heterogeneous configurations for one workflow execution.</p><p>Experiment 1: with caching. In this basic experiment, we compare two workflow executions: with caching, using <software>CacheA</software> and bStorage; and without caching, using <software ContextAttributes="used">ActGreedy</software>. We consider one re-execution of the workflow on different input datasets, from 0% to 60% of same reused data.</p><p><software>CacheA</software> outperforms <software ContextAttributes="used">ActGreedy</software> from 20% of reused data. Below 20%, the overhead of caching outweighs its benefit. For instance, with no reuse (0%), the total time with <software ContextAttributes="used">CacheA</software> is 16% higher than with <software ContextAttributes="used">ActGreedy</software>. But with 30%, it is 11% lower, and with 60%, it is 42% lower.</p><p>Experiment 2: multiple users. Figure <ref type="figure" target="#fig_2">2</ref> shows the total time of the workflow for the three scheduling methods, four users, H = 0.7 and our two cache site selection methods: (a) bStorage, and (b) bCompute.</p><p>Let us first analyze the results in Figure <ref type="figure" target="#fig_2">2</ref>.a (bStorage method). For the first user execution, <software ContextAttributes="used">CacheA</software> outperforms <software ContextAttributes="used">Sgreedy</software> in terms of execution time by 8% and in terms of data and intermediate data transfer times by 51% and 63%, respectively. The reason <software ContextAttributes="used">Sgreedy</software> is slower is that it schedules some computeintensive fragments at Site 1, which has the lowest computing resources. Furthermore, it does not consider data placement and transfer time when scheduling fragments.</p><p>Again for the first user execution, <software>CacheA</software> outperforms <software ContextAttributes="used">Agreedy</software> in terms of total time by 24%, when considering data transfer time to the cache. However, <software ContextAttributes="used">CacheA</software> execution time is a bit slower (by 9%). The reason that <software ContextAttributes="used">Agreedy</software> is slower in terms of total time is that it does not take into account the placement of the cached data, which leads to larger amounts (by 67%) of cache data to transfer. For other users' executions (when cached data exists), <software ContextAttributes="used">CacheA</software> outperforms <software ContextAttributes="used">Sgreedy</software> in terms of execution time by 29%, and for the fourth user execution, by 20%. This is because <software ContextAttributes="used">CacheA</software> better selects the cache site in order to reduce the execution time of the future re-executions. In addition, <software ContextAttributes="used">CacheA</software> balances the cached data and computations. It outperforms <software ContextAttributes="used">Sgreedy</software> and <software ContextAttributes="used">Agreedy</software> in terms of  intermediate data transfer times (by 59% and 15%, respectively), and cache data transfer times (by 82% and 74%, respectively). Overall, <software ContextAttributes="used">CacheA</software> outperforms <software ContextAttributes="used">Sgreedy</software> and <software ContextAttributes="used">Agreedy</software> in terms of total times by 61% and 43%, respectively. The workflow fragments are not necessarily scheduled to the site with shortest execution time, but to the site that minimizes overall total time. Considering the multiuser perspective, <software ContextAttributes="used">CacheA</software> outperforms baseline methods, reducing the total time for each new user (up to 6% faster for the fourth user compared to the second).</p><p>Let us now consider Figure <ref type="figure" target="#fig_2">2</ref>.b (bCompute method). For the first user execution, <software ContextAttributes="used">CacheA</software> outperforms <software ContextAttributes="used">Sgreedy</software> and <software ContextAttributes="used">Agreedy</software> in terms of total time by 36% and 10% respectively. bCompute stores the cache data on the site with most idle CPUs, which is often the site with the most CPUs. This leads the cached data to be stored close to where it is generated, thus reducing data transfers when adding data to the cache. For the second user, <software ContextAttributes="used">CacheA</software> outperforms <software ContextAttributes="used">Sgreedy</software> and <software ContextAttributes="used">Agreedy</software> in terms of total time by 46% and 21% respectively. The cached data generated by the first user is stored on the sites with more available CPUs, which minimizes the intermediate and reused cached data transfers. From the third user, the storage at some site gets full, i.e. for the third user's execution, Site 3 storage is full and from the fourth user's execution, Site 2 storage is full. Thus, the performance of the three scheduling methods decreases due to higher cache data transfer times. Yet, <software ContextAttributes="used">CacheA</software> still outperforms <software ContextAttributes="used">Sgreedy</software> and <software ContextAttributes="used">Agreedy</software> in terms of total time by 49% and 25% respectively. Experiment 3: cloud site heterogeneity. We now compare the three methods in the case of heterogeneous sites by considering the amount of data transferred and execution time. In this experiment (see Figure <ref type="figure" target="#fig_3">3</ref>), we consider only one user who executes the workflow and that previous executions with 60% of the same raw data have generated some cached data. We use the bStorage method for cache site selection.  With heterogeneous sites (H &gt; 0), the sites with more CPUs have less available storage but can execute more tasks, which leads to a larger amount of intermediate and cached data transferred between the sites. For H = 0.3, <software ContextAttributes="used">CacheA</software> outperforms <software ContextAttributes="used">Sgreedy</software> and <software ContextAttributes="used">Agreedy</software> in terms of total time (by 40% and 18%, respectively) and amount of data transferred (by 47% and 21%, respectively).</p><p>With H = 0.7, <software>CacheA</software> outperforms <software ContextAttributes="used">Sgreedy</software> and <software ContextAttributes="used">Agreedy</software> in terms of total time (by 58% and 42%, respectively) and in terms of amount of data transferred (by 55% and 31%, respectively). <software ContextAttributes="used">CacheA</software> is faster because its scheduling leads to a smaller amount of cached data transferred when reused (48% smaller than <software ContextAttributes="used">Agreedy</software>) and added to the cache (62% smaller than <software ContextAttributes="used">Agreedy</software>).</p></div>
<div><head n="5.3">Concluding Remarks</head><p>Our cache-aware scheduling method <software>CacheA</software> always outperforms the two baseline methods (which also benefit from our cache/data selection method), both in the case of multiple users and heterogeneous sites.</p><p>The first experiment (with caching) shows that storing and reusing cached data becomes beneficial when 20% or more of the input data is reused. The second experiment (multiple users) shows that <software>CacheA</software> outperforms <software ContextAttributes="used">Sgreedy</software> and <software ContextAttributes="used">Agreedy</software> in terms of total time by up to 61% and 43%, respectively. It also shows that, with increasing numbers of users, the performance of the three scheduling methods decreases due to higher cache data transfer times. The third experiment (heterogeneous sites) shows that <software ContextAttributes="used">CacheA</software> adapts well to site heterogeneity, minimizing the amount of cached data transferred and thus reducing total time. It outperforms <software ContextAttributes="used">Sgreedy</software> and <software ContextAttributes="used">Agreedy</software> in terms of total time by up to 58% and 42% respectively.</p><p>Both cache site selection methods bCompute and bStorage have their own advantages. bCompute outperforms bStorage in terms of data transfer time by 13% for the first user and up to 17% for the second user. However, it does not scale with the number of users, and the limited storage capacities of Site 2 and 3 lead to a bottleneck. On the other hand, bStorage balances the cached data among sites and prevents the bottleneck when accessing the cached data, thus reducing re-execution times. In summary, bCompute is best suited for computeintensive workflows that generate smaller intermediate datasets while bStorage is best suited for data-intensive workflows where executions can be performed at the site where the data is stored.</p></div>
<div><head n="6">Conclusion</head><p>In this paper, we proposed a solution for distributed caching of scientific workflows in a cloud with heterogeneous sites (including on premise servers and shared-nothing clusters). Based on a distributed and parallel architecture, we proposed algorithms for adaptive caching, cache site selection and dynamic workflow scheduling. We implemented our solution in <software>OpenAlea</software>, together with a multisite scheduling algorithm. Using a real data-intensive application in plant phenotyping (Phenomenal), our extensive experimental evaluation using a cloud with three heterogeneous sites shows that our solution can yield major performance gains. In particular, it reduces much execution times and data transfers, compared to two baseline scheduling methods (which also use our cache/data selection method).</p></div><figure xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: Multisite Workflow System Architecture</figDesc><graphic coords="6,188.88,412.67,237.60,118.80" type="bitmap" /></figure>
<figure xml:id="fig_1"><head /><label /><figDesc>(a) bStorage method (b) bCompute method</figDesc></figure>
<figure xml:id="fig_2"><head>Fig. 2 :</head><label>2</label><figDesc>Fig. 2: Total times for multiple users (60% of same raw data per user) for three scheduling methods (Sgreedy, Agreedy and CacheA).</figDesc><graphic coords="13,134.77,120.15,172.91,146.69" type="bitmap" /></figure>
<figure xml:id="fig_3"><head>Fig. 3 :</head><label>3</label><figDesc>Fig. 3: Execution times and amounts of data transferred for one user (60% of same raw data used), on heterogeneous sites with three scheduling methods (Sgreedy, Agreedy and CacheA).</figDesc><graphic coords="14,134.77,159.73,172.91,140.10" type="bitmap" /></figure>
<figure xml:id="fig_4"><head>Figure 3</head><label>3</label><figDesc>Figure 3 shows the execution times and the amount of data transferred using the three scheduling methods in case of heterogeneous sites. With homogeneous sites (H = 0), the three methods have almost the same execution time. CacheA outperforms Sgreedy in terms of amount of intermediate data transferred and total time by 44% and 26%, respectively. CacheA has execution time similar to Agreedy (3.1% longer). The cached data is balanced as the three sites have same storage capacities. Thus, the total times of CacheA and Agreedy are almost the same.With heterogeneous sites (H &gt; 0), the sites with more CPUs have less available storage but can execute more tasks, which leads to a larger amount of intermediate and cached data transferred between the sites. For H = 0.3, CacheA outperforms Sgreedy and Agreedy in terms of total time (by 40% and 18%, respectively) and amount of data transferred (by 47% and 21%, respectively).With H = 0.7, CacheA outperforms Sgreedy and Agreedy in terms of total time (by 58% and 42%, respectively) and in terms of amount of data transferred (by 55% and 31%, respectively). CacheA is faster because its scheduling leads to a smaller amount of cached data transferred when reused (48% smaller than Agreedy) and added to the cache (62% smaller than Agreedy).</figDesc></figure>
			<note place="foot" n="5" xml:id="foot_0"><p>https://www.phenome-emphasis.fr/phenome_eng/</p></note>
			<note place="foot" n="6" xml:id="foot_1"><p>https://cassandra.apache.org</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="7">Acknowledgments</head><p>This work was supported by the #<rs type="funder">DigitAg French initiative</rs> (www.hdigitag.fr), the <rs type="projectName">SciDISC and HPDaSc Inria associated</rs> teams with Brazil, the <rs type="projectName">Phenome-Emphasis project</rs> (<rs type="grantNumber">ANR-11-INBS-0012</rs>) and <rs type="projectName">IFB</rs> (<rs type="grantNumber">ANR-11-INBS-0013</rs>) from the <rs type="funder">Agence Nationale de la Recherche</rs> and the <rs type="funder">France Grille Scientific Interest Group</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_z5DFHtC">
					<orgName type="project" subtype="full">SciDISC and HPDaSc Inria associated</orgName>
				</org>
				<org type="funded-project" xml:id="_FDruKP7">
					<idno type="grant-number">ANR-11-INBS-0012</idno>
					<orgName type="project" subtype="full">Phenome-Emphasis project</orgName>
				</org>
				<org type="funded-project" xml:id="_8JRs6fG">
					<idno type="grant-number">ANR-11-INBS-0013</idno>
					<orgName type="project" subtype="full">IFB</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Provenance collection support in the kepler scientific workflow system</title>
		<author>
			<persName><forename type="first">I</forename><surname>Altintas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Barney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Jaeger-Frank</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Provenance and Annotation Workshop</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="118" to="132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Openalea.phenomenal: A workflow for plant phenotyping</title>
		<author>
			<persName><forename type="first">S</forename><surname>Artzet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Brichet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chopard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mielewczik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fournier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Pradal</surname></persName>
		</author>
		<idno type="DOI">10.5281/zenodo.1436634</idno>
		<ptr target="https://doi.org/10.5281/zenodo.1436634" />
		<imprint>
			<date type="published" when="2018-09">Sep 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Vistrails: visualization meets data management</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Callahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Freire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Scheidegger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">T</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">T</forename><surname>Vo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGMOD Int. Conf. on Management of Data (SIGMOD)</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="745" to="747" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Heterogeneous cloud computing</title>
		<author>
			<persName><forename type="first">S</forename><surname>Crago</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Dunn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Eads</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Hochstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">I</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Modium</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Suh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Walters</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Cluster Computing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011">2011. 2011</date>
			<biblScope unit="page" from="378" to="385" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Common motifs in scientific workflows: An empirical analysis</title>
		<author>
			<persName><forename type="first">D</forename><surname>Garijo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Alper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Belhajjame</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Corcho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Goble</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Future Generation Computer Systems (FGCS)</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="338" to="351" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Adaptive caching for data-intensive scientific workflows in the cloud</title>
		<author>
			<persName><forename type="first">G</forename><surname>Heidsieck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>De Oliveira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Pacitti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Pradal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Tardieu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Valduriez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Database and Expert Systems Applications (DEXA)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="452" to="466" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Data-intensive science: a new paradigm for biodiversity studies</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kelling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Hochachka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Fink</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Riedewald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Caruana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ballard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hooker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BioScience</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="613" to="620" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Efficient scheduling of scientific workflows using hot metadata in a multisite cloud</title>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">P</forename><surname>Morales</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Pacitti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Costan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Valduriez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Antoniu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mattoso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="page" from="1" to="20" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A survey of data-intensive scientific workflow management</title>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Pacitti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Valduriez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mattoso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Grid Computing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="457" to="493" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Multi-objective scheduling of scientific workflows in multisite clouds</title>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Pacitti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Valduriez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>De Oliveira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mattoso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Future Generation Computer Systems(FGCS)</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="page" from="76" to="95" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Improving multisite workflow performance using model-based scheduling</title>
		<author>
			<persName><forename type="first">K</forename><surname>Maheshwari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vishwanath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kettimuthu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE nt. Conf. on Parallel Processing (ICPP)</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="131" to="140" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Towards a taxonomy for cloud computing from an e-science perspective</title>
		<author>
			<persName><forename type="first">D</forename><surname>De Oliveira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">A</forename><surname>Baião</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mattoso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Cloud Computing. Computer Communications and Networks</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="47" to="62" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Principles of Distributed Database Systems</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Özsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Valduriez</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
	<note>Fourth Edition</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Openalea: scientific workflows combining data analysis and simulation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Pradal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fournier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Valduriez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cohen-Boulakia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Scientific and Statistical Database Management (SSDBM)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Plant phenomics, from sensors to knowledge</title>
		<author>
			<persName><forename type="first">F</forename><surname>Tardieu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cabrera-Bosquet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pridmore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bennett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Current Biology</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">15</biblScope>
			<biblScope unit="page" from="R770" to="R783" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A highly practical approach toward achieving minimum data sets storage cost in the cloud</title>
		<author>
			<persName><forename type="first">D</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Parallel and Distributed Systems</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1234" to="1244" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Scheduling of scientific workflow in non-dedicated heterogeneous multicluster platform</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Dong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Systems and Software</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1806" to="1818" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>