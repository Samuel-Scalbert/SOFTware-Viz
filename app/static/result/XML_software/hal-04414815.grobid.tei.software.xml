<TEI xmlns="http://www.tei-c.org/ns/1.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xml:space="preserve" xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Efficient and Robust Active Learning Methods for Interactive Database Exploration</title>
			</titleStmt>
			<publicationStmt>
				<publisher />
				<availability status="unknown"><licence /></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Enhui</forename><surname>Huang</surname></persName>
							<email>enhui.huang@polytechnique.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">École Polytechnique</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yanlei</forename><surname>Diao</surname></persName>
							<email>yanlei.diao@polytechnique.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">École Polytechnique</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Massachusetts Amherst</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Anna</forename><surname>Liu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Massachusetts Amherst</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Liping</forename><surname>Peng</surname></persName>
							<email>lppeng@cs.umass.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Massachusetts Amherst</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Luciano</forename><surname>Di</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">École Polytechnique</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Luciano</forename><forename type="middle">Di</forename><surname>Palma</surname></persName>
							<email>luciano.di-palma@polytechnique.edu</email>
						</author>
						<title level="a" type="main">Efficient and Robust Active Learning Methods for Interactive Database Exploration</title>
					</analytic>
					<monogr>
						<imprint>
							<date />
						</imprint>
					</monogr>
					<idno type="MD5">0CB3E6DD12C47597383E0CC01189E888</idno>
					<idno type="DOI">10.1007/s00778-</idno>
					<note type="submission">Received: date / Accepted: date</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-04-12T14:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid" />
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Interactive Data Exploration</term>
					<term>Active Learning</term>
					<term>Label Noise</term>
				</keywords>
			</textClass>
			<abstract>
<div><p>There is an increasing gap between fast growth of data and the limited human ability to comprehend data. Consequently, there has been a growing demand of data management tools that can bridge this gap and help the user retrieve high-value content from data more effectively. In this work, we propose an interactive data exploration system as a new database service, using an approach called "explore-by-example." Our new system is designed to assist the user in performing highly effective data exploration while reducing the human effort in the process. We cast the explore-by-example problem in a principled "active learning" framework. However, traditional active learning suffers from two fundamental limitations: slow convergence and lack of robustness under label noise. To overcome the slow convergence and label noise problems, we bring the properties of important classes of database queries to bear on the design of new algorithms and optimizations for active learning-based database exploration. Evaluation results using real-world datasets and user interest patterns show that our new system, both in the noise-free case and in the label noise case, significantly outperforms state-of-the-art active learning techniques and data exploration systems in accu-</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div><head n="1">Introduction</head><p>Today data is being generated at an unprecedented rate. However, the human ability to comprehend data remains as limited as before. Consequently, there has been a growing demand of data management tools that can bridge the gap between the data growth and limited human ability, and help retrieve high-value content from data more effectively.</p><p>To respond to such needs, we build a new database service for interactive exploration in a framework called "explore-by-example" <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24]</ref>. In this framework, the database content is considered as table, and the user is interested in a subset of its tuples but not all. In the data exploration process, the system allows the user to interactively label tuples as "interesting" or "not interesting," so that it can construct an increasingly-more-accurate model of the user interest. Eventually, the model is turned into a user interest query <ref type="foot" target="#foot_0">1</ref> that will run the model as a user defined function (UDF) on the database to retrieve all relevant tuples.</p><p>In this work, we consider several target applications. First, when a scientist comes to explore a large sky survey database such as the Sloan Digital Sky Survey (SDSS) <ref type="bibr" target="#b68">[69]</ref>, he/she may not be able to express his/her data interest precisely. Instead, the scientist may prefer to navigate through a region of the sky, see a few examples of sky objects, provide yes or no feedback, and ask the system to find all other (potentially many more) relevant sky objects from the database.</p><p>Second, consider many web applications backed by a large database, such as E-commerce websites and housing websites, which provide a simple search interface but leave the job of filtering through a long list of returned objects to the user. The new database service in the explore-by-example framework will provide these applications with a new way to interact with the user and, more importantly, help the user filter through numerous objects more efficiently.</p><p>To build an explore-by-example system, our approach is to cast it in an "active learning" framework: We treat the modeling of the user interest as a classification problem where all the user labeled examples thus far are used to train a classification model. Then active learning <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b65">66,</ref><ref type="bibr" target="#b72">73]</ref> decides how to choose a new example, from the unlabeled database, for the user to label next so that the system can learn the user interest efficiently. The active learning based explore-by-example approach offers potential benefits over alternative methods such as faceted search <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b61">62]</ref> and semantic windows <ref type="bibr" target="#b43">[44]</ref> for the following reasons.</p><p>First, the user interest may include varying degrees of complexity, or the user does not have prior knowledge about the complexity and expects the system to learn a model as complex as necessary for his/her interest. For example, the user interest may involve more complex constraints, e.g., "( rowc-a1 b1 ) 2 + ( colc-a2 b2 ) 2 &lt; c" , from the SDSS example query set <ref type="bibr" target="#b67">[68]</ref>, where rowc and colc refer to the row and column center positions of each photometric object, or "length * width &gt; c" from our Car User Study. If the user knows the function shape and constants in advance, some of the above examples (e.g., the ellipse pattern) can be supported by semantic windows <ref type="bibr" target="#b43">[44]</ref> as pre-defined patterns. However, if the user does not have such knowledge, neither faceted search nor semantic windows can be applied, but explore-by-example still works regardless of how complex the predicates are, which functions and constants are used in the predicates, etc.</p><p>Second, increased dimensionality makes it harder for semantic windows to scale. For example, when the interest of the SDSS user involves both an ellipse pattern based on the location and a log pattern based on brightness, it will be more difficult for the system and the user to handle multiple semantic windows in data exploration (if such patterns can be predefined). In contrast, as the dimensionality increases, explore-by-example can keep the same user interface for data exploration and handles increased complexity via its learning algorithm "behind the scenes."</p><p>While prior work on explore-by-example has employed active learning <ref type="bibr" target="#b23">[24]</ref>, a main issue is the large number of labeled examples needed to achieve high accuracy. For instance, in the use case studied in <ref type="bibr" target="#b23">[24]</ref>, 300-500 labeled examples were needed to reach 80% accuracy, which is undesirable in many applications. This problem, referred to as slow convergence, is exacerbated when the user interest cov-ers only a small fraction of the database (i.e., low selectivity) or the number of the attributes chosen for exploration is large (i.e., high dimensionality). Another major issue is that most existing active learning methods assume the labels provided by the user to be uncorrupted <ref type="bibr" target="#b13">[14]</ref>. In practice, label noise often occurs in the user labeling process due to two main reasons. First, the user does not fully understand his/her interest and may have trouble classifying some examples correctly, especially those close to the decision boundary. Second, some examples are mislabeled accidentally. As stated in the survey <ref type="bibr" target="#b30">[31]</ref>, the occurrence of label noise may lead to (i) deterioration of prediction performance, (ii) requirement of more labeled examples and more complex learned models, and (iii) distortion of observed class distribution. Although a few pioneering efforts have considered the label noise problem in the context of active learning <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b70">71,</ref><ref type="bibr" target="#b81">82,</ref><ref type="bibr" target="#b82">83]</ref>, they often focus on random classification noise. It is challenging to deal with more general and realistic label noise models.</p><p>In this work, we take a new approach to active learningbased database exploration. Instead of improving active learning in isolation from the database, we treat it as an internal module of the database system and ask the question: what query and data properties from the database can we leverage to address the slow convergence problem and the label noise problem? Based on the common properties that we observed in query traces from the SDSS <ref type="bibr" target="#b67">[68]</ref> and a car database (described more in Section 6), we can indeed design new techniques that overcome or alleviate the slow convergence and label noise problems. Some of the key query and data properties we explore include:</p><p>Subspatial Convexity: Consider the database attributes as dimensions of a data space D and map the tuples to the space based on the values of their attributes. Then all the tuples that match the user interest form the positive region in D; others form the negative region. We observe that in some lower-dimensional subspaces of D, the projected positive or negative region is a convex object. For example, the SDSS query trace <ref type="bibr" target="#b67">[68]</ref> includes 116 predicates, among which 107 predicates define a convex positive region in the subspace formed by the attributes used in the predicate, and 3 predicates define a convex negative region in their subspaces. For the car database, the 70 predicates defined on numerical attributes all form a convex positive region. Figure <ref type="figure" target="#fig_2">2</ref> shows a range of predicates from these two databases and their positive and negative regions.</p><p>Conjunctivity: Conjunctive queries are a major class of database queries that have been used in numerous applications. For data exploration, we are interested in the "conjunctive" property of the set of predicates that characterize the user interest. Among 45 queries provided by SDSS <ref type="bibr" target="#b67">[68]</ref>, 40 queries are conjunctive queries. For the car database, all user queries use the conjunctive property.</p><p>In this paper, we bring the subspatial convexity and conjunctivity of database queries, treated as true (yet unknown) user interest queries, to bear on the design of new algorithms and optimizations for active learning-based database exploration. These techniques allow the database system to overcome fundamental limitations of traditional active learning, i.e., the slow convergence problem when data exploration is performed with high dimensionality and low selectivity of the user interest query, and the label noise problem when the labels given by the user are corrupted. More specifically, our paper makes the following contributions.</p><p>1. Dual-Space Model (Section 3): By leveraging the subspatial convex property, we propose a new "dual-space model" (DSM) that builds not only a classification model, F V , from labeled examples, but also a polytope model of the data space, F D . On one hand, active learning theory improves F V by choosing the next example that enables reduction of the version space V (the space of all classification models consistent with labeled data). On the other hand, our polytope model offers a more direct description of the data space D including the areas known to be positive, areas known to be negative, and areas with unknown labels. We use both models to predict unlabeled examples and choose the best example to label next. In addition, DSM allows us to prove exact and approximate lower bounds on the model accuracy in terms of the F1-score. To the best of our knowledge, this is the first provable result of an active learning method in the F1-score, while existing learning theory offers bounds only on classification errors <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b34">[35]</ref><ref type="bibr" target="#b35">[36]</ref><ref type="bibr" target="#b36">[37]</ref>, which treats positive and negative classes equally and hence does not suit queries of very low selectivity.</p><p>2. High-dimensional Exploration (Section 4): When the user interest involves a large number of attributes, by leveraging the conjunctive and subspatial convexity properties of user interest queries, we factorize a high-dimensional data space into low-dimensional subspaces, in some of which the projections of user positive or negative regions are convex. Our dual-space model with factorization, DSM F , runs the polytope model in each subspace where the convex property holds. We formally define the class of queries that DSM F supports, the decision function it utilizes, and prove that it achieves a better lower bound of the F1-score than DSM without factorization.</p><p>3. Learning with Label Noise (Section 5): To handle noisy labels with minimum labeled examples, we develop a new active learner called Robust Dual-Space Model (RDSM), which is a seamless integration of a recently proposed automatic noise distillation method (auto-cleansing) <ref type="bibr" target="#b16">[17]</ref> and our proposed DSM-based denoising methods. Regarding the user labeling process, we propose an adaptive method to strategically trade off the savings in user labeling effort against the risk of introducing more label noise. We also extend RDSM with factorization (RDSM F ) for data exploration in high dimensional space, and design optimization strategies, such as distributed computing and hyperparameter tuning, to improve our system in accuracy, interactive performance, and generality.</p><p>4. Evaluation (Section 6): We evaluated our system using two real datasets. The SDSS dataset <ref type="bibr" target="#b68">[69]</ref> includes 190M tuples, for which the user interests are selective and their decision boundaries present varied complexity for detection. Without knowing these queries in advance, DSM F can achieve the F1-score of 95% within 10 labeled examples if the decision boundary B falls in a sparse region, and otherwise requires 40-160 labeled examples for up to 6D queries while maintaining per-iteration time within 1-2 seconds. For these queries, DSM F significantly outperforms learning methods including Active Learning (AL) <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b27">28]</ref> and Active Search <ref type="bibr" target="#b32">[33]</ref>, as well as recent explore-by-example systems, Aide <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24]</ref> and <software ContextAttributes="used">LifeJoin</software> <ref type="bibr" target="#b18">[19]</ref>.</p><p>Our user study of a 5622-tuple car database validates the subspatial convexity and conjunctivity properties of user interest queries. It also shows the benefits of DSM F (a median of 10 labeled examples to reach high accuracy) over manual exploration (where the user wrote 12 queries and reviewed 98 tuples as the median), as well as over other AL methods.</p><p>In the face of label noise, evaluation using SDSS and car datasets shows the following results: (1) Our algorithm substantially outperforms alternative algorithms in accuracy for all label noise levels while maintaining per-iteration time within 1-3 seconds. In particular, compared to traditional active learning, our algorithm achieves 0.88x-22.14x higher accuracy for SDSS 4D-6D queries, and 0.14x-3.72x higher accuracy for the car queries obtained in our user study.</p><p>(2) When the noise rate increases, while all algorithms decrease performance, our algorithm reduces at the slowest pace. (3) All of our techniques, auto-cleansing, RDSM, and factorization, contribute to alleviate the label noise problem. (4) Our distributed computing method improves the time per iteration from 20-30 seconds without parallelism to a few seconds, which improves the scalability of our system.</p></div>
<div><head n="2">Background</head><p>In this section, we review our design of an explore-byexample system and present background on active learning.</p></div>
<div><head n="2.1">System Overview</head><p>Our data exploration system is depicted in Figure <ref type="figure" target="#fig_1">1</ref>. In the following discussion, we use the query Q F , "price ≤ 30000 and length * width &gt; 10.1," extracted from our Car user study to exemplify the main concepts and core workflow. Suppose that the user has an implicit exploration goal Q I : purchasing a spacious car at an affordable price. Since there  is a tradeoff between spaciousness and price, the user cannot come up with the query Q F directly, but when presented with a concrete example, the user can determine if this car meets the expectations or not.</p><p>Initial Sampling. When exploring the database, the user is first presented with the database schema for browsing. Based on his/her best understanding of the implicit exploration goal, the user chooses an initial set of attributes, {A i }, i = 1, . . . , d, from the database. They form a superset of the relevant attributes that will eventually be discovered by the system. Let us consider the projection of the underlying database table <ref type="foot" target="#foot_1">2</ref> to {A i }, and pivot the projected table such that each A i becomes a dimension and the projected tuples are mapped to points in this d-dimensional space -the resulting space is called a data space where the user exploration will take place. For our example Q I above, the data space involves attributes price, length, and width, as well as other irrelevant attributes that will be eventually filtered out in the data exploration process <ref type="foot" target="#foot_2">3</ref> .</p><p>To bootstrap data exploration, the user is asked to give a positive example and a negative example. If the user does not have such examples, the system can run initial sampling over the data space to help find such examples. Since the initial sampling problem has been studied before <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b48">49]</ref>, our work in this paper focuses on data exploration after such initial examples are identified.</p><p>Iterative Learning and Exploration starts with a positive example and a negative example, which form the labeled dataset. In each iteration, four steps are executed in order, resulting in a new example labeled, as shown in Figure <ref type="figure" target="#fig_1">1</ref>.</p><p>1. Classification. The labeled dataset is used to train a user interest model. Here, training is fast due to the small number of labeled examples.</p><p>2. Convergence Test. Our system assesses the current model to decide whether more iterations are needed. This unique feature, which was not available in earlier data exploration systems <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24]</ref>, is enabled by our advanced data space model. We present the main idea below while deferring the details to Section 3. At the high level, our model builds a partitioning of the data space based on all available labeled examples. The data space is divided into three disjoint regions: the positive region, negative region, and unknown region. With the increase of labeled examples, both positive and negative regions expand while the uncertain region shrinks. With sufficient data, the uncertain region will eventually become empty and the positive region will converge to the user interest region. Based on the volumes of these regions, a lower bound of the F1-score can be obtained as the estimated accuracy for the classification model. If the estimated accuracy exceeds the user-defined threshold, the exploration will be terminated automatically by our system.</p><p>3. Space Exploration. The trained model is explored to retrieve a new example for user labeling which, among all unlabeled examples, best expedites the model convergence.</p><p>4. Request of User Feedback. The user labels the new example as positive or negative. Such feedback can be collected explicitly through a graphical interface as in our past work <ref type="bibr" target="#b21">[22]</ref>. The newly labeled example is added to the labeled dataset. For our example Q I , the user provides feedback based on the price, length, and width of a specific car in display. Note that although our final learned query is written as "Q F : price ≤ 30000 and length * width &gt; 10.1," the user does not need to do such calculation. Instead, the user just looks at a particular car and decides whether it is a good tradeoff between space and price. It is our system that will learn the query Q F to enclose all positive examples in a user interest region; that is, the boundary of the region is a learned concept and does not necessarily reflect how the user makes a decision. Our system also has an extension for the user to label the example for its subspaces, which we will defer to Section 4 when we introduce factorization.</p><p>The above process repeats until the model reaches a user-specified accuracy level or the maximum level of iterations (labeled examples) that the user can tolerate. At this point, the model for the positive class is translated to a user interest query, which encodes the model in a UDF and retrieves from the database all the tuples that the model classifies as relevant. In the car query example, the implicit goal Q I is eventually characterized by the output of Q F .</p></div>
<div><head n="2.2">Active Learning for Data Exploration</head><p>The problem of seeking the next example for labeling from a database of unlabeled tuples is closely related to active learning (AL). Its recent results are surveyed in <ref type="bibr" target="#b65">[66]</ref>. Below, we summarize the results most relevant to our work.</p><p>Pool-Based Sampling. Many real-world problems fit the following scenario: there is a small set of labeled data L and a large pool of unlabeled data U available. In active learning, an example is chosen from the pool U in a greedy fashion, according to a utility measure used to evaluate all instances in the pool (or, if U is large, a subsample thereof). In our setting of database exploration, the labeled data L is what the user has provided thus far. The pool U is a subsample of size m of the unlabeled part of the database. The utility measure depends on the classifier in use, as discussed below.</p><p>Classification Model. Previous explore-by-example systems <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24]</ref> used decision trees to build a classification model. It works well if the user interest pattern is a hyperrectangle in the data space, whereas real-world applications may use more complex predicates. To support higher complexity, our system uses more powerful classifiers such as Support Vector Machines (SVM) or Gradient Boosting. The new techniques proposed in our work do not depend on the specific classifier; they can work with most existing classifiers. But the implementation of active learning does depend on the classifier in use. For ease of composition, in this paper we use SVM as an example classifier.</p><p>Active Learning Strategies have been studied to determine which examples should be labeled next to expedite model convergence, which are also called "query strategies" in the AL literature. Some of the most popular strategies <ref type="bibr" target="#b65">[66]</ref> include: Uncertainty sampling queries the examples for which the current model is least certain of its prediction. For binary classification, the examples selected are those closest to the decision boundary. Query by committee maintains a committee of models trained on the current labeled dataset, allows all models to vote on the predictions for unlabeled data, and queries the examples on which the committee disagrees the most. Expected error reduction queries the examples that would minimize the model's generalization error. Variance reduction selects the examples that would minimize output variance. Density-weighted methods take into account not only the most uncertain examples but also the most representative ones of the underlying distribution (e.g. in dense regions) in the query strategy.</p><p>It is important to note that our work aims to develop a new approach that overcomes the slow convergence observed for many of the above AL methods <ref type="bibr" target="#b56">[57,</ref><ref type="bibr" target="#b65">66]</ref> by adding new insights from the data and user interest patterns in database exploration. Our solution combines a new data space model and any AL method to work in parallel, where the data space model is orthogonal to the AL method used. As such, our approach is tailored for database exploration while being general with respect to the AL method in use.</p><p>More specifically, our implementation of the AL method is uncertainty sampling due to its wide use <ref type="bibr" target="#b65">[66]</ref> and high efficiency in execution. A final note is that a formal description of uncertainty sampling is through the notion of version space, which is the space of all configurations of the classification model consistent with labeled data. For SVM classifiers, for example, the version space includes all possible Uncertainty sampling is known to be an approximation of an optimal algorithm that bisects the version space with each selected example <ref type="bibr" target="#b72">[73]</ref>.</p><p>For the above reason, we call the learning algorithm in uncertainty sampling version space-based as they are designed to reduce the version space, and we will further augment it with a new data space model.</p></div>
<div><head n="3">Dual-Space Model</head><p>With the goal to provide a service for database exploration, our work takes a new, a database-centric approach to tackle the slow convergence problem of existing active learning methods. We observe from existing query traces that in lower-dimensional subspaces, the projected positive or negative region of user interest query is often a convex object.</p><p>Figure <ref type="figure" target="#fig_2">2</ref> shows 6 examples from the SDSS query trace <ref type="bibr" target="#b67">[68]</ref>, where 107 out of 116 predicates used define a convex positive region (colored in green) in the subspace formed by the attributes in the predicate, and 3 predicates define a convex negative region (colored in red) in their subspaces.</p><p>In this section, we utilize such subspatial convexity and introduce a dual-space (data and version space) model, which enables improved accuracy and provable lower bounds on the model accuracy. We begin with the simple case that the convex property holds for the user interest query over the entire data space D, without factorization, and defer the extension to subspaces to Section 4. We refer to this class of queries as "convex pattern queries",</p><formula xml:id="formula_0">Q c ≡ Q + c ∪ Q - c</formula><p>, where Q + c and Q - c denote those queries whose positive and negative regions are convex, respectively.</p></div>
<div><head n="3.1">A Polytope Model in Data Space</head><p>The key idea behind our data space model is that at each iteration we use all available labeled examples to build a partitioning of the data space. It divides the data space into the  positive region (any point inside which is known to be positive), the negative region (any point inside which is known to be negative) and the uncertain region. As more examples are labeled, we have more knowledge about the uncertain region, so part of it will be converted to either the positive or the negative region in later iterations. Eventually, with enough training data, the uncertain region becomes empty, and the positive region converges to the query region.</p><p>For simplicity, we begin with queries whose positive region is convex (Q + c ). When the query region Q is convex, any point on the line segment between two points x 1 ∈ Q and x 2 ∈ Q is also in Q. Then we have the following: Definition 1 (Positive Region) Denote the examples that have been labeled as "positive" as L + = {e + i | i = 1, . . . , n + }. The convex hull of L + is known to be the smallest convex set that contains L + <ref type="bibr" target="#b45">[46]</ref> and is called the positive region, denoted as R + .</p><p>It is known that the convex hull of a finite number of points is a convex polytope <ref type="bibr" target="#b33">[34]</ref>. For example, the green triangle in Figure <ref type="figure" target="#fig_4">3</ref>(a) and the green tetragon in Figure <ref type="figure" target="#fig_19">3(b)</ref> are the positive regions formed by three and four positive examples, respectively, which are an approximation of the query region marked by the green ellipse. We can prove the following property of the positive region for convex queries, assuming that user labels are consistent with her interest: Proposition 1 All points in the positive region R + are positive.</p><p>All the proofs in this section and the next are left to our tech report <ref type="bibr" target="#b38">[39]</ref> due to space limitations. Definition 2 (Negative Region) For a negative example e - i , we can define a corresponding negative region R - i such that the line segment connecting any point x ∈ R - i and e - i does not overlap with the positive region R + , but the ray that starts from x ∈ R - i and passes through e - i will overlap with R + . More formally,</p><formula xml:id="formula_1">R - i = {x|xe - i ∩R + = ∅∧ --→ xe - i ∩R + = ∅}.</formula><p>Given n -negative examples, the negative region R -is the union of the negative region for each negative example, i.e., R -=</p><formula xml:id="formula_2">n - i=1 R - i .</formula><p>From the definition, we know that R - i is a convex cone generated by the conical combination of the vectors from the positive examples to the given negative example, i.e., ---→ e + j e - i (j = 1, . . . , n + ). The red triangle in Figure <ref type="figure" target="#fig_4">3</ref>(a) depicts such a convex cone. However, the union of R - i , i = 1, 2, . . . is non-convex. For example, the union of the five red polygons in Figure <ref type="figure" target="#fig_4">3</ref>(b) is non-convex. Given more labeled examples, the result of the union will be more accurate for approximating the true negative region, which is outside the ellipse. We prove the following property of the negative region: Proposition 2 All points in the negative region R -are negative.</p><p>Definition 3 (Uncertain Region) Denote the data space as R d , the uncertain region</p><formula xml:id="formula_3">R u = R d -R + -R -.</formula><p>Formally, the polytope model makes a decision about an example x based on the following decision function, which takes values in {-1, 0, 1} corresponding to R -, R u , and R + defined above:</p><formula xml:id="formula_4">F D (x) = 1 • 1(x ∈ R + ) -1 • 1(x ∈ R -).<label>(1)</label></formula><p>Our work also supports the case that the negative region of the query is convex (Q - c ). We can simply switch the above definitions such that we build a convex polytope for the negative region, and a union of convex cones for the positive region, one for each positive example. We also offer a test at the beginning of the data exploration process to choose between two polytope models,</p><formula xml:id="formula_5">Q ∈ Q + c or Q ∈ Q - c .</formula><p>The details are deferred to Section 4 where we offer a test procedure for all the assumptions made in the work.</p><p>Three-Set Metric. Our goal is not only to provide a new data space model, as described above, but also to design a new learning algorithm that enables a provable bound on the model accuracy. As stated before, our accuracy measure is the F1-score. Formally, the F1-score is evaluated on a test set D test = {(x i , y i )}, where x i denotes a database object and y i denotes its label according to the classification model. Then the F1-score is defined as, F1-score = 2 • precision•recall precision+recall , where precision is the fraction of points returned by the model from D test that are truly positive, and recall is the fraction of positive points in D test that are actually returned by the model.</p><p>However, capturing the F1-score in our data exploration procedure is difficult because we do not have such a labeled test set, D test , available. We cannot afford to ask the user to label more to produce one since the user labor is an important concern. To bound the F1-score with limited labeled data, our idea is to run our polytope model, F D : R d → {-1, 0, 1}, on an evaluation set. We define the evaluation set D eval as the projection of D test without labels y i 's. Then for each data point in D eval , depending on which region it falls into, D eval can be partitioned into three sets accordingly, denoted as D + , D -and D u . We can compute a metric from the number of data points in the three sets, as follows.</p><formula xml:id="formula_6">Definition 4 (Three-Set Metric) Denote D + = D eval ∩ R + , D -= D eval ∩ R -, D u = D eval ∩ R u ,</formula><p>and |S| means the size of set S. At a specific iteration of exploration, the three-set metric is defined to be |D + | |D + |+|D u | . We will prove shortly that this metric is a lower bound of the F1-score evaluated on D test . As more labeled examples are provided, data points will be moved from D u to either D + or D -. Eventually, with enough training data the uncertain set shrinks to an empty set and the Three-Set Metric rises to 100%, reaching convergence.</p></div>
<div><head n="3.2">Dual-Space Model and Algorithm</head><p>We now propose a new algorithm for interactive data exploration by exploiting models from two spaces, including our data space model F D with the Three-Set Metric, and a classification model F V with uncertainty sampling derived from the version space.</p><p>We first define the dual-space model (DSM) by considering two functionalities of a model for data exploration.</p><p>(1) Prediction: Given an unlabeled example, DSM predicts the class label first based on the data space model F D . If the example falls in the positive region R + , it is predicted to be positive; if it falls in the negative region R -, it is predicted to be negative. If the example falls in the unknown region R u , then the classification model F V is used to predict the example to be positive or negative.</p><p>(2) Sampling: In the AL framework, the model is also used to guide the choice of the next example for labeling to expedite model convergence. As discussed in Section 2, active learning can use an uncertainty sampling method, S V , for a given classification model to choose the next example. However, direct application of uncertainty sampling to our dual-space model raises a problem: the example chosen by S V may fall in the known positive or negative region of our data space model F D , hence wasting computing resources on such examples. In our work, we propose a new sampling method that is restricted to the unknown region of our data space model, denoted as S D . However, if we sample only from our data space model, we may not get a representative sample to train the classifier. Therefore, we use a sampling ratio, γ, to alternate between the sampling methods, S D and Algorithm 1 Dual-Space Algorithm for Convex Queries Input: database D, initial labeled data set D 0 , accuracy threshold λ, sampling ratio γ, unlabeled pool size m 1: D lu ← <software>getUserLabel</software>(x) 28: until accu ≥ λ or reachedMaxNum() 29: finalRetrieval(D, (R + , R -), classif ier) S V . For instance, when γ = 1/3, in each iteration we use S D with probability 1/3 and use S V otherwise. Now we present the full algorithm for interactive data exploration, as shown in Algorithm 1. The input is the database D, the initial labeled data set D 0 (a positive example x + , a negative example x -), a user-defined accuracy threshold λ, the sampling ratio γ, and the size of unlabeled pool m. First, we assign the initial labeled and unlabeled datasets (line 1). In particular, we assume the evaluation set D eval to be the entire database D, which can be replaced with a sample of D if D is too large. More details can be found in Section 3.4. Then we initialize data structures, including setting the data space model (R + , R -) as empty sets, setting the partitions D, and assigning the user-labeled set D lu and the DSM-labeled set D ld as empty sets. The algorithm next goes through iterative exploration.</p><formula xml:id="formula_7">D labeled ← D 0 , D unlabeled ← D \ D 0 2: R + ← ∅, R -← ∅ 3: D + ← ∅, D -← ∅, D u ← D 4: D ← (D + , D -, D u ) 5: D lu ← D 0 , D ld ← ∅ 6: repeat //</formula><p>Lines 7-13 update our DSM model. The data space model, R + and R -, is updated with the newly labeled example(s) by the user (lines 7-8). This step incrementally updates our convex polytope for R + and the union of convex polytopes for R -based on computational geometry <ref type="bibr" target="#b8">[9]</ref>. Afterwards, the corresponding partitions D are incrementally updated (line 9). In this step, some examples are removed from the unknown partition D u and placed to the positive partition D + or the negative partition D -. The accuracy is then estimated using the Three-Set Metric. We also keep track of the labeled and unlabeled examples using D labeled and D unlabeled . We use the labeled examples to train a classifier, that is, the version space model in our DSM.</p><p>Then lines 14-27 implement uncertainty sampling using DSM. With probability γ, we perform uncertainty sampling from a pool restricted to the unknown partition of the evaluation set, D u . Then the example chosen randomly is labeled by the user. With probability 1 -γ, we perform uncertainty sampling from a pool that is a subsample of all unlabeled examples in the database. Then the example chosen by uncertainty sampling is first run through our data space model, (R + , R -), to see if it falls in the positive or negative region and hence can be labeled directly by the model (we call this functionality auto-labeling from DSM). Otherwise, it will be labeled by the user. Note that auto-labeling can reduce the number of user-labeled examples required to reach high accuracy, achieving faster convergence. Due to autolabeling, our labeled set includes both user-labeled examples and DSM-labeled examples.</p><p>Then the algorithm repeats until it has met the user accuracy requirement based on the lower bound offered by our Three-Set Metric, or reached the maximum of iterations allowed (line 28). Finally, we run the DSM model over the database to retrieve all tuples predicated to be positive.</p></div>
<div><head n="3.3">Lower Bounds of the F1-score</head><p>Given how the DSM algorithm works, we now present formal results on the model accuracy achieved by DSM.</p><p>Exact Lower Bound. We begin with an exact lower bound of our accuracy measure, the F1-score.</p><p>Theorem 1 The Three-Set Metric evaluated on D eval is a lower bound of the F1-score if DSM is evaluated on D test .</p><p>The lower bound of DSM has several features. First, it is an exact lower bound throughout the exploration process for any evaluation set D eval . Second, the metric is monotonic in the sense that points in the uncertain region D u can be moved to the positive or negative region later, but not vice versa, and the metric goes to 1 when D u = ∅. If the metric is above the desired accuracy threshold at some iteration, it is guaranteed to be greater than the threshold in later iterations, so we can safely stop the exploration.</p><p>Approximate Lower Bound. When D eval is too large, we employ a sampling method to reduce the time to evaluate the Three-Set Metric. Let p and q be the true proportions of the positive and negative examples in D eval , i.e., p</p><formula xml:id="formula_8">= |D + | |D eval | and q = |D -| |D eval | . Then the Three-Set Metric is b = p 1 -q .</formula><p>Let p and q be the observed proportions of the positive and negative examples in a random draw of n examples from D eval , and let X n = p 1 -q . Our goal is to find the smallest sample size n such that the error of the estimation X n from the exact Three-Set Metric is less than δ with probability no less than λ. That is, Pr(|X n -b| &lt; δ) ≥ λ.</p><p>The following theorem helps find the lower bound of n. With the theorem, we can approximate the sample size by</p><formula xml:id="formula_9">Theorem 2 sup P r( √ n|X n -b| &lt; )-2Φ( (1-q) p(1-p-q) ) -1 = O(1/ √ n) for</formula><formula xml:id="formula_10">2Φ( √ nδ(1 -q) p(1 -p -q) ) -1 ≥ λ. Since p(1 -p -q)/(1 -q) 2 ≤ 1/4, it is sufficient for n to satisfy 2Φ(2 √ nδ) -1 ≥ λ and therefore n ≥ Φ -1 λ+1 2 2 /(4δ 2 ).</formula></div>
<div><head n="3.4">Optimization</head><p>We improve time efficiency by designing new sampling methods and leveraging distributed computing. We begin by considering the sampling procedures in Algorithm 1. We present their implementation and optimization as follows.</p><p>Sampling for creating the evaluation set (OPT1). In line 1 of Algorithm 1, we assign the evaluation set D eval to be the entire database D, to develop a lower bound of the DSM model. Ideally, we want D eval to be as large as possible. However, for efficiency, we can only afford to have a memory-resident sample. The analysis in Theorem 2 indicates that we can choose a sample of size n from the database, and achieve an approximate lower bound of the F1-score of our DSM algorithm when evaluated on the database. The sample, denoted as Deval , will expedite line 9 of Algorithm 1. For example, the SDSS database used in our experiments contains 190 million tuples. Denote the true lower bound as b. When n = 50k, our approximate lower bound b approximates b by ≤ .005 with probability 0.975 or higher. When n = 1.9 million, b approximates b by ≤ .001 with probability 0.994 or higher.</p><p>Sampling for pool-based active learning. Algorithm 1 contains two subsample procedures for pool-based uncertainty sampling, i.e., choosing the most uncertain example from the pool for labeling next. The subsample routine in line 16 creates a pool of unlabeled examples from the uncertain partition of the evaluation set, D u , which is memoryresident. This can be implemented using reservoir sampling. The subsample routine in line 20, however, creates a pool, from the unlabeled portion of the entire database, which is large and not materialized. We devise a sampling-based optimization to avoid scanning the unlabeled database in each iteration. Due to space constraints, the interested reader is referred to <ref type="bibr" target="#b38">[39]</ref> for details.</p><p>Distributed computing (OPT2). Given a large dataset, it is time-consuming to update the data space model by building the three (positive, negative, and uncertain) partitions for the entire dataset every time a user-labeled example is received. We apply an open-source system Ray 4 to parallelize our code, by splitting the entire dataset into several subsets evenly and building the partitions within each subset. As such, we divide the partition-building task for the entire dataset into multiple independent sub-tasks, reducing the time per iteration from 20-30 seconds to a few seconds.</p></div>
<div><head n="4">Factorized Dual-Space Model</head><p>Although DSM can reduce user labeling effort and offer better accuracy than traditional active learning, increased dimensionality will make the volume of its uncertain region grow fast and degrade its performance. To handle this issue, we leverage the property of conjunctive queries to factorize a high-dimensional data space into a set of lowdimensional spaces. By running the polytope model in each low-dimensional space, we can "simulate" DSM in the highdimensional space with improved performance.</p><p>This extension, denoted as DSM F , may require the user to label examples in some subspaces. Revisit our example query Q I from Section 2. We observe that often when a user labels an example, the decision making process can be broken into a set of smaller questions, whose answers are then combined, through conjunctivity, to derive the final answer. In the example of Q I , the user asks two smaller questions: one concerns price (P ) and the other concerns length and width (LW ) for spaciousness. If a car is labeled positive, then its partial labels on P and LW are inferred to be positive. Otherwise, the user will be asked which values are not satisfactory. For example, if the user thinks the price is too high, through clicking a "dislike" button in the GUI, the price P will receive a negative partial label for the given car. As the user has gone through thinking to derive the overall negative label, specifying a subset of attributes that lead to the negative decision will not incur much more work.</p></div>
<div><head n="4.1">Factorization</head><p>Formally, factorization concerns a user interest query Q that is defined on an attribute set A of size d and can be written in the conjunctive form,</p><formula xml:id="formula_11">Q 1 ∧. . .∧Q m . Each Q i , i ∈ [1 . . . m],</formula><p>4 Ray provides fast distributed computing at https://ray.io/ uses a subset of attributes</p><formula xml:id="formula_12">A i = {A i1 , • • • , A idi }. The fam- ily of attribute sets, A = (A 1 , • • • , A m ), is pairwise dis- joint with d = m i=1</formula><p>d i , and called the factorization structure.</p><p>In practice, we can derive the factorization structure from available data in a database. If a query trace is available to show how attributes are used in predicates, e.g., individually or in a pair, we can run a simple algorithm over the query trace: Initially assign each attribute to its own partition. As the query trace is scanned, two partitions are merged if a predicate uses attributes from the two partitions. At the end, all the remaining partitions become the factorization structure A. Even if a query trace is not available, it is still possible to work with domain experts and extract a factorization structure that reflects how attributes are used based on their semantics, e.g., length and width are often used in the same predicate to capture the spaciousness of the car.</p><p>DSC Queries. We next define a class of user interest queries that DSM F supports with benefits over active learning: that is, ∀i = 1, . . . , m, either the positive region in the i th subspace, defined as</p><formula xml:id="formula_13">{x i ∈ R |Ai| |Q i (x i ) &gt; 0}, is convex (in the Q + c class), or the negative region, {x i ∈ R |Ai| |Q i (x i ) &lt; 0} is convex (in Q - c ).</formula><p>We call such queries Decomposable Subspatial Convex (DSC) queries.</p><p>DSC allows us to combine a subspace whose positive region is convex with another subspace whose negative region is convex. However, the global query is not necessarily convex in either the positive or negative region. Figure <ref type="figure">4(a)</ref> shows an example query, Q = x 2 + y 2 &gt; 0.2 2 ∧ 480 &lt; z &lt; 885, which combines the ellipse pattern in Fig. <ref type="figure" target="#fig_2">2(d)</ref>, whose negative region is convex, with the line pattern in Fig. <ref type="figure" target="#fig_2">2(a)</ref>, whose positive region is convex. In the 3D space, the negative region (marked in red) is the union of the red cylinder in the middle of the figure and the red rectangles above and below the cylinder, while the positive region (in green) is the complement of it. Neither of the positive nor the negative region of Q is convex although it belongs to DSC.</p><p>p-DSC Queries. We also support a superset of DSC queries where the convex assumption holds only in some subspaces. We call this set partial factorization or p-DSC.</p><p>As a special case, if none of the subspaces permits the convexity property, we call such queries Zero DSC or 0-DSC.</p></div>
<div><head n="4.2">Factorized DSM Model</head><p>We next extend our DSM model with factorization, denoted as DSM F . The new model is composed of a factorized polytope model and a factorized classification model.</p><p>Factorized polytope model. Given the factorization structure, the polytope model runs in each subspace where the subspatial convexity holds. In the i th subspace defined by A i , Q i 's positive and negative regions, denoted as R +  Cartesian product of convex sets is convex, so R + f is convex by definition. At each iteration, given a finite set of positive points {x}, R + and R + f are constructed on {x}. Since R + as a convex polytope is the smallest convex set that contains {x}, we conclude</p><formula xml:id="formula_14">R + ✓ R + f .</formula><p>Proposition 4.2 All points in the negative region R f are negative and R ✓ R f at each iteration of the data exploration process.</p><formula xml:id="formula_15">PROOF. If a point x = (x1, • • • , xm) 2 R f ,</formula><p>then there exists some subspace spanned by attributes AI such that xI 2 R I . Since all points in R I are negative for QI based on proposition 3.2, and the target query Q is in a conjunctive form, x is negative for Q.</p><p>To prove R ✓ R f , it is sufficient to prove that every arbitrary convex cone that R is built upon, is contained by R f . Let e be the apex of a convex cone R e in Definition 3.2. Since e is negative for Q, there exists I such that its factorization vector e I is negative for QI . Let ⇡I (R e ) be the projection of the cone R e onto the subspace spanned by AI . Then for a point where each QI is convex in the positive region <ref type="foot" target="#foot_3">6</ref> and contains an attribute set AI = {AI1, • • • , AId I }, with AIj representing the j th attribute in the I th partition. A family of attribute sets, A = (A1, • • • , Am), is pairwise disjoint. The total dimension is d = P m I=1 dI . Then in the Ith subspace defined by AI , the positive and negative regions of QI can be defined as in Definition 3.1 and 3.2, but based on the "positive" and "negative" examples labeled for QI . We denote these positive and negative regions in the I th subspace as R + I and R I , respectively. We note that a positive example with respect to Q implies that its projection in every I th subspace satisfies Qi. However, a negative example with respect to Q implies that there exists at least one subspace, I, where the example's projection is negative for QI , while there may still other subspaces, I 0 6 = I, where the example's projection satisfies QI0 .</p><p>Next we build the positive and negative regions of the query Q from the positive and negative regions in the subspaces:</p><formula xml:id="formula_16">R + f = ⇥ m I=1 R + I , R f = ⇥ m I=1 (R I ) c<label>(1)</label></formula><p>where each R + I (or R I ) is interpreted equally as a geometric object or the set of all possible points enclosed in the object, ⇥ represents the Cartesian product between two sets, and R c represents the complement of set R. Therefore, the uncertain region of Q is</p><formula xml:id="formula_17">R u f = R d R + f R f</formula><p>Finally, denote the entire dataset as D and the evaluation set as Deval. The definitions of positive, negative, and uncertain partitions of Deval are the same as in Definition 3.4 with the three regions replaced by R + f , R f , and R u f . Formal results on factorization. When the true query is conjunctive, taking advantage of the factorization property of the data space proves to be effective for enlarging the positive and negative regions known to the DSM algorithm and hence reducing the uncertain region. Formally, we have the following results. 6 [As mentioned before, each QI can also be convex in the negative region. However, in the interest space our discussion focuses on the case of convex positive regions.] f polytope is the smallest convex set that contains {x}, we conclude</p><formula xml:id="formula_18">R + ✓ R + f .</formula><p>Proposition 4.2 All points in the negative region R f are negative and R ✓ R f at each iteration of the data exploration process.</p><p>PROOF. If a point x = (x1, • • • , xm) 2 R f , then there exists some subspace spanned by attributes AI such that xI 2 R I . Since all points in R I are negative for QI based on proposition 3.2, and the target query Q is in a conjunctive form, x is negative for Q.</p><p>To prove R ✓ R f , it is sufficient to prove that every arbitrary convex cone that R is built upon, is contained by R f . Let e be the apex of a convex cone R e in Definition 3.2. Since e is negative for Q, there exists I such that its factorization vector e I is negative for QI . Let ⇡I (R e ) be the projection of the cone R e onto the subspace spanned by AI . Then for a point projection satisfies QI0 .</p><p>Next we build the positive and negative regions of the query Q from the positive and negative regions in the subspaces:</p><formula xml:id="formula_19">R + f = ⇥ m I=1 R + I , R f = ⇥ m I=1 (R I ) c<label>(1)</label></formula><p>where each R + I (or R I ) is interpreted equally as a geometric object or the set of all possible points enclosed in the object, ⇥ represents the Cartesian product between two sets, and R c represents the complement of set R. Therefore, the uncertain region of Q is</p><formula xml:id="formula_20">R u f = R d R + f R f</formula><p>Finally, denote the entire dataset as D and the evaluation set as Deval. The definitions of positive, negative, and uncertain partitions of Deval are the same as in Definition 3.4 with the three regions replaced by R + f , R f , and R u f . Formal results on factorization. When the true query is conjunctive, taking advantage of the factorization property of the data space proves to be effective for enlarging the positive and negative regions known to the DSM algorithm and hence reducing the uncertain region. Formally, we have the following results. =1 dI . Then in the Ith subspace defined by AI , the positive negative regions of QI can be defined as in Definition 3.1 and but based on the "positive" and "negative" examples labeled QI . We denote these positive and negative regions in the I th pace as R + I and R I , respectively. e note that a positive example with respect to Q implies that its ection in every I th subspace satisfies Qi. However, a negative ple with respect to Q implies that there exists at least one pace, I, where the example's projection is negative for QI , le there may still other subspaces, I 0 6 = I, where the example's ection satisfies Q I 0 . ext we build the positive and negative regions of the query Q the positive and negative regions in the subspaces:</p><formula xml:id="formula_21">R + f = ⇥ m I=1 R + I , R f = ⇥ m I=1 (R I ) c<label>(1)</label></formula><p>re each R + I (or R I ) is interpreted equally as a geometric object e set of all possible points enclosed in the object, ⇥ represents artesian product between two sets, and R c represents the coment of set R. Therefore, the uncertain region of Q is  mentioned before, each Q I can also be convex in the negative region. ever, in the interest space our discussion focuses on the case of convex tive regions.]</p><formula xml:id="formula_22">R u f = R d R + f R f inally,</formula><p>Cartesian product of convex sets is convex, so R + f is convex by definition. At each iteration, given a finite set of positive points {x}, R + and R + f are constructed on {x}. Since R + as a convex polytope is the smallest convex set that contains {x}, we conclude</p><formula xml:id="formula_23">R + ✓ R + f .</formula><p>Proposition 4.2 All points in the negative region R f are negative and R ✓ R f at each iteration of the data exploration process.</p><p>PROOF. If a point x = (x1, • • • , xm) 2 R f , then there exists some subspace spanned by attributes AI such that xI 2 R I . Since all points in R I are negative for QI based on proposition 3.2, and the target query Q is in a conjunctive form, x is negative for Q.</p><p>To prove R ✓ R f , it is sufficient to prove that every arbitrary convex cone that R is built upon, is contained by R f . Let e be the apex of a convex cone R e in Definition 3.2. Since e is negative for Q, there exists I such that its factorization vector e I is negative for QI . Let ⇡I (R e ) be the projection of the cone R e onto the subspace spanned by AI . Then for a point where each QI is convex in the positive region 6 and contains an attribute set AI = {AI1, • • • , AId I }, with AIj representing the j th attribute in the I th partition. A family of attribute sets, A = (A1, • • • , Am), is pairwise disjoint. The total dimension is d = P m I=1 dI . Then in the Ith subspace defined by AI , the positive and negative regions of QI can be defined as in Definition 3.1 and 3.2, but based on the "positive" and "negative" examples labeled for QI . We denote these positive and negative regions in the I th subspace as R + I and R I , respectively. We note that a positive example with respect to Q implies that its projection in every I th subspace satisfies Qi. However, a negative example with respect to Q implies that there exists at least one subspace, I, where the example's projection is negative for QI , while there may still other subspaces, I 0 6 = I, where the example's projection satisfies QI0 .</p><p>Next we build the positive and negative regions of the query Q from the positive and negative regions in the subspaces:</p><formula xml:id="formula_24">R + f = ⇥ m I=1 R + I , R f = ⇥ m I=1 (R I ) c<label>(1)</label></formula><p>where each R + I (or R I ) is interpreted equally as a geometric object or the set of all possible points enclosed in the object, ⇥ represents the Cartesian product between two sets, and R c represents the complement of set R. Therefore, the uncertain region of Q is</p><formula xml:id="formula_25">R u f = R d R + f R f</formula><p>Finally, denote the entire dataset as D and the evaluation set as Deval. The definitions of positive, negative, and uncertain partitions of Deval are the same as in Definition 3.4 with the three regions replaced by R + f , R f , and R u f . Formal results on factorization. When the true query is conjunctive, taking advantage of the factorization property of the data space proves to be effective for enlarging the positive and negative regions known to the DSM algorithm and hence reducing the uncertain region. Formally, we have the following results. an example, the label is certainly positive in each subspace. However, if the user label is negative for an example, she will be asked to highlight the specific set of attributes (and the subspaces thereof) that lead to the negative label. This can be easily achieved through a graphical interface such as in <ref type="bibr" target="#b11">[12]</ref>.</p><p>Positive and negative regions in factorized space. Formally, factorization concerns a user interest query Q that involve d attributes and can be written in the conjunctive form, Q1 ^• • • ^Qm, where each QI is convex in the positive region 6 and contains an attribute set AI = {AI1, • • • , AId I }, with AIj representing the j th attribute in the I th partition. A family of attribute sets, A = (A1, • • • , Am), is pairwise disjoint. The total dimension is d = P m I=1 dI . Then in the Ith subspace defined by AI , the positive and negative regions of QI can be defined as in Definition 3.1 and 3.2, but based on the "positive" and "negative" examples labeled for QI . We denote these positive and negative regions in the I th subspace as R + I and R I , respectively. We note that a positive example with respect to Q implies that its projection in every I th subspace satisfies Qi. However, a negative example with respect to Q implies that there exists at least one subspace, I, where the example's projection is negative for QI , while there may still other subspaces, I 0 6 = I, where the example's projection satisfies QI0 .</p><p>Next we build the positive and negative regions of the query Q from the positive and negative regions in the subspaces:</p><formula xml:id="formula_26">R + f = ⇥ m I=1 R + I , R f = ⇥ m I=1 (R I ) c<label>(1)</label></formula><p>where each R + I (or R I ) is interpreted equally as a geometric object or the set of all possible points enclosed in the object, ⇥ represents the Cartesian product between two sets, and R c represents the complement of set R. Therefore, the uncertain region of Q is</p><formula xml:id="formula_27">R u f = R d R + f R f</formula><p>Finally, denote the entire dataset as D and the evaluation set as Deval. The definitions of positive, negative, and uncertain partitions of Deval are the same as in Definition 3.4 with the three regions replaced by R + f , R f , and R u f . Formal results on factorization. When the true query is conjunctive, taking advantage of the factorization property of the data space proves to be effective for enlarging the positive and negative regions known to the DSM algorithm and hence reducing the uncertain region. Formally, we have the following results. projection satisfies QI0 .</p><p>Next we build the positive and negative regions of the query Q from the positive and negative regions in the subspaces:</p><formula xml:id="formula_28">R + f = ⇥ m I=1 R + I , R f = ⇥ m I=1 (R I ) c<label>(1)</label></formula><p>where each R + I (or R I ) is interpreted equally as a geometric object or the set of all possible points enclosed in the object, ⇥ represents the Cartesian product between two sets, and R c represents the complement of set R. Therefore, the uncertain region of Q is</p><formula xml:id="formula_29">R u f = R d R + f R f</formula><p>Finally, denote the entire dataset as D and the evaluation set as Deval. The definitions of positive, negative, and uncertain partitions of Deval are the same as in Definition 3.4 with the three regions replaced by R + f , R f , and R u f . Formal results on factorization. When the true query is conjunctive, taking advantage of the factorization property of the data space proves to be effective for enlarging the positive and negative regions known to the DSM algorithm and hence reducing the uncertain region. Formally, we have the following results. an example, the label is certainly positive in each subspace. However, if the user label is negative for an example, she will be asked to highlight the specific set of attributes (and the subspaces thereof) that lead to the negative label. This can be easily achieved through a graphical interface such as in <ref type="bibr" target="#b11">[12]</ref>.</p><p>Positive and negative regions in factorized space. Formally, factorization concerns a user interest query Q that involve d attributes and can be written in the conjunctive form, Q1 ^• • • ^Qm, where each QI is convex in the positive region 6 and contains an attribute set AI = {AI1, • • • , AId I }, with AIj representing the j th attribute in the I th partition. A family of attribute sets, A = (A1, • • • , Am), is pairwise disjoint. The total dimension is d = P m I=1 dI . Then in the Ith subspace defined by AI , the positive and negative regions of QI can be defined as in Definition 3.1 and 3.2, but based on the "positive" and "negative" examples labeled for QI . We denote these positive and negative regions in the I th subspace as R + I and R I , respectively. We note that a positive example with respect to Q implies that its projection in every I th subspace satisfies Qi. However, a negative example with respect to Q implies that there exists at least one subspace, I, where the example's projection is negative for QI , while there may still other subspaces, I 0 6 = I, where the example's projection satisfies QI0 .</p><p>Next we build the positive and negative regions of the query Q from the positive and negative regions in the subspaces:</p><formula xml:id="formula_30">R + f = ⇥ m I=1 R + I , R f = ⇥ m I=1 (R I ) c<label>(1)</label></formula><p>where each R + I (or R I ) is interpreted equally as a geometric object or the set of all possible points enclosed in the object, ⇥ represents the Cartesian product between two sets, and R c represents the complement of set R. Therefore, the uncertain region of Q is</p><formula xml:id="formula_31">R u f = R d R + f R f</formula><p>Finally, denote the entire dataset as D and the evaluation set as Deval. The definitions of positive, negative, and uncertain partitions of Deval are the same as in Definition 3.4 with the three regions replaced by R + f , R f , and R u f . Formal results on factorization. When the true query is conjunctive, taking advantage of the factorization property of the data space proves to be effective for enlarging the positive and negative regions known to the DSM algorithm and hence reducing the uncertain region. Formally, we have the following results. based on the "positive" and "negative" labels of projected examples for Q i , as mentioned above. Then we build the positive and negative regions of Q from the positive and negative regions in the subspaces via the conjunctive property:</p><formula xml:id="formula_32">R + f = × m i=1 R + i , R - f = × m i=1 (R - i ) c c<label>(2)</label></formula><p>where × denotes the Cartesian product between two sets, and R c denotes the complement of set R. Then the uncertain</p><formula xml:id="formula_33">region of Q is, R u f = R d -R + f -R - f .</formula><p>Next we formally define the decision function for the polytope model with factorization. In each subspace defined on A i , let F Ai : R |Ai| → {-1, 0, 1} be the decision function that divides the subspace into three disjoint regions corresponding to the negative, unknown, and positive regions, respectively. As in (Eq. 1),</p><formula xml:id="formula_34">F Ai (x) = 1 • 1(x ∈ R + i ) -1 • 1(x ∈ R - i ).<label>(3)</label></formula><p>For p-DSC queries, if the subspatial convexity does not hold in a subspace, the value of its decision function is set to zero. The global decision function for the polytope model with factorization over the entire data space is then</p><formula xml:id="formula_35">F D f (x) = m min i=1 F Ai (x i )<label>(4)</label></formula><p>where x = (x 1 , . . . , x d ), and x i denotes the projection of x on the i th subspace defined by A i . Factorized classification model. To apply factorization to the classifier, we train a classifier in each subspace h i : R |Ai| → {-1, 1}, i = 1, • • • , m, and combine them to be a classifier F V f : R d → {-1, 1} over the entire data space. According to the conjunctive property, we have</p><formula xml:id="formula_36">F V f = m min i=1 h i (x i )<label>(5)</label></formula><p>We call F V f a factorized classifier. We further propose a new sample acquisition method for F V f . The sample acquisition criterion can be defined as:</p><formula xml:id="formula_37">next_sample = arg min x m i=1 |d(h i , x i )|<label>(6)</label></formula><p>The example selected from the unlabeled data by the above criterion has, on average, the shortest distance to each decision boundary on subspaces. The reason behind using L 1 (Manhattan) distance metric to combine the distance information from all subspaces is that for the commonly used L k norm, lower values of k are preferable in high dimensional space as shown in <ref type="bibr" target="#b3">[4]</ref>. In addition, we observe from empirical results that when the sample acquisition methods are performed with high dimensionality, the examples selected by our criterion are usually closer to the true decision boundary, compared to the traditional uncertainty sampling. Finally, consider the dual-space model. Let DSM F be the combination of DSM in §3.2 with the polytope data space model F D replaced by the polytope model with factorization F D f , and the classification model F V replaced by the factorized classifier F V f . Then the dual-space decision function of DSM F , H : R d → {-1, 1}, is:</p><formula xml:id="formula_38">H(x) = F D f (x), F D f (x) = 0 F V f (x), otherwise<label>(7)</label></formula><p>H is our final prediction model returned to approximate the user interest query. For 0-DSC queries, DSM F simply runs the classification model. Illustration. Before presenting the formal results, we illustrate the intuition that factorization allows us to construct the positive and negative regions (R + f , R - f ) as supersets of (R + , R -), hence reducing the unknown region and offering better accuracy. Figure <ref type="figure">4(b)</ref> shows four positive points A, B, C, D when the 3D space (x-y-z) is factorized into two subspaces (x-y) and (z). It depicts the positive region R + as the pyramid shaded in grey and marked by the green lines. When we factorize R + into (x-y) and (z) planes, we have R + xy as a triangle marked ABC and R + z as a line segment projected onto z. Then we construct R + f from the triangle and the line segment based on Eq. 2, we obtain a prism marked by the blue lines, which is much bigger than R + .</p><p>Figure <ref type="figure">4</ref>(c) shows a negative point e -and the negative region built for it. R -is a convex cone shaded in grey and bounded by the solid purple rays emitting from e -. When e -is projected onto (x-y), it is negative and defines a convex cone R - xy marked by the two solid red lines in the (x-y) plane. When e -is projected onto z, it lies in the positive region. According to Eq. 2, the new negative region R - f extends the convex cone R - xy by all possible values of z, yielding a geometric shape enclosed by the three solid red lines in the figure. Again, the new R - f is much bigger than R -. Formal results. For both DSC and p-DSC queries, we offer formal results of the polytope model with factorization. Proposition 3 and Proposition 4 state that the positive and negative regions constructed via factorization, (R + f , R - f ), are a superset of (R + , R -) that the original DSM offers. Hence, the lower bound of the F1-score built from (R + f , R - f ) is higher than that from (R + , R -) based on Def. 4.</p><p>Testing assumptions of DSM F . Factorization relies on two assumptions: The first assumption is that we have a correct factorization structure, A = (A 1 , • • • , A m ), derived from a query trace or the database schema. The second assumption is that user interests take the form a conjunctive query (CQ). When either assumption is wrong, we may see conflicting examples in a polytope model for a specific subspace, i.e., a positive example labeled by the user appears in the negative region or vice versa. Our system uses a procedure to test online both assumptions behind DSM F : At the beginning of data exploration, we build two polytope models for each subspace, one under the assumption that the positive region is convex, Q + c , the other under the assumption that the negative region is convex, Q - c . Over iterations, we count the number of conflicting examples of these two polytope models, and use a threshold, T , to turn off a polytope model if its count exceeds T . If both polytope models remain active in a given iteration, the one with the smaller count will be used; in the case of a tie, the model for Q + c will be used as more query patterns belong to this class. When both polytope models for a subspace are turned off, we use partial factorization until they are turned off for all subspaces, when we resort to the classifier.</p></div>
<div><head n="5">Learning with Label Noise</head><p>Traditional active learning implicitly assumes that uncorrupted labels are provided by the user. However, in practice, label noise often occurs during the process of manual annotation. The user may not fully understand his/her interest and may have trouble labeling some examples correctly, especially for the ambiguous examples close to the underlying decision boundary. In other cases, the user may just mislabel some examples occasionally. Based on the observations of labeling behaviors in our user study, we assume in this work that the user exploration is overall moving toward a target interest region (e.g., not switching mind between two different regions), but the user may mislabel some examples. To improve the robustness of our AL-based data exploration system, we explore suitable label noise models to characterize the human annotator noise and propose denoising methods that are customized for our DSM model. We also analyze the effect of factorization on the label noise problem and provide several optimization methods to improve our system.</p></div>
<div><head n="5.1">Label Noise Models and Noise Distillation</head><p>In this section, we present background on label noise models and some analysis about the data cleansing method used in our algorithm -automatic noise distillation.</p></div>
<div><head n="5.1.1">Label Noise Models</head><p>There are mainly three types of label noise models <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b30">31]</ref>: (i) In the random classification noise (RCN) model, the label flip probability (noise rate) is independent of the example and the corresponding true label (i.e., all examples have the same label flip probability). (ii) In the class-conditional random label noise (CCN) model, the label flip probability is independent of the example but dependent on the true label. The examples from the same class have the same label flip probability. (iii) The instance-and label-dependent label noise (ILN) model, where the label flip probability depends on both the example and true label, is the most general case of label noise, hence most complex yet broadly applicable. Compared to the RCN and CCN models, the ILN model has not been widely studied in the literature. In this work, we consider ILN for AL-based data exploration.</p><p>In particular, we use the assumption on label noise proposed by <ref type="bibr" target="#b16">[17]</ref>, shown as Assumption 1 below, to deal with a specific type of ILN. Let X denote the observation, Y the uncorrupted but unobserved label, and Ỹ the observed but noisy label. We assume that (X, Y, Ỹ ) ∈ X × Y × Y are jointly distributed according to an unknown distribution, where X ⊆ R d is the data space and Y = {-1, 1} is the label space. Given an example x and its corresponding true label y, the noise rate model in <ref type="bibr" target="#b16">[17]</ref> is defined as ρ y (x) = P ( Ỹ = -y | X = x, Y = y). For RCN, ρ +1 (x) = ρ -1 (x) = ρ (ρ is a constant). For CCN, ρ y (x) is independent of X but dependent on Y . For ILN, ρ y (x) is dependent on both X and Y . A particular case of ILN is called bounded instance-and label-dependent noise (BILN) in <ref type="bibr" target="#b16">[17]</ref>, if the following assumption holds.</p><p>Assumption 1 (Key assumption in <ref type="bibr" target="#b16">[17]</ref>) ∀x ∈ X , we have the following bounds on the noise rates:</p><formula xml:id="formula_39">   0 ≤ ρ +1 (x) ≤ ρ +1 max &lt; 1, 0 ≤ ρ -1 (x) ≤ ρ -1 max &lt; 1, 0 ≤ ρ +1 (x) + ρ -1 (x) &lt; 1.</formula><p>where ρ +1 max and ρ -1 max are upper bounds of noise rates for positive and negative examples respectively.</p><p>The first two restrictions on label noise rates indicate that the label noise rates (label flipping probabilities) of labeled examples must have upper bounds less than 1. The last restriction 0 ≤ ρ +1 (x)+ρ -1 (x) &lt; 1 encodes the assumption made in <ref type="bibr" target="#b50">[51]</ref> that a majority of the labels in the ILN model are correct on average, and it is derived from <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b63">64]</ref> where ρ ±1 are constant. In the rest of this paper, we always assume that Assumption 1 holds.</p><p>Boundary-Consistence Noise (BCN). Although our proposed algorithm is designed for any kind of label noise that satisfies Assumption 1, to demonstrate the effectiveness of our algorithm in AL-based data exploration, we focus on one special case of ILN, Boundary-Consistence Noise (BCN), in evaluation. In the BCN model, the examples closer to the decision boundary are more likely to be mislabeled. As stated in <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b50">51]</ref>, the BCN model is a reasonable model for human annotator noise, and it can be applied to many real-world applications. More details of the BCN model can be found in Section 5.4.</p></div>
<div><head n="5.1.2">Automatic Noise Distillation and Limitations</head><p>We now introduce a data cleansing method with theoretical guarantees (first proposed in <ref type="bibr" target="#b16">[17]</ref>), which automatically collects confident examples out of a potentially corrupted dataset for the label noise problem under Assumption 1. We refer to this method as auto-cleansing.</p><p>Lemma 1 Denote by η(x) the conditional probability</p><formula xml:id="formula_40">P (Y = +1 | X = x).</formula><p>The Bayes optimal classifier is given by g * (x) = sgn(η(x) -1 2 ). Corollary 1 (Corollary in <ref type="bibr" target="#b16">[17]</ref>) Denote by η(x) the conditional probability P ( Ỹ = +1 | X = x), ∀x ∈ X , and then we have</p><formula xml:id="formula_41">η(x) &lt; 1 -ρ +1 max 2 =⇒ (x, Y = -1) is distilled; η(x) &gt; 1 + ρ -1 max 2 =⇒ (x, Y = +1) is distilled.</formula><p>When η, ρ +1 max and ρ -1 max are given, according to Corollary 1, a noisy example (X i , Ỹi ) is identified automatically as positive (negative</p><formula xml:id="formula_42">) if η (X i ) &gt; 1+ρ-1 max 2 (η (X i ) &lt; 1-ρ+1 max 2</formula><p>). The labels of distilled examples are identical to those assigned by the Bayes optimal classifier under the clean distribution, i.e., g * . In practice, η, ρ +1 max and ρ -1 max are usually unknown, but they can be estimated. More details can be found in <ref type="bibr" target="#b16">[17]</ref>.</p><p>This automatic noise distillation method is applicable to general label noise with robust performance guaranteed by the above theoretical results. Its effectiveness has been demonstrated empirically in the context of supervised learning. However, in the AL framework, under the combined effects of adverse factors (limited labeled data and severe class imbalance), the estimated η, ρ +1 max and ρ -1 max learned from the existing labeled examples are usually not accurate enough. Consequently, the confident examples collected by auto-cleansing may include a certain amount of label noise.</p></div>
<div><head n="5.2">Robust Dual-Space Model</head><p>Given the limitations of auto-cleansing in the context of active learning, we propose a Robust Dual-Space Model (DSM) to further filter label noise and eventually build an accurate DSM. Here, we leverage the geometrical property of DSM and a k-nearest neighbors method to further remove noisy labels from the distilled examples collected by auto-cleansing. Since there is a tradeoff between introducing DSM-labeled examples to save user labeling effort and avoiding additional noisy labels from DSM-labeled examples, we also design an adaptive method based on estimated noise rates to bring in DSM-labeled examples strategically.</p></div>
<div><head n="5.2.1">Data Space Model Refinement</head><p>To further filter noisy labels from the distilled examples collected by auto-cleansing, we propose a DSM-based data cleansing method called Data Space Model Refinement. In the following, we suppose that the subspatial convexity assumption holds for the data space under consideration. We use D ch and D cc to denote the labeled examples for building the convex hull and the convex cones, respectively.</p><p>Illustration. Before presenting the complete algorithm, we illustrate the intuition that the subset of examples in D ch that are very close to the boundary of the convex hull are likely to be noisy. Our approach to cleansing the convex hull is that for each vertex of the current convex hull, if its knearest neighbors contain a conflicting example from D cc , then we remove from the convex hull this vertex and all its neighbors that lie within a l-distance radius of this vertex, where l is defined to be the distance between the vertex and the discovered conflicting example. As shown in Figure <ref type="figure" target="#fig_21">5(a)</ref>, the green tetragon represents the convex hull formed by the examples in D ch (green dots), and red triangles represent some examples from D cc but inside the convex hull. Take the vertex o for example: its nearest neighbor is g 1 and its second nearest neighbor is r 1 , regardless of the choice of k. If the assumption of subspatial convexity holds, both o Multiple-round noise filtering for the convex hull. One round of noise filtering, however, may not be adequate. We next demonstrate the effectiveness of the iterative refinement of the convex hull by displaying the gradually shrinking convex hulls over rounds of noise filtering. For ease of composition, we assume that the user interest query is in Q + c , and the convex hull of DSM is created from the positive examples. In addition, we use k = 3 for finding the k-nearest neighbors in the example in Figure <ref type="figure" target="#fig_20">5</ref>. Since there still exist negative examples inside the convex hull, we perform another round of noise filtering and eventually obtain a conservative and much more accurate convex hull, as shown in Figure <ref type="figure" target="#fig_21">5(d)</ref>.</p><p>Refinement of the convex cones. To refine the convex cones, we perform simple filtering by checking whether an example x in D cc is inside the refined hull. Only when x / ∈ hull, will x be used to build a convex cone. build the data space model with refinement, and 3) compute estimated noise rates to control auto-labeling, which takes effect later in the sampling acquisition process.</p><p>We now present the full algorithm (Algorithm 2) of applying RDSM into active learning-based interactive data exploration and highlight the lines (marked in blue) that differ from Algorithm 1. Since the framework remains similar, we only discuss the two major differences.</p><p>Update of RDSM. accu ← threeSetMetric(D) // applying the combined query strategy: 16:</p><formula xml:id="formula_43">D lu ← ∅, D ld ← ∅, x * ← ∅ 17:</formula><p>if rand() ≤ γ then //sampling from the uncertain region : 18:</p><p>pool ← subsample(D u , m) 19:</p><p>x * ← sample_acquisition(pool, classif ier) 20:</p><p>D lu ← get_labels_from_user(x * ) 21:</p><p>else //uncertainty sampling around the boundary: 22:</p><p>pool ← subsample(D unlabeled , m) 23:</p><p>x * ← sample_acquisition(pool, classif ier) 24:</p><p>for x ∈ x * do 25: amples. Otherwise, they are updated incrementally. The implicit assumption of the incremental update method is that D ± (D u ) built at a certain iteration must be a superset (subset) of D ± (D u ) built at any previous iteration. We use D (±, t) and D (u, t) to denote D ± and D u built at iteration t respectively and then the assumption can be formalized as: if t 1 &lt; t 2 , then D (±, t1) ⊆ D (±, t2) and D (u, t2) ⊆ D (u, t1) . However, this assumption does not hold in the presence of label noise. As a consequence, we need to correct the existing partitions when receiving newly labeled examples from the user. We believe that the data space model created with more labeled examples is more accurate than that created with fewer labeled examples, so we correct the existing partitions by rebuilding the partitions according to the latest data space model. Rebuilding the partitions enables RDSM to self-correct the partitions and thus alleviates the effect of label noise on its performance.</p><formula xml:id="formula_44">if x ∈ (R + ∪ R -)</formula><p>Adaptive Auto-labeling. Line 9 determines whether to utilize RDSM later for auto-labeling. Recall that our la-beled set includes both user-labeled and DSM-labeled examples. Hence, we compute the error rates for these two subsets, respectively, and compare them to decide whether auto-labeling is enabled. The estimated noise rate of userlabeled (DSM-labeled) examples ρu (ρ d ) is defined to be the ratio of estimated mislabeled examples in user-labeled (DSM-labeled) examples. The on-off switch auto_labeling is assigned to be T rue if ρd &lt; ρu , otherwise F alse. Line 25 shows that only when auto_labeling is T rue, RDSM is qualified to provide labels. This adaptive strategy, which not only takes advantage of auto-labeling to save user labeling effort but also prevents introducing label noise from RDSM, is more realistic and applicable than using auto-labeling all the time. More details can be found in <ref type="bibr" target="#b38">[39]</ref>.</p></div>
<div><head n="5.3">High-dimensional Exploration with Label Noise</head><p>In this section, we present how to optimize RDSM further through factorization for high-dimensional exploration and analyze how factorization affects learning with label noise.</p><p>For data exploration performed with high dimensionality, to further boost performance we leverage the idea of factorization to devise the factorized RDSM, denoted as RDSM F . This method divides the high dimensional space into a set of lower dimensional subspaces and builds a RDSM in each subspace. RDSM F follows the same factorization rules as those of DSM F , as described in Section 4.</p><p>We provide further analysis of the effect of factorization on learning with label noise. We prove that if the factorization structure has the conjunctive property, the noise rate of positive examples in each subspace is lower than or equal to that in the original data space. Besides, we explain how factorization affects the mislabeled negative examples.</p><p>Mislabeled positive examples. The user interest queries in our data exploration setting usually have very low selectivity, i.e., the user interest examples are scarce and hard to hit during the data exploration. It is well known that such class imbalance pushes the decision boundary of classification models towards minority examples <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b71">72,</ref><ref type="bibr" target="#b78">79,</ref><ref type="bibr" target="#b79">80]</ref>. Further, if some minority examples are mislabeled while the majority examples are clean, the class imbalance problem becomes more severe. In general, mislabeled minority examples are more destructive than mislabeled majority examples. In our problem setting, the minority examples are positive examples, which are often rare when the user comes to explore a very large dataset. Hence, our design pays more attention to mislabeled positive examples.</p><p>Without loss of generality, we assume that the factorization structure consists of two groups of attributes (X 1 , X 2 ), X j ∈ R dj , j = 1, 2 and {X 1 , X 2 } are independent. In the jth subspace, we use Y j and Ỹ j to denote the true label and the observed label of the projected examples X j , and we define the noise rate of an example x in this subspace as, ρ j y j (x) = P ( Ỹ j = -y j | X j = x j , Y j = y j ), y j ∈ {-1, +1}. According to the conjunctivity in the original high-dimensional data space, we have the clean but unobserved label Z = min(Y 1 , Y 2 ), the observed but corrupted label Z = min( Ỹ 1 , Ỹ 2 ), and the overall noise rate is defined as</p><formula xml:id="formula_45">ρ z (x) = P ( Z = -z | X = x, Z = z), where x = (x 1 , x 2 ), x j ∈ R dj , j = 1, 2.</formula><p>Lemma 2 Denote by F the factorization structure composed of two groups of attributes (X 1 , X 2 ), X j ∈ R dj , j = 1, 2, and {X 1 , X 2 } are independent, if the conjunctive property holds in F , we have ρ j +1 ≤ ρ +1 , j = 1, 2. Proof With the conjunctive property, we have</p><formula xml:id="formula_46">ρ +1 (x) = P ( Z = -1 | X = x, Z = +1) = P ( Z = -1, X = x, Z = +1) P (X = x, Z = +1) = P (X = x, Z = +1) -P ( Z = +1, X = x, Z = +1) P (X = x, Z = +1) = 1 - i=1,2 P ( Ỹ i = +1, X i = x i , Y i = +1) P (X i = x i , Y i = +1) = 1 -1 -ρ 1 +1 (x))(1 -ρ 2 +1 (x) = ρ 1 +1 (x) + ρ 2 +1 (x) -ρ 1 +1 (x)ρ 2 +1 (x) Then ρ +1 (x) -ρ 1 +1 (x) = ρ 2 +1 (x)(1 -ρ 1 +1 (x)) ≥ 0 = == ⇒ ρ 1 +1 (x) ≤ ρ +1 (x). Similarly, we can prove ρ 2 (x) ≤ ρ +1 (x).</formula><p>It is trivial to extend the Lemma 2 from two independent groups of attributes to a finite number of independent groups and obtain the following theorem.</p><p>Theorem 3 Denote by F the factorization structure composed of m groups of attributes</p><formula xml:id="formula_47">(X 1 , X 2 , • • • , X m ), X j ∈ R dj , j = 1, 2, • • • , m, and {X 1 , X 2 , • • • , X m } are inde- pendent, if the conjunctive property holds in F , we have ρ j +1 ≤ ρ +1 , j = 1, 2, • • • , m.</formula><p>Theorem 3 demonstrates that given a factorization structure with conjunctive property, the noise rate of positive examples in each subspace is lower than that in the original high-dimensional space. It coincides with the intuition that when a positive example is labeled as negative and the user is asked to specify which subspaces cause the negative label, the user may not label all the subspaces as negative. Thus, the classifiers can always gain useful information from the subspaces that obtain the correct subspatial labels.</p><p>Mislabeled negative examples. We now focus on negative examples. The projection of negative examples may fall into the projected positive region (user interest region) in some subspace. For example, a small black car does not suit the user who seeks a large black car, but it captures the user interest partially along the color dimension, leading to an overall negative label but a positive subspatial label for the color. When a negative example is mislabeled as positive in the original high-dimensional space, according to the conjunctive property, its subspatial label in each subspace is inferred to be positive. Since the actual subspatial labels of this example in some subspaces might be positive, these subspatial labels remain correct and can offer useful information in the corresponding subspaces. Compared to standard classifiers, which only learn from the mislabeled negative examples in high-dimensional space, the classifiers with factorization gain greater insight from the subspaces and become more robust.</p></div>
<div><head n="5.4">Simulation of Label Noise Models</head><p>Boundary-Consistence Noise (BCN). As mentioned in Section 5.1.1, we focus on Boundary-Consistence Noise (BCN) for evaluation as the BCN model is considered a reasonable model for human annotator noise. Inspired by <ref type="bibr" target="#b24">[25]</ref>, we assume that the label noise is distributed as an unnormalized Gaussian centered at decision boundary with variance σ 2 and thus, the BCN model can be represented by</p><formula xml:id="formula_48">P (ỹ = y | x) = ρ max exp - (dist(x, B)) 2 2σ 2 = ρ max exp - d 2 x λ<label>(8)</label></formula><p>where d x = dist(x, B) represents the distance from the example x to the true decision boundary B and λ = 2 * σ 2 is a parameter to determine the label noise level. When d x = 0, i.e. the example x lies on the decision boundary, P (ỹ = y | x) reaches its maximum ρ max . Given fixed ρ max and an example x, larger λ leads to higher noise rate. When λ is large enough, the BCN model approximates a RCN model with the label flip probability equal to ρ max .</p><p>To apply the BCN model for our simulation of human annotator noise, we need to address the following issues.</p><p>Estimation of d x . When the query pattern is not linear, it is nontrivial to attain d x for any example x. Although a perfect SVM classifier is trained based on the ground truth to compute d x in <ref type="bibr" target="#b39">[40]</ref>, the estimated distance provided by SVM cannot perfectly reflect the true distance, which eventually leads to a distorted distribution of the BCN model. Therefore, in this work, we propose a new method to estimate d x more precisely. The new distance estimation method mainly consists of three steps: 1) Collect border points: We run a k-nearest neighbors algorithm (provided by the <software ContextAttributes="used">scikit-learn</software> library 5 ) on the entire dataset and collect the examples whose neighbors contain at least t (t ≥ 1) examples(s) from the other class. These collected examples must be very close to the boundary, and we call them border points. 2) Compute distance: Given an example x, we treat the distance from x to its nearest border points as the estimated d x . 3) Scaling d x to [0, 1] within each class: According to the ground truth of the user interest, we separate the entire dataset into two sets, i.e., a positive set and a negative set. Then we scale d x to [0, 1] by its maximum absolute value within each set.</p><p>Setup of ρ max . We treat ρ max as a parameter to control different noise settings. We vary them in the range, [0.05, 0.1, 0.2, 0.3, 0.4, 0.5], in our experimental evaluation.</p><p>Tuning of λ. We assume that the examples farthest to the decision boundary have very low probability (at most τ ) to be mislabeled under the BCN model. Given ρ max and the upper bound τ (τ = 10 -12 in our experiments), we have</p><formula xml:id="formula_49">ρ max exp - 1 λ τ = ==== ⇒ λ -1 log(τ /ρ max )</formula><p>.</p><p>Noise injection during data exploration. We simulate noisy labels in our experimental study as follows: For a fixed user interest pattern and a fixed noise level ρ max , we set up λ as -1 log(τ /ρmax) and precompute the label flip probabilities in advance according to the BCN model defined in Equation <ref type="formula" target="#formula_48">8</ref>. In the simulated user labeling process of data exploration, we flip a coin for the selected example x with the label flip probability, P (ỹ = y | x), to determine whether a noisy label is generated or not for this example.</p></div>
<div><head n="6">Experimental Evaluation</head><p>We implemented all of our proposed techniques in a Pythonbased prototype for data exploration, which connects to a <software ContextAttributes="used">PostgreSQL</software> database. In this section, we evaluate our techniques and compare to recent active learning algorithms <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b27">28]</ref>, active search <ref type="bibr" target="#b32">[33]</ref>, and explore-by-example systems including Aide <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24]</ref> and <software ContextAttributes="used">LifeJoin</software> <ref type="bibr" target="#b18">[19]</ref>. Datasets: Our evaluation used two datasets. ( <ref type="formula" target="#formula_4">1</ref>) SDSS (190 million tuples) contains the "PhotoObjAll" table with 510 attributes. By default, we used 1% sample (1.9 million tuples, 4.9GB) to create an evaluation set for DSM and for pool-based uncertainty sampling -our formal results in Section 3.4 allowed us to use the 1% sample for data exploration, yet with bounded difference of ≤ .001 from the accuracy achieved over the full database with probability ≥0.994. (2) Car database (5622 tuples): this small dataset is used for our user study because it is more intuitive for users to perform explorative analytics. We defer a detailed discussion to §6.3. User Interest Queries: We extracted 7 query templates from the SDSS query release <ref type="bibr" target="#b67">[68]</ref> to represent user interests. They allow us to run simulations of user exploration sessions, as in <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24]</ref>, by using the true query answers as the proxy for user labeling. The 7 templates are shown in </p><formula xml:id="formula_50">(rectangle): rowc ∈ [a 1 , a 2 ] ∧ colc ∈ [b 1 , b 2 ] Q2 (ellipse): ((rowc -a 1 )/b 1 ) 2 + ((colc -a 2 )/b 2 ) 2 &lt; c 2 Q3 (rectangle): ra ∈ [a 1 , a 2 ] ∧ dec ∈ [b 1 , b 2 ]</formula><p>Q4 (outside a circle): rowv 2 + colv 2 &gt; c 2 Q5: 4D queries combining two queries from Q1-Q4 Q6: 6D queries combining three from Q1-Q4 Q7: 4D, (</p><formula xml:id="formula_51">x 1 &gt; a + b • x 2 ) ∧ (x 3 + c • log 10 x 2 4 &lt; d)</formula><p>Table <ref type="table" target="#tab_1">1</ref>. Each template is instantiated with different constants to vary query selectivity in [0.01%, 10%]. In particular, Q1-Q3 represent patterns that are convex in the positive region (Q + c ). Q4 retrieves tuples outside a circle, hence in the Q - c class. Q5-Q6 combine these attributes for scalability tests. Q7 includes a predicate on x1 and x 2 that belongs to Q + c , and a log predicate on x 3 and x 4 that belongs to</p><formula xml:id="formula_52">Q - c if x 4 &gt; 0 or is non-convex if x 4 ∈ R.</formula><p>We use Q7 to test partial factorization as discussed in §4. Servers: Our experiments were run on four servers, each with 40-core Intel(R) Xeon(R) CPU E5-2640 v4 @ 2.40GHz, 128GB memory, Python 3.7 on CentOS 7.</p></div>
<div><head n="6.1">Dual-Space Algorithm with Factorization</head><p>We evaluate our Dual-Space Model (DSM) and compare it to two ML techniques: (i) Active Learning (AL) runs uncertainty sampling <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b27">28]</ref> to obtain labeled examples for building a classifier, which is the version space part of DSM. We run AL with an SVM or a standard kNN classifier (kNN + ).</p><p>(ii) Active Search (AS) <ref type="bibr" target="#b32">[33]</ref> also follows an iterative procedure of asking the user to label an example and training a new model to guide the selection of the next example, but uses a strategy to maximize the number of positive examples in the set of selected examples, not classification accuracy. It uses a special variant of kNN classifier to enable optimization, denoted as kNN -. In all plots, the x-axis is the number of labeled examples. Expt 1 (2D queries): We run 2D queries from templates Q1 to Q4. Since the results show similar trends, we show the results for Q3 (1%) in Figure <ref type="figure" target="#fig_27">7(a)-7(d)</ref>.</p><p>Regarding F1-score, DSM outperforms AL, and AL outperforms AS. (1) An important factor is the data distribution around the decision boundary B of the user interest query. If B falls into a sparse region, separating the positive and negative classes is relatively easy for DSM and AL. The time cost of DSM, shown in Fig. <ref type="figure" target="#fig_27">7(d)</ref>, varies with the sampling method. Recall from Alg. 1 that with probability γ, it samples from the uncertain partition of the polytope model, D u ; otherwise, it does so from a subsample of the unlabeled examples in the database, D unlabeled . γ=0 leads to high time costs because the examples retrieved from D unlabeled may repeatedly fall in the positive or negative region of the polytope model, wasting resources with no new information. γ=1 and γ=0.5 significantly reduce the time cost, with γ=0.5 offering slightly better accuracy due to balanced sampling. However, both settings exhibit a spike in time cost in the early phase, which is the time to build the polytope model the first time by scanning the entire evaluation set. Finally, we improve γ=0.5 with the optimization (OPT1) from Section 3.4, where we start with a smaller evaluation set (n=50k) to avoid the initial spike, but later switch to a larger evaluation set (n=1.9 million) once its polytope model is built completely in the background. This optimization keeps the time cost per iteration within a second. Another way to reduce the time cost is to apply distributed computing (OPT2) from Section 3.4, which cuts down the per-iteration time to within 1-2 seconds when the size of the evaluation set is always 1.9 million. OPT1 and OPT2 can be combined to further improve the time performance; however, OPT1 may reduce accuracy due to use of a smaller sample. Since OPT2 keeps the time cost per iteration within a reasonable range, OPT2 with a 1.9-million evaluation set will be used as the default setting of our algorithm.</p><p>Expt 2 (4D-6D queries): We next show results of 4D-6D queries in dense regions as they present harder workloads. The main results of Figure <ref type="figure" target="#fig_27">7</ref>  Expt 3 (Partial factorization): We next generate two queries from Q7 with 7.8% selectivity for Q7.v 1 and 0.7% selectivity for Q7.v 2 (based on the constants used in SDSS). Q7.v 1 is in the DSC family because its positive region is convex in the (x1, x 2 ) subspace, and its negative region is convex in (x 3 , x 4 ). Q7.v 2 is in the p-DSC family because it is non-convex in (x 3 , x 4 ). Hence, DSM supports Q7.v 2 with partial factorization. Figure <ref type="figure" target="#fig_27">7</ref>(i) shows that DSM F works better for the DSC query than p-DSC query, as expected, but even partial factorization works much better than (i) AL, which does not reach 60% after 200 labeled examples, and (ii) AS, which cannot achieve more than 5% accuracy.</p></div>
<div><head n="6.2">Comparison to Alternative Systems</head><p>We next compare our system to two state-of-the-art exploreby-example systems: 1) <software ContextAttributes="used">LifeJoin</software> <ref type="bibr" target="#b18">[19]</ref> uses SVM for classification and active learning for seeking the next example for labeling. But it differs in the SVM implementation from our work: <software ContextAttributes="used">LifeJoin</software> proposed a hybrid approach that utilizes program synthesis to generate a number of interest functions (each function can be treated as a weak learner and the group forms an ensemble), and uses an SVM to find the weights for the generated functions. The example on which the weighted functions disagree the most is selected as the next example for labeling. We implemented <software ContextAttributes="used">LifeJoin</software> as additional methods in our system. 2) Aide <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24]</ref> uses decision trees as the classification model and customized methods for seeking the next example for labeling. We obtained the source code from the authors. When comparing these systems, we exclude the overhead to find the first positive example as these systems use different methods / assumptions to find them.</p><p>Expt 4: The F1-score is reported in Figure <ref type="figure">8</ref>(a) for a 2D query, and in Figure <ref type="figure">8</ref>(b) for 4D and 6D queries. (1) For the 2D query, our system outperforms Aide, which further outperforms <software ContextAttributes="used">LifeJoin</software>. Overall, <software ContextAttributes="used">LifeJoin</software> is significantly worse in accuracy. (2) For the 4D query, Aide and <software ContextAttributes="used">LifeJoin</software> drop to below 10% in accuracy, while our system achieves 99% within 140 iterations. For the 6D query, again Aide and Life-Join fail to work. This observation remains for any query that we tried beyond 2D. This is due to the combination of low selectivity and high dimensionality in data exploration. Additional analysis of these systems is available in <ref type="bibr" target="#b38">[39]</ref>.</p></div>
<div><head n="6.3">User Study using a Car Database</head><p>We conducted a user study by building a car database <ref type="foot" target="#foot_5">6</ref> . The database includes 5622 vehicles with 27 attributes such as the model, year, length, height, engine power, and retail price. Our study has two objectives: (1) build a query trace to understand the characteristics of data exploration tasks in this domain; <ref type="bibr" target="#b1">(2)</ref> use this trace as the ground truth of user interests to evaluate our system.</p><p>To develop the query trace, we designed task scenarios with different price constraints and usage patterns, e.g., buying a car for everyday city commute, outdoor sports, an elderly in the family, or a small business. The 18 users in our study belong to two groups: the first 11 users are CS professors and graduate students, while the rest of 7 are non-technical people. We asked each user to find all the cars that meet the requirements of the assigned task so that he can provide a recommendation service to customers who belong to the given scenario. Each user proceeds in three phases: 1) Review the schema: Since this is a familiar domain, most users can understand the schema quickly. 2) Initial exploration: We next show sample data to help the user understand the data and materialize the user preference. We also ask the user to come up with the first positive example via (a) naming a car that he/she already has in mind, or (b) reviewing a sample set precomputed for the given task based on the price constraints, body type, year, etc., and finding one that appears relevant. Two users chose option (a) while the others chose option (b). 3) Iterative manual exploration: Next, the user is asked to specify her interest precisely by (i) sending a SQL query Table <ref type="table">2</ref>: Results of the car database using Manual Exploration, DSM F , Active Learning (AL). "#attrs" shows the number of attributes used in the user interest and "# encoded features" shows the number of features (with one-hot encoding) after transforming all categorical attributes for use by the classifier. For manual exploration, "# tuples INIT" shows the number of tuples reviewed in initial exploration (the 2nd phase) for the user to find the first positive example; and "# tuples ITER" shows those reviewed in iterative exploration (3rd phase). Last three rows show the global Min, Max and Mdn respectively. For DSM F and AL, the algorithm marked by '-' never reached the desired accuracy within 100 iterations. The notation "SIM RES" over the last 6 columns indicates that these results are based on the simulation of user behaviors. to the database (for the 7 non-technical people, we offered help to translate their requirements to SQL); (ii) reviewing a subset of the returned tuples; (iii) going back to revise the query. The steps are repeated until the user is satisfied with all returned tuples of the final query. First, we verify that the selectivities of all 18 queries are in the range of [0.2%, 0.9%]. They contain 117 predicates in total: 70 of them are defined on numerical attributes and are all convex in the positive region. The rest 47 use categorical attributes, for which the notion of convexity does not apply. All the queries are conjunctive; the only disjunction is applied to the categorical attributes.</p><p>Second, we evaluate our system by running simulations using these queries as the true user interest. While the queries involve 4 to 10 attributes, the classifier requires categorical attributes to be transformed using one-hot encoding, resulting in 4 to 418 features. As for the initial examples for each experiment, the initial positive example is provided by the user at Step 2, and the initial negative example is randomly picked from the examples that are "far away" from the initial positive example. Table <ref type="table">2</ref> shows in the "DSM" column family that DSM F achieve 99% for all queries, with a median of 10 labeled examples, despite the high-dimensionality. In contrast, the users manually wrote a series of queries, with a median of 12, and reviewed many tuples, with a median of 98. The results from two user groups are largely consistent, with a small difference that non-technical people tend to specify simpler queries, hence easier for DSM to learn. Note that the initial cost of finding the first positive example, with a median of 10 tuples, can be added fairly to both exploration modes.</p><p>Third, the study verified our benefits over active learning (AL), which cannot reach 95% accuracy for most queries within 100 iterations, and for 80% accuracy requires a median of 29 labeled examples. Even though the decision boundary is often in a sparse region in this dataset, high dimensionality makes AL suffer in performance.</p></div>
<div><head n="6.4">Learning with Label Noise</head><p>We now evaluate our algorithm for handling label noise and demonstrate its advantages over alternative methods. The previous results showed that in the noise-free cases, (1) AL-SVM outperforms AL-KNN + , AL-KNN -and AS, (2) the alternative systems Aide and <software>LifeJoin</software> are greatly inferior to DSM F and they are often defeated by AL-SVM. Besides, both Aide and <software ContextAttributes="used">LifeJoin</software> assume noise-free conditions and cannot handle label noise. Thus, the following experiments use AL-SVM as the baseline and focus on the harder problem in which the decision boundary falls into dense regions.</p><p>Expt 5 (Evaluation of RDSM without factorization): Figure <ref type="figure">9</ref>(a) shows the averaged F-score comparison of RDSM, AL-SVM + auto (AL-SVM combined with auto-cleansing), and AL-SVM over all 2D queries (queries from SDSS query templates Q1-Q4) at iteration 200 in the form of a heat map. In the heat map, the x-axis labels correspond to varied ρ max from 5% to 50%, and the y-axis labels correspond to various algorithms. The number inside the cell (i, j) corresponds to the averaged F-score over all 2D queries when algorithm j is run with ρ max = i. For example, as the red cell in the upper-left corner shows, when ρ max = 5%, RDSM reaches up to 99% accuracy on average.</p><p>The main observations are: (1) RDSM outperforms the other two algorithms under every noise rate. (2) Given a fixed ρ max , there always exists a gap (2%-4%) between (3) As ρ max becomes larger (i.e., the noise rate increases), all algorithms reduce accuracy while RDSM reduces at the slowest pace. For example, with ρ max ranging from 5% to 30%, RDSM drops from 99% to 82%, AL-SVM + auto drops from 95% to 78% and AL-SVM drops from 95% to 63%. (4) AL-SVM + auto performs similarly to AL-SVM when ρ max is relatively small, but it achieves substantially higher accuracy than AL-SVM when ρ max becomes larger.</p><p>We now present some detailed results of different algorithms in terms of the F-score and time measurement. Since the results for different queries show similar trends, and Q3 (0.1%) is one of the most challenging 2D query patterns to learn, we present only Q3 (0.1%) in the interest of space. Figures <ref type="figure" target="#fig_1">10(a</ref>  <ref type="formula" target="#formula_34">3</ref>) It is worth noting that compared to 2D queries, the gap between RDSM F and the second-best algorithm becomes larger. On the one hand, even with factorization, the other algorithms fail to achieve high accuracy for high-dimensional queries. On the other hand, RDSM F takes advantage of the factorized data space model to dramatically shrink the uncertain region and capture the true decision boundary in high dimensional space rapidly. (4) For each algorithm, its accuracy decreases as ρ max increases. Particularly, RDSM F drops at the slowest rate, which empirically attests to the robustness of RDSM F .</p><p>(5) Compared to AL-SVM, AL-SVM F improves the accuracy dramatically. Except for 50% ρ max , AL-SVM F surpasses AL-SVM by around 40% accuracy, which shows the benefit of factorization. (6) Similar to previous observations, due to the usage of auto-cleansing, AL-SVM F + auto outperforms AL-SVM F when ρ max is high, but these two methods are similar for lower ρ max .</p><p>We next show some details in terms of the F1-score and time measurement. Since the experimental results have similar trends for all high-dimensional queries and the 4D query Q5 (0.01%) and the 6D query Q6 (0.01%) have the It achieves above 90% accuracy even when ρ max rises up to 30%, while the other algorithms have accuracy below 80% for ρ max = 30%. AL-SVM F + auto and AL-SVM F perform similarly for low ρ max , but for high ρ max , AL-SVM F drops faster than AL-SVM F + auto. As for AL-SVM its highest accuracy is below 80% for Q5(0.01%) with ρ max = 5%, while in other cases, its accuracy is close to 0. Regarding time measurement, like 2D queries, the time cost per iteration of each algorithm for high dimensional queries remains similar across different levels of label noise. Figure <ref type="figure" target="#fig_1">11(c</ref>) and 11(f) show that AL-SVM F + auto, AL-SVM F and AL-SVM have time cost per iteration below 0.5 second. The highest time cost per iteration of RDSM F is around 2.5 seconds for Q5 (0.01%) and around 3 seconds for Q6 (0.01%). In conclusion, for all the experiments, the time cost for RDSM F is within 3 seconds per iteration, which is reasonable for the requirement of interactive performance. As our work aims to reduce user labeling efforts, 200 labeled examples and within 3 seconds per iteration seem to be acceptable. The most time-intensive procedure is updating the three-set based data space model at each iteration. If more iterations are needed, instead of building the data space model on the entire dataset, we can further keep the iteration time within 3 seconds via subsampling methods.</p><p>Expt 7 (Learning car queries with synthetic noisy labels): In this experiment, we simulate the Boundary-Consistence Noise using the approaches described in Section 5.4, and conduct the evaluation for the 18 car queries obtained from our user study. Figure <ref type="figure" target="#fig_30">12</ref>(a) shows the averaged F-scores of RDSM F , AL-SVM F + auto, AL-SVM F , and AL-SVM over these queries. The main results include: (1) Consistent with the results of SDSS queries, RDSM F substantially outperforms other algorithms under every noise rate. In particular, RDSM F improves the performance from AL-SVM by 12%-69% (48% on average).</p><p>(2) RDSM F is the most robust one whose accuracy drops at the slowest pace as the noise rate rises. It achieves 85% when ρ max is up to 50%, while other algorithms, no matter the noise rate is low or high, cannot reach 85% in most cases. <ref type="bibr" target="#b2">(3)</ref> The considerable improvement from AL-SVM F + auto to RDSM F can be attributed to the factorized data space model. (4) For each noise rate, AL-SVM F + auto performs slightly better than AL-SVM F . ( <ref type="formula" target="#formula_36">5</ref>) For lower noise rates, AL-SVM achieves better performance than AL-SVM F while for higher noise rates, AL-SVM F significantly outperforms AL-SVM.</p><p>Among all car queries, Q5 (0.231%) has the highest dimensionality, including 418 attributes after one-hot encoding of the categorical attributes. In Figure <ref type="figure" target="#fig_30">12</ref>(b), we show the trends of the algorithms for the car query Q5 using an example ρ max = 40%. Consistently, we observe that RDSM F significantly outperforms other algorithms. AL-SVM F + auto and AL-SVM F have similar performances, and their accuracies lie in between RDSM F and AL-SVM.</p><p>Expt 8 (Learning car queries with real noisy labels): We next evaluate our algorithm using real noisy labels from our Car User Study. However, this study has some constraints: the user recorded only "positive examples" during initial exploration (Step 2) and at the end of manual exploration (Step 3) when the user verified the final results before terminating the exploration. So, we can treat the examples returned by the final query as the true positive examples. In contrast, during initial exploration, the user was not very clear about her true interest and might provide false positive examples. As such, this study provides only limited noisy labels in the initial exploration phase and in the form of false positives.</p><p>To generalize the noisy labels to all iterations in our experiment, we use label propagation to characterize the false positive region. More specifically, we obtain false positive examples from the initial exploration in the user trace, preprocess our dataset by propagating noisy labels to the k nearest neighbors of the false positives, and execute the data exploration on the polluted dataset to test the robustness of our model. In the process of label propagation, we flip the labels only for the negative examples in the neighborhood of the false positive examples under the assumption that the examples close to the false positives are more likely to be positive. We tune the value of k to control the label noise incurred in the data exploration to obtain a percentage error rate.</p><p>As shown in Figure <ref type="figure" target="#fig_30">12</ref>(c), RDSM F consistently outperforms the other algorithms by a large margin (14%-59% times higher), and AL-SVM F beats AL-SVM in most cases except in the 5% case. The scores in Expt 8 are not comparable to those in Expt 7 due to different distributions of label noise: In Expt 7, the label noise follows a Gaussian distribution. In Expt 8, to ensure that the k-nearest neighbors are actually hit as noisy labels in iterative data exploration, we impose a Bernoulli distribution on the label noise: if an example is in the k-nearest neighbor set, it is a noisy label; otherwise, it is not. The noise in Expt 8 often occurs close to the decision boundary, while the noise in Expt 7 may occur far from the boundary -hence the former has a more severe impact on performance than the latter. Overall, the results in Expt 8 demonstrate not only the robustness of our model to real noise, but also the capability of our model to address different types of noisy labels.</p></div>
<div><head n="7">Related Work</head><p>Data Exploration. Faceted search iteratively recommends query attributes for drilling down into the database, but the user is often asked to provide attribute values until the desired tuple(s) are returned <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b61">62]</ref> or offer an "interestingness" measure and its threshold <ref type="bibr" target="#b20">[21]</ref>. Semantic windows <ref type="bibr" target="#b43">[44]</ref> are pre-defined multidimensional predicates that a user can explore. These methods are different from our active learning approach. Recent work also supports time series data <ref type="bibr" target="#b53">[54]</ref> or avoids false discoveries of statistical patterns <ref type="bibr" target="#b83">[84]</ref> during interactive data exploration. Finding best database objects based on user preferences <ref type="bibr" target="#b69">[70]</ref> assumes a numeric weight per database attribute, which is different from the active learning approach to discover the user interest on the fly. The kBM-IGS framework proposed in <ref type="bibr" target="#b84">[85]</ref> handles interactive search for multiple labels using a constrained budget. While our work only considers single label cases, the extension for multiple labels will be a promising direction. By interactive data exploration, the DEXPLORER system in <ref type="bibr" target="#b57">[58]</ref> aims to discover and rank examples and the EDA4Sum system in <ref type="bibr" target="#b80">[81]</ref> detects highly uniform and diverse itemsets to provide data summaries, which are different from our scenario of finding desired examples. The ATENA system in <ref type="bibr" target="#b25">[26]</ref> auto-generates EDA notebooks to offer the user more insights before he/she actively explores the dataset, which is complementary to our work. Recent work <ref type="bibr" target="#b54">[55]</ref> proposes GUIDES, an innovative framework for guided Text-based Item Exploration (TIE), to explore structured and unstructured data in tandem and provide guidance in the dual space of attributes and text. However, GUIDES leverages an action-driven approach, example-based TIE operators are yet to be fully explored. A new query workload augmentation with labeling estimation framework DATA-FARM <ref type="bibr" target="#b77">[78]</ref> also adopts active learning for label forecasting, but it focuses on generating query workloads with labels, which is different from our goal. Beyond the requirements of regular data exploration, the authors of <ref type="bibr" target="#b6">[7]</ref> address the joint exploration of items, people, and people's opinions on items in the Subjective Data Exploration (SDE) framework. Instead of finding interesting data sets, the approach in <ref type="bibr" target="#b64">[65]</ref> aims to find the target users through guided exploration.</p><p>Query by Example is a specific framework for data exploration. Earlier work on QBE focused on a visualization front-end that aims to minimize the user effort to learn the SQL syntax <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b55">56]</ref>. Recent work <ref type="bibr" target="#b52">[53]</ref> proposes exemplar queries which treat a query as a sample from the desired result set and retrieve other tuples based on similarity metrics, but for graph data only. The work <ref type="bibr" target="#b66">[67]</ref> considers data warehouses with complex schemas and learns the minimal project-join queries from a few example tuples efficiently. It does not consider selection with complex predicates, which is a focus of our work. The work <ref type="bibr" target="#b42">[43]</ref> helps users construct join queries for exploring relational databases, and <ref type="bibr" target="#b46">[47]</ref> does so by asking the user to determine whether a given output table is the result of her intended query on a given database. INODE <ref type="bibr" target="#b5">[6]</ref> as an end-to-end data exploration system also provides explore-by-example approaches for finding interesting data patterns across multiple semantics, but it adopts existing approaches rather than developing new methods. Query Formulation has been surveyed in <ref type="bibr" target="#b17">[18]</ref>. The closest to our work is <software ContextAttributes="used">LifeJoin</software> <ref type="bibr" target="#b18">[19]</ref>, which we compared in Section 6.2. Query By Output (QBO) <ref type="bibr" target="#b73">[74]</ref> takes the output of one query on a database, and constructs another query such that running these two queries on the database are instanceequivalent. Dataplay <ref type="bibr" target="#b1">[2]</ref> provides a GUI for users to directly construct and manipulate query trees. It assumes that the user can specify the value assignments used in his intended query, and learns conjunctions of quantified Horn expressions (with if-then semantics) over nested relations <ref type="bibr" target="#b0">[1]</ref>. Active Learning. Tong and Koller <ref type="bibr" target="#b72">[73]</ref> provide a theoretical motivation on selecting new examples using the notion of a version space, but with unknown convergence speed. Related to our work is a lower bound on the probability of misclassification error on the unlabeled training set <ref type="bibr" target="#b15">[16]</ref>. However, it relies on user labeling of an additional sample from the unlabeled pool, which is not required in our work. Recent papers <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b34">[35]</ref><ref type="bibr" target="#b35">[36]</ref><ref type="bibr" target="#b36">[37]</ref> offer probabilistic bounds for the classification error and sample complexity. Our work differs in that we focus on F1-score, which suits selective user interest queries (imbalanced classes in classification).</p><p>Most learning theory makes no assumptions on convex data/class distributions <ref type="bibr" target="#b37">[38]</ref>. Clustering techniques can assume that data is clustered in convex sets <ref type="bibr" target="#b37">[38]</ref>, but address a different problem from ours. In Active Learning, convexity assumptions occur in the Version Space <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b72">73]</ref>, which is the set of classifiers consistent with training data. DSM can embrace any classifier developed through the version space, but also includes the new polytope model. Active Search <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b74">75]</ref> aims to maximize the number of positive examples discovered, called the Target Set Accuracy, within a limited budget of user labeling effort. In comparison, our work aims to maximize the F-score of the model learned to approximate the true user interest. We reported the performance difference from <ref type="bibr" target="#b32">[33]</ref> in the previous section. The works <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b74">75]</ref> use a kernel function to measure similarity of items in order to maximize the utility of a set of selected items. Such kernel methods have the smoothness requirement, i.e., similar items have similar utility values, and require training data to tune the kernel for each use case (user interest), which do not suit our problem setting. Crowdsourcing-based Active Learning with Label Noise. Many existing methods in the active learning literature address the label noise problem based on crowdsourcing techniques <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b81">82]</ref>. However, these techniques depend on adding redundancy by collecting labels for each example from multiple users and aggregating the labels using some methods such as majority voting. In this context, these crowdsourcing techniques usually result in an additional labeling cost and can not support the scenarios with a single user. While our work assumes that only one user is involved, if combined with some crowdsourcing techniques to aggregate labels from the users, our proposed algorithms have the potential to support multiple users. Active Learning with Relabeling Noisy Labels. In the active learning literature, a popular approach to deal with label noise is to relabel suspiciously mislabeled examples. Recent works <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b82">83]</ref> treat relabeling as an individual process triggered at each iteration or under some conditions. Other works <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b47">48]</ref> integrate relabeling into the sample acquisition process, which generalizes conventional sampling strategies with a tradeoff between assigning labels for more unlabeled examples and relabeling potentially noisy examples in the existing labeled data set to improve the label quality. Instead of relabeling, our work takes advantage of noise removal methods. On the one hand, label noise cleansing methods have the following advantages: (1) removed noisy examples have no effect on modeling <ref type="bibr" target="#b31">[32]</ref>, (2) it's observed that retaining mislabeled examples may be more harmful than removing some correctly labeled examples <ref type="bibr" target="#b14">[15]</ref>, and (3) in general, removing noisy examples is more effective than relabeling them and a hybrid approach which combines removal and relabeling <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b51">52]</ref>. On the other hand, relabeling may be too costly since humans may make similar mistakes repeatedly <ref type="bibr" target="#b70">[71]</ref>, and there is no guarantee that the relabeled examples are noise-free. Recent work <ref type="bibr" target="#b70">[71]</ref> proposed a Bidirectional Active Learning with human Training (BALT) model to improve the expertise of labelers and relabeling quality simultaneously. However, the BALT model relies on some gold instances (previously labeled and noisefree) which are not available in our setting. Data Programming. When ground truth labels are not available, recent studies <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b75">76]</ref> have employed the data programming paradigm to build training data for developing high-quality models. Furthermore, Snorkel <ref type="bibr" target="#b58">[59]</ref>, a first end-to-end system based on data programming, is designed for training machine learning models with weak supervision. Snorkel asks the users to write labeling functions (LFs) that express various weak supervision sources (e.g., domain expertise) and learns an accurate model through weak supervision on LFs. However, the typical LFs development cycles include multiple iterations of ideation, refining, evaluation, and debugging, which can be very time-consuming and expensive. In addition, it may be unrealistic to request a reasonable set of LFs from non-expert users. Even though recent work <ref type="bibr" target="#b76">[77]</ref> proposed an approach to generate LFs using an initially labeled dataset automatically, the size of the labeled datasets needed reaches a magnitude of hundreds. Data Discovery. Recent work considered correlated dataset search by capturing the relationships between datasets. Aurum <ref type="bibr" target="#b29">[30]</ref> leverages an enterprise knowledge graph (EKG) to retrieve relevant datasets for a user-provided discovery query. The recent work <ref type="bibr" target="#b62">[63]</ref> proposes a QCR index-based approach to efficiently support correlated table search for generalized join-correlation queries. COCOA <ref type="bibr" target="#b28">[29]</ref> accelerates the calculation of correlation coefficients to select the most correlated features for user-defined machine learning tasks efficiently. The goal of our work is different: we aim to find examples of user interest in a dataset, rather than identify the relationships between datasets/tables and obtain the most correlated ones. However, these methods can be used to discover correlated features in our future work.</p></div>
<div><head n="8">and Future Work</head><p>We presented new algorithms for interactive data exploration in the active learning framework, which leverage the subspatial convex and conjunctive properties of database queries to overcome the slow convergence problem and the label noise problem. Our main results include the following: <ref type="bibr" target="#b0">(1)</ref> In the absence of label noise, our algorithm significantly outperforms active learning <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b27">28]</ref>, active search <ref type="bibr" target="#b32">[33]</ref>, and existing explore-by-example systems including Aide <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24]</ref> and <software ContextAttributes="used">LifeJoin</software> <ref type="bibr" target="#b18">[19]</ref>, in accuracy and convergence speed. (2) For bounded instance-and labeldependent label noise, our algorithm substantially improves accuracy and convergence speed, while other alternative algorithms fail to handle noisy labels. (3) Our algorithm maintains the per-iteration time within 1-3 seconds.</p><p>In future work, we plan to extend query patterns to multiple disjoint areas using exploration versus exploitation. Toward this goal, a possible approach is to explore the entire dataset to identify possible subregions, partition the data space based on the detected subregions such that each partition contains only one subregion, and then apply our algorithm in each partition respectively. It is also interesting to tackle noisy labels intrinsically by building probabilistic convex hulls. Another direction for future work is to generalize our techniques from database exploration, where subspatial convexity and conjunctivity properties are likely to hold, to more general active learning problems, e.g., for text/image classification, and understand their applicability and potential benefits in those settings.</p></div><figure xml:id="fig_1"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: System architecture for explore by example.</figDesc></figure>
<figure xml:id="fig_2"><head>Fig. 2 :</head><label>2</label><figDesc>Fig.2: Positive region (green) and negative region (red) of 6 example predicates from SDSS queries, where one of the two regions is convex decision boundaries of the SVM that lead to correct classification of the training examples. Uncertainty sampling is known to be an approximation of an optimal algorithm that bisects the version space with each selected example<ref type="bibr" target="#b72">[73]</ref>. For the above reason, we call the learning algorithm in uncertainty sampling version space-based as they are designed to reduce the version space, and we will further augment it with a new data space model.</figDesc></figure>
<figure xml:id="fig_4"><head>Fig. 3 :</head><label>3</label><figDesc>Fig. 3: Positive and negative regions in the polytope model.</figDesc></figure>
<figure xml:id="fig_5"><head /><label /><figDesc>any , where Φ is the cumulative distribution function of the standard Normal distribution.</figDesc></figure>
<figure xml:id="fig_6"><head>i</head><label /><figDesc>and R - i , are built according to Definition 3.1 and 3.2, but Enhui Huang † et al.(a) A query in the DSC family is not convex in either the positive region (green) or the negative region (red).</figDesc></figure>
<figure xml:id="fig_8"><head /><label /><figDesc>x 2 R e , xI 2 ⇡I (R e ). Now define the convex cone construction in the subspace spanned by AI with e I as apex as R e I = {xI 2 R d I |xI e I \ R + I = ; ^ ! xI e I \ R + I 6 = ;}. We will now show that ⇡I (R e ) ✓ R e I . We have ⇡I (R e ) = {xI 2 R d I |xI e I \⇡I (R + ) = ;^ ! xI e I \⇡I (R + ) 6 = ;}. Since xI 2 ⇡I (R e ) and ⇡I (R e ) is a negative convex cone, xI e I \ R + I = ; is true. On the other hand, since ⇡I (R + ) ✓ R + I , ! xI e I \ ⇡I (R + ) 6 = ; implies ! xI e I \ R + I 6 = ;. This shows ⇡I (R e ) ✓ R e I . Therefore xI 2 ⇡I (R e ) ✓ R e I ✓ R I , and x 2 R f . Finally, R e ✓ R f for any e , and hence R ✓ R f . Proposition 4.3 Factorized DSM improves the Three-Set Metric, the lower bound of the model accuracy in F1-score.</figDesc></figure>
<figure xml:id="fig_9"><head>Proposition 4 . 1</head><label>41</label><figDesc>All points in the positive region R + f are positive and R + ✓ R + f at each iteration of the data exploration process.</figDesc></figure>
<figure xml:id="fig_10"><head /><label /><figDesc>x 2 R e , xI 2 ⇡I (R e ). Now define the convex cone construction in the subspace spanned by AI with e I as apex as R e I = {xI 2 R dI |xI e I \ R + I = ; ^ ! xI e I \ R + I 6 = ;}. We will now show that ⇡I (R e ) ✓ R e I . We have ⇡I (R e ) = {xI 2 R dI |xI e I \⇡I (R + ) = ;^ ! xI e I \⇡I (R + ) 6 = Since xI 2 ⇡I (R e ) and ⇡I (R e ) is a negative convex cone, xI e I \ R + I = ; is true. On the other hand, since ⇡I (R + ) ✓ R + I , ! xI e I \ ⇡I (R + ) 6 = ; implies ! xI e I \ R + I 6 = ;. This shows ⇡I (R e ) ✓ R e I . Therefore xI 2 ⇡I (R e ) ✓ R e I ✓ R I , and x 2 R f . Finally, R e ✓ R f for any e , and hence R ✓ R f . Proposition 4.3 Factorized DSM improves the Three-Set Metric, the lower bound of the model accuracy in F1-score.</figDesc></figure>
<figure xml:id="fig_11"><head>Proposition 4 . 1</head><label>41</label><figDesc>All points in the positive region R + f are positive and R + ✓ R + f at each iteration of the data exploration process.6 [As mentioned before, each QI can also be convex in the negative region. However, in the interest space our discussion focuses on the case of convex positive regions.] e negative for Q, there exists I such that its factorization v is negative for QI . Let ⇡I (R e ) be the projection of the c onto the subspace spanned by AI . Then for a point x xI 2 ⇡I (R e ). Now define the convex cone construction in the subspac by AI with e I as apex as R e I = {xI 2 R dI |xI e I \ R + I = ; ^ ! xI e I \ R + I We will now show that ⇡I (R e ) ✓ R e I . We have ⇡I (R e ) = {xI 2 R dI |xI e I \⇡I (R + ) = ;^ ! xI e I \⇡I Since xI 2 ⇡I (R e ) and ⇡I (R e ) is a negative conv xI e I \ R + I = ; is true. On the other hand, since ⇡I (R + ! xI e I \ ⇡I (R + ) 6 = ; implies ! xI e I \ R + I 6 = ;. Th ⇡I (R e ) ✓ R e I . Therefore xI 2 ⇡I (R e ) ✓ R e I ✓ R I , and x 2 R f R e ✓ R f for any e , and hence R ✓ R f . Proposition 4.3 Factorized DSM improves the Three-Se the lower bound of the model accuracy in F1-score. x-y z (b) A positive region built from 4 positive points, {A, B, C, D} A D C B ositive and negative regions in factorized space. Formally, orization concerns a user interest query Q that involve d attes and can be written in the conjunctive form, Q1 ^• • • ^Qm, re each QI is convex in the positive region 6 and contains an bute set AI = {AI1, • • • , AId I }, with AIj representing the attribute in the I th partition. A family of attribute sets, A = , • • • , Am), is pairwise disjoint. The total dimension is d =</figDesc></figure>
<figure xml:id="fig_12"><head /><label /><figDesc>denote the entire dataset as D and the evaluation set as al. The definitions of positive, negative, and uncertain partis of Deval are the same as in Definition 3.4 with the three res replaced by R +f , R f , and R u f . ormal results on factorization. When the true query is contive, taking advantage of the factorization property of the data e proves to be effective for enlarging the positive and negaregions known to the DSM algorithm and hence reducing the rtain region. Formally, we have the following results.</figDesc></figure>
<figure xml:id="fig_13"><head>position 4 . 1</head><label>41</label><figDesc>All points in the positive region R + f are positive R + ✓ R + f at each iteration of the data exploration process.</figDesc></figure>
<figure xml:id="fig_14"><head /><label /><figDesc>x 2 R e , xI 2 ⇡I (R e ). Now define the convex cone construction in the subspace spanned by AI with e I as apex as R e I = {xI 2 R d I |xI e I \ R + I = ; ^ ! xI e I \ R + I 6 = ;}. We will now show that ⇡I (R e ) ✓ R e I . We have ⇡I (R e ) = {xI 2 R d I |xI e I \⇡I (R + ) = ;^ ! xI e I \⇡I (R + ) 6 = ;}. Since xI 2 ⇡I (R e ) and ⇡I (R e ) is a negative convex cone, xI e I \ R + I = ; is true. On the other hand, since ⇡I (R + ) ✓ R + I , ! xI e I \ ⇡I (R + ) 6 = ; implies ! xI e I \ R + I 6 = ;. This shows ⇡I (R e ) ✓ R e I . Therefore xI 2 ⇡I (R e ) ✓ R e I ✓ R I , and x 2 R f . Finally, R e ✓ R f for any e , and hence R ✓ R f . Proposition 4.3 Factorized DSM improves the Three-Set Metric, the lower bound of the model accuracy in F1-score.</figDesc></figure>
<figure xml:id="fig_15"><head>Proposition 4 . 1</head><label>41</label><figDesc>All points in the positive region R + f are positive and R + ✓ R + f at each iteration of the data exploration process. polytope is the s R + ✓ R + f . Proposition 4.2 and R ✓ R f a PROOF. If a p some subspace s all points in R I the target query To prove R convex cone tha be the apex of a negative for Q, is negative for Q onto the subspa xI 2 ⇡I (R e ). Now define th by AI with e I a R e I = {xI We will now sho We have ⇡I (R e ) = {xI Since xI 2 ⇡I xI e I \ R + I = ; ! xI e I \ ⇡I (R + ⇡I (R e ) ✓ R e I Therefore xI R e ✓ R f for Proposition 4.3 the lower bound</figDesc></figure>
<figure xml:id="fig_16"><head>Proposition 4 . 1</head><label>41</label><figDesc>All points in the positive region R + f are positive and R + ✓ R + f at each iteration of the data exploration process.</figDesc></figure>
<figure xml:id="fig_17"><head>Proposition 4 . 1 Figure 5 :</head><label>415</label><figDesc>Figure 5: Illustration of positive and negative regions in factorized space.</figDesc></figure>
<figure xml:id="fig_18"><head>Proposition 4 . 1 Proposition 4 . 2 Fig. 4 :</head><label>41424</label><figDesc>Fig. 4: Illustration of factorization when 3D space (x-y-z) is factorized into two subspaces (x-y) and (z).</figDesc><graphic coords="11,37.71,94.43,141.71,102.04" type="bitmap" /></figure>
<figure xml:id="fig_19"><head>Proposition 3</head><label>3</label><figDesc>All points in the positive region R + f are positive, and R + ⊆ R + f in each iteration of data exploration. Proposition 4 All points in the negative region R - f are negative, and R -⊆ R - f in each iteration of data exploration. Proposition 5 DSM F improves the Three-Set Metric, the lower bound of the model accuracy in F1-score, over DSM.</figDesc></figure>
<figure xml:id="fig_20"><head>Fig. 5 :</head><label>5</label><figDesc>Fig. 5: Illustration of data space model refinement and g 1 are likely to be noisy examples, so we remove both o and g 1 to avoid the over-expansion of the convex hull. We refer to the process of constructing the convex hull based on current D ch and examine every vertex of the convex hull to remove noise as one round of noise filtering.</figDesc><graphic coords="14,57.10,94.43,127.56,99.21" type="bitmap" /></figure>
<figure xml:id="fig_21"><head>Figure 5 (</head><label>5</label><figDesc>Figure 5(b) shows the initial convex hull (enclosed by red dashed lines) built on the confident examples distilled by auto-cleansing. Here, the true query region is marked by the dashed black rectangle, the red points denote distilled positive examples, and the blue points denote distilled negative examples. Figure 5(c) shows the intermediate result of the convex hull after one round of noise filtering, and Figure 5(d) shows that after two-rounds of noise filtering. It is evident that in Figure 5(b), six negative examples (six red points outside the dashed black rectangle) are erroneously labeled, which leads to the over-expansion of the convex hull. Three out of them are the vertices of the initial convex hull, and have negative examples among their 3-nearest neighbors. By removing these mislabeled positive examples and some other positive examples from their neighbors, an intermediate convex hull is derived as shown in Figure 5(c). Since there still exist negative examples inside the convex hull, we perform another round of noise filtering and eventually obtain a conservative and much more accurate convex hull, as shown in Figure 5(d).</figDesc></figure>
<figure xml:id="fig_22"><head>Fig. 6 :</head><label>6</label><figDesc>Fig. 6: RDSM update 5.2.2 Robust Dual-Space Model for Data Exploration We now propose a Robust Dual-Space Model (RDSM) by integrating into DSM (i) the existing data cleansing method, auto-cleansing [17], (ii) our own data cleansing method, data space model refinement, and (iii) a new adaptive autolabeling method in the sample acquisition procedure. Like DSM, RDSM fulfills two functionalities, prediction and sampling, as an active learner for data exploration. But there are two major differences, in both model update and sample acquisition methods: At each iteration, we update DSM based on the labeled examples directly, and always allow DSM for auto-labeling in the sample acquisition process (lines 22-25 in Algorithm 1). In contrast, to update RDSM, as shown in Figure 6, we first use auto-cleansing to collect confident examples from labeled examples and then based on the confident examples, we 1) update the classifier, 2)build the data space model with refinement, and 3) compute estimated noise rates to control auto-labeling, which takes effect later in the sampling acquisition process.We now present the full algorithm (Algorithm 2) of applying RDSM into active learning-based interactive data exploration and highlight the lines (marked in blue) that differ from Algorithm 1. Since the framework remains similar, we only discuss the two major differences.Update of RDSM. Lines 7-15 update the RDSM. More specifically, we collect clean labeled examples D clean distilled by auto-cleansing (line 7) and then use D clean to train both a data space model with refinement (line 10) and a classifier (line 8). Lines 11-14 update the three-set partitions (D + , D -, D u ): the partitions must be rebuilt if there exist user-labeled examples (D lu ) in the newly labeled ex-</figDesc><graphic coords="14,315.96,241.11,198.42,90.70" type="bitmap" /></figure>
<figure xml:id="fig_23"><head /><label /><figDesc>Lines 7-15 update the RDSM. More specifically, we collect clean labeled examples D clean distilled by auto-cleansing (line 7) and then use D clean to train both a data space model with refinement (line 10) and a classifier (line 8). Lines 11-14 update the three-set partitions (D + , D -, D u ): the partitions must be rebuilt if there exist user-labeled examples (D lu ) in the newly labeled ex-Algorithm 2 Robust Dual-Space Model Input: database D, initial labeled data set D 0 , accuracy threshold λ, sampling ratio γ, unlabeled pool size m 1: D labeled ← D 0 , D unlabeled ← D \ D 0 2: R + ← ∅, R -← ∅ 3: D + ← ∅, D -← ∅, D u ← D 4: D ← (D + , D -, D u ) 5: D lu ← D 0 , D ld ← ∅ 6: repeat // training the Robust Dual-Space Model: 7: D clean ← distill_by_auto(D labeled ) 8: classif ier ← train_classifier(D clean ) 9: auto_labeling ← compare_rates(D labeled , D clean ) 10: (R + , R -) ← train_data_space_model(D clean ) 11: if D lu = ∅ and D ld = ∅ then 12: D ← threeSets(R + , R -, D) 13: else 14: D ← rebuilding_threeSets(R + , R -, D) 15:</figDesc></figure>
<figure xml:id="fig_24"><head>and auto_labeling then 26 :</head><label>26</label><figDesc>D ld ← D ld ∪ {get_labels_from_rdsm(x)} 27: else 28: D lu ← D lu ∪ {get_labels_from_user(x)} // updating the labeled and unlabeled data sets: 29: D labeled ← D labeled ∪ D lu ∪ D ld 30: D unlabeled ← D unlabeled \ x * 31: until accu ≥ λ or reachedMaxNum() 32: finalRetrieval(D, (R + , R -), classif ier)</figDesc></figure>
<figure xml:id="fig_25"><head>Figure 7 Fig. 7 :Fig. 8 :</head><label>778</label><figDesc>Fig. 7: DSM with factorization for SDSS 2D, 4D, and 6D queries, compared to AL and AS algorithms</figDesc></figure>
<figure xml:id="fig_26"><head /><label /><figDesc>(e)-7(h) are: (1) Without factorization, DSM performs similarly to AL as the polytope model is dominated by the uncertain region. DSM F dramatically shrinks the uncertain region, and improves the lower bound and F1-score. (2) Consistently, DSM F outperforms AL, which further outperforms AS.</figDesc></figure>
<figure xml:id="fig_27"><head>Figure 7 (</head><label>7</label><figDesc>g) shows that for the 6D query, DSM F reaches 96% with 80 examples, AL-SVM cannot reach 20% with 200 examples, and AS is close to 0%. (3) DSM F has the time per iteration within 1-2 seconds, as shown in Figure 7(h) for the 6D query.</figDesc></figure>
<figure xml:id="fig_28"><head>Fig. 9 :Fig. 10 :</head><label>910</label><figDesc>Fig. 9: Results for SDSS queries in the presence of label noise</figDesc><graphic coords="21,48.09,243.22,155.91,99.21" type="bitmap" /></figure>
<figure xml:id="fig_29"><head /><label /><figDesc>) -10(b) show that RDSM surpasses the other algorithms in accuracy almost at every iteration. With respect to time measurement, the time cost of each algorithm remains similar under different levels of label noise. As shown in Figure 10(c), both AL-SVM + auto and AL-SVM have time cost per iteration below 0.5 second. The time cost per iteration of RDSM is within 2 seconds, which is reasonable for the requirement of interactive performance. Expt 6 (Evaluation of RDSM with factorization (RDSM F )): The heat map in Figure 9(b) shows the averaged F-score comparison of RDSM F , AL-SVM, AL-SVM F (AL with a factorized SVM), and AL-SVM F + auto (AL-SVM F combined with auto-cleansing) for 4D-6D queries at iteration 200. The heat map is drawn in the same format as that in Figure 9(a). The main results are: (1) Consistently, RDSM F outperforms the other algorithms under every noise rate. (2) Thanks to the factorized data space model, RDSM F always outperforms the second-best algorithm AL-SVM F + auto by a wide margin (13%-40%). (</figDesc></figure>
<figure xml:id="fig_30"><head>Fig. 12 :</head><label>12</label><figDesc>Fig. 11: RDSM F for SDSS 4D and 6D queries, compared to AL-SVM F + auto, AL-SVM F and AL-SVM</figDesc><graphic coords="22,48.09,211.99,155.91,99.21" type="bitmap" /></figure>
<figure type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Query templates with different selectivity values</figDesc><table><row><cell>Query template</cell></row><row><cell>Q1</cell></row></table></figure>
			<note place="foot" n="1" xml:id="foot_0"><p>In our work we use the term, user interest query, to refer to the final query that represents the user interest, and user interest model to refer to an immediate model before it converges to the true user interest.</p></note>
			<note place="foot" n="2" xml:id="foot_1"><p>This work considers a dataset that consists of a single table. It is left to our future work to study the extension for join queries.</p></note>
			<note place="foot" n="3" xml:id="foot_2"><p>Feature selection to filter irrelevant attributes is addressed in our previous paper<ref type="bibr" target="#b39">[40]</ref>. Due to space constraints, in this paper we assume that feature selection is performed the same as<ref type="bibr" target="#b39">[40]</ref> and focus on other data exploration issues.</p></note>
			<note place="foot" n="6" xml:id="foot_3"><p>[As mentioned before, each QI can also be convex in the negative region. However, in the interest space our discussion focuses on the case of convex positive regions.]</p></note>
			<note place="foot" n="5" xml:id="foot_4"><p>https://scikit-learn.org/stable/</p></note>
			<note place="foot" n="6" xml:id="foot_5"><p>The content is extracted from http://www.teoalida.com/</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning and verifying quantified boolean queries by example</title>
		<author>
			<persName><forename type="first">A</forename><surname>Abouzied</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Angluin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PODS</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="49" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Playful query specification with dataplay</title>
		<author>
			<persName><forename type="first">A</forename><surname>Abouzied</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Hellerstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Silberschatz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PVLDB</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1938" to="1941" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Greedy search for active learning of OCR</title>
		<author>
			<persName><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chaudhury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDAR</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="837" to="841" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">On the surprising behavior of distance metrics in high dimensional spaces</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hinneburg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Keim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDT</title>
		<meeting><address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="420" to="434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Applying support vector machines to imbalanced datasets</title>
		<author>
			<persName><forename type="first">R</forename><surname>Akbani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kwek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Japkowicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECML</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="39" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">INODE: building an end-to-end data exploration system in practice</title>
		<author>
			<persName><forename type="first">S</forename><surname>Amer-Yahia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGMOD Rec</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="23" to="29" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Exploring ratings in subjective databases</title>
		<author>
			<persName><forename type="first">S</forename><surname>Amer-Yahia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Milo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Youngmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="62" to="75" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning the structure of generative models without labeled data</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ratner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ré</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="273" to="282" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The quickhull algorithm for convex hulls</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">B</forename><surname>Barber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Dobkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Huhdanpaa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TOMS</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="469" to="483" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Active sampling for entity matching with guarantees</title>
		<author>
			<persName><forename type="first">K</forename><surname>Bellare</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Iyengar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Parameswaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Rastogi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Knowledge Discovery from Data</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2013-09">Sept. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Berthon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.03772</idno>
		<title level="m">Confidence scores make instance-dependent label-noise learning possible</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Combining labeled and unlabeled data with co-training</title>
		<author>
			<persName><forename type="first">A</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLT</title>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="92" to="100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Fast kernel classifiers with online and active learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ertekin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1579" to="1619" />
			<date type="published" when="2005-12">Dec. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Agreeing to disagree: active learning with noisy labels without crowdsourcing</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bouguelia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nowaczyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">C</forename><surname>Santosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Verikas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJMLC</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1307" to="1319" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Identifying mislabeled training data</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Brodley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Friedl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Artif. Intell. Res</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="131" to="167" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Query learning with large margin classifiers</title>
		<author>
			<persName><forename type="first">C</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Cristianini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="111" to="118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning with bounded instance and labeldependent label noise</title>
		<author>
			<persName><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1789" to="1799" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Computer-assisted query formulation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Solar-Lezama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends R in Programming Languages</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="94" />
			<date type="published" when="2016-06">June 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Using program synthesis for social recommendations</title>
		<author>
			<persName><forename type="first">A</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Solar-Lezama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1732" to="1736" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Automatic labeling inconsistencies detection and correction for sentence unit segmentation in conversational speech</title>
		<author>
			<persName><forename type="first">S</forename><surname>Cuendet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hakkani-Tür</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int'l Workshop on MLMI</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="144" to="155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Dynamic faceted search for discovery-driven analysis</title>
		<author>
			<persName><forename type="first">D</forename><surname>Dash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Megiddo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="3" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">AIDE: an automatic user navigation system for interactive data exploration</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Diao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PVLDB</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1964" to="1967" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Explore-byexample: an automatic query steering framework for interactive data exploration</title>
		<author>
			<persName><forename type="first">K</forename><surname>Dimitriadou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Papaemmanouil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Diao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="517" to="528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Aide: an active learning-based approach for interactive data exploration</title>
		<author>
			<persName><forename type="first">K</forename><surname>Dimitriadou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Papaemmanouil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Diao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TKDE</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2842" to="2856" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Modelling class noise with symmetric and asymmetric distributions</title>
		<author>
			<persName><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2589" to="2595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Automatically generating data exploration sessions using deep reinforcement learning</title>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">B</forename><surname>El</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Milo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Somech</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIG-MOD</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1527" to="1537" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Active learning via perfect selective classification</title>
		<author>
			<persName><forename type="first">R</forename><surname>El-Yaniv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wiener</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="255" to="279" />
			<date type="published" when="2012-02">Feb. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning on the border: Active learning in imbalanced data classification</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ertekin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="127" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">COCOA: correlation coefficient-aware data augmentation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Esmailoghli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Quiané-Ruiz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EDBT</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="331" to="336" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Aurum: A data discovery system</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Fernandez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Abedjan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDE</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1001" to="1012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Classification in the presence of label noise: A survey</title>
		<author>
			<persName><forename type="first">B</forename><surname>Frénay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Verleysen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="845" to="869" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Noise elimination in inductive concept learning: A case study in medical diagnosis</title>
		<author>
			<persName><forename type="first">D</forename><surname>Gamberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Lavrač</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Džeroski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on ALT</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="199" to="212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Bayesian optimal active search and surveying</title>
		<author>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="843" to="850" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Convex polytopes</title>
		<author>
			<persName><forename type="first">B</forename><surname>Grünbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Convex Polytopes</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
	<note>2 edition</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Rates of convergence in active learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hanneke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="333" to="361" />
			<date type="published" when="2011-02">02 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Theory of disagreement-based active learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hanneke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends in Machine Learning</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="131" to="309" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Refined error bounds for several learning algorithms</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hanneke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="4667" to="4721" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">The Elements of Statistical Learning</title>
		<author>
			<persName><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Friedman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Active Learning Methods for Interactive Exploration on Large Databases</title>
		<author>
			<persName><forename type="first">E</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
		<respStmt>
			<orgName>Institut Polytechnique de Paris</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Theses</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Optimization for active learning-based interactive database exploration</title>
		<author>
			<persName><forename type="first">E</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PVLDB</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="71" to="84" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Repeated labeling using multiple noisy labelers</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">G</forename><surname>Ipeirotis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Min. Knowl. Discov</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="402" to="441" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A Generalized Query-by-Example Data Manipulation Language Based on Database Logic</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">E</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Walczak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Software Engineering</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="40" to="57" />
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Interactive browsing and navigation in relational databases</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kahng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PVLDB</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1017" to="1028" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Interactive data exploration using semantic windows</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kalinin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Cetintemel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zdonik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="505" to="516" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Distributed and Interactive Cube Exploration</title>
		<author>
			<persName><forename type="first">N</forename><surname>Kamat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Jayachandran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Tunga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nandi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDE</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="472" to="483" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Convex Sets and Their Applications</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Lay</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
			<publisher>Dover Publications</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Query from examples: An iterative, data-driven approach to query construction</title>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-Y</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Maier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PVLDB</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page" from="2158" to="2169" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Re-active learning: Active learning with relabeling</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Mausam</surname></persName>
		</author>
		<author>
			<persName><surname>Weld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1845" to="1852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">An analysis of query-agnostic sampling for interactive data exploration</title>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Diao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications in Statistics-Theory and Methods</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">16</biblScope>
			<biblScope unit="page" from="3820" to="3837" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Σ-optimality for active learning on gaussian random fields</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Schneider</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="2751" to="2759" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Learning from binary labels with instancedependent noise</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Menon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">107</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1561" to="1595" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Use of classification algorithms in noise detection and elimination</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L B</forename><surname>Miranda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HAIS</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="417" to="424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Exemplar queries: Give me an example of what you need</title>
		<author>
			<persName><forename type="first">D</forename><surname>Mottin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PVLDB</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="365" to="376" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Interactive time series analytics powered by ONEX</title>
		<author>
			<persName><forename type="first">R</forename><surname>Neamtu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1595" to="1598" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Guided text-based item exploration</title>
		<author>
			<persName><forename type="first">B</forename><surname>Omidvar-Tehrani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Personnaz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="3410" to="3420" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Example-Based Graphical Database Query Languages</title>
		<author>
			<persName><forename type="first">G</forename><surname>Özsoyoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="25" to="38" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">New Algorithms and Optimizations for Human-inthe-Loop Model Development</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">D</forename><surname>Palma</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<pubPlace>France</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Polytechnic Institute of Paris</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Interactively discovering and ranking desired tuples by data exploration</title>
		<author>
			<persName><forename type="first">X</forename><surname>Qin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">VLDB J</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="753" to="777" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Snorkel: Rapid training data creation with weak supervision</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ratner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. VLDB Endow</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="269" to="282" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Data programming: Creating large training sets, quickly</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Ratner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3567" to="3575" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Minimum-effort driven dynamic faceted search in structured databases</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">B</forename><surname>Roy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="13" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Dynacet: Building dynamic faceted search systems over databases</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">B</forename><surname>Roy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDE</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1463" to="1466" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">A sketch-based index for correlated dataset search</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S R</forename><surname>Santos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDE</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="2928" to="2941" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Classification with asymmetric label noise: Consistency and maximal denoising</title>
		<author>
			<persName><forename type="first">C</forename><surname>Scott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">COLT</title>
		<imprint>
			<biblScope unit="page" from="489" to="511" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Guided exploration of user groups</title>
		<author>
			<persName><forename type="first">M</forename><surname>Seleznova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PVLDB</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1469" to="1482" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Active Learning</title>
		<author>
			<persName><forename type="first">B</forename><surname>Settles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Synthesis Lectures on Artificial Intelligence &amp; Machine Learning</title>
		<imprint>
			<publisher>Morgan Claypool Publishers</publisher>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Discovering queries based on example tuples</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="493" to="504" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<ptr target="http://skyserver.sdss.org/dr8/en/help/docs/realquery.asp" />
		<title level="m">Sloan digital sky survey: DR8 sample SQL queries</title>
		<imprint />
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Designing and mining multi-terabyte astronomy archives: The Sloan digital sky survey</title>
		<author>
			<persName><forename type="first">A</forename><surname>Szalay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGMOD</title>
		<imprint>
			<biblScope unit="page" from="451" to="462" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Determining the impact regions of competing options in preference space</title>
		<author>
			<persName><forename type="first">B</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="805" to="820" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Bidirectional active learning with gold-instance-based human training</title>
		<author>
			<persName><forename type="first">F</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5989" to="5996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Svms modeling for highly imbalanced classification</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Syst. Man Cybern. Part B</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="281" to="288" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Support vector machine active learning with applications to text classification</title>
		<author>
			<persName><forename type="first">S</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="45" to="66" />
			<date type="published" when="2002-03">Mar. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Query by output</title>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">T</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-Y</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Parthasarathy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="535" to="548" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Discovering valuable items from massive data</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">P</forename><surname>Vanchinathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1195" to="1204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Inferring generative model structure with static analysis</title>
		<author>
			<persName><forename type="first">P</forename><surname>Varma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="240" to="250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Snuba: Automating weak supervision to label training data</title>
		<author>
			<persName><forename type="first">P</forename><surname>Varma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ré</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. VLDB Endow</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="223" to="236" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Expand your training limits! generating training data for ml-based data management</title>
		<author>
			<persName><forename type="first">F</forename><surname>Ventura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGMOD</title>
		<imprint>
			<biblScope unit="page" from="1865" to="1878" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Class probability estimates are unreliable for imbalanced data (and how to fix them)</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">C</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">J</forename><surname>Dahabreh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDM</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="695" to="704" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">KBA: kernel boundary alignment considering imbalanced data distribution</title>
		<author>
			<persName><forename type="first">G</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">Y</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Knowl. Data Eng</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="786" to="795" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Guided exploration of data summaries</title>
		<author>
			<persName><forename type="first">B</forename><surname>Youngmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Amer-Yahia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Personnaz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PVLDB</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1798" to="1807" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Active learning with imbalanced multiple noisy labeling</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Cybern</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1081" to="1093" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Bidirectional active learning: A two-way exploration into unlabeled and labeled data set</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3034" to="3044" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Controlling false discoveries during interactive data exploration</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="527" to="540" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Budget constrained interactive search for multiple targets</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PVLDB</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="890" to="902" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>