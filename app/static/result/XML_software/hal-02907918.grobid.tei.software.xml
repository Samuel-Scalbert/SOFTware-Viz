<TEI xmlns="http://www.tei-c.org/ns/1.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xml:space="preserve" xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Comparative Study of Speech Anonymization Metrics</title>
				<funder ref="#_FrqkYZa">
					<orgName type="full">Agence Nationale de la Recherche</orgName>
					<orgName type="abbreviated">ANR</orgName>
				</funder>
				<funder>
					<orgName type="full">CNRS</orgName>
				</funder>
				<funder ref="#_UtWrju8">
					<orgName type="full">European Union</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher />
				<availability status="unknown"><licence /></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Mohamed</forename><surname>Maouche</surname></persName>
							<affiliation key="aff0">
								<address>
									<settlement>Inria</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Brij</forename><surname>Mohan</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Lal</forename><surname>Srivastava</surname></persName>
							<affiliation key="aff0">
								<address>
									<settlement>Inria</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Nathalie</forename><surname>Vauquier</surname></persName>
							<affiliation key="aff0">
								<address>
									<settlement>Inria</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Aurélien</forename><surname>Bellet</surname></persName>
							<affiliation key="aff0">
								<address>
									<settlement>Inria</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Marc</forename><surname>Tommasi</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Université de Lille</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Emmanuel</forename><surname>Vincent</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution" key="instit1">Université de Lorraine</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<orgName type="institution" key="instit3">Inria</orgName>
								<orgName type="institution" key="instit4">LORIA</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Comparative Study of Speech Anonymization Metrics</title>
					</analytic>
					<monogr>
						<imprint>
							<date />
						</imprint>
					</monogr>
					<idno type="MD5">7FA8006131258924CD886823E8E7B492</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-04-12T14:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid" />
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>anonymization</term>
					<term>voice conversion</term>
					<term>speaker recognition</term>
					<term>privacy metrics</term>
				</keywords>
			</textClass>
			<abstract>
<div><head>Speech anonymization techniques have recently been proposed</head><p>for preserving speakers' privacy. They aim at concealing speakers' identities while preserving the spoken content. In this study, we compare three metrics proposed in the literature to assess the level of privacy achieved. We exhibit through simulation the differences and blindspots of some metrics. In addition, we conduct experiments on real data and state-of-the-art anonymization techniques to study how they behave in a practical scenario. We show that the application-independent log-likelihood-ratio cost function C min llr provides a more robust evaluation of privacy than the equal error rate (EER), and that detection-based metrics provide different information from linkability metrics. Interestingly, the results on real data indicate that current anonymization design choices do not induce a regime where the differences between those metrics become apparent.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div><head n="1.">Introduction</head><p>With the increasing popularity of smart devices, more users have access to voice-based interfaces. They offer simple access to modern technologies and enable the development of new services. The building blocks behind these speech-based technologies are no more handcrafted but learned from large sets of data. This is the case for instance of automatic speech recognition (ASR), where vast volumes of speech in different languages are needed and continuously collected to improve performance and adapt to new domains. The collection and exploitation of speech data raises privacy threats. Indeed, speech contains private or sensitive information about the speaker (e.g., gender, emotion, speech content) <ref type="bibr" target="#b0">[1]</ref> and it is a biometric characteristic that can be used to recognize the speaker through, e.g., i-vector <ref type="bibr" target="#b1">[2]</ref> or x-vector <ref type="bibr" target="#b2">[3]</ref> based speaker verification.</p><p>To address this privacy issue, various anonymization techniques have been studied in the literature <ref type="foot" target="#foot_0">1</ref> . Their purpose is to transform speech signals in order to preserve all content except features related with the speaker identity. These techniques include noise addition <ref type="bibr" target="#b3">[4]</ref>, speech transformation <ref type="bibr" target="#b4">[5]</ref>, voice conversion <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8]</ref>, speech synthesis <ref type="bibr" target="#b8">[9]</ref>, or adversarial learning <ref type="bibr" target="#b9">[10]</ref>. As a privacy preservation mechanism, they must achieve a suitable privacy/utility trade-off. The utility is typically assessed in terms of the accuracy of downstream processing steps (e.g., the word error rate achieved by an ASR system). The measurement of privacy is the topic we tackle in this paper.</p><p>Historically, the usual metrics employed in the speaker verification community have been used to assess the (in)ability of an attacker to recognize the speaker, which is considered as a proxy for privacy. The most widely used metric is the equal error rate (EER): it considers an attacker that makes a decision by comparing speaker similarity scores with a threshold and it assigns the same cost to false alarms and misses <ref type="bibr" target="#b10">[11]</ref>. The application-independent log-likelihood-ratio cost function C min llr generalizes the EER by considering optimal thresholds over all possible priors and all possible error costs <ref type="bibr" target="#b11">[12]</ref>. In the following, we consider a third metric called linkability which has recently emerged from the biometric template protection community but has received little attention in the speech community so far <ref type="bibr" target="#b12">[13]</ref>. This metric, denoted as D sys ↔ , estimates the distributions of scores for mated (same-speaker) vs. non-mated (different-speaker) trials and computes their overlap.</p><p>The goal of this paper is to assess the suitability of these three metrics for the evaluation of speaker anonymization. In addition to comparing the metrics in their form and substance, we generate simulated data to exhibit their blindspots. We also conduct experiments on real speech data processed by state-ofthe-art anonymization techniques against different attackers (ignorant, semi-informed, or informed <ref type="bibr" target="#b13">[14]</ref>). Overall, we aim to understand the complementary factors underlying different metrics and ensure that the anonymization techniques being evaluated were not designed to fool attackers that follow one specific speaker verification method but would fail with others.</p><p>We describe the attack model in Section 2 and introduce the metrics in Section 3. We present the simulations used to exhibit their blindspots in Section 4. Section 5 reports the results of the evaluation on real data with various anonymization techniques and attack types. We conclude in Section 6.</p></div>
<div><head n="2.">Attack Model</head><p>The attack scenario is depicted in Fig. <ref type="figure" target="#fig_0">1</ref>. Speakers process their voice through an anonymization technique. This anonymization step takes as input one or more private speech utterances along with some configuration parameters, and outputs a new speech signal or some kind of derived representation. The transformed utterances from one or more speakers form a public speech dataset that is processed by a third-party user for, e.g., ASR training/decoding or any other downstream task.</p><p>Given unprocessed or anonymized utterances from a known speaker, an attacker attempts to find which anonymized utterances in the public dataset are spoken by this speaker <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b13">14]</ref>. Formally, an attacker has access to two sets of utterances: A (enrollment/found data) and B (trial/public speech), but knows the corresponding speakers in A only. The attacker designs a linkage function LF (a, b) that outputs a score for any a ∈ A and b ∈ B. Typically, this score is a similarity score obtained through a speaker verification system. The attacker then makes a decision (same vs. different) based on this score.</p><p>Anonymization techniques must achieve a suitable privacy/ utility trade-off. Utility is measured by the performance of the desired downstream task(s), e.g., the word error rate of an ASR system or the intelligibility for a human listener. Different privacy metrics exist in the literature.</p></div>
<div><head n="3.">Privacy Metrics</head><p>We describe three candidate privacy metrics, which model the attacker's decision making process or the score distribution.</p></div>
<div><head n="3.1.">Equal Error Rate (EER)</head><p>The EER is the classical metric used in speaker recognition. It assumes a threshold-based decision on the score. If LF (a, b) is greater than a certain threshold t, the two utterances a and b are considered to be mated. Two types of errors can be made: false alarms with rate P fa (t), and misses with rate Pmiss(t). The EER is the error rate corresponding to the threshold t * for which the two types of errors are equally likely: EER = Pmiss(t * ) = P fa (t * ).</p><p>(1)</p><p>3.2. Log-Likelihood-Ratio Cost Function C llr and C min llr C llr is also a common speaker recognition metric <ref type="bibr" target="#b11">[12]</ref>. It is application-independent in the sense that it pools across all possible costs for false alarm vs. miss errors, and all possible priors for mated vs. non-mated trials. Let M (resp., M ) be the set of mated (resp., non-mated) trials and |M | (resp., |M |) its cardinality. Denoting by llr(p) be the log-likelihood ratio for trial p = (a, b), C llr is defined as</p><formula xml:id="formula_0">C llr = 1 log 2 1 |M | p∈M log 1 + e -llr(p) + 1 |M | p∈M log 1 + e llr(p) . (<label>2</label></formula><formula xml:id="formula_1">)</formula><p>C llr assesses the overall detection which includes both discrimination and calibration. In practice, discrimination alone is more relevant as a privacy metric. To measure it, a derived metric called C min llr can be computed by optimal calibration of the scores LF (p) into log-likelihood ratios using a monotonic rising transformation. This transformation is found via the Pool Adjacent Violators algorithm (PAV), see <ref type="bibr" target="#b16">[17]</ref> for details.</p></div>
<div><head n="3.3.">Linkability</head><p>A linkability metric was proposed in <ref type="bibr" target="#b12">[13]</ref> for biometric template protection systems. This metric can be generalized for any two sets of items. Denoting by H (resp., H) the binary variable expressing whether two random utterances a and b are mated (resp., non-mated), the local linkability metric for a score s = LF (a, b) is defined as p(H | s) -p(H | s). When the local metric is negative, an attacker can deduce with some confidence that the two utterances are from different speakers. The authors of <ref type="bibr" target="#b12">[13]</ref> argued that the local metric should estimate the strength of the link described by a score rather than measure how much a score describes non-mated relationships. Therefore they propose a clipped version of the difference:</p><formula xml:id="formula_2">D↔(s) = max(0, p(H | s) -p(H | s)).<label>(3)</label></formula><p>The global linkability metric D sys ↔ is the mean value of D↔(s) over all mated scores:</p><formula xml:id="formula_3">D sys ↔ = p(s | H) • D↔(s) ds.</formula><p>In practice, D↔(s) is rewritten as </p><formula xml:id="formula_4">(2•ω•lr(s))/(1+ω</formula></div>
<div><head n="3.4.">Comparison of the Metrics</head><p>Based on the above definitions, we already note that the three metrics do not provide the same information. Both the EER and C min llr measure the probability of error of an attacker that makes decisions based on a threshold on the linkage function (one particular threshold for EER and all possible ones for C min llr ). Linkability measures something different: it evaluates how different the distributions of mated vs. non-mated scores are. There is no attacker making a decision and there is no threshold or, from another perspective, the best possible oracle attacker (not necessarily threshold-based) is assumed. In addition, if we consider how general are the metrics, on the one hand C min llr is a direct extension of the EER as it does not focus on one single threshold. On the other hand, D sys ↔ is evaluated over all the encountered mated scores. In the next section, we provide experimental examples that highlight the differences of information provided and generality of the metrics.</p></div>
<div><head n="4.">Exhibiting Differences and Blindspots through Simulation</head><p>We design two experiments over simulated scores in order to exhibit the differences between the metrics. The first experiment relies on discrete scores to highlight the lack of generality of the EER. The second experiment relies on Gaussian distributed scores to exhibit the differences between C min llr and linkability. All of the metrics are integrated in the Voice Privacy Challenge 2020 <ref type="foot" target="#foot_1">2</ref> and we developed an easy to use toolkit <ref type="foot" target="#foot_2">3</ref></p></div>
<div><head n="4.1.">Discrete Scores</head><p>Let us assume that there are 8 trials p1, . . . , p8 and that the score for the i-th trial is given by the integer LF (pi) = i. The values of EER and C min llr vary with the label (mated vs. non-mated) of each trial. In Table <ref type="table" target="#tab_1">1</ref>, we show 3 particular cases where only the labels of the last three trials (associated with scores 6, 7, and 8) change. We notice that this has an effect on C min llr but not on the EER. This is because the EER searches for a single llr averages over all possible thresholds that the attacker might choose. We also notice that the EER indicates a privacy of 0.25 that is half of the best achievable privacy (0.5), while C min llr increases from half of the best achievable privacy (0.5 over 1) to higher values (0.69).</p></div>
<div><head n="4.2.">Gaussian Scores</head><p>Since D sys ↔ relies on density estimation, we now generate Gaussian distributed scores to compare D sys ↔ and C min llr . We consider three Gaussians: G1 ∼ N (1, σ1), G2 ∼ N (2, σ2) and G3 ∼ N (3, σ3). Each Gaussian Gi is used to sample either mated or non-mated scores according to a key ki ∈ {H, H}. In total, we have four different cases depending on the values of (k1, k2, k3): Mated higher for (H, H, H) or (H, H, H); Nonmated higher for (H, H, H) or (H, H, H); Mated in-between for (H, H, H); Non-mated in-between for (H, H, H). We sample from those three distributions in order to obtain 5, 000 mated and 5, 000 non-mated scores. Multiple standard deviations are chosen to obtain different degrees of overlap between the distributions: (σ1, σ2, σ3) ∈ {0.1, 0.5, 1, 1.5} 3 .</p><p>The results are presented in Fig. <ref type="figure" target="#fig_2">2</ref>. We consider that C min llr and D sys ↔ are equivalent when C min llr is equal to 1-D sys ↔ (diagonal line). The two metrics agree to a large extent only when the mated scores are higher. When the non-mated scores are higher, C min llr is always close to 1 while D sys ↔ varies depending on the overlap between the distributions. In the two remaining cases when the mated scores are surrounded by the non-mated scores or vice-versa, C min llr is lower-bounded by 0.6 and the two metrics do not agree on the strength of anonymization. This is explained by the fact that threshold-based decision is meaningful in the mated higher case and its performance is then strongly related to the overlap between distributions, while it fails partially or totally in the three other cases.  To illustrate why this an issue and how this may happen in practice, in Figure <ref type="figure" target="#fig_4">3</ref>, we draw (simulated) x-vectors for multi-ple utterances of two speakers, which have all been anonymized by mapping them to another (pseudo) speaker's voice. Each utterance of speaker A has been randomly mapped to the left or the right cluster, while the utterances of speaker B have been mapped to the center cluster. The resulting score distributions match the non-mated in-between case above. As expected, the two metrics strongly disagree: D sys ↔ = 0.99 (low privacy) and C min llr = 0.81 (high privacy). While this situation is unlikely to occur with unprocessed data (scores are then expected to match the mated higher case), it becomes likely once the utterances have been anonymized and the anonymization design choices (see <ref type="bibr" target="#b17">[18]</ref> for example choices) result in multimodal score distributions.  </p></div>
<div><head n="5.">Evaluation on Real Anonymized Speech</head><p>In order to further compare D sys ↔ , C min llr and the EER, we conduct a second experiment on real speech data. In the following, we present the dataset, the anonymization techniques and the attackers considered. Then we discuss the results.</p><p>The experiment is conducted on <software ContextAttributes="used">LibriSpeech</software> <ref type="bibr" target="#b18">[19]</ref>. The train-clean-460 set (∼1k speakers, ∼130k utterances and 460 hours of speech) is used to train the x-vector model and the probabilistic linear discriminant analysis (PLDA) model with Kaldi <ref type="bibr" target="#b19">[20]</ref>. Part of the test-clean set (40 speakers, 1,496 utterances) is anonymized to form the trial/public data. The remaining part (29 speakers, 438 utterances) is considered as unprocessed enrollment/found data.</p></div>
<div><head n="5.1.">Anonymization Techniques and Target Selection</head><p>We use the following four anonymization techniques. Except for the first one, these are voice conversion techniques which map the input (source) signal to another (target) speaker's voice.</p><p>VoiceMask (VM) <ref type="bibr" target="#b20">[21]</ref> is a frequency warping method. It has two parameters, α and β, they are chosen uniformly at random from a predefined range which is found to produce intelligible speech while perceptually concealing the speaker identity.</p><p>VTLN-based VC <ref type="bibr" target="#b21">[22]</ref> clusters each speaker's data into unsupervised pseudo-phonetic classes. For each source speaker class, the closest target speaker class is found and the corresponding warping parameters are applied to the input signal.</p><p>The third approach is based on disentangled representation (DAR) <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24]</ref>. It uses a speaker encoder and a content encoder to separate speaker and content information and replace the source speaker information by that of the target speaker.</p><p>Finally, the primary baseline of the VoicePrivacy Challenge 2020 (VPC) <ref type="bibr" target="#b24">[25]</ref> uses a neural synthesizer <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b25">26]</ref> to synthesize speech given the target x-vector and fundamental frequency and bottleneck features extracted from the source.</p><p>VTLN and DAR require speakers to be anonymized using target speakers from a given pool. Following <ref type="bibr" target="#b13">[14]</ref>, we evaluate three different target selection strategies: (1) CONST: all utterances of all source speakers are mapped to one single target speaker; (2) PERM: each source speaker has all her utterances mapped to one specific target speaker; (3) RAND: each utterance of each speaker is mapped to a random target speaker. Rather than an actual target speaker, VPC constructs a target x-vector by averaging several x-vectors from the pool.</p></div>
<div><head n="5.2.">Attacker Knowledge and Linkage Function</head><p>Following <ref type="bibr" target="#b13">[14]</ref>, we also consider different attackers based on their knowledge about the anonymization. (1) Ignorant: the attacker has no knowledge of the anonymization and uses unprocessed enrollment data; (2) Informed: the attacker has complete knowledge of the anonymization technique including the target speakers, and he/she processes the enrollment data accordingly;</p><p>(3) Semi-informed: the attacker knows the anonymization technique and the target selection strategy but not the particular target speaker selected for a given source speaker, and she processes the enrollment data accordingly. The attacker performs linkage attacks by computing the x-vectors of a trial utterance and an enrollment utterance and comparing them using one of three linkage functions: PLDA affinity, cosine distance, or Euclidean distance. This results in a total of 72 combinations of anonymization techniques, target selection strategies, attacker knowledge levels, and linkage functions.</p></div>
<div><head n="5.3.">Results</head><p>Figures 4 and 5 compare the resulting metrics, where each dot corresponds to one of the 72 combinations above. The comparison between the EER and C min llr (Fig. <ref type="figure">4</ref>) shows a clear relation between the two metrics. In some cases the EER is stable and C min llr varies a little bit but not significantly so. Regarding the comparison between D sys ↔ and C min llr , we see a clear difference between Fig. <ref type="figure">5</ref> on real data and Fig. <ref type="figure" target="#fig_2">2</ref> on simulated Gaussian scores: on real data, the two metrics follow a clear relation.</p><p>These results can be explained by the fact that, with few exceptions, the score distributions for the specific target selection and attack strategies considered here fall into the mated higher case, as can be seen from the colors associated with the dots. It is however likely that advanced target selection strategies aiming for score distributions akin to Fig. <ref type="figure" target="#fig_2">2</ref> will be developed in the near future, as these would provide an advantage against attackers making threshold-based decisions. For that reason, we believe D sys ↔ should be privileged as a privacy metric, since it provides very similar results to established metrics with current target selection and attack strategies, while being more robust to advanced strategies that will likely be developed soon.</p></div>
<div><head n="6.">Conclusion</head><p>In this study, we compare three metrics to assess the effectiveness of anonymization: the EER, the application-independent log-likelihood-ratio min cost function ↔ when the mated scores are lower or interleaved with non-mated scores. While such situations were unlikely to occur in the field of speaker verification, which involves unprocessed speech data, we expect them to become frequent in the field of anonymization when more advanced target selection and attack strategies are built. For this reason, we advocate for the use of D sys ↔ as a robust privacy metric capable of handling both current approaches and future developments in this field.</p></div><figure xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Anonymization procedure and attack model.</figDesc><graphic coords="1,314.96,600.94,226.77,108.86" type="bitmap" /></figure>
<figure xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: C min llr vs. 1 -D sys ↔ on simulated Gaussian scores.</figDesc></figure>
<figure xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Simulated 'non-mated in-between' data. Top: xvectors visualized in 2D. Bottom: resulting score distributions.</figDesc></figure>
<figure xml:id="fig_5"><head>Figure 4 :Figure 5 :</head><label>45</label><figDesc>Figure 4: C min llr vs. EER on real data. The color scale µ -µ is the difference of the means of mated and non-mated scores.</figDesc></figure>
<figure type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>C min llr and EER with discrete scores in {1, . . . , 8}. H (resp. H) denote mated (resp. non-mated) scores.</figDesc><table><row><cell>Score</cell><cell>1 2 3 4 5 6 7 8 C min llr</cell><cell>EER</cell></row><row><cell cols="3">Case 1 H H H H H H H H 0.50 0.25</cell></row><row><cell cols="3">Case 2 H H H H H H H H 0.59 0.25</cell></row><row><cell cols="3">Case 3 H H H H H H H H 0.65 0.25</cell></row><row><cell cols="2">threshold of the linkage function while C min</cell><cell /></row></table></figure>
			<note place="foot" n="1" xml:id="foot_0"><p>In the legal community, the term "anonymization" means that this goal has been achieved. Following the VoicePrivacy Challenge<ref type="bibr" target="#b24">[25]</ref>, we use it to refer to the task, even when the technique has failed.</p></note>
			<note place="foot" n="2" xml:id="foot_1"><p>https://www.voiceprivacychallenge.org/#Soft</p></note>
			<note place="foot" n="3" xml:id="foot_2"><p>https://gitlab.inria.fr/magnet/anonymization˙metrics</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="7.">Acknowledgments</head><p>This work was supported by the <rs type="funder">French National Research Agency</rs> under project <rs type="projectName">DEEP-PRIVACY</rs> (<rs type="grantNumber">ANR-18-CE23-0018</rs>) and by the <rs type="funder">European Union</rs>'s <rs type="programName">Horizon 2020 Research and Innovation Program</rs> under Grant Agreement No. <rs type="grantNumber">825081 COM-PRISE</rs> (https://www.compriseh2020.eu/). Experiments presented in this paper were carried out using the Grid'5000 testbed, supported by a scientific interest group hosted by <rs type="institution">Inria</rs> and including <rs type="funder">CNRS</rs>, RENATER and several Universities as well as other organizations (see https: //www.grid5000.fr).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_FrqkYZa">
					<idno type="grant-number">ANR-18-CE23-0018</idno>
					<orgName type="project" subtype="full">DEEP-PRIVACY</orgName>
				</org>
				<org type="funding" xml:id="_UtWrju8">
					<idno type="grant-number">825081 COM-PRISE</idno>
					<orgName type="program" subtype="full">Horizon 2020 Research and Innovation Program</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Preserving privacy in speaker and speech characterisation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Nautsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jimenez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Treiber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kolberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Jasserand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Kindt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Delgado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Todisco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Hmani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mtibaa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Abdelraheem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Abad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Teixeira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Matrouf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gomez-Barrero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Petrovska-Delacrétaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chollet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-F</forename><surname>Bonastre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Raj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Trancoso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Busch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech and Language</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page" from="441" to="480" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Front-end factor analysis for speaker verification</title>
		<author>
			<persName><forename type="first">N</forename><surname>Dehak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Kenny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Dehak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dumouchel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ouellet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="788" to="798" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">X-vectors: Robust DNN embeddings for speaker recognition</title>
		<author>
			<persName><forename type="first">D</forename><surname>Snyder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Garcia-Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5329" to="5333" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Privacy-preserving sound to degrade automatic speaker verification performance</title>
		<author>
			<persName><forename type="first">K</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yamagishi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Echizen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="5500" to="5504" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Voicemask: Anonymize and sanitize voice input on mobile devices</title>
		<author>
			<persName><forename type="first">J</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X.-Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.11460</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Speaker deidentification via voice transformation</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Toth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Schultz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU)</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="529" to="533" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Online speaker de-identification using voice transformation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Pobar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Ipšić</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">37th International Convention on Information and Communication Technology, Electronics and Microelectronics</title>
		<meeting><address><addrLine>MIPRO</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1264" to="1267" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Convolutional neural network based speaker de-identification</title>
		<author>
			<persName><forename type="first">F</forename><surname>Bahmaninezhad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H L</forename><surname>Hansen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Odyssey</title>
		<imprint>
			<biblScope unit="page" from="255" to="260" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Speaker anonymization using x-vector and neural waveform models</title>
		<author>
			<persName><forename type="first">F</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yamagishi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Echizen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Todisco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-F</forename><surname>Bonastre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">10th ISCA Speech Synthesis Workshop (SSW)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="155" to="160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Privacy-preserving adversarial representation learning in ASR: Reality or illusion?</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M L</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bellet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tommasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Interspeech</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3700" to="3704" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<idno>ISO/IEC 19795-1:2006</idno>
		<title level="m">Information Technology -Biometric performance testing and reporting -Part 1: Principles and framework</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Application-independent evaluation of speaker detection</title>
		<author>
			<persName><forename type="first">N</forename><surname>Brümmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Du Preez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech and Language</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="230" to="275" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">General framework to evaluate unlinkability in biometric template protection systems</title>
		<author>
			<persName><forename type="first">M</forename><surname>Gomez-Barrero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Galbally</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rathgeb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Busch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Forensics and Security</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1406" to="1420" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Evaluating voice conversion-based privacy protection against informed attackers</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M L</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Vauquier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sahidullah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bellet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tommasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2802" to="2806" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Towards privacy-preserving speech data publishing</title>
		<author>
			<persName><forename type="first">J</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X.-Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Conference on Computer Communications (INFOCOM)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1079" to="1087" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Evaluating voice conversion-based privacy protection against informed attackers</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M L</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Vauquier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sahidullah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bellet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tommasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2802" to="2806" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">An introduction to application-independent evaluation of speaker recognition systems</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Van Leeuwen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Brümmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Speaker Classification I: Fundamentals, Features, and Methods</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="330" to="353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Design choices for x-vector based speaker anonymization</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M L</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Tomashenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yamagishi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Maouche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bellet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tommasi</surname></persName>
		</author>
		<imprint />
	</monogr>
	<note>in Interspeech, submitted</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Librispeech: An ASR corpus based on public domain audio books</title>
		<author>
			<persName><forename type="first">V</forename><surname>Panayotov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="5206" to="5210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The Kaldi speech recognition toolkit</title>
		<author>
			<persName><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ghoshal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Boulianne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Glembek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hannemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Motlíček</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Silovský</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Stemmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Veselý</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU)</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Hidebehind: Enjoy voice input with voiceprint unclonability and anonymity</title>
		<author>
			<persName><forename type="first">J</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X.-Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">16th ACM Conference on Embedded Networked Sensor Systems (SenSys)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="82" to="94" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">VTLN-based voice conversion</title>
		<author>
			<persName><forename type="first">D</forename><surname>Sundermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd IEEE International Symposium on Signal Processing and Information Technology (ISSPIT)</title>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="556" to="559" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">One-shot voice conversion by separating speaker and content representations with instance normalization</title>
		<author>
			<persName><forename type="first">J.-C</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-Y</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Interspeech</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="664" to="668" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Improved texture networks: Maximizing quality and diversity in feed-forward stylization and texture synthesis</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ulyanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="6924" to="6932" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Introducing the VoicePrivacy initiative</title>
		<author>
			<persName><forename type="first">N</forename><surname>Tomashenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M L</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nautsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yamagishi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Patino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-F</forename><surname>Bonastre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-G</forename><surname>Noé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Todisco</surname></persName>
		</author>
		<ptr target="https://hal.inria.fr/hal-02562199" />
		<imprint />
	</monogr>
	<note>in Interspeech, submitted</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Neural harmonic-plus-noise waveform model with trainable maximum voice frequency for textto-speech synthesis</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yamagishi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">10th ISCA Speech Synthesis Workshop (SSW)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>