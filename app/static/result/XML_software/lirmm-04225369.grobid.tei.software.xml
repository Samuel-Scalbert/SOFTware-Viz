<TEI xmlns="http://www.tei-c.org/ns/1.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xml:space="preserve" xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd">
	<teiHeader xml:lang="fr">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">kNN matrix profile for knowledge discovery from time series</title>
				<funder>
					<orgName type="full">Safran Data Analytics Lab</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher />
				<availability status="unknown"><licence /></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Tanmoy</forename><surname>Mondal</surname></persName>
							<email>tanmoy.mondal@imt-atlantique.fr</email>
							<affiliation key="aff0">
								<orgName type="laboratory">ZENITH Team</orgName>
								<orgName type="institution">INRIA &amp; LIRMM</orgName>
								<address>
									<settlement>Montpellier</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Mathematical &amp; Electrical Engineering Department</orgName>
								<orgName type="institution">IMT Atlantique</orgName>
								<address>
									<settlement>Brest</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Reza</forename><surname>Akbarinia</surname></persName>
							<email>reza.akbarinia@inria.fr</email>
							<affiliation key="aff1">
								<orgName type="department">Mathematical &amp; Electrical Engineering Department</orgName>
								<orgName type="institution">IMT Atlantique</orgName>
								<address>
									<settlement>Brest</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Florent</forename><surname>Masseglia</surname></persName>
							<email>florent.masseglia@inria.fr</email>
							<affiliation key="aff1">
								<orgName type="department">Mathematical &amp; Electrical Engineering Department</orgName>
								<orgName type="institution">IMT Atlantique</orgName>
								<address>
									<settlement>Brest</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">kNN matrix profile for knowledge discovery from time series</title>
					</analytic>
					<monogr>
						<imprint>
							<date />
						</imprint>
					</monogr>
					<idno type="MD5">981F6D4AA7902154D7761D7224926872</idno>
					<idno type="DOI">10.1007/s10618-022-</idno>
					<note type="submission">Submitted on 2 Oct 2023</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-04-12T14:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid" />
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Time series analysis</term>
					<term>STAMP</term>
					<term>STOMP</term>
					<term>All-pairs-similarity search</term>
					<term>Motifs and discord discovery</term>
					<term>Outliers detection</term>
					<term>Anomaly detection</term>
					<term>Joins</term>
				</keywords>
			</textClass>
			<abstract>
<div><p>et à la diffusion de documents scientifiques de niveau recherche, publiés ou non, émanant des établissements d'enseignement et de recherche français ou étrangers, des laboratoires publics ou privés.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="fr">
		<body>
<div><head n="1">Introduction</head><p>A time series is a series of data points ordered in time. As examples of time series, we can mention the height of ocean tides level captured every minute, the vibration of an aircraft engine captured every second, or the number of steps measured by a kNN Matrix Profile smart watch day after day. Analyzing the time series can give us precious information about the underlying applications, e.g., the anomalies in an aircraft engine. Matrix Profile (MP) <ref type="bibr" target="#b0">[1]</ref> has been proposed as a powerful technique for time series analysis, e.g., detecting motifs or anomalies. Efficient algorithms have been proposed for MP computation, e.g., <software ContextAttributes="used">STAMP</software> <ref type="bibr" target="#b1">[2]</ref>, STOMP <ref type="bibr" target="#b1">[2]</ref> and <software ContextAttributes="used">SCRIMP++</software> <ref type="bibr" target="#b2">[3]</ref>. The definition of MP in the literature is as follows <ref type="bibr" target="#b0">[1]</ref>. Given a time series T and a subsequence length m, the MP returns for each subsequence included in T its distance to the most similar subsequence (1N N ) in the time series. We call this type of MP as 1NN MP. It is very useful for data analysis, e.g., detecting the motifs (represented by low values), or anomalies (represented by high values).</p><p>Although 1NN MP has been shown useful for knowledge discovery, it has its own drawbacks and can miss some important motifs and anomalies (also called discords). Particularly, it does not allow to detect a cluster of discords, e.g., two subsequences that are similar to each other, but dissimilar to all other subsequences. In addition, 1NN MP does not permit to distinguish a weak motif (i.e., a subsequence that has only one similar subsequence in the series) from strong motifs (i.e., those that have several similar subsequences).</p><p>We believe that a more powerful MP based on kNN search is of high interest, where for each subsequence, its k th nearest neighbor is used for generating the MP. We call it kNN MP. An example is depicted in Fig. <ref type="figure" target="#fig_0">1</ref> where 1NN and kNN (4NN in this case) MPs are drawn for a time series of protein spectrum, representing protein rates measured in 10 different products (only certain portion of the complete MP is shown here). As seen in Fig. <ref type="figure" target="#fig_0">1</ref>, the strong motifs can be detected in 4NN MP. In the figure, the distance values in 1N N and 4N N MPs, which are less than a user defined low threshold value (marked as black dotted line at the bottom of the Fig. <ref type="figure" target="#fig_0">1</ref>), are termed as "strong motifs". This analogy signifies that the subsequence has not only one close match (i.e., 1NN which may occur due to some noise or outlier elements) but multiple close matches, i.e., 1NN, 2NN, 3NN and 4NN. This reasoning helps to increase the certitude of validating a subsequence as motif.</p><p>There are also situations where the distance in 1N N MP for an anomaly is quite low because the subsequence has one close match but the match can simply be another anomaly. An interesting example is shown in Fig. <ref type="figure" target="#fig_0">1</ref> (Inset 1), where based on the user defined threshold (dotted red line in Fig. <ref type="figure" target="#fig_0">1</ref>), the discord can be detected in 4N N MP, while it is not detected in 1N N MP (see more examples in <ref type="bibr">Section 6)</ref>.</p><p>In this paper, we propose the kNN MP and illustrate its utility for knowledge discovery from time series. Our contributions are as follows:</p><p>1. We define the kNN MP, and propose a fast algorithm to calculate it in a time series. 2. We propose a technique for parallel execution of the proposed algorithm by using multiple cores of an off-the-shelf computer. 3. We evaluate the performance of our technique experimentally by using multiple real datasets, e.g., UCR dataset and Yahoo anomaly detection dataset. The experimental evaluation illustrates the efficiency of our solution for kNN MP computation <ref type="foot" target="#foot_0">1</ref> . The results also show how qualitatively the kNN MP is useful for knowledge discovery compared to the 1NN MP. For example, the accuracy of anomaly detection can be improved from 37% with 1N N MP to 99% with 10N N MP, for one of the benchmarks of the Yahoo dataset.</p><p>The rest of the paper is organized as follows. In Section 2, we give the necessary background and definitions. In Section 3, we discuss the related work. Our solution for sequential computations of kNN MP is presented in Section 4. The parallel computation of kNN MP is presented in Section 5. In Section 6, an extensive experimental evaluation is reported. Finally, Section 7 concludes.</p></div>
<div><head n="2">Problem Definition</head><p>In this section, we give the formal definition of kNN MP, and describe the problem we address. A summary of notations is shown in Table <ref type="table" target="#tab_0">1</ref>.</p><p>Definition 1. Time series: A time series T is a sequence of real-valued numbers T = ⟨t 1 , . . . , t n ⟩ where n is the length of T .</p><p>A subsequence of a time series is defined as follows.</p><p>Definition 2. Subsequence: Let m be a given integer value such that 1 ≤ m ≤ n. A subsequence T i of a time series T is a continuous sequence of values in T of length m starting from position i. Formally, T i = ⟨t i , . . . , t i+m-1 ⟩ where 1 ≤ i ≤ n -m + 1. We call i the start position of T i . A user given threshold Dist(T i, T j) or D i,j</p><p>Euclidean distance of subsequences T i and T j QT i,j Dot product between the subsequence T i and T j µ i</p><p>Mean of subsequence T i σ i</p><p>Standard deviation (STD) of subsequence T i I or I T</p><p>The vector or array which contains the indexes of the time series T P T</p><p>The vector or array which contains the matrix profile distance of the time series T D T  A dataset which contains several small time series</p><p>One of the primary goals of time series analysis is to perform time series subsequence matching. Given a positive real number τ (called threshold) and a time series T , two subsequences T i (beginning at index i) and T j (beginning at index j) of length m, and a distance function Dist(T i , T j ) that measures the Euclidean distance between T i and T j . If Dist(T i , T j ) ≤ τ , then T j is called a matching subsequence of T i .</p></div>
<div><head>Definition 3. Distance profile:</head><p>The distance between a subsequence T i with all other subsequences of the time series T gives a vector of distances, called distance profile of T i .</p><p>The minimum value of this distance profile represents the closest match or 1N N and the top k minimum values of this vector represent the kN N matches of the subsequence T i . The classical MP is defined as a vector of nearest neighbor (1N N ) distances of all the subsequences of time series T . Definition 4. kNN MP: The kN N MP of a time series T is a vector P = ⟨p 1 , . . . , p n-m+1 ⟩ such that p i is the distance of the subsequence T i to its k th nearest neighbor among the subsequences of T .</p><p>The kNN MP index is a vector I = ⟨s 1 , . . . , s n-m+1 ⟩ such that s i is the index of the k th nearest neighbor of the subsequence T i in the time series T . Definition 5. Motif: A motif pair is defined as a pair of subsequences ⟨T i , T j ⟩ whose distance is less than a user defined threshold τ (i.e., Dist(T i , T j ) &lt; τ ), and their starting positions are at least w elements apart ( |i -j|≥ w ), where w is a user defined threshold which tells that two subsequences in a pair should be w elements apart. If |i -j|&lt; w, then the motif pair ⟨T i , T j ⟩ is called a trivial match.</p><p>From the definition of 1NN motif, we can define the kNN motif as follows. Definition 6. kNN Motif: A subsequence T i is a kNN motif if its distance to its k th nearest neighbor (say T k j ) is less than the user defined threshold τ (i.e. Dist(T i , T k j ) &lt; τ ) and their starting positions are at least w elements apart.</p><p>Traditionally, a discord is defined as a subsequence (say T i ) whose distance from all other subsequences of the time series is higher than a given threshold ω. Hence, the distance between T i and it's nearest neighbor is higher than ω. We call this type of discord as 1NN discord. Let us now define the kN N discord. Definition 7. kNN Discord: Let T be a time series, and ω a high distance threshold given by the user. A subsequence T i ∈ T is a kNN discord, if the distance of T i to its k th nearest neighbor is higher than ω.</p><p>Let us now define the problem which we address in this paper. Given a number k, a time series T , and a subsequence length m, our goal is to efficiently compute the kN N MP.</p></div>
<div><head n="3">Related Work</head><p>MP is an efficient solution to the problem of similarity join, which can be defined as: given a set of data objects (for our case subsequences), retrieve the nearest neighbors for each object. The solution to this problem can be useful for motif and anomaly discovery from time series in many application domains such as bioinformatics <ref type="bibr" target="#b3">[4]</ref>, speech processing <ref type="bibr" target="#b4">[5]</ref>, Seismology <ref type="bibr" target="#b5">[6]</ref>, etc. Similarity join can be categorized into two principal categories, 1NN similarity join and kNN similarity join. With kNN similarity join, for each given object the k nearest neighbors are returned, where k is a positive number given by the user. The 1NN similarity join is a special case of kNN similarity join with k = 1. To the best of our knowledge, the MP algorithms in the literature only take into account the 1NN category.</p><p>The authors in <ref type="bibr" target="#b6">[7]</ref> propose an approach to optimize the calculation of Euclidean distance between all subsequences. The idea is to interleave the early abandoning calculations of Euclidean distance with the concept of online z-normalization. In <ref type="bibr" target="#b7">[8]</ref>, the authors propose an algorithm based on intelligent caching, reusing computations and the pruning of the search space. By reusing the computations of z-normalized distances for overlapping subsequences, the solution highly saves the computation time and reduces the search space.</p><p>In <ref type="bibr" target="#b8">[9]</ref> [8], Mueen et al. propose <software ContextAttributes="used">MASS</software>, an efficient algorithm for similarity search in time series. It exploits the consecutive subsequence overlapping property to calculate Z normalized distance by Fast Fourier Transform (FFT) based convolution. Thanks to the use of FFT, in recent years the <software ContextAttributes="used">MASS</software> algorithm has emerged as a significant contribution in subsequence similarity search for many similarity based pattern matching problems such as motif and discord discovery, nearest neighbor matching, etc. <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b12">[12]</ref>, <ref type="bibr" target="#b13">[13]</ref>.</p><p>Yeh et al. <ref type="bibr" target="#b1">[2]</ref> proposed MP, an efficient technique for similarity join in time series <ref type="bibr" target="#b14">[14]</ref>. The authors use the convolution property of FFT and Inverse FFT for the fast calculation of MP by an algorithm called <software ContextAttributes="used">STAMP</software>. Furthermore, an incremental algorithm, called STOMP, is adapted for distance computation of overlapping sequential subsequences in which <software ContextAttributes="used">MASS</software> algorithm is used for time series similarity search by computing z-normalized euclidean distance between the subsequences. However, the techniques proposed in <ref type="bibr" target="#b1">[2]</ref> are mainly designed for 1NN similarity join.</p><p>In <ref type="bibr" target="#b2">[3]</ref>, the authors introduce an anytime algorithm, named <software ContextAttributes="used">SCRIMP++</software> by combining the best features of <software ContextAttributes="used">STAMP</software> and STOMP for fast MP computation. But this technique is also designed for 1NN MP. SCAMP <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b16">16]</ref> is an efficient GPU-based extension of STOMP algorithm for computing 1NN matrix profile using GPUs. It is an improved version of the earlier GPU-STOMP algorithm.</p><p>Neighbor profile <ref type="bibr" target="#b17">[17]</ref> is a new technique for knowledge discovery from time series. The idea is to take multiple small samples from the set of subsequences and to find the anomalies/motifs inside the samples. In this way, if an abnormal subsequence has some similar anomalies, then probably they will not be in the same sample, and thus the subsequence may be returned as a discord from the sample. This may particularly avoid defiant-gravity behavior. However, the neighbor profile may increase the probability of false anomalies, i.e., those that are not really anomalies but are far from other subsequences in the small chosen sample.</p><p>The state-of-the-art MP techniques (e.g. <software>STAMP</software> and STOMP) compute only 1N N MP. To the best of our knowledge, in the literature there is no efficient solution for computing kN N MP. As illustrated in Section 1, this type of MP can be very useful for knowledge discovery from time series. In this paper, we propose an efficient solution for computing it.</p></div>
<div><head n="4">kNN MP</head><p>Here, we discuss our technique for kNN MP computation of a time series T in a sequential computing environment.</p></div>
<div><head n="4.1">MP Algorithm</head><p>Given a time series T , a subsequence size m, and a number k, the goal is to find the k closest matches of all the subsequences T i in T . Our algorithm for computing kN N MP of the time series T is inspired by the STOMP algorithm <ref type="bibr" target="#b18">[18]</ref>. Let us briefly present the idea behind STOMP.</p></div>
<div><head n="4.1.1">Brief description of STOMP algorithm</head><p>The Scalable Time Series Ordered Search MP (STOMP) algorithm <ref type="bibr" target="#b10">[11]</ref> is a variant of <software ContextAttributes="used">STAMP</software> (see Appendix: IV for more details) in which we perform an ordered search (from left to right). To calculate the distance, STOMP takes benefit of the common part between two adjacent subsequences which is the same except the fist and last elements. The Z-normalized euclidean distance (D i,j ) between two time series subsequences T j and T i is calculated by using the following Equation 1. The dot product between these two subsequences are mentioned as QT i,j .</p><formula xml:id="formula_0">D i,j = 2m 1 - QT i,j -mµ i µ j mσ i σ j (1)</formula><p>where m is the subsequence length, µ i and µ j are the mean of T i and T j subsequences respectively, σ i is the standard deviation of T i , σ j is the standard deviation of T j , and QT i,j is the dot product between two subsequences T i and T j . The dot product QT i,j can be computed in O(1) time when QT i-1,j-1 has already been calculated. The term QT i-1,j-1 can be decomposed as :</p><formula xml:id="formula_1">QT i-1,j-1 = m-1 k=0 T i-1+k T j-1+k</formula><p>and the term QT i,j can be decomposed as : QT i,j = m-1 k=0 T i+k T j+k Thus by combining these two terms we can get :</p><formula xml:id="formula_2">QT i,j = QT i-1,j-1 -T i-1 T j-1 +T i+m-1 T j+m-1 .</formula><p>The relationship between QT i,j and QT i-1,j-1 indicates that from the distance profile of query subsequence T j-1,m , we can compute the distance profile of T j,m in O(1) time.</p></div>
<div><head n="4.1.2">1NN Matrix Profile algorithm</head><p>The pseudocode of the algorithm for computing 1NN MP is shown in Algorithm 1. Let's consider T be a big time series, which may be obtained by concatenating several small individual time series.The goal is to find the closest match of all the subsequences T [i] with all the remaining subsequences of T and these matches should be separated by w elements from the subsequence in question i.e., T [i]. The fast computation of mean and standard deviation (STD) (see Appendix: II for more details) of the time series T is performed in Line 3 of Algorithm 1. Then, the <software ContextAttributes="used">MASS</software> algorithm (see Appendix: III for more details) is applied in Line 5 to compute the dot product (QT ) (distance D ignore is ignored) between the first subsequence (subSeq 1 ) and other subsequences of time series T . The arguments of <software ContextAttributes="used">MASS</software> are: 1 st subsequence (subSeq 1 ), mean (µ T <ref type="bibr" target="#b0">[1]</ref>) and STD (σ T <ref type="bibr" target="#b0">[1]</ref>) of subSeq 1 , complete time series T , mean (µ T ) and STD (σ T ) vector (i.e., mean and STD of all the subsequences) of T .</p><p>Then, the algorithm loops through all the subsequences of T (see Line 9) and takes each subsequence (cutSubSeq) in Line 10. Only for the first subsequence (when i = 1), it applies <software>MASS</software> algorithm to compute the dot product (QT ) and distance vector (Dist cutSubSeq ) between first target subsequence (cutSubSeq) and the remaining subsequences of time series T . For i &gt; 1 on-wards, the distance of target subsequence (cutSubSeq) with all the remaining subsequences of T is incrementally calculated by using <software ContextAttributes="used">IndependentSTOMP</software> algorithm (see Appendix: IV.1 for more details). The following arguments are passed in <software ContextAttributes="used">IndependentSTOMP</software> for the distance calculation: each subsequences (cutSubSeq), the mean (µ T [i]) and STD (σ T [i]) of T , the dot product value (QT initial [i]) of very first subsequence (subSeq 1 ) and i th subsequences (T [i to (i+m-1)]), already computed dot product vector (QT ), mean (µ T ) and STD (σ T ) of T .</p><p>Then the matrix profile array (P T ) and index profile array (I T ) are updated based on the distance of each subsequence (cutSubSeq) of T to its nearest neighbor (see Lines <ref type="bibr">15 -16)</ref>. Finally, these two vectors (P T ) and (I T ) are returned as the results of this algorithm.</p></div>
<div><head>Algorithm 1: 1NN-MP(T , m)</head><p>Input: Time series T , and the subsequence length m Output: Matrix profile (P T ) and its associated index (I T ) 1 n T ← length(T ) // get the length of time series T 2 Idx T ← n T -m + 1 // get the total number of subsequences in T</p><formula xml:id="formula_3">3 [µ T , σ T ] ← ComputeM eanStd(T ) 4 subSeq 1 ← T [1 to (1 + m -1)] // get the 1 st subsequence from Q 5 [QT, D ignore ] ← M ASS(subSeq 1 , µ T [1], σ T [1], T , µ T , σ T ) // apply M ASS</formula><p>with arguments as 1 st subsequence of T i.e. subSeq 1 and remaining subsequences of time series T . See Algorithm 7 in Appendix: III 6 QT initial ← QT // keeping a copy of the very first dot product 7 P T ← Initialize this 1D vector with inf 8 I T ← Initialize this 1D vector with zeros</p><formula xml:id="formula_4">9 for i ← 1 to Idx T do 10 cutSubSeq ← T [i to (i + m -1)] // get target subsequence by chopping T from index i to (i + m -1) 11 if i == 1 then 12 [QT, Dist cutT arget ] ← M ASS(cutSubSeq, µ T [i], σ T [i], T , µ T , σ T ) //</formula><p>apply M ASS with arguments as 1 st subsequence of T and complete time series T . </p><formula xml:id="formula_5">13 else 14 [QT, Dist cutSubSeq ] ← IndependentST OM P (cutSubSeq, µ T [i], σ T [i], QT initial [i], T , QT , µ T , σ T ) //</formula></div>
<div><head n="4.2">Computing kNN Matrix Profile</head><p>The MP algorithms in the literature (e.g., <software ContextAttributes="used">STAMP</software> and STOMP algorithms <ref type="bibr" target="#b1">[2]</ref>) are designed to find the best match (1N N ) of each subsequence. For computing a kNN matrix profile algorithm, the main issue is the management of k nearest neighbors of each subsequence T i . In fact, efficient methods are needed to update the kNN matches of T i after computing its distance with another subsequence. In this section, we propose three techniques to find and manage the kN N matches of each subsequence: 1) Sort based; 2) Maximum based; 3) Heap based.</p></div>
<div><head>Algorithm 2: SORTING-BASED-KNN-MP (T , m, k)</head><p>Input: The time series T , the subsequence length m, user given number k Output: A MP P T and associated MP index I T .... .... </p><formula xml:id="formula_6">7 P T ← Initialize a 2D vector of size {(k + 1) × Idx T } with inf 8 I T ← Initialize a 2D vector of size {(k + 1) × Idx T } with zeros 9 for i ← 2 to Idxs conCat do 10 cutSubSeq ← ... .... .... 15 if i &lt;= k then 16 for p ← 1 to Idx T do 17 P T [i, p] ← Dist cutT arget [p] 18 I T [i, p] ← i else 19 for p ← 1 to Idx T do 20 P T [(k + 1), p] ← Dist cutT arget [p] 21 I T [(k + 1), p] ← i 22 [sortV als, sortIdxs] ← Sort(P T [1 to (k + 1), p]) 23 for t ← 1 to k do 24 P T [t, p] ← sortV als[t, p] 25 I T [t, p] ← I T [sortIdxs[t], p] 26 P f inal T ← P T [1 to k, 1 to Idx T ] //</formula></div>
<div><head n="4.2.1">Sort based kNN search</head><p>Given a time series T , we need to keep updated the list of the k nearest neighbors of each subsequence T i , when its distance with a subsequence of T is calculated. The idea of the sort based approach is to create a list containing the distance of the current kN N matches and the new computed distance, sort the list and take the first k distances and their corresponding subsequences.</p><p>The pseudo-code of the sort based approach is mentioned in Algorithm 2 which is the same as Algorithm 1 until Line 6 (so we avoid to mention it again). In Lines 7 -8, two 2D arrays, named as : P T and I T are created for storing kN N . Both of these arrays are of size {(k + 1) × Idx T }. (k + 1) number of rows are needed to keep k nearest neighbors, and the (k +1) th row is needed to temporarily hold newly calculated distance. In Line 9, we loop through all the subsequences of time series (T ) and kNN Matrix Profile in Line 10, each subsequence is chopped as usual (same as Algorithm 1)). After that until Line 14, we perform the same operations as in Line 11 -14 of Algorithm 1. The initial k number of distances and index profiles are simply stored in P T and I T arrays (see Line <ref type="bibr">15 -18)</ref>. From (k + 1) th subsequence on-wards (the else portion in Line 19 -25), the newly calculated distance profile is saved at k + 1 th row (Line 20) and consequently the target subsequence index i is stored in k + 1 th row (Line 21). Then the distances stored from index 1 to (k + 1) are sorted in ascending order and from this sorting operation, we will get the sorted distance values (sortV als) and their corresponding indexes (sortIdxs) (see Line 22). After that the top k sorted values are updated in P T matrix (Line 24) and corresponding stored indexes are also updated (Line 25) with the help of sorted indexes (sortIdxs). This process is repeated iteratively for all the subsequences, and finally the MP P T and the MP index I T from the index 1 to k are returned (as P f inal T and I f inal T ) as the output of this algorithm. The average time complexity of sorting k elements is O(k log k) and we need to perform this O((n -m) 2 ) times, where n is the length of the time series T , and m the subsequence length. Hence, the total complexity is O((n -m) 2 × k log k). To improve the computational time, in the next subsection we propose to replace sorting by finding maximum of top k distance values for each subsequences.</p><formula xml:id="formula_7">Algorithm 3: MAX-BASED-KNN-MP (T , m, k) .... .... else 19 for p ← 1 to Idx T do 20 P T [(k + 1), p] ← Dist cutT arget [p] 21 I T [(k + 1), p] ← i 22 [maxV al, maxIdx] ← F indM ax(P T [1 to k, p]) 23 if (P T [(k + 1), p] &lt; maxV al) then 24 P T [maxIdx, p] ← P T [(k + 1), p] 25 I T [maxIdx, p] ← I T [(k + 1), p] 26 P f inal T ← P T [1 to k, 1 to Idx T ] 27 I f inal T ← I T [1 to k, 1 to Idx T ]</formula><p>.... return P T and I T // return the P T and I T array</p></div>
<div><head n="4.2.2">Maximum based kNN search</head><p>In this method, instead of sorting, we use the maximum distance of the subsequence T i to its kNN matches (i.e., the distance to its k nearest neighbor), and then compare this maximum value with the newly computed distance. The time complexity of finding maximum in a list of k elements is O(k), which is already less than the time complexity of the sort based algorithm, i.e., O(k log k).</p><p>The pseudo code of this approach is mentioned in Algorithm 3. The Else portion of Algorithm 2 (Line 19-25), needs to be replaced by the pseudo code of Algorithm 3 (Line 19-25). As usual, the newly calculated distance profile and subsequence index are stored at (k + 1) th row of P T and I T matrix (Line 20 -21). Then, we find the maximum value of top k elements and corresponding index from P T matrix (Line 22). Now this maximum value is compared with the newly arrived value, which is temporarily kept at (k + 1) th index (Line 23). If the newly arrived value is less than the existing maximum value, then the old maximum value is replaced by the new value (P T [maxIdx, p]) (Line 24) and it's index is stored in I T matrix (Line 25). This process is repeated for all the subsequences and finally the results P f inal T and I f inal T are returned as output of the algorithm.</p></div>
<div><head n="4.2.3">Heap based kNN search</head><p>Finding the maximum value in a vector of size k in the classical manner has a time complexity of O(k). Here, we propose to find the maximum of a vector by using the heap based priority queue whose time complexity is O log(k). At first, we need to organize the kNN matches into a heap structure, thus the first element in the heap will contain the maximum value of the array.  The pseudo-code of the heap based approach is shown in Algorithm 4. Until Line 18, the algorithm remains the same as Algorithm 2. The Else portion of Algorithm 2 (Line 19-25), needs to be replaced by the pseudo code of Algorithm 4 (Line 19-33).</p><formula xml:id="formula_8">I T [(k + 1), p] ← i 22 if i == (k + 1) then 23 for p ← 1 to Idx T do 24 [P T [1 to k, p], heapSortIdxs] ← BuildM axHeap(P T [1 to k, p], k) 25 I T [1 to k, p] = I T [heapSortIdxs[1 to k], p] 26 for p ← 1 to Idx T do 27 if (P T [(k + 1), p] &lt; P T [(1), p]) then 28 P T [1, p] ← P T [(k + 1), p] 29 I T [1, p] ← I T [(k + 1), p] 30 [P T [1 to k, p], heapSortIdxs] ← BuildM axHeap(P T [1 to k, p], k) 31 I T [1 to k, p] = I T [heapSortIdxs[1 to k], p] 32 P f inal T ← P T [1 to k, 1 to Idx T ] 33 I f inal T ← I T [1 to k,</formula><p>In Line 22, we verify whether i == (k + 1), i.e. when we are handling (k + 1) th subsequence of target, the elements of P T array are organized for the first time in the structure of heap based priority queue. So in Line 23, we loop through each subsequence, and for each of such subsequences we organize the elements (k elements) of each column in a heap based priority queue. The heap structure is updated in P T (Line 24). Thanks to this operation, we will have the maximum value at the first row of P T matrix. We loop through all subsequences (Line 26) and compare the maximum value with the newly arrived value at (k + 1) th index. If this newly arrived value is less than the existing maximum value (stored in first row) then we replace the value in the first row with the new value. Then, we apply the restructuring operation on the heap based priority queue to put the maximum value out of top k values at the first row. This process is repeated for all the subsequences in order to produce the final k distances and corresponding indexes in P f inal T and I f inal T matrices.</p></div>
<div><head n="5">Multi-core based parallel computing</head><p>In this section, we propose an approach to perform the parallel computation of kN N MP by exploiting multiple cores. We consider the situation where several small time series are concatenated to generate one big time series. Let's say there are n D T number of individual time series in the time series dataset D T i.e. {T 1 , T 2 , T 3 , ...., T n D T } ∈ D T . By concatenating all these time series, we can obtain a big time series T . The objective is to compute kNN MP of T , such that we should be able to identify the matches, coming from which individual time series along with the index of the match in the time series.</p><p>Let n cores be the total number of available cores, then the idea is to divide the big time series T into g portions (i.e., G 1 , G 2 , ...,G g ) and to give each portion to one individual core to process. The number of groups are determined by the availability of total number of cores of the processor (say g). An example of kN N MP computation by using multiple cores is depicted in Fig. <ref type="figure" target="#fig_5">2</ref>. As seen, the time series T is equally divided into 6 groups, and each group is processed by a separate core. The pseudo code of this technique is shown in Algorithm 5.</p><p>The core algorithm for performing parallel computation of kNN MP is presented in Algorithm 5. The concatenation of n D T number of small time series of different lengths from the time series database D T is done by using concatenate TimeSeries() function in Line 1. After concatenating, we obtain a big time series<ref type="foot" target="#foot_1">2</ref> T . The fast computation of mean and standard deviation of all the subsequences in T is computed in Line 2.</p><p>Then in Line 3, we obtain the number of available cores. If the remainder after division between the total number of individual time series, i.e., n D T and total number of available cores i.e. n cores is zero then the variable Y (Line 8) will simply hold  Input: The target time series data base (D T , m) Output: A MP (P T ) and associated MP index  T <ref type="bibr" target="#b0">[1]</ref> and</p><formula xml:id="formula_9">(I T ) 1 [n D T , Inf o conCat , T ] ← concatenate T imeSeries(D T ) 2 [µ T , σ T ] ← ComputeM eanStd(T ) // see</formula><formula xml:id="formula_10">Y[iClus][1] ← s; Y[iClus][2] ← s + g -1; s = s + g // the indexes of first and last individual time series is stored in Y 9 else if (n D T % n cores &gt; 0) then s ← 1; r ← n D T /n cores ; g ← (n D T -r)/n cores for iClus ← 1 to n cores -1 do Y[iClus][1] ← s; Y[iClus][2] ← s + g -1; s = s + g Y[n cores ][1] ← s; Y[n cores ][2] ← n D T ; //</formula><formula xml:id="formula_11">I All T [1] nLen ← n cores × kN N for i ← 2 to n cores do P concat T ← [P concat T , P All T [i]]; I concat T ← [I concat T , I All T [i]] // concatenating the matrices P sort [1 : nLen][1 : Idx conCat ], indx sort [1 : nLen][1 : Idx conCat ] ← sortColW ise ( P concat T [1 : nLen][1 : Idx conCat ]</formula><p>) // sort the concatenated matrix</p><formula xml:id="formula_12">P concat T column wise I sort [1 : nLen][1 : Idx conCat ] ← I concat T ( indx sort [1 : nLen][1 : Idx conCat ] ) // using the sorted index i.e. indxsort rearrange I concat T return P F inal T ← P sort [1 : kN N ][1 : Idx conCat ] and I F inal T ← I sort [1 : kN N ][1 : Idx conCat ]</formula><p>Function knn MP Parallel(T, µ T , σ T , st, ed, m): ..... Figure <ref type="figure">3</ref>: The start and end indexes of each portions/parts of time series T , after dividing it into g (= 6 here) number of groups (please see the online color version for better visibility).</p><formula xml:id="formula_13">4 subSeq 1 ← T [st to (st + m -1)] ▷ get the 1 st subsequence 5 [QT, D ignore ] ← M ASS(subSeq 1 , µ T [st], σ T [st], T , µ T , σ T ) ......</formula><p>individual time series of all the groups), we can obtain start and end indexes of each parts/portions of time series T , which is divided into g parts (Line 17 -18). An example is shown in Fig. <ref type="figure">3</ref>, where the start (St) and end (Ed) indexes of each part are visually depicted. In every core, the local kN N matches are calculated in parallel for all the subsequence of T by using the "knn MP Parallel ()" function (Line The function "knn MP Parallel ()" is similar to Algorithm 4, except that "knn MP Parallel ()" takes a portion of T that starts from st and ends to ed, and for each subsequence of this portion finds the kNN matches among all subsequences of T .</p></div>
<div><head n="6">Experimental Evaluation</head><p>In this section, we evaluate the performance of our proposed kN N MP technique and illustrate its utility for motif and anomaly detection from time series. kNN Matrix Profile</p></div>
<div><head n="6.1">Setup</head><p>In our experiments, we have used several datasets. The first dataset is chemometric data, representing protein rate measured on 10 different products. The second dataset is accelerometer data obtained by attaching accelerometer at the neck of 13 sheep. The third dataset is seismic data and the fourth dataset is a synthetic random walk dataset. We have also used some datasets from the UCR time series data mining archive <ref type="bibr" target="#b19">[19]</ref> (see the list in Table <ref type="table" target="#tab_5">2</ref>).</p><p>The multi-core experiments were performed on a computational server having 36 physical cores. The other experiments were performed on an off-the-shelf computer, having Intel(R) Core(TM) i7-8850H CPU @ 2.60 GHz processor with 32 GB RAM<ref type="foot" target="#foot_2">3</ref> .</p></div>
<div><head n="6.2">Utility Experiments</head><p>We illustrate the utility of kNN MP for knowledge discovery in three case studies including chemometric, accelerometer, UCR archive, and Yahoo anomaly detection dataset.</p></div>
<div><head n="6.2.1">Case study: chemometric data</head><p>The experiments were performed with the dataset of 4075 spectrum, each having 680 dimensions. These spectrum represents the protein rates, measured on 10 different products: rapeseed (CLZ), corn gluten (CNG), sun flower seed (SFG), grass silage (EHH), full fat soya (FFS), wheat (FRG), sun flower seed (SFG), animal feed (ANF), soyameal (TTS), maïs (PEE), milk powder and whey (MPW). The complete data can be imagined as a matrix of 4075 × 680, where each row represents a time series of 680 elements. To build the kNN MP on a big time series, we concatenated the 4075 individual time series together and applied our kNN MP algorithm on it. We created the kNN MP by considering the subsequence length m = 40. Fig. <ref type="figure" target="#fig_3">4</ref> shows the 1N N vs 2N N and 4N N MPs for the chemometric time series. Note that we have only plotted a small part (i.e., 640 elements) of the whole MP. A low threshold is defined to obtain the motifs (shown as dotted black line at the bottom of the curve).If we consider only the curve for 1N N (blue color) then there are several subsequences, which can be taken as motifs. We consider these motifs as weak motifs. We can detect the strong motifs by checking their existence in kNN MP. If a subsequence appears as motif in kN N MP, then it has at least k similar subsequences in the time series.</p><p>In Fig. <ref type="figure" target="#fig_3">4</ref>, we see some strong motifs (marked as green circles) by considering k = 2 (top image) and k = 4 (bottom image). The motifs that appear in 4N N MP are stronger than those detected by using 2N N MP because they are repeated in higher values of k. There are some weak motifs (encircled by red color) that are shown in Fig <ref type="figure" target="#fig_3">4</ref> which appear only in 1NN MP but not in either 2N N or in 4N N MPs. Another nice illustration is also shown in Fig. <ref type="figure" target="#fig_10">5</ref> where we can see the cases of strong motifs and weak motifs by considering the 1N N vs 2N N MP and 4N N MP.</p><p>The discords are the subsequences whose distance with other subsequences is high. Let's consider Fig. <ref type="figure">6</ref> that shows the sorted distances of the matches of three individual subsequences. The distances are shown along Y -axis and the subsequences   will be detected as outlier (this is normal as this subsequence has close matches). But in the case of third curve, the top two distance values are less than the defined discord threshold, and the other distance values are more than the threshold. So, from the nature of the curve it can be depicted that this particular subsequence has two close matches, but it is highly different than all other remaining subsequences. Thus, if we consider 1N N and 2N N then this particular subsequence will not be detected as outlier. But, if we consider 3N N , 4N N and more then it will be detected as an outlier. Logically, this subsequence should be detected as outlier as it has only two very close neighbors (which can be outliers), and all of it's other neighbors are very different.</p><p>This scenario is confirmed by kNN MP in Fig. <ref type="figure">7</ref> (top). The 1 st subsequence has many matches shown as pink color (Fig. <ref type="figure">7</ref>   </p></div>
<div><head n="6.2.2">kNN similarity search : case study on accelerometer data</head><p>This real world dataset corresponds to more than 8000 time series which have been measured by attaching accelerometer at the neck of 13 sheep. Acelerometers captured 3-axial acceleration at a constant rate of 100Hz. Each of the three axial acceleration gives a different information for the zoologist, but for the simplicity and to show the interest of proposed method, here we only consider X axis data. The accelerometer data are manually labeled into one of six activities: STANDING-GRAZING, STANDING-EATING BRUSH, STANDING-RUMINATING, WALKING, RUN-NING, STANDING IMMOBILE. The sensor signals were pre-processed and for each activity of interest, a time series of 5 seconds (500 elements per time series) were constituted. By this manner, a dataset with 8532 time series is obtained where each of these time series is manually labeled.  Here we create a big time series by concatenating all the 8532 time series and the subsequence length (m) is taken as 50. The total number of subsequences obtained from the concatenated long time series is (500 × 8531) -50 + 1 = 42, 65, 451. An interesting part of the kN N MP for the case of 1N N vs 2N N MP and 1N N vs 4N N MP is shown in Fig. <ref type="figure" target="#fig_13">8a</ref> and Fig. <ref type="figure" target="#fig_13">8b</ref>. It can be seen from these figures that at several locations the distance values in 1N N MP are less than the defined threshold for motif, but not in the 2N N MP (see the green circles). So these low points can not be considered as strong motifs. For the case of detecting outliers, several points are marked in Fig. <ref type="figure" target="#fig_13">8a</ref> and Fig. <ref type="figure" target="#fig_13">8b</ref>, where some outliers can be detected only in 2N N or 4N N (see the red circled points in Fig. <ref type="figure" target="#fig_13">8a</ref> and Fig. <ref type="figure" target="#fig_13">8b</ref>).</p><p>In many cases, the 1N N and 4N N MPs follow almost the same trajectory and such an example is shown in Fig. <ref type="figure" target="#fig_13">8c</ref>. Another portion of the complete MP is shown in Fig. <ref type="figure" target="#fig_13">8d</ref> in which we illustrate the 1N N and 4N N MPs, and several interesting cases (by green and violet encircles). The points marked by green circles are strong motifs where both of the 1N N and 4N N MPs satisfy the defined threshold, whereas the points marked by violet circles are weak motifs. The region marked by dotted red rectangle shows an interesting situation where the 4N N MP shows a completely different trajectory (upward) than 1N N MP (downward). This can mainly happen when a particular subsequence has the 1 st nearest neighbor with whom it has small distance, but its distance with other neighbors is high (see Fig. <ref type="figure">6</ref>). Such subsequences are usually anomalies.</p></div>
<div><head n="6.2.3">Case study: UCR repository</head><p>Here we show some interesting results for kNN MP by using datasets from the UCR time series archive <ref type="bibr" target="#b19">[19]</ref> (see the list of datasets in Table <ref type="table" target="#tab_5">2</ref>). To create a single big time series from each dataset, we have sequentially concatenated the individual time series from training and testing set. Fig. <ref type="figure" target="#fig_7">9</ref> shows a portion of the generated MP for different datasets. As seen in Fig. <ref type="figure" target="#fig_7">9</ref>  From these plots, we can visualize that the curves for 1N N MP and 2N N or 4N N MPs don't follow the same trajectory. Hence, the detection of motifs and discords from 1N N MP could be wrongly validated if we don't verify their presence in kN N (i.e., 2N N...kN N ) MPs. From Fig. <ref type="figure" target="#fig_14">9 (c</ref>) also, we observe that there are detected motifs present in only 1N N MP, but not in the other MPs. Probably these motifs are resulted due to the presence of noise and should be considered with precaution. Hence, from these experiments with UCR datasets, we can clearly visualize the usefulness of kN N MP over 1N N MP for the detection followed by confirmation of motifs and discords. </p></div>
<div><head n="6.2.4">Case study: Yahoo anomaly detection dataset</head><p>In this section, we show some interesting illustrations to depict the usefulness of kNN MP, using the Yahoo time series dataset that includes labeled anomalies <ref type="bibr" target="#b20">[20]</ref>. This dataset contains several files (around 370), among them one part is based on real data (around 95), based on production traffic in some Yahoo services, whereas the other part contains synthetic (i.e., simulated) data. The anomalies in the simulated data were algorithmically generated, and those in the real-traffic data were manually labeled by Yahoo experts. The dataset is divided in 4 benchmarks, which are named as: "A1Benchmark-Real", "A2Benchmark-Synthetic", "A3Benchmark-Synthetic" and "A4Benchmark-Synthetic". In Fig. <ref type="figure" target="#fig_16">10a</ref> to Fig. <ref type="figure" target="#fig_16">10d</ref>, we depicted some interesting examples of anomaly detection by kNN MPs. In each figure, the time series is plotted at the top, followed by 1N N , 2N N , 3N N , 4N N , and 5N N MPs. We see that the kNN MPs find more relevant anomalies than 1N N MP. In Fig. <ref type="figure" target="#fig_16">10a</ref>, for instance, the 1NN MP (see the second plot) is not able to detect the second and third anomalies which are marked by red-colored star on the time series plot, wheeras the 2NN, 3NN, 4NN and 5NN MPs (see the third until sixth plots in Fig. <ref type="figure" target="#fig_16">10a</ref>) are able to detect them. The plausible reason is that these anomalies which are not detected in 1NN MP have similar subsequences (anomalies) in the time series, thus the distance values of these anomalies to their 1N N are not high enough (see Fig. <ref type="figure">6</ref>, Fig. <ref type="figure">7</ref> and it's corresponding discussions for further explanation). Whereas, the 2NN, 3NN, 4NN and 5NN have high distance values with these specific subsequences (i.e. subsequences where the outliers are present in the time series), hence they can be detected by kNN MPs. A very similar characteristic is also visible in Fig. <ref type="figure" target="#fig_16">10b</ref>. Some more nice examples can be seen in Fig. <ref type="figure" target="#fig_16">10c</ref> and Fig. <ref type="figure" target="#fig_16">10d</ref>. There are three outlier elements in the time series (shown in the topmost plot of Fig. <ref type="figure" target="#fig_16">10c</ref>) which have appeared as unusual spikes. If we observe carefully, then we can see that these three spikes (or their corresponding subsequences) look very similar to each other. Hence, the 1NN of these anomalies is a close match among two other similar anomalies. Even the 2NN of these anomalies will be a close match corresponding to other similar anomalies. Hence, it is not possible to detect them in 1NN (see the second plot in Fig. <ref type="figure" target="#fig_16">10c</ref>) and 2NN MPs (see the third plot in Fig. <ref type="figure" target="#fig_16">10c</ref>). But, if we compute 3NN, 4NN, and 5NN (see the fourth, fifth and sixth plots respectively in Fig. <ref type="figure" target="#fig_16">10c</ref>) MPs, then these outliers can be detected because their corresponding subsequences will have a high distance value to their third, fourth and fifth nearest neighbors. A very similar characteristic is also visible in Fig. <ref type="figure" target="#fig_16">10d</ref>. We have also performed experiments to evaluate the accuracy of kNN MPs in comparison with 1NN MP for anomaly detection by using all the labeled benchmarks from the Yahoo dataset. In our experiments, we generate the kNN matrix profiles for 1 ≤ k ≤ 10. We consider a subsequence as a discord if its value in the matrix profile is higher than a predefined threshold. To automatically calculate the threshold for the detection of discords, we adopt a simple way by taking 95%, 90% and 85% of the maximum value of 1NN MP. In this manner, for each time series, we can obtain 3 individual thresholds and based on these 3 threshold values, we have detected the discords of 1N N, 2N N, 3N N, ......, 10N N MP.</p><p>For each labeled outlier, we look within a horizontal window of size 2m to find any occurrence of the outlier among the detected discords in each of the 1N N, 2N N, 3N N, ......, 10N N MP, where m (= 32) represents the subsequence MP to find the existence of any detected (based on the chosen threshold) discords.</p><p>If we find a discord within this range, then we consider it as a success, otherwise it is considered as failure in detection. Simply speaking, for each labeled anomaly, we consider it as detected, if its subsequence overlaps with one of the detected discords in the matrix profile. Thus, the accuracy of anomaly detection by a matrix profile is measured as the fraction of detected anomalies over the total number of anomalies.</p><p>In this manner, we have computed the average accuracies of outlier detection over Yahoo time series. Tables <ref type="table" target="#tab_6">3</ref> and<ref type="table" target="#tab_7">4</ref> show the accuracy of 1N N and kN N matrix profiles (for k = 1, 2, ..., 10), for A1Benchmark-Real and A2Benchmark-Synthetic benchmarks from the Yahoo dataset. The results show that the accuracy can significantly be improved by using kN N MP. As seen in Table <ref type="table" target="#tab_6">3</ref>, for the A1Benchmark-Real benchmark, the accuracy increases from 41% with 1N N MP to 64% with 10N N MP (when the threshold is taken as 90%) and 46% with 1N N MP to 72% with 10N N MP (when the threshold is taken as 85%).</p><p>The accuracy gain is even more impressive for the A2Benchmark-Synthetic benchmark. As seen in Table <ref type="table" target="#tab_7">4</ref>, the accuracy increases from 37% with 1N N MP to 99% with 10N N MP (when the threshold is taken as 85%). The accuracy also increases from 51% with 1N N MP to 99% with 10N N MP (when the threshold is taken as 90%) and 64% with 1N N MP to 99% with 10N N MP (when the threshold is taken as 85%). The reason for this significant accuracy gain is that some of the (ground truth) anomalies have similar subsequences in the dataset (which can be other anomalies). This is why these anomalies do not appear as discords in the 1N N MP, as the distance value to their nearest neighbor is low. However, their distance value to other subsequences may be high (e.g., to 2NN, 3NN, etc), this is why they can be detected by kN N MPs.</p></div>
<div><head n="6.3">Scalability of kNN similarity search</head><p>In this section, we study the scalability of our solution for building the kN N MP, by varying several parameters such as k, number of cores, length of time series (n) and subsequence length (m). In our experiments, we used several datasets. The first one is We also concatenated the time series of this database to create a big time series. The initial three experiments (Fig. <ref type="figure" target="#fig_5">12a</ref>, Fig. <ref type="figure" target="#fig_5">12b</ref>, Fig. <ref type="figure" target="#fig_0">13a</ref>) were performed by using these two datasets. For the experiments on the number of cores in (Fig. <ref type="figure" target="#fig_0">13b</ref>), we have used the Hyper-spectral data of protein levels and accelerometer datasets (which are are used in other experiments, mentioned in Section 6.2.1 and 6.2.2). The main reason of choosing these datasets is the higher length of their time series (680 and 500 respectively). As like the previous datasets, here also we have concatenated all the time series to build a big time series. In our experiments the default value for k is taken as 10.</p><p>For the first experiment, we incremented the value of k to study its effect on the time required for computing kNN MP. As seen in Fig. <ref type="figure" target="#fig_5">12a</ref>, with increasing k, the required computational time increases linearly for both datasets. But, the computational time doesn't increase drastically for higher values of k, and the proposed </p><p>Figure <ref type="figure" target="#fig_0">13</ref>: The comparative performance of our proposed algorithm and the technique by Yeh et.al <ref type="bibr" target="#b1">[2]</ref> on random-walk, seismic, protein and sheep datasets: (a) The variation of computational time with increasing the length of subsequence (m) for the protein and sheep datasets respectively. (b) The computational time with increasing the number of cores for the random-walk and seismic datasets respectively (please see the online color version for better visibility). computational time increases with increasing time series length. In Fig. <ref type="figure" target="#fig_0">13a</ref>, it can be seen that computational time is almost linear with increasing the subsequence length. kNN Matrix Profile Fig. <ref type="figure" target="#fig_0">13b</ref> shows the evolution of the computational time with increasing the number of cores. As seen, the computational time firstly decreases significantly with the increase of number of cores for both datasets, but after a certain number of cores, it gets constant (after 17 cores).</p></div>
<div><head n="6.3.1">Comparison with adapted STOMP</head><p>We have compared the computational time of STOMP algorithm by Yeh et.al <ref type="bibr" target="#b1">[2]</ref> with our proposed technique for kN N MP. In general, the STOMP algorithm is designed to compute 1N N MP. We adapted it for kN N MP by applying it in k iterations, such that the best matches obtained in each iteration are excluded before the next iteration. Fig. <ref type="figure" target="#fig_5">12</ref> and<ref type="figure" target="#fig_0">13</ref> show the computational time of our proposed algorithm and STOMP algorithm by using the previously mentioned seismic and random walk datasets. The variation of computational time is depicted by increasing k by using 1000 individual time series from each dataset (seismic and random walk datasets, shown in Fig. <ref type="figure" target="#fig_5">12a</ref>). It can be seen that our algorithm outperforms significantly the STOMP technique for computing kN N MP.</p><p>The second experiment, shown in Fig. <ref type="figure" target="#fig_5">12b</ref> is performed to compare the computational time with increasing the time series length. The x-axis in the plot represents the number of individual time series which are concatenated to generate the single big time series to calculate kNN MP. In this experiment, our proposed technique has outperformed the STOMP algorithm. In the third experiment (shown in Fig. <ref type="figure" target="#fig_0">13a</ref>), where the comparison is performed by increasing the subsequence length, our proposed technique has also better execution time than the STOMP algorithm, for computing kNN matrix profile.</p></div>
<div><head n="7">Conclusion</head><p>In this paper, we proposed an efficient technique for computing kNN MP. Our technique is fast, simple and parallelizable on multiple cores of an off-the-shelf computer. Using several real and synthetic datasets, we illustrated the effectiveness of generating kNN MP by our technique. Our results show how qualitatively the kNN MP is useful for knowledge discovery compared to the 1NN MP. For example, they show that the accuracy of anomaly detection can be improved significantly, e.g., from 37% with 1N N MP to 99% with 10N N MP for A2Benchmark-Synthetic of the Yahoo dataset.</p><p>As a future work, we plan to extend our parallel technique for GPUs. Moreover, we would like to extend the kNN MP for the case of multi-dimensional data. Another possible research direction is to develop automatic methods for choosing the best values for kNN matrix profile parameters (such as the subsequence length or k) for anomaly/motif detection from a given dataset. Actually, these parameters should be mainly chosen by the experts of the domain. reversed query so that like T a , q ra will have 2n elements. This is done to facilitate element wise multiplication in frequency domain. In Line 5, the Fourier transform of q ra and T a is performed to transform time domain signals into frequency domain signals. Then in Line 6, an element wise multiplication is performed between two complex valued vectors, followed by inverse FFT on the resultant product.</p></div>
<div><head>Appendix: I.1 Relation between convolution in time domain with frequency domain</head><p>The time domain convolution of two signals T = [t 1 , t 2 , ....., t n ] and q = [q 1 , q 2 , ....., q m ]; m &lt;&lt; n can be calculated by sliding q upon T . For implementation, we would need m 2 number of zeros padding at the beginning and at the end of original T vector. The convolution between these two vectors is represented by (T * q) which is a vector c = [c 1 , c 2 , ....., c n+m-1 ] denoted as : C p = u T u q p-u+1 where u = [max (1, p -u + 1)].....[min (n, m)] and u ranges over all the sub-scripts for T u and q p-u+1 .</p><p>The convolution in time domain can be quickly calculated by element-wise multiplication in frequency domain by taking Fourier Transform of two signals, multiplying them element-wise and then applying inverse Fourier transform. Performing full convolution (in both time and frequency domains) between two 1D (same for 2D also) signals of size n and m results in an output of size (n + m -1) elements. Usually, the two signals are made of same size by padding zeros at the end to facilitate the multiplication operation.</p></div>
<div><head>Appendix: II Fast Calculation of Mean and Standard</head><p>Deviation :</p><p>The fast calculation of mean (µ) and standard deviation (σ) of a vector of elements (x) is proposed by Rakthanmanon et.al <ref type="bibr" target="#b6">[7]</ref>. The technique needs only one scan through the sample to compute the mean and standard deviation of all the subsequences. The mean of the subsequences can be calculated by keeping two running sums of the long time series which have a lag of exactly m values.</p><formula xml:id="formula_15">µ = 1 m k i=1 x i - k-m i=1 x i σ 2 = 1 m k i=1 x 2 i - k-m i=1 x 2 i -µ 2 (2)</formula><p>In the same manner, the sum of squares of the subsequences can also be calculated which are used to compute the standard deviation of all the subsequences by using Equations 2.</p></div>
<div><head>Appendix: III Brief description of MASS algorithm :</head><p>The <software ContextAttributes="used">MASS</software> algorithm is mentioned in Algorithm 7. In Line 1 of this algorithm, the sliding dot product is calculated by using Algorithm 6. The z-normalized Euclidean distance (D i ) is calculated between two time series subsequences q and T i by using the dot product between them (qT i ). The formula to and t <ref type="bibr" target="#b0">[1]</ref> represents the 1 st element of the target subsequence (t). In the same manner, Q[j + m -1] and t[1 + m -1] terms represent the last element of j th query subsequence (current one) and the last element of the subsequences t. In Line 4, the dot product value between the first query subsequence (j = 1) and the specific target subsequence (t) is copied in QT <ref type="bibr" target="#b0">[1]</ref>. This value is taken as input in this algorithm (i.e., tqSingleV al) The distance profile is calculated in Line 5 by using the dot product profile (QT ), mean (µ t ) and STD (σ t ) value of target subsequence t, mean (µ Q ) and STD (σ Q ) vector of query time series. Finally the distance vector (Dist cutQuery ) and the dot product vector (QT ) are returned as the output from the algorithm.</p><p>The time complexity of classical STOMP is O(n) 2 which is O(log n) improvement over <software>STAMP</software> algorithm. This improvement is highly useful for computing MP over big time series.</p></div><figure xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The 1NN and 4NN MP are plotted with different colors in which the motifs and discords are marked (please see the online color version for better visibility).</figDesc></figure>
<figure xml:id="fig_1"><head>15 P 16 I</head><label>1516</label><figDesc>calculate distance between a subsequence and all the remaining subsequences in T . T ← computeElementwiseM in(Dist cutSubSeq , P T ) // perform element-wise minimum value comparison of P T and Dist cutSubSeq , and maintain the minimum values in P T T ← i // update I T at the indexes where element-wise minimum operation replaces the previously stored value in P T with the new value from Dist cutSubSeq 17 return P T and I T // return the P T and I T array</figDesc></figure>
<figure xml:id="fig_2"><head /><label /><figDesc>perform element-wise minimum of value comparison of P T and Dist cutSubSeq then keep the minimum values in P T 27 I f inal T ← I T [1 to k, 1 to Idx T ] // update I T at indexes where element-wise minimum operation replaces the previously stored value in P T with the new value from Dist cutSubSeq return P f inal T and I f inal T // return the P f inal T and I f inal T array</figDesc></figure>
<figure xml:id="fig_3"><head>Algorithm 4 :</head><label>4</label><figDesc>HEAPMAX-BASED-KNN-MP (T , m, k) .... .... else 19 for p ← 1 to Idx T do 20 P T [(k + 1), p] ← Dist cutT arget [p] 21</figDesc></figure>
<figure xml:id="fig_4"><head>Function</head><label /><figDesc>concatenate TimeSeries(D T ): n D T ← length(D T ) // count the total number of time series in the data base D T Len 1 ← lenth(D T [1]) // get the length of first time series from database D T T ← D T [1] // create a new vector T and initialize it by copying the first time series D T [1] in it Inf o conCat ← [1, Len 1 , 'f ile1.csv'] // when an individual time series is merged in T , keep the indexes at where it starts and ends in concatenated time series T for iSeries ← 2 to n D T do T ← [T, D T [iSeries]] // keep concatenating individual time series from D T Inf o conCat ← [startIdx, endIdx, f ileN ame] // store the start, end indexes and the file name after concatenating an individual time series in T return n D T ; Inf o conCat and T the indexes of first and last values of T assigned to each group. Otherwise, if the remainder is more than zero then the number of individual time series in each group is calculated by subtracting r from n D T and then dividing it by n cores (Line 11). The indexes of first and last values of all the groups except last group are stored in Y (Line 13). The remaining r number of time series, belongs to the last group are stored at the last cell of Y in Line 14.</figDesc></figure>
<figure xml:id="fig_5"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The proposed architecture of multi core based parallel processing.</figDesc></figure>
<figure xml:id="fig_6"><head /><label /><figDesc>the indexes of start and last individual time series of last group are saved in the last cell of Y P All T , I All T ← (n cores × 1) array // these are 1D vectors each cell contains / * // following for loop is run in parallel i.e. the contents inside the loop are given to each core * / for iCore ← 1 to n cores do ST = Inf o conCat [ Y[iCore][1] ] // get the starting index of the part/portion of full time series, handled by this core ED = Inf o conCat [ Y[iCore][2] ] // get the end index of the part/portion of full time series, handled by this core [P core T , I core T ] ← knn M P P arallel(T, µ T , σ T , ST , ED, m) P All T [iCore] ← P core T ; I All T [iCore] ← I core T P concat T ← P All T [1]; I concat T ← I All T [1] // initialized with the first matrices i.e. P All</figDesc></figure>
<figure xml:id="fig_7"><head>9 fori</head><label>9</label><figDesc>← st to ed -m + 1 do 10 cutSubSeq ← T [i to (i + m -1)] ▷ get target subsequence by chopping T from index i to (i + m -1)</figDesc></figure>
<figure xml:id="fig_9"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: The extracted motifs and discords are illustrated in the plot : (a) The 1N N vs 2N N MP and (b) 1N N vs 4N N MP of a part of the time series is plotted (please see the online color version for better visibility).</figDesc><graphic coords="18,35.75,7.95,690.64,409.88" type="bitmap" /></figure>
<figure xml:id="fig_10"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: The extracted motifs and discords are illustrated in the plot : (a) The 1N N vs 2N N MP and (b) 1N N vs 4N N MP of a part of the time series is plotted (please see the online color version for better visibility).</figDesc><graphic coords="19,40.33,25.50,680.33,403.76" type="bitmap" /></figure>
<figure xml:id="fig_11"><head /><label /><figDesc>top), this is why 1N N , 2N N , 3N N MPs (Fig. 7 bottom) show lower value at the index of this subsequence. On the other hand, the 3 rd subsequence (shown in green color) has no matches, hence the 1N N , 2N N , 3N N MPs show high values for the 3 rd subsequence. But, for the case of 2 nd subsequence, it has a close match (shown in red color). Hence these two subsequences (which are marked in red color) would closely match with each other. Accordingly the 1N N MP (shown in Fig. 7 bottom) shows a very low value for these subsequences (follow the pink colored curve). Selected Selected Distribution of distances between subSeq1 and all other sub-sequences Distribution of distances between subSeq2 and all other sub-sequences Distribution of distances between subSeq3 and all other sub-sequences</figDesc></figure>
<figure xml:id="fig_12"><head>Figure 6 :Figure 7 :</head><label>67</label><figDesc>Figure 6: One special case of outliers detection (please see the online color version for better visibility)</figDesc><graphic coords="20,50.64,252.89,338.02,195.55" type="bitmap" /></figure>
<figure xml:id="fig_13"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: (a) The 1NN and 2NN MPs are plotted for a specific portion of the time series, having the label as: WALKING. (b) The 1NN and 4NN MPs are plotted of the same portion of the time series. (c) The 1NN and 4NN MPs (which follow very similar trajectories) are plotted for another portion of the time series, having labeled as WALKING. (d) The 1NN and 4NN MPs are plotted for a randomly chosen portion of the time series, having labeled as: RUNNING. (please see the online color version for better visibility)</figDesc><graphic coords="22,53.27,364.87,163.43,102.76" type="bitmap" /></figure>
<figure xml:id="fig_14"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: The results of UCR dataset : (a) FacesUCR dataset: the motifs and discords are shown by green and red circles respectively; (b) Beef dataset: presence of motifs in 4N N MP are not supported by 1N N MP; (c) CinCECGTorso dataset: the highlighted zone shows the contrary nature of 1N N vs 2N N MPs; (d) BME dataset: the highlighted zone shows the contrary nature of 1N N vs 2N N MPs; (e) BME dataset : some (weak) motifs in 1N N MP are not present in 4N N MPs; (f) Adiac dataset: the highlighted zones show the contrast between 1N N vs 2N N MPs where the weak motifs detected in 1N N MP are not present in 2N N MP and the discords detected in 2N N MP are not present in 1N N MP.</figDesc><graphic coords="23,33.45,30.97,368.21,260.10" type="bitmap" /></figure>
<figure xml:id="fig_15"><head /><label /><figDesc>(a), (b), (d), (e), (f), at some places the 1N N MP shows different trajectory than kN N (i.e., 2N N and 4N N ) MPs where the detected weak motifs (shown in dotted green circle) in 1N N MP are absent in kN N MP. There are also discords (shown in dotted red circle) detected in kN N MPs, but absent in 1N N MP.</figDesc></figure>
<figure xml:id="fig_16"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: The usefulness of the kNN MP technique is shown using Yahoo dataset. In each figure, the time series is plotted at the top followed by 1N N , 2N N , 3N N , 4N N , and 5N N MPs, plotted subsequently (the subsequence length (m) is taken as 32). The outlier elements/points, obtained from the available ground truth of the dataset are marked by the red star at the topmost plot of each figure (please see the online color version for better visibility).</figDesc><graphic coords="25,40.31,200.32,188.98,141.68" type="bitmap" /></figure>
<figure><head /><label /><figDesc /><graphic coords="21,66.79,243.22,300.38,171.74" type="bitmap" /></figure>
<figure><head /><label /><figDesc /><graphic coords="21,69.67,64.37,297.98,170.78" type="bitmap" /></figure>
<figure type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Summary of notations</figDesc><table><row><cell>Notation</cell><cell>Description</cell></row><row><cell>T</cell><cell>Time series</cell></row><row><cell>T i</cell><cell>Subsequence beginning at index i</cell></row><row><cell>n</cell><cell>Time series length</cell></row><row><cell>m</cell><cell>Subsequence length</cell></row><row><cell>τ</cell><cell /></row></table></figure>
<figure type="table" xml:id="tab_1"><head /><label /><figDesc>1 to Idx T ] .... return P T and I T // return the P T and I T array</figDesc><table /></figure>
<figure type="table" xml:id="tab_2"><head /><label /><figDesc>Equation 2 in Appendix: II 3 n cores ← cluster.numW orkers // get the number of workers/cores available 4 Y ← (n cores × 2) array // these are 2D matrix, initialized with zeros 5 if (n D T % n cores == 0) then 6 s ← 1; g ← n D T /n cores // number of individual time series in each group is calculated in g 7 for iClus ← 1 to n cores do 8</figDesc><table /></figure>
<figure type="table" xml:id="tab_3"><head /><label /><figDesc>17 return P T and I T ▷ return the P T and I T array</figDesc><table><row><cell>St1</cell><cell /><cell /><cell /><cell /><cell /><cell cols="2">Ed1</cell><cell>St2</cell><cell /><cell /><cell /><cell /><cell /><cell /><cell>Ed2</cell><cell>St3</cell><cell /><cell /><cell /><cell /><cell /><cell /><cell>Ed3</cell><cell>St4</cell><cell /><cell /><cell /><cell /><cell /><cell /><cell>Ed4</cell><cell>St5</cell><cell /><cell /><cell /><cell /><cell /><cell /><cell>Ed5</cell><cell>St6</cell><cell /><cell /><cell /><cell /><cell /><cell /><cell /><cell /><cell>Ed6</cell></row><row><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>7</cell><cell>8</cell><cell>9</cell><cell>10</cell><cell>11</cell><cell>12</cell><cell>13</cell><cell>14</cell><cell>15</cell><cell>16</cell><cell>17</cell><cell>18</cell><cell>19</cell><cell>20</cell><cell>21</cell><cell>22</cell><cell>23</cell><cell>24</cell><cell>25</cell><cell>26</cell><cell>27</cell><cell>28</cell><cell>29</cell><cell>30</cell><cell>31</cell><cell>32</cell><cell>33</cell><cell>34</cell><cell>35</cell><cell>36</cell><cell>37</cell><cell>38</cell><cell>39</cell><cell>40</cell><cell>41</cell><cell>42</cell><cell>43</cell><cell>44</cell><cell>45</cell><cell>46</cell><cell>47</cell><cell>48</cell><cell>49</cell><cell>50</cell></row></table></figure>
<figure type="table" xml:id="tab_4"><head /><label /><figDesc><ref type="bibr" target="#b19">19)</ref>. The idea is to calculate kN N matches of all the subsequences of T , where the matches come from one part/portion of the time series T , handled by an individual core. Then, the partial kN N MPs from different cores are merged and the global MP is produced. Line 21). The concatenated MP P concat T is then sorted column wise to obtain the sorted MP P sort and sorted indexes indx sort . They are then used to obtain the sorted index profile I sort (Line 26). Finally, the top kN N rows of P sort and I sort are returned in P F inal</figDesc><table><row><cell cols="3">The MP P core T I All T respectively (Line 20). These matrices are concatenated in P concat and index profile I core T , obtained from each core are saved in P All T and T and I concat T</cell></row><row><cell>respectively (T</cell><cell>and I F inal T</cell><cell>respectively in Line 27.</cell></row></table></figure>
<figure type="table" xml:id="tab_5"><head>Table 2 :</head><label>2</label><figDesc>Dataset details from UCR archive</figDesc><table><row><cell>Dataset Name</cell><cell>Size of</cell><cell>Size of</cell><cell>Time series</cell></row><row><cell /><cell>training set</cell><cell>testing set</cell><cell>length</cell></row><row><cell>FacesUCR</cell><cell>200</cell><cell>2050</cell><cell>131</cell></row><row><cell>Beef</cell><cell>30</cell><cell>30</cell><cell>471</cell></row><row><cell>CinCECGTorso</cell><cell>40</cell><cell>1380</cell><cell>1639</cell></row><row><cell>BME</cell><cell>30</cell><cell>150</cell><cell>128</cell></row><row><cell>Adiac</cell><cell>390</cell><cell>391</cell><cell>176</cell></row></table></figure>
<figure type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>The outlier detection accuracy of various kNN MP based on 3 high thresholds on the Yahoo dataset ("A1Benchmark-Real")</figDesc><table><row><cell /><cell /><cell>Accuracies</cell><cell /></row><row><cell>kNN MP</cell><cell>Threshold</cell><cell>Threshold</cell><cell>Threshold</cell></row><row><cell /><cell>(95%)</cell><cell>(90%)</cell><cell>(85%)</cell></row><row><cell>1NN</cell><cell>0.317</cell><cell>0.413</cell><cell>0.469</cell></row><row><cell>2NN</cell><cell>0.349</cell><cell>0.485</cell><cell>0.556</cell></row><row><cell>3NN</cell><cell>0.386</cell><cell>0.509</cell><cell>0.584</cell></row><row><cell>4NN</cell><cell>0.439</cell><cell>0.522</cell><cell>0.630</cell></row><row><cell>5NN</cell><cell>0.458</cell><cell>0.553</cell><cell>0.653</cell></row><row><cell>6NN</cell><cell>0.490</cell><cell>0.566</cell><cell>0.673</cell></row><row><cell>7NN</cell><cell>0.500</cell><cell>0.610</cell><cell>0.686</cell></row><row><cell>8NN</cell><cell>0.509</cell><cell>0.622</cell><cell>0.698</cell></row><row><cell>9NN</cell><cell>0.522</cell><cell>0.629</cell><cell>0.704</cell></row><row><cell>10NN</cell><cell>0.542</cell><cell>0.643</cell><cell>0.720</cell></row></table></figure>
<figure type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>The outlier detection accuracy of various kNN MP based on 3 high thresholds on the Yahoo dataset ("A2Benchmark-Synthetic") For example, let's say in any particular time series T , there is a labeled outlier in the ground truth at the l th location. Now, we look within the range of [(l -m) to (l + m)] positions in 1N N, 2N N, 3N N, ......, 10N N</figDesc><table><row><cell /><cell /><cell>Accuracies</cell><cell /></row><row><cell>kNN MP</cell><cell>Threshold</cell><cell>Threshold</cell><cell>Threshold</cell></row><row><cell /><cell>(95%)</cell><cell>(90%)</cell><cell>(85%)</cell></row><row><cell>1NN</cell><cell>0.374</cell><cell>0.512</cell><cell>0.648</cell></row><row><cell>2NN</cell><cell>0.606</cell><cell>0.794</cell><cell>0.835</cell></row><row><cell>3NN</cell><cell>0.711</cell><cell>0.850</cell><cell>0.913</cell></row><row><cell>4NN</cell><cell>0.803</cell><cell>0.863</cell><cell>0.934</cell></row><row><cell>5NN</cell><cell>0.857</cell><cell>0.948</cell><cell>0.991</cell></row><row><cell>6NN</cell><cell>0.876</cell><cell>0.963</cell><cell>0.992</cell></row><row><cell>7NN</cell><cell>0.944</cell><cell>0.988</cell><cell>0.997</cell></row><row><cell>8NN</cell><cell>0.978</cell><cell>0.993</cell><cell>0.997</cell></row><row><cell>9NN</cell><cell>0.989</cell><cell>0.992</cell><cell>0.997</cell></row><row><cell>10NN</cell><cell>0.991</cell><cell>0.993</cell><cell>0.997</cell></row><row><cell cols="2">length, used for MP computation.</cell><cell /><cell /></row></table></figure>
<figure type="table" xml:id="tab_8"><head /><label /><figDesc>Figure 11: The execution time for computing kN N MP, with increasing the number k. The time is shown for three different proposed approaches, i.e., sort based (refer to Section 4.2.1), max based (refer to Section 4.2.2 ), and heap-max based (refer to Section 4.2.3 ) (please see the online color version for better visibility). called as seismic dataset, obtained from the domain of seismology, containing 50000 individual time series. The length of each time series is 200. By concatenating all time series, we obtained a big time series of 1 million values. The second dataset is called random walk dataset, having 50000 individual time series. The length of each time series is 256.</figDesc><table><row><cell>800</cell><cell /><cell /><cell /><cell /><cell /><cell /><cell /><cell /><cell /><cell /><cell /><cell /></row><row><cell /><cell cols="5">kNN Heap-Max-Based on RandomWalk Data(50000 * 256)</cell><cell /><cell /><cell /><cell /><cell /><cell /><cell /></row><row><cell /><cell cols="5">kNN Sorting-Based on RandomWalk Data(50000 * 256)</cell><cell /><cell /><cell /><cell /><cell /><cell /><cell /></row><row><cell /><cell cols="4">kNN Max-Based on RandomWalk Data(50000 * 256)</cell><cell /><cell /><cell /><cell /><cell /><cell /><cell /><cell /></row><row><cell>700</cell><cell cols="4">kNN Heap-Max-Based on Seismic Data(50000 * 200)</cell><cell /><cell /><cell /><cell /><cell /><cell /><cell /><cell /></row><row><cell /><cell cols="4">kNN Sorting-Based on Seismic Data(50000 * 200)</cell><cell /><cell /><cell /><cell /><cell /><cell /><cell /><cell /></row><row><cell /><cell cols="4">kNN Max-Based on Seismic Data(50000 * 200)</cell><cell /><cell /><cell /><cell /><cell /><cell /><cell /><cell /></row><row><cell>600</cell><cell /><cell /><cell /><cell /><cell /><cell /><cell /><cell /><cell /><cell /><cell /><cell /></row><row><cell>500</cell><cell /><cell /><cell /><cell /><cell /><cell /><cell /><cell /><cell /><cell /><cell /><cell /></row><row><cell>300 400</cell><cell>Time Needed (sec.)</cell><cell /><cell /><cell /><cell /><cell /><cell /><cell /><cell /><cell /><cell /><cell /></row><row><cell>200</cell><cell /><cell /><cell /><cell /><cell /><cell /><cell /><cell /><cell /><cell /><cell /><cell /></row><row><cell>100</cell><cell /><cell /><cell /><cell /><cell /><cell /><cell /><cell /><cell /><cell /><cell /><cell /></row><row><cell /><cell /><cell /><cell /><cell /><cell /><cell cols="2">k Nearest Neighbors</cell><cell /><cell /><cell /><cell /><cell /></row><row><cell>0</cell><cell>5</cell><cell>10</cell><cell>15</cell><cell>20</cell><cell>25</cell><cell>30</cell><cell>35</cell><cell>40</cell><cell>45</cell><cell>50</cell><cell>55</cell><cell>60</cell></row></table></figure>
<figure type="table" xml:id="tab_9"><head /><label /><figDesc>The execution time of our proposed algorithm and that of STOMP on random-walk, seismic, protein and sheep datasets: (a) The variation of execution time with increasing k for generating kN N MP. The colors red and blue represent the plot for random-walk and seismic datasets respectively. (b) The variation of execution time with increasing the length of time series (x-axis is representing the number of individual time series, which are concatenated to generate a single big time series) (please see the online color version for better visibility).</figDesc><table><row><cell>14000 14000</cell><cell /><cell /><cell /></row><row><cell /><cell cols="2">Proposed kNN MP on RandomWalk Data (1000 * 256 = 256,000)</cell><cell /></row><row><cell /><cell cols="2">Adapted STOMP for kNN MP on RandomWalk Data (1000 * 256 = 256,000)</cell><cell /></row><row><cell>12000 12000</cell><cell cols="2">Proposed kNN MP on Seismic Data (1000 * 200 = 200,000) Adapted STOMP for kNN MP on Seismic Data (1000 * 200 = 200,000)</cell><cell /></row><row><cell>2000 4000 6000 8000 10000 4000 6000 8000 10000</cell><cell>Time Needed (sec.) Time Needed (sec.)</cell><cell>Proposed kNN MP on Protien Data (200 * 680 = 2.77 M) Adapted STOMP for kNN MP on Protien Data (200 * 680 = 2.77 M) Proposed kNN MP on Sheep Data (200 * 500 = 8.44 M) Adapted STOMP for kNN MP on Sheep Data (200 * 500 = 8.44 M)</cell><cell /></row><row><cell /><cell /><cell>Sub-sequence length</cell><cell /></row><row><cell cols="3">10 0 100 0 100 200 300 400 500 600 700 800 900 1000 2000 0 1000 2000 3000 4000 5000 6000 7000 Time Needed (sec.) Time Needed (sec.) Proposed kNN MP on RandomWalk Data (no. of time series * 256) 30 50 k Nearest Neighbors (a) 300 500 No. of Time Series Adapted STOMP for kNN MP on RandomWalk Data (no. of time series * 256) Proposed kNN MP on Seismic Data (no. of time series * 200) Adapted STOMP for kNN MP on Seismic Data (no. of time series * 200) (b) 356 456 556 656 (a) 2 7 12 17 22 No. of Cores Proposed kNN MP on RandomWalk Data (no. of time series * 256) 70 700 756 27 Adapted STOMP for kNN MP on RandomWalk Data (no. of time series * 256) 856 Proposed kNN MP on Seismic Data (no. of time series * 200) Figure 12: 256 Adapted STOMP for kNN MP on Seismic Data (no. of time series * 200)</cell><cell>32</cell><cell>956</cell><cell>90 900</cell></row></table><note><p><p><p>algorithm is able to give output of high kN N values with small change in computational time. From the second experiment shown in Fig.</p>12b</p>, it can be seen that the</p></note></figure>
			<note place="foot" n="1" xml:id="foot_0"><p>The source code and the tested datasets are publicly available at: https://sites.google.com/view/knnmatrixprofile/ home</p></note>
			<note place="foot" n="2" xml:id="foot_1"><p>this case is only applicable when we don't have a single big time series, but several small time series in the database. Hence, a big time series is formed by concatenating these small time series</p></note>
			<note place="foot" n="3" xml:id="foot_2"><p>We have shared the code, datasets and instructions in : https://github.com/tanmayGIT/kNN Matrix Profile</p></note>
			<note place="foot" xml:id="foot_3"><p>kNN Matrix Profile</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgment</head><p>We greatly acknowledge the funding from <rs type="funder">Safran Data Analytics Lab</rs>. The authors are grateful to <rs type="institution">Inria Sophia Antipolis -Méditerranée</rs> "Nef" computation cluster for providing resources and support.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="annex">
<div><head>Appendix: I Fast Calculation of Distance</head><p>Mueen et. al proposed a technique, known as Mueen's Algorithm for Similarity Search (<software ContextAttributes="used">MASS</software>) <ref type="bibr" target="#b8">[9]</ref>  <ref type="bibr" target="#b7">[8]</ref> for the fast calculation of z-normalized Euclidean Distance between query subsequence and the subsequence of target time series, by exploiting Fast Fourier Transform (FFT). Let us first explain Algorithm 6 that generates the dot product of a subsequence (q), and the subsequences of a target time series (T ).</p></div>
<div><head>Algorithm 6: SLIDINGDOTPRODUCT</head><p>Input: A query subsequence (q), A target time series (T ) Output: The dot product (qT ) between single query subsequence and all the target subsequences 1 n ← no. of elements in T ; s ← no. of elements in q 2 T a ← double the length of T by appending n number of zeros at the end 3 q a ← reverse the elements in q so that the last element become first and vice versa 4 q ra ← append (2n -m) zeros at the end of q r 5 q raf ← do F F T (q ra ); T af ← do F F T (T a ) 6 M ← elementwise multiplication of q raf and T af // as both q raf and T af are of the same size, so we can easily multiply element to elements 7 qT ← InverseF F T (M ) 8 return P T In Line 4, both vectors q and T are made to be of the same length (see Section Appendix: I.1) by appending the required amount of zeros (2n -m) to the Algorithm 7: <software ContextAttributes="used">MASS</software> (q, µ q , σ q , T , µ T , σ T ) Input: A query subsequence (q), mean of q (µ q ), standard deviation of q (σ q ), Target time series (T ), mean of T (µ T ), standard deviation of T (σ T ) Output: Distance profile (D), Dot product (qT ) 1 qT ← SlidingDotP roducts(q, T ) //see Algorithm 6 2 D ← CalculateDistanceP rof ile(qT, µ q , σ q , µ T , σ T ) // see Equation <ref type="formula">3</ref>3 return qT, D calculate the distance (D i ) is shown below.</p><p>In this Equation <ref type="formula">3</ref>, m is the subsequence length, µ q is the mean of query sequence q, µ Ti is the mean of T i , σ q is the standard deviation of q and σ Ti is the standard deviation of T i .</p></div>
<div><head>Appendix: IV Brief description of STAMP algorithm :</head><p>The Scalable Time Series Anytime MP (<software ContextAttributes="created">STAMP</software>) algorithm, proposed by Yeh et.al <ref type="bibr" target="#b1">[2]</ref> (outlined in Algorithm 8) calculates the closest match (1NN) of every subsequence in a time series T , based on the calculated distance (called as distance profile) between any particular subsequence with all the remaining subsequence in T .</p><p>The pseudo-code is mentioned in Algorithm 8. A for loop is used in Line 6 to chop each subsequence (consider as query for better understanding). Then, a distance vector (Dist cutQuery ) is computed by calculating the distances between all other subsequences in T and the query. In each iteration, the smallest distance and its corresponding index are kept in P T and I T vectors.</p></div>
<div><head>Appendix: IV.1 Two independent time series matching using STOMP</head><p>In Section 4.1.1, we have explained the general principal of STOMP algorithm for the computation of MP for a single time series. In case of two independent time series, the basic STOMP algorithm needs to be marginally modified. The pseudocode of modified STOMP algorithm, named as <software ContextAttributes="created">IndependentSTOMP</software> is mentioned in Algorithm 9. While called, this algorithm calculates the distances from 2 query subsequence until the last query subsequences i.e., (Idxs) (see Line 2) because before calling this algorithm, we have already computed the distances between 1 st query subsequence and target subsequence t. Now let's concentrate on the principal part of STOMP. Remind that the dot product profile (QT ) of any particular subsequence, can be derived from the dot product profile of it's previous subsequence (see <ref type="bibr">Section 4.1.1)</ref>. So, in Algorithm 8: <software ContextAttributes="created">STAMP</software>(T , m) Input: The user given time series T , subsequence length m Output: The MP P T and associated matrix profile index I T 1 n T ← length(T ) // get the no. of elements in T 2 P T ← Initialize this 1D vector with inf 3 I T ← Initialize this 1D vector with zeros 4 Idxs ← (n T -m + 1) // total number of possible subsequences 5 µ T , σ T ← ComputeM eanStd(T ) // see Equation <ref type="formula">2</ref>6 for i ← 1 to Idxs do 7 cutQuery ← T [i to (i + m -1)] // get query subsequence by chopping T from index i to </p><p>the distance profile 6 return Dist cutQuery and QT // return the Dist cutQuery and QT array Line 3 we repetitively calculate the dot product profile between each query subsequence (from 2 nd subsequence onward) and the target subsequence (t). The 1 st term i.e. (QT [j -1]) at the right hand side of Line 3 holds the dot product profile of the previous target subsequence (in comparison with t) and j th query subsequence. The Q[j -1] in 2 nd term represents the 1 st element of the previous query subsequence</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Matrix Profile {III:} The Matrix Profile Allows Visualization of Salient Subsequences in Massive Time Series</title>
		<author>
			<persName><forename type="first">C.-C</forename><forename type="middle">M</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">V</forename><surname>Herle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Keogh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Data Mining (ICDM)</title>
		<meeting>the International Conference on Data Mining (ICDM)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="579" to="588" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Time series joins, motifs, discords and shapelets: a unifying view that exploits the matrix profile</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C M</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ulanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Begum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">A</forename><surname>Dau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zimmerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">F</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mueen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Keogh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="83" to="123" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Matrix Profile XI: SCRIMP++: Time Series Motif Discovery at Interactive Speeds</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-C</forename><forename type="middle">M</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zimmerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kamgar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Keogh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Data Mining (ICDM)</title>
		<meeting>the International Conference on Data Mining (ICDM)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="837" to="846" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Discriminative motifs</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sinha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth Annual International Conference on Computational Biology</title>
		<meeting>the Sixth Annual International Conference on Computational Biology</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="291" to="298" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Discovering Multidimensional Motifs in Physiological Signals for Personalized Healthcare</title>
		<author>
			<persName><forename type="first">A</forename><surname>Balasubramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Prabhakaran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Sel. Topics Signal Processing</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="832" to="841" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">ParCorr: efficient parallel methods to identify similar time series pairs across sliding windows</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Yagoubi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Akbarinia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kolev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Levchenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Masseglia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Valduriez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Shasha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Mining and Knowledge Discovery</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1481" to="1507" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Searching and mining trillions of time series subsequences under dynamic time warping</title>
		<author>
			<persName><forename type="first">T</forename><surname>Rakthanmanon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">J L</forename><surname>Campana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mueen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E A P A</forename><surname>Batista</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">B</forename><surname>Westover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zakaria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Keogh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<editor>
			<persName><forename type="first">Q</forename><surname>Yang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Agarwal</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Pei</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="262" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Logical-shapelets: an expressive primitive for time series classification</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mueen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Keogh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">E</forename><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Apté</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Ghosh</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Smyth</surname></persName>
		</editor>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">August 21-24, 2011. 2011</date>
			<biblScope unit="page" from="1154" to="1162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Time series join on subsequence correlation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mueen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hamooni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Estrada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Data Mining, ICDM 2014</title>
		<editor>
			<persName><forename type="first">R</forename><surname>Kumar</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Toivonen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Pei</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">Z</forename><surname>Huang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</editor>
		<meeting><address><addrLine>Shenzhen, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">December 14-17, 2014. 2014</date>
			<biblScope unit="page" from="450" to="459" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Matrix profile I: all pairs similarity joins for time series: A unifying view that includes motifs, discords and shapelets</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ulanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Begum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">A</forename><surname>Dau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">F</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mueen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Keogh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 16th International Conference on Data Mining, ICDM 2016</title>
		<editor>
			<persName><forename type="first">F</forename><surname>Bonchi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Domingo-Ferrer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Baeza-Yates</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</editor>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">December 12-15, 2016. 2016</date>
			<biblScope unit="page" from="1317" to="1322" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Matrix profile II: exploiting a novel algorithm and gpus to break the one hundred million barrier for time series motifs and joins</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zimmerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">S</forename><surname>Senobari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Funning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mueen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Brisk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Keogh</surname></persName>
		</author>
		<imprint />
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">F</forename><surname>Bonchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Domingo-Ferrer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Baeza-Yates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<title level="m">IEEE 16th International Conference on Data Mining, ICDM 2016</title>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">December 12-15, 2016. 2016</date>
			<biblScope unit="page" from="739" to="748" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Matrix Profile {XVII:} Indexing the Matrix Profile to Allow Arbitrary Range Queries</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-C</forename><forename type="middle">C M</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zimmerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Keogh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Data Engineering (ICDE)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1846" to="1849" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">MERLIN: parameterfree discovery of arbitrary length anomalies in massive time series archives</title>
		<author>
			<persName><forename type="first">T</forename><surname>Nakamura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Imamura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mercer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Keogh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">20th IEEE International Conference on Data Mining, ICDM 2020</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Plant</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Cuzzocrea</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Zaniolo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</editor>
		<meeting><address><addrLine>Sorrento, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">November 17-20, 2020. 2020</date>
			<biblScope unit="page" from="1190" to="1195" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Matrix profile XXIII: contrast profile: A novel time series primitive that allows real world classification</title>
		<author>
			<persName><forename type="first">R</forename><surname>Mercer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Alaee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Abdoli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Murillo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Keogh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Data Mining, ICDM 2021</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Bailey</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Miettinen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><forename type="middle">S</forename><surname>Koh</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</editor>
		<meeting><address><addrLine>Auckland, New Zealand</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">December 7-10, 2021. 2021</date>
			<biblScope unit="page" from="1240" to="1245" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Matrix profile XIV: scaling time series motif discovery with gpus to break a quintillion pairwise comparisons a day and beyond</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zimmerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kamgar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">S</forename><surname>Senobari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Crites</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Funning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Brisk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Keogh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM Symposium on Cloud Computing, SoCC 2019</title>
		<meeting>the ACM Symposium on Cloud Computing, SoCC 2019<address><addrLine>Santa Cruz, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">November 20-23, 2019. 2019</date>
			<biblScope unit="page" from="74" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Breaking computational barriers to perform time series pattern mining at scale and at the edge</title>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">P</forename><surname>Zimmerman</surname></persName>
		</author>
		<ptr target="https://escholarship.org/content/qt51z7d647/qt51z7d647.pdf" />
		<imprint>
			<date type="published" when="2019">2019</date>
			<pubPlace>Riverside</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of California</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Neighbor profile: Bagging nearest neighbors for unsupervised time series mining</title>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">36th IEEE International Conference on Data Engineering, ICDE 2020</title>
		<meeting><address><addrLine>Dallas, TX, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">April 20-24, 2020. 2020</date>
			<biblScope unit="page" from="373" to="384" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Matrix Profile {II:} Exploiting a Novel Algorithm and GPUs to Break the One Hundred Million Barrier for Time Series Motifs and Joins</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zimmerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">S</forename><surname>Senobari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-C</forename><forename type="middle">M</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Funning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mueen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Brisk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Keogh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Data Mining (ICDM)</title>
		<meeting>the International Conference on Data Mining (ICDM)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="739" to="748" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">Hoang</forename><surname>Dau</surname></persName>
		</author>
		<author>
			<persName><surname>Anh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eamonn</forename><surname>Keogh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaveh</forename><surname>Kamgar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chin-Chia</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaghayegh</forename><surname>Gharghabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ratanamahatana</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Chotirat</forename><surname>Ann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanping</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Begum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nurjahan</forename><surname>Bagnall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Mueen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abdullah</forename><surname>Batista</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gustavo</forename></persName>
		</author>
		<ptr target="https://www.cs.ucr.edu/∼eamonn/timeseriesdata2018/" />
		<title level="m">The UCR Time Series Classification Archive</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">Laptev</forename><surname>Nikolay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amizadeh</forename><surname>Saeed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Billawala</forename><surname>Youssef</surname></persName>
		</author>
		<ptr target="https://yahooresearch.tumblr.com/post/114590420346/a-benchmark-dataset-for-time-series-anomaly" />
		<title level="m">A Benchmark Dataset for Time Series Anomaly Detection</title>
		<imprint />
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>