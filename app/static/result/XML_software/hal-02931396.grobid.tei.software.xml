<TEI xmlns="http://www.tei-c.org/ns/1.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xml:space="preserve" xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Classifying Candidate Axioms via Dimensionality Reduction Techniques</title>
				<funder ref="#_Tm2jHY8">
					<orgName type="full">Agence Nationale de la Recherche</orgName>
					<orgName type="abbreviated">ANR</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher />
				<availability status="unknown"><licence /></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Dario</forename><surname>Malchiodi</surname></persName>
							<email>dario.malchiodi@unimi.it</email>
							<affiliation key="aff0">
								<orgName type="department">Dipartimento di Informatica</orgName>
								<orgName type="institution">Università degli Studi di Milano</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Célia</forename><surname>-7574-697x]</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Université Côte d'Azur</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<address>
									<region>I3S</region>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><surname>Da Costa Pereira</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Andrea</forename><forename type="middle">G B</forename><surname>Tettamanzi</surname></persName>
							<email>andrea.tettamanzi@unice.fr</email>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Université Côte d'Azur</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<address>
									<region>I3S</region>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Classifying Candidate Axioms via Dimensionality Reduction Techniques</title>
					</analytic>
					<monogr>
						<imprint>
							<date />
						</imprint>
					</monogr>
					<idno type="MD5">CE7D573CC8E396CF4D3BA8518B0D49C9</idno>
					<idno type="DOI">10.1007/978-3-030-</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-04-12T14:50+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid" />
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Possibilistic Axiom Scoring • Dimensionality reduction</keywords>
			</textClass>
			<abstract>
<div><p>We assess the role of similarity measures and learning methods in classifying candidate axioms for automated schema induction through kernel-based learning algorithms. The evaluation is based on (i) three different similarity measures between axioms, and (ii) two alternative dimensionality reduction techniques to check the extent to which the considered similarities allow to separate true axioms from false axioms. The result of the dimensionality reduction process is subsequently fed to several learning algorithms, comparing the accuracy of all combinations of similarity, dimensionality reduction technique, and classification method. As a result, it is observed that it is not necessary to use sophisticated semantics-based similarity measures to obtain accurate predictions, and furthermore that classification performance only marginally depends on the choice of the learning method. Our results open the way to implementing efficient surrogate models for axiom scoring to speed up ontology learning and schema induction methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div><head n="1">Introduction</head><p>Among the various tasks relevant to ontology learning <ref type="bibr" target="#b9">[10]</ref>, schema enrichment is a hot field of research, due to the ever-increasing amount of Linked Data published on the semantic Web. Its goal is to extract schema axioms from existing ontologies (typically expressed in OWL) and instance data (typically represented in RDF) <ref type="bibr" target="#b6">[7]</ref>. To this aim, induction-based methods akin to inductive logic programming and data mining are exploited. They range from using statistical schema induction to enrich the schema of an RDF dataset with property axioms <ref type="bibr" target="#b4">[5]</ref> or of the DBpedia ontology <ref type="bibr" target="#b24">[24]</ref> to learning dataypes within ontologies <ref type="bibr" target="#b5">[6]</ref> or even developing light-weight methods to create a schema of any knowledge base accessible via SPARQL endpoints with almost all types of OWL axioms <ref type="bibr" target="#b2">[3]</ref>.</p><p>All these approaches critically rely on (candidate) axiom scoring. In practice, testing an axiom boils down to computing an acceptability score, measuring the extent to which the axiom is compatible with the recorded facts. Methods to approximate the semantics of given types of axioms have been thoroughly investigated in the last decade (e.g., approximate subsumption <ref type="bibr" target="#b20">[20]</ref>) and some related heuristics have been proposed to score concept definitions in concept learning algorithms <ref type="bibr" target="#b19">[19]</ref>. The most popular candidate axiom scoring heuristics proposed in the literature are based on statistical inference (see, e.g., <ref type="bibr" target="#b2">[3]</ref>), but alternative heuristics based on possibility theory have also been proposed <ref type="bibr" target="#b22">[22]</ref>. While it appears that these latter may lead to more accurate results, their heavy computational cost makes it hard to apply them in practice. However, a promising alternative to their direct computation is to train a surrogate model on a sample of candidate axioms for which the score is already available, to learn to predict the score of a novel, unseen candidate axiom.</p><p>In <ref type="bibr" target="#b10">[11]</ref>, two of us proposed a semantics-based similarity measure to train surrogate models based on kernel methods. However, a doubt remained whether the successful training of the surrogate model really depended on the choice of such a measure (and not, for example, on the choice of the learning method). Furthermore, it was not clear if a similarity measure has to capture the semantics of axioms to give satisfactory results or if any similarity measure satisfying some minimal requirements would work equally well. The goal of this paper is, therefore, to shed some light on these issues.</p><p>To this aim, we (i) introduce three different syntax-based measures, having an increasing degree of problem-awareness, computing the similarity/distance between axioms; (ii) starting from these measures, we use two alternative kernelbased dimensionality reduction techniques to check how well the images of true and false axioms are separated; and (iii) we apply a number of supervised machine learning algorithms to these images in order to learn how to classify axioms. This allows us to determine the combinations of similarity measure, dimensionality reduction technique, and classification method giving the most accurate results. It should be stressed that we do not address here the broader topic of how axiom scoring should be used to perform knowledge base enrichment, axiom discovery, or ontology learning. Here, we focus specifically on the problem of learning good surrogate models of an axiom scoring heuristics whose exact computation has proven to be extremely expensive <ref type="bibr" target="#b21">[21]</ref>.</p></div>
<div><head n="2">The Hypothesis Language</head><p>The hypotheses we are interested in classifying as true or false are OWL 2 axioms, expressed in functional-style syntax <ref type="bibr" target="#b13">[14]</ref>, and their negations. In particular, we focus on subsumption axioms, whose syntax is described by the production: Axiom := 'SubClassOf' '(' ClassExpression ' ' ClassExpression ')'. Although a ClassExpression can be quite complicated, according to the full OWL syntax, here, for the sake of simplicity, we will restrict ourselves to class expressions consisting of just one atomic class, represented by its internationalized resource identifier (IRI), possibly abbreviated as in SPARQL, i.e., as a prefix, followed by the class name, like dbo:Country. The formulas we consider are thus described by the production Formula := Axiom | '-' Axiom. If a formula consists of an axiom (the first alternative in the above production) we will say it is positive; otherwise (the second alternative), we will say it is negative. If φ is a formula, we define sgn(φ) = 1 if φ is positive, -1 otherwise. Alongside the "sign" of a formula, it is also convenient to define a notation for the OWL axiom from which the formula is built, i.e., what remains if one removes a minus sign prepended to it: abs(φ) = φ if φ is positive, ψ if φ = -ψ, with ψ an axiom.</p><p>Because the OWL syntax tends to be quite verbose, when possible we will write axioms in description logic (DL) notation. In such notation, a subsumption axiom has the form A B, where A and B are two class expressions. For its negation, we will write ¬(A B), although this would not be legal DL syntax. <ref type="foot" target="#foot_0">3</ref>With this DL notation, the definition of abs(φ) can be rewritten as follows:</p><formula xml:id="formula_0">abs(φ) = φ if sgn(φ) = 1, ¬φ if sgn(φ) = -1.</formula><p>We will denote with [C] = {a : C(a)} the extension of an OWL class C in an RDF dataset, and with E the cardinality of set E.</p><p>In particular, the dataset we processed has been built considering 722 formulas, together with their logical negations, thus gathering a total of 1444 elements. Each item is associated to a score inspired by possibility theory <ref type="bibr" target="#b3">[4]</ref>, called acceptance-rejection index (ARI for short), introduced in <ref type="bibr" target="#b10">[11]</ref> and numerically summarizing the suitability of that formula as an axiom within a knowledge base expressed through RDF. More precisely, the ARI of a formula φ has been defined as the combination of the possibility and necessity measures of φ as follows:</p><formula xml:id="formula_1">ARI(φ) = Π(φ) -Π(¬φ) ∈ [-1, 1]</formula><p>, so that a negative ARI(φ) suggests rejection of φ (Π(φ) &lt; 1), whilst a positive ARI(φ) suggests its acceptance (N (φ) &gt; 0), <ref type="foot" target="#foot_1">4</ref>with a strength proportional to its absolute value. A value close to zero reflects ignorance about the status of φ. For all φ, ARI(¬φ) = -ARI(φ).</p><p>The 722 formulas of our dataset were exactly scored against DBpedia, which required a little less than 290 days of CPU time on quite a powerful machine <ref type="bibr" target="#b23">[23]</ref>.</p></div>
<div><head n="3">Formula Translation</head><p>Among the landscape of dimensionality reduction techniques (see for instance <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b17">17]</ref>), formulas have been processed using (i) kernel-based Principal Component Analysis (PCA) <ref type="bibr" target="#b18">[18]</ref> and (ii) t-distributed Stochasting Neighbor Embedding (t-SNE) <ref type="bibr" target="#b8">[9]</ref>. The first technique applies standard PCA <ref type="bibr" target="#b15">[15]</ref> to data nonlinarly mapped onto a higher-dimensional space. On the other hand, t-SNE describes data using a probability distribution linked to a similarity measure, minimizing its Kullback-Leibler divergence with an analogous distribution in map space. We did not use such techniques to reduce data dimensionality <ref type="bibr" target="#b25">[25]</ref>, but as a way to map formulas into R d , for arbitrary choices of d, based on the similarity mea- sures detailed below. The result of this mapping is considered as an intermediate representation to be further processed as explained in the rest of the paper.</p><p>Length-based similarity This similarity is obtained by comparing the textual representation length of two formulas φ 1 and φ 2 as follows:</p><formula xml:id="formula_2">s len (φ 1 , φ 2 ) = 1 - |#φ 1 -#φ 2 | max{#φ 1 , #φ 2 } ,<label>(1)</label></formula><p>where #φ denotes the length of the textual representation of φ. Normalization is required in order to transform the distance |#φ 1 -#φ 2 | into a similarity. <ref type="foot" target="#foot_2">5</ref>Such measure, however, cannot be reasonably expected to be able to capture meaningful information, as it merely relies on string length. For instance, two formulas whose formal description have equal length will always exhibit maximal similarity, regardless of the concepts they express. Nonetheless, such similarity has been included as a sort of litmus test. Anyhow, (1) is excessively naive, for it would assign quasi-maximal similarity to a formula and its logical negation: indeed, their descriptions would differ only for a trailing minus character. This inconvenience can be easily fixed by complementing to 1 the above definition Fig. <ref type="figure">3</ref>. Scatter plots of the output of PCA, followed by t-SNE. Same notations as in Figure <ref type="figure">1</ref>.</p><p>when the signs of the two formulas are opposed and limiting the comparison to the base axiom only:</p><formula xml:id="formula_3">sim len (φ 1 , φ 2 ) = 1 -s len (abs(φ 1 ), abs(φ 2 )) if sgn(φ 1 ) = sgn(φ 2 ), s len (φ 1 , φ 2 ) otherwise. (<label>2</label></formula><formula xml:id="formula_4">)</formula><p>This basically ensures, as one would intuitively expect, that the similarity between a formula and its negation be zero.</p><p>Hamming similarity Despite its simplicity, length-based similarity catches very simple instances of syntactical similarities (e.g. when identical-or almost identical-formulas only differ from synonymous class names); in this respect, it is reasonable to obtain better results if the distance is defined in a smarter way. This can be achieved for instance considering sim H (φ 1 , φ 2 ) = H(abs(φ 1 ), abs(φ 2 )), if sgn(φ 1 ) = sgn(φ 2 ), 1 -H(φ 1 , φ 2 ) otherwise, where H is the normalized Hamming distance between the textual representations of two formulas, that is the fraction of positions where such strings contain a different character, aligning them on the left and ignoring the extra characters of the longest string.</p></div>
<div><head>Levenshtein similarity</head><p>We can aim at grasping more sophisticated forms of syntactical similarities, and even simple semantic similarities. To such extent, we considered sim edit (φ 1 , φ 2 ) = Lev(abs(φ 1 ), abs(φ 2 )) if sgn(φ 1 ) = sgn(φ 2 ), 1 -Lev(φ 1 , φ 2 ) otherwise, where Lev is the normalized Levenshtein distance <ref type="bibr" target="#b7">[8]</ref> between strings representing two formulas, intended as the smallest number of atomic operations allowing to transform one string into the other one.</p><p>Jaccard similarity Semantics heavily depends on context. This is why we also consider the similarity measure introduced in <ref type="bibr" target="#b10">[11]</ref> exploiting notation of Sect. 1:</p><formula xml:id="formula_5">sim J (φ 1 , φ 2 ) = [A] ∩ [B] ∪ [C] ∩ [D] [A] ∪ [C] ,<label>(3)</label></formula><p>where φ 1 is the subsumption A B, and φ 2 is C D. Note that among all the considered similarities, this is the only one taking into account both the specific form of the formulas (subsumptions) and their meaning within a dataset. It has been termed Jaccard similarity because (3) racalls the Jaccard similarity index. Figures <ref type="figure">1</ref> and<ref type="figure" target="#fig_2">2</ref> show the obtained set of points in R 2 when applying t-SNE and kernel PCA, coupled with the above similarities, to the considered subsumption formulas <ref type="foot" target="#foot_3">6</ref> . In each plot, + and -denote the images of positive and negative formulas in R 2 through the reduction technique; the color of each bullet reflects the ARI associated to a formula (cf. Sect. 2), with minimal and maximal values being mapped onto dark and light gray. Figure <ref type="figure">3</ref> shows the same scatter plots when PCA and t-SNE are applied in chain, as commonly advised <ref type="bibr" target="#b8">[9]</ref>. In particular, not having here an explicit dimension for the original space of formulas, we extracted 300 principal components via PCA and considered the cumulative fraction of explained variance (EV) for each of the extracted components, after the latter were sorted by nondecreasing EV value. In such cases, the number of components to be considered is the one explaining a fixed fraction of the total variance, and we set such fraction to 75%, which led us to roughly 150 components, to be further reduced onto R 2 via t-SNE.</p><p>Recalling that our final aim is to tell "good" formulas from "bad" ones, where "good" means a high ARI, i.e., light gray symbols in the scatter plots, the obtained results highlight the following facts.</p><p>-In all sim len plots, light and dark bullets tend to heavily overlap, thus confirming that sim len is not a suitable choice for our classification purposes. -A similar behavior is generally exhibited by PCA; however, shapes in Leaving apart the length-based similarity and the sole kernel PCA technique, we are left with six possibilities, which are hard to rank via qualitative judgment only. We can only remark that sim H tends to generate plots where the two classes do not appear as sharply distinguished as in the remaining cases. However, this criterion is too weak and, therefore, a quantitative approach is needed. This is why we considered the radius statistics of the clusters of points in R 2 referring to a same concept. More precisely:</p><p>each concept is identified by an OWL class (such as for instance Coach, TennisLeague, Eukaryote, and so on); given a concept, we identify all positive formulas having it as antecedent, <ref type="foot" target="#foot_4">7</ref>and consider the positive cluster of corresponding points in R 2 ; we proceed analogously for the negative cluster ; we subsequently obtain the centroids of the two clusters and compute their Euclidean distance; moreover, we consider the population of distances between each point in a cluster and the corresponding centroid. Such population is described in terms of basic descriptive indices (namely, maximum, mean and standard deviation, median and interquartile range (IQR)).</p><p>Thus, for each combination of reduction technique, similarity measure, and concept, we obtain a set of measurements. In order to reduce this information to a manageable size, we average all indices across concepts: Tables 1-3 summarize the results, for which we propose the following interpretation:</p><p>the use of kernel PCA results in loosely decoupled clusters: indeed, independently of the considered similarity measure, the distance between positive and negative formulas is generally smaller than the sum of the radii of the corresponding clusters (identified with the max index); moreover, all measured indices tend to take up similar values; as already found out in the qualitative analysis of Figures <ref type="figure">1</ref><ref type="figure" target="#fig_2">2</ref><ref type="figure">3</ref>, a weakness analogous to that illustrated in the previous point tends to be associated to sim len , regardless of the chosen reduction technique;</p><p>quite suprisingly, sim H is the only similarity measure which results in sharply distinguished clusters when coupled with t-SNE.</p></div>
<div><head n="4">Learning ARI</head><p>As all considered formulas have been mapped to points in a Euclidean space, it is now easy to use the images of the considered mappings as patterns to be fed to a supervised learning algorithm, whose labels are the ARI of the relevant formulas. More precisely, we considered the following models (and, specifically, their implementation in <software ContextAttributes="used">scikit</software>-learn <ref type="bibr" target="#b16">[16]</ref>), each described together with the hyperparameters involved in the model selection phase:</p><p>-Decision trees (DT), parameterized on purity index, maximal number of leaves, maximal number of features, and maximal tree depth; -Random forests (RF), parameterized on the number of trees, as well as on the same quantities as DT; -Naive Bayes with Gaussian priors (NB), without hyperparameters; -Linear Discrimimant Analysis (LDA), without hyperparameters; -Three-layered feed-forward neural networks (NN), parameterized on the number of hidden neurons; -Support vector classifiers (SVC), parameterized on the tradeoff constant and on the used kernel.</p><p>When applicable, model selection was carried out via a repeated holdout consisting of five iterations, each processing a grid of the above-mentioned values for hyperparameters, shuffling available data, and considering a split assigning 80%, 10%, and 10% of points, respectively, to train, validation, and test. When model selection was not involved, respectively 80% and 20% of the available examples where assigned to training and test.</p><p>In order to build examples to be fed to learning algorithms, each formula needed to be associated with a binary label. That was done through binarization of the ARI values, using a threshold α ∈ {i/10 for i = 1, . . . , 9}. More precisely, an experiment was carried out for each of the possible thresholding levels. Finally, the transformation of formulas into vectors was done by considering different values for the dimension d of the resulting Euclidean space, namely the values in {2, 3, 5, 10, 30} were tested. Summing up, for each reduction technique R (that is, either PCA or t-SNE coupled with any similarity measure) and considered dimension d, the following holdout protocol was iterated ten times:</p><p>1. R was applied to the available data in order to obtain a set of vectors in R d ; 2. for each model M such vectors were randomly shuffled and subsequently divided into training, validation (if needed), and test set; 3. for each threshold value α, a model selection for M using the data in training and validation set was carried out, testing the selected model against generalization using the test set; 4. the model whose value of α gave the best generalization was retained. Table <ref type="table" target="#tab_3">4</ref> shows the results obtained when using kernel PCA with Jaccard similarity. For each model, the optimal value of α is shown, as well as the estimated generalization capability, in terms of mean and median accuracy on the test set. Tables <ref type="table" target="#tab_4">5</ref> and<ref type="table" target="#tab_5">6</ref> show a more compact representation of the corresponding results when considering the possible combinations of learning algorithm, similarity measure, reduction technique, and dimension of the space onto which formulas are mapped. Because, as expected, the length-based distance always gave the worst results, it has been omitted from the tables to save space. The same applies to the results obtained when setting d = 30, which in this case did not give any significant improvement w.r.t. the figures for d = 10.</p><p>Looking at these results, it is immediately clear that SVC is systematically outperformed by the remaining models, and the results it obtains are independent of the combination of reduction technique, similarity measure, and space dimension. Moreover, the best results (highlighted using boldface in the tables) are almost always higher when using kernel PCA, thus somehow in contrast with the preliminary results obtained in Sect. <ref type="bibr" target="#b2">3</ref>. In order to further analyze this trend, Figure <ref type="figure" target="#fig_3">4</ref> graphically shows a subset of the obtained results, where SVC is left out. The graphs highlight that the space dimension d has a weak influence on the results, and that sim len always attains a low score, thus it has been excluded from the rest of the analysis. Experiments can be divided into three groups:</p><p>all combinations of LDA with sim H , getting a bottom-line performance, a majority of cases where median accuracy lays roughly between 60 and 70%, notably containing all remaining results based on t-SNE, and a top set of experiments with median accuracy &gt; 80%, including all those involving kernel PCA with sim J and any learning algorithm but DT.</p><p>In order to verify that there is a significant difference between the experiments in these groups, we executed the Cramér-Von Mises (CVM) test <ref type="bibr" target="#b1">[2]</ref>. For the sake of conciseness, we will call a case any combination of learning algorithm, dimensionality reduction technique, dimension of the resulting space, and similarity measure (excluding, as previously stated, SVC and sim len ). For each case, we repeated the experiments ten times, thus getting a sample of ten values for the median test set error. We then gathered cases in the following categories:</p><p>G top PCA , G mid PCA , G btm PCA : respectively the best, middle, and low performing cases using PCA (corresponding to the three items in the previous list); G tSNE : all cases using tSNE, i.e. those shown in Figure <ref type="figure" target="#fig_3">4</ref>(a). Now, given a generic pair of cases taken from G top PCA and G mid PCA , the hypothesis that the corresponding test error samples are drawn from the same distribution is strongly rejected by the CVM test, with p &lt; 0.01. The same happens when considering any other pair of groups related to PCA, and these results do not change if G mid PCA is swapped with G tSNE . On the other hand, the same test run on two cases from G top PCA didn't reject the same hypothesis in 81% of the times.</p></div>
<div><head n="5">Conclusions</head><p>Our results show that it is possible to learn reasonably accurate predictors of the score of candidate axioms without having to resort to sophisticated (and expensive-to-compute) semantics-based similarity measures. Indeed, we showed that using a dimensionality-reduction technique to map candidate axioms to a low-dimension space allows supervised learning algorithms to effectively train predictive models of the axiom score. This agrees with the observation that there are no obvious indicators to inform the decision to choose between a cheap or expensive similarity measure based on the properties of an ontology <ref type="bibr" target="#b0">[1]</ref>.</p><p>Our findings can contribute to dramatically speed up ontology-learning approaches based on exhaustive or stochastic generate-and-test strategies, like <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b11">12]</ref>. As a further development, we plan to apply to the same dataset more sophisticated techniques able to learn the ARI without the need of binarizing labels.</p></div><figure xml:id="fig_0"><head>Fig. 1 .Fig. 2 .</head><label>12</label><figDesc>Fig. 1. t-SNE-based scatter plots for the considered similarity measures. Positive and negative formulas are marked using + and -, respectively, using a gray shade reflecting ARI (dark shades for low ARI and vice versa.)</figDesc><graphic coords="5,137.07,269.42,83.00,79.21" type="bitmap" /></figure>
<figure xml:id="fig_1"><head /><label /><figDesc>(a) sim len (b) simH (c) sim edit (d) simJ</figDesc></figure>
<figure xml:id="fig_2"><head>Fig- ures 2</head><label>2</label><figDesc>(a) and 2(d) strongly recall the projection onto R 2 of a saddle-shaped manifold, thus suggesting a low-dimensional intrinsic dimensionality.</figDesc></figure>
<figure xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Comparison of the more significative results in learning ARI when using t-SNE (a) and PCA (b). Rows are associated to learning algorithms, using the abbreviations introduced in Sect. 4, while columns refer to similarity measures (E: sim edit , H: simH, L: sim len , J: simJ). Each graph plots test set accuracy vs. number of dimensions.</figDesc><graphic coords="11,164.74,115.83,138.33,208.55" type="bitmap" /></figure>
<figure type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Radius statistics for PCA-based clusters. Rows: similarity measures; Dist: average Euclidean distance between postivie and negative clusters; Max, m, IQR, µ, and σ: maximum, median, interquartile range, mean, and standard deviation for cluster radius. Indices are computed separately for positive and negative formulas and shown columns marked with + and -.</figDesc><table><row><cell>Dist Max +</cell><cell>m + IQR +</cell><cell>µ + σ + Max -</cell><cell>m -IQR -</cell><cell>µ -σ -</cell></row><row><cell cols="5">sim len 0.05 5.0e-1 1.7e-1 0.26 2.0e-1 0.16 5.0e-1 1.7e-1 0.25 2.0e-1 0.16</cell></row><row><cell cols="5">simH 0.45 8.2e-2 3.3e-2 0.03 3.3e-2 0.02 8.2e-2 3.2e-2 0.03 3.3e-2 0.02</cell></row><row><cell cols="5">sim edit 0.41 3.1e-1 1.1e-1 0.14 1.2e-1 0.09 3.0e-1 1.0e-1 0.14 1.1e-1 0.09</cell></row><row><cell cols="5">simJ 0.30 5.2e-1 2.3e-2 0.39 1.8e-1 0.23 5.2e-1 2.3e-2 0.39 1.8e-1 0.23</cell></row></table></figure>
<figure type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Radius statistics for clusters of points obtained through t-SNE. Same notations as in Table1.</figDesc><table><row><cell>Dist Max +</cell><cell>m + IQR +</cell><cell>µ +</cell><cell>σ + Max -</cell><cell>m -IQR -</cell><cell>µ -σ -</cell></row><row><cell cols="6">sim len 15.80 7.8e+1 3.4e+1 36.62 3.4e+1 23.12 8.0e+1 3.7e+1 37.35 3.6e+1 24.01</cell></row><row><cell cols="6">simH 60.29 1.2e+1 3.0e+0 7.10 4.4e+0 3.98 1.0e+1 2.1e+0 5.26 3.4e+0 3.23</cell></row><row><cell cols="6">sim edit 57.66 2.7e+1 8.3e+0 11.42 9.6e+0 7.62 2.6e+1 7.6e+0 9.31 8.9e+0 6.96</cell></row><row><cell cols="6">simJ 2.33 3.8e+0 9.1e-1 2.75 1.5e+0 1.44 3.3e+0 6.3e-1 2.54 1.3e+0 1.33</cell></row></table></figure>
<figure type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Radius statistics for clusters of points obtained chaining kernel PCA and t-SNE. Same notations as in Table1.</figDesc><table><row><cell>Dist Max +</cell><cell>m + IQR +</cell><cell>µ + σ + Max -</cell><cell>m -IQR -</cell><cell>µ -σ -</cell></row><row><cell cols="5">sim len 17.76 7.9e+1 3.5e+1 36.68 3.5e+1 3.65 8.0e+1 3.7e+1 37.87 3.7e+1 24.45</cell></row><row><cell cols="5">simH 56.19 3.3e+0 1.1e+0 1.51 1.3e+0 0.91 3.1e+0 1.1e+0 1.41 1.2e+0 0.87</cell></row><row><cell cols="5">sim edit 60.56 3.1e+1 1.0e+1 14.28 1.2e+1 9.22 3.2e+1 1.1e+1 13.97 1.2e+1 9.45</cell></row><row><cell cols="5">simJ 13.02 2.3e+1 1.3e+0 16.47 7.8e+0 9.72 2.6e+1 1.4e+0 16.46 7.9e+0 10.05</cell></row></table></figure>
<figure type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Test set accuracy for the considered learning algorithms, when fed with points in R 2 representing the original formulas transformed through kernel PCA using simJ.</figDesc><table><row><cell>Model</cell><cell cols="3">Test accuracy α mean median</cell><cell>σ</cell><cell>Model</cell><cell cols="3">Test accuracy α mean median</cell><cell>σ</cell></row><row><cell>DT</cell><cell>0.8</cell><cell>0.71</cell><cell cols="2">0.64 0.12</cell><cell cols="2">LDA 0.9</cell><cell>0.82</cell><cell>0.82 0.01</cell></row><row><cell>RF</cell><cell>0.8</cell><cell>0.84</cell><cell cols="2">0.83 0.01</cell><cell cols="2">MLP 0.2</cell><cell>0.83</cell><cell>0.82 0.02</cell></row><row><cell>NB</cell><cell>0.9</cell><cell>0.82</cell><cell cols="2">0.82 0.00</cell><cell>SVC</cell><cell>0.9</cell><cell>0.60</cell><cell>0.60 0.00</cell></row></table></figure>
<figure type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>Comparison between test set median accuracy of algorithms using PCA. Rows: models; columns: similarity; d: space dimension.</figDesc><table><row><cell /><cell>d = 2</cell><cell>d = 3</cell><cell>d = 5</cell><cell cols="2">d = 10</cell></row><row><cell>H</cell><cell>J edit H</cell><cell>J edit H</cell><cell cols="2">J edit H</cell><cell>J edit</cell></row><row><cell cols="6">DT 0.62 0.64 0.68 0.62 0.60 0.66 0.62 0.60 0.66 0.62 0.64 0.66</cell></row><row><cell cols="6">RF 0.67 0.83 0.67 0.66 0.81 0.70 0.66 0.82 0.68 0.64 0.81 0.68</cell></row><row><cell cols="6">NB 0.62 0.82 0.69 0.63 0.80 0.68 0.63 0.83 0.68 0.62 0.81 0.68</cell></row><row><cell cols="6">LDA 0.52 0.82 0.68 0.52 0.82 0.67 0.52 0.83 0.68 0.53 0.83 0.68</cell></row><row><cell cols="6">MLP 0.62 0.82 0.66 0.61 0.83 0.68 0.63 0.80 0.68 0.61 0.82 0.68</cell></row><row><cell cols="6">SVC 0.60 0.60 0.60 0.60 0.60 0.60 0.60 0.60 0.60 0.60 0.60 0.60</cell></row></table></figure>
<figure type="table" xml:id="tab_5"><head>Table 6 .</head><label>6</label><figDesc>Comparison between algorithms using t-SNE. Same notations as in Table5.</figDesc><table><row><cell /><cell>d = 2</cell><cell /><cell>d = 3</cell><cell>d = 5</cell><cell cols="2">d = 10</cell></row><row><cell>H</cell><cell>J edit</cell><cell>H</cell><cell>J edit H</cell><cell>J edit</cell><cell>H</cell><cell>J edit</cell></row><row><cell cols="7">DT 0.66 0.62 0.62 0.65 0.62 0.60 0.65 0.62 0.60 0.67 0.61 0.60</cell></row><row><cell cols="7">RF 0.64 0.66 0.65 0.66 0.64 0.66 0.65 0.63 0.65 0.66 0.66 0.66</cell></row><row><cell cols="7">NB 0.69 0.65 0.63 0.69 0.66 0.64 0.65 0.65 0.65 0.67 0.65 0.62</cell></row><row><cell cols="7">LDA 0.67 0.65 0.62 0.68 0.68 0.62 0.68 0.67 0.62 0.70 0.68 0.62</cell></row><row><cell cols="7">MLP 0.70 0.66 0.63 0.68 0.60 0.61 0.64 0.57 0.61 0.68 0.58 0.60</cell></row><row><cell cols="7">SVC 0.60 0.60 0.60 0.60 0.60 0.60 0.60 0.60 0.60 0.60 0.60 0.60</cell></row></table></figure>
			<note place="foot" n="3" xml:id="foot_0"><p>It should always be borne in mind that this is just a shorthand notation for the underlying OWL 2 functional-style syntax extended with the "minus" operator as explained above.</p></note>
			<note place="foot" n="4" xml:id="foot_1"><p>We recall that N (φ) = 1 -Π(¬φ) and Π(φ) = 1 -N (¬φ).</p></note>
			<note place="foot" n="5" xml:id="foot_2"><p>The used implementations of t-SNE and PCA in <software ContextAttributes="used">scikit</software>-learn<ref type="bibr" target="#b16">[16]</ref> accept, respectively, a distance and a similarity matrix. Thus we normalized all similarities.</p></note>
			<note place="foot" n="6" xml:id="foot_3"><p>Code and data to replicate all experiments described in the paper is available at https://github.com/dariomalchiodi/MDAI2020.</p></note>
			<note place="foot" n="7" xml:id="foot_4"><p>We also repeated all experiments considering both antecedents and consequents, obtaining comparable results.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>Part of this work was done while <rs type="person">D. Malchiodi</rs> was visiting scientist at <rs type="institution">Inria Sophia-Antipolis/I3S CNRS Université Côte d'Azur</rs>. This work has been supported by the <rs type="funder">French government</rs>, through the <rs type="programName">3IA Côte dAzur "Investments in the Future</rs>" project of the <rs type="projectName">Nat'l Research Agency</rs>, ref. no. <rs type="grantNumber">ANR-19-P3IA-0002</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_Tm2jHY8">
					<idno type="grant-number">ANR-19-P3IA-0002</idno>
					<orgName type="project" subtype="full">Nat'l Research Agency</orgName>
					<orgName type="program" subtype="full">3IA Côte dAzur "Investments in the Future</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Measuring conceptual similarity in ontologies: How bad is a cheap measure?</title>
		<author>
			<persName><forename type="first">T</forename><surname>Alsubait</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Parsia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Sattler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Description Logics</title>
		<imprint>
			<biblScope unit="page" from="365" to="377" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">On the distribution of the two-sample cramer-von mises criterion</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">W</forename><surname>Anderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Mathematical Statistics</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1148" to="1159" />
			<date type="published" when="1962">1962</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Universal OWL axiom enrichment for large knowledge bases</title>
		<author>
			<persName><forename type="first">L</forename><surname>Bühmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lehmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EKAW 2012</title>
		<imprint>
			<biblScope unit="page" from="57" to="71" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Possibility Theory-An Approach to Computerized Processing of Uncertainty</title>
		<author>
			<persName><forename type="first">D</forename><surname>Dubois</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Prade</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1988">1988</date>
			<publisher>Plenum Press</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Mining RDF data for property axioms</title>
		<author>
			<persName><forename type="first">D</forename><surname>Fleischhacker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Völker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Stuckenschmidt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OTM</title>
		<imprint>
			<biblScope unit="page" from="718" to="735" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Datil: Learning fuzzy ontology datatypes</title>
		<author>
			<persName><forename type="first">I</forename><surname>Huitzil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Straccia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Díaz-Rodríguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bobillo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IPMU 2018</title>
		<imprint>
			<biblScope unit="page" from="100" to="112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Perspectives on Ontology Learning</title>
	</analytic>
	<monogr>
		<title level="m">Studies on the Semantic Web</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Lehmann</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Völker</surname></persName>
		</editor>
		<meeting><address><addrLine>Amsterdam</addrLine></address></meeting>
		<imprint>
			<publisher>IOS Press</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Binary codes capable of correcting deletions, insertions, and reversals</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">I</forename><surname>Levenshtein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Soviet Physics Doklady</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="707" to="710" />
			<date type="published" when="1966-02">February 1966</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">V D</forename><surname>Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008-11">Nov. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Ontology learning for the semantic web</title>
		<author>
			<persName><forename type="first">A</forename><surname>Maedche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Staab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Intelligent Systems</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="72" to="79" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Predicting the possibilistic score of owl axioms through modified support vector clustering</title>
		<author>
			<persName><forename type="first">D</forename><surname>Malchiodi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G B</forename><surname>Tettamanzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SAC 2018</title>
		<imprint>
			<biblScope unit="page" from="1984" to="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning class disjointness axioms using grammatical evolution</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">H</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tettamanzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EuroGP</title>
		<imprint>
			<biblScope unit="page" from="278" to="294" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Multidimensional projection for visual analytics: Linking techniques with distortions, tasks, and layout enrichment</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">G</forename><surname>Nonato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Aupetit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2650" to="2673" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">OWL 2 web ontology language structural specification and functional-style syntax</title>
		<author>
			<persName><forename type="first">B</forename><surname>Parsia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Motik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Patel-Schneider</surname></persName>
		</author>
		<imprint />
	</monogr>
	<note>second edition</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<ptr target="http://www.w3.org/TR/2012/REC-owl2-syntax-20121211/" />
		<title level="m">W3C recommendation</title>
		<imprint>
			<date type="published" when="2012-12">December 2012</date>
		</imprint>
	</monogr>
	<note>W3C</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">LIII. on lines and planes of closest fit to systems of points in space</title>
		<author>
			<persName><forename type="first">K</forename><surname>Pearson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="559" to="572" />
			<date type="published" when="1901">1901</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Scikit-learn: Machine learning in Python</title>
		<author>
			<persName><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Dubourg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cournapeau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brucher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Perrot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Duchesnay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Visual interaction with dimensionality reduction: A structured literature analysis</title>
		<author>
			<persName><forename type="first">D</forename><surname>Sacha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sedlmair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Peltonen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Weiskopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>North</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Keim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on visualization and computer graphics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="241" to="250" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Kernel principal component analysis</title>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Neural Networks (ICANN)</title>
		<imprint>
			<biblScope unit="page" from="583" to="588" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">pFOIL-DL: Learning (fuzzy) EL concept descriptions from crisp OWL data using a probabilistic ensemble estimation</title>
		<author>
			<persName><forename type="first">U</forename><surname>Straccia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mucci</surname></persName>
		</author>
		<editor>SAC</editor>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="345" to="352" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Partial matchmaking using approximate subsumption</title>
		<author>
			<persName><forename type="first">H</forename><surname>Stuckenschmidt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Second AAAI Conference on Artificial Intelligence</title>
		<meeting>the Twenty-Second AAAI Conference on Artificial Intelligence<address><addrLine>Vancouver, British Columbia, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">July 22-26, 2007</date>
			<biblScope unit="page" from="1459" to="1464" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Dynamically time-capped possibilistic testing of subclassof axioms against rdf data to enrich schemas</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G B</forename><surname>Tettamanzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Faron-Zucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Gandon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">K-CAP</title>
		<imprint>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Possibilistic testing of OWL axioms against RDF data</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G B</forename><surname>Tettamanzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Faron-Zucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Gandon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Approximate Reasoning</title>
		<imprint>
			<biblScope unit="volume">91</biblScope>
			<biblScope unit="page" from="114" to="130" />
			<date type="published" when="2017-12">December 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Testing OWL axioms against RDF facts: A possibilistic approach</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G B</forename><surname>Tettamanzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Faron-Zucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">L</forename><surname>Gandon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EKAW 2014</title>
		<imprint>
			<biblScope unit="page" from="519" to="530" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Dbpedia ontology enrichment for inconsistency detection</title>
		<author>
			<persName><forename type="first">G</forename><surname>Töpper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Knuth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sack</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">I-SEMANTICS</title>
		<imprint>
			<biblScope unit="page" from="33" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Nonlinear dimensionality reduction and data visualization: a review</title>
		<author>
			<persName><forename type="first">H</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ternational Journal of Automation and Computing</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="294" to="303" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>