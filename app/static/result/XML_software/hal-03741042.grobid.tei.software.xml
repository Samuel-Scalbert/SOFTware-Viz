<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">S-LIME: Reconciling Locality and Fidelity in Linear Explanations</title>
				<funder>
					<orgName type="full">French National Research Agency (ANR JCJC FAbLe)</orgName>
				</funder>
				<funder ref="#_kwrvyJn">
					<orgName type="full">Inria Project Lab &quot;</orgName>
				</funder>
				<funder ref="#_HKwUeBz">
					<orgName type="full">&quot; (HyAIAI)</orgName>
				</funder>
				<funder ref="#_jutsP6p">
					<orgName type="full">EU</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Romaric</forename><surname>Gaudel</surname></persName>
							<email>romaric.gaudel@ensai.fr</email>
							<affiliation key="aff0">
								<orgName type="department">Ensai</orgName>
								<orgName type="institution" key="instit1">author) Univ. Rennes</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<orgName type="institution" key="instit3">CREST</orgName>
								<address>
									<settlement>Rennes</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Luis</forename><surname>Galárraga</surname></persName>
							<email>luis.galarraga@inria.fr</email>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Univ. Rennes</orgName>
								<orgName type="institution" key="instit2">Inria</orgName>
								<address>
									<settlement>Irisa</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Julien</forename><surname>Delaunay</surname></persName>
							<email>julien.delaunay@inria.fr</email>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Univ. Rennes</orgName>
								<orgName type="institution" key="instit2">Inria</orgName>
								<address>
									<settlement>Irisa</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Laurence</forename><surname>Rozé</surname></persName>
							<email>laurence.roze@insa-rennes.fr</email>
							<affiliation key="aff2">
								<orgName type="institution" key="instit1">Univ. Rennes</orgName>
								<orgName type="institution" key="instit2">Insa</orgName>
								<orgName type="institution" key="instit3">Inria</orgName>
								<address>
									<settlement>Irisa, Rennes</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Vaishnavi</forename><surname>Bhargava</surname></persName>
							<email>vaishnavi.bhargava2605@gmail.com</email>
							<affiliation key="aff3">
								<orgName type="institution">Inria/Irisa</orgName>
								<address>
									<settlement>Rennes</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">S-LIME: Reconciling Locality and Fidelity in Linear Explanations</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">F703A8C0515559076F8083A7767C67CB</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-04-12T14:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Explainable AI • Interpretability</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The benefit of locality is one of the major premises of LIME, one of the most prominent methods to explain black-box machine learning models. This emphasis relies on the postulate that the more locally we look at the vicinity of an instance, the simpler the black-box model becomes, and the more accurately we can mimic it with a linear surrogate. As logical as this seems, our findings suggest that, with the current design of LIME, the surrogate model may degenerate when the explanation is too local, namely, when the bandwidth parameter σ tends to zero. Based on this observation, the contribution of this paper is twofold. Firstly, we study the impact of both the bandwidth and the training vicinity on the fidelity and semantics of LIME explanations. Secondly, and based on our findings, we propose S-LIME, an extension of LIME that reconciles fidelity and locality.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The pervasiveness of complex automatic decision-making nowadays has raised multiple concerns about the implications of AI for the values of fairness, trust, transparency, and privacy <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b12">13]</ref>. These concerns have propelled a plethora of work in explainable AI, a domain concerned with the design of models that can provide high-level comprehensive explanations for their answers. These models can be either explainable-by-design, or rely on external modules that compute explanations a posteriori. This need for posthoc explainability is particularly compelling for sophisticated machine learning models, e.g., neural networks, whose logic is perceived as a black box by lay users.</p><p>One of the most prominent modules to compute post-hoc explanations for blackbox supervised ML models is LIME <ref type="bibr" target="#b14">[15]</ref>. This approach builds upon the notion of local feature attribution via a linear surrogate. Feature attribution means that the explanation quantifies the contribution of a set of features to the black box's answer. This allows users to build a ranking of the features that play the biggest role in the model's logic. We say the explanation is local because it only holds for a target instance and its vicinity.</p><p>By focusing on a region of the feature space, LIME reduces the complexity of the black box and can approximate it using a surrogate sparse linear function whose coefficients constitute the feature attribution scores of the explanation. To learn this surrogate, LIME constructs a training set by generating artificial instances -called neighbors -around the target instance, and labeling them using the black box. The neighbors may not lie in the original feature space, but rather on a surrogate space that is meaningful to humans, e.g., image segments instead of pixels for images. The neighbors are weighted using an exponential kernel that depends on the distance to the target and a bandwidth parameter σ ∈ R + . The weighting process controls the level of locality of the explanation: the smaller σ is, the more local the explanation becomes as closer neighbors are weighted higher than farther ones. More locality also implies focusing on a smaller region where the black box is presumably easier to approximate.</p><p>As logical as this sounds, our experiments suggest that small values of σ can yield unfaithful or even trivially empty explanations. This counter-intuitive result has thus motivated this work, which brings two contributions: (a) A study of the impact of the bandwidth and the training vicinity on the fidelity and semantics of LIME, namely the meaning of the feature attribution scores <ref type="foot" target="#foot_0">5</ref> ; and (b) S-LIME, an extension of LIME that can solve the locality-fidelity paradox.</p><p>This paper is structured as follows. In Section 2 we introduce some background concepts and notations. We elaborate on our contributions in Sections 3 and 4. Section 5 presents an experimental evaluation of S-LIME. In Section 6 we survey the state of the art. Section 7 concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Preliminaries</head><p>Black Boxes and Linear Surrogates. We assume our black box is a classification function f : R d → R (d ∈ Z + ) that predicts the probability that a target instance x ∈ R d belongs to a given class. We denote by x[i] the i-th feature of x. Conversely, the explanation g : R d → R ( d ∈ Z + ) is a linear surrogate function that approximates f in the locality of x, i.e., g(x) = α0 + 1≤i≤ d αi x[i]. Note that g may be defined on a surrogate space different from f 's. This implies the existence of a conversion function η x : R d → R d from the surrogate to the original space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LIME.</head><p>In <ref type="bibr" target="#b14">[15]</ref>, the authors propose a model-agnostic method to compute local explanations for ML models in the form of sparse linear surrogates. LIME learns an explanation g for a black box f and an instance x by solving the following minimization problem:</p><formula xml:id="formula_0">g = argmin g∈G: α 0 k L x (f, g)<label>(1)</label></formula><p>In other words, the surrogate g is chosen such that it minimizes the error L x w.r.t. the answers of f on a neighborhood X around a target instance x. To keep the explanation meaningful to humans, LIME restricts itself to surrogate functions g with less than k non-zero parameters, where k is a user-configurable hyper-parameter set by default to 6. LIME does not assume access to the training data of the black box <ref type="foot" target="#foot_1">6</ref> , therefore the neighbors z ∈ X take the form z = η x (ẑ) where ẑ ∈ X ⊆ {0, 1} d is a synthetic instance that lies on a binary space. This space is interpretated as the presence or absence of features of the target x, so that x = η x (x) with x = 1 d. The neighbors in X are obtained by toggling off bits in x's binary representation x. When a bit is set to zero in the surrogate space, the conversion function η x must map the resulting vector to the original space. For images, this can be achieved by replacing the toggled-off super-pixels with a baseline monochrome segment or with a patch from another image <ref type="bibr" target="#b15">[16]</ref>. LIME weighs the neighbors in X according to a kernel function π σ x (based on a distance D and a bandwidth hyper-parameter σ ∈ R + ) on the surrogate space, that is,</p><formula xml:id="formula_1">L x (f, g) = ẑ∈ X π σ x (ẑ)(f (η x (ẑ)) -g(ẑ)) 2 , with π σ x (ẑ) = exp -D(x,ẑ) 2 /σ 2 .</formula><p>The hyper-parameter σ controls the locality of the explanation so that smaller values give more weight to the instances that lie close to x, i.e., those instances with fewer toggled-off bits. LIME does not make any assumptions about the inner-workings of f , however the distance D and the conversion functions η x depend on f 's original space, which at the same time depends on the instances' data type.</p><p>Quality Metrics. The quality of the local surrogate g is evaluated in terms of its fidelity, which can be measured via the surrogate's adherence to the black box f in the vicinity of x. Adherence is usually measured via the coefficient of determination R 2 <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b19">20]</ref>. The R 2 score measures the similarity between the predictions of both functions, compared to the variance of the black-box prediction. This coefficient lies in (-∞, 1], where R 2 = 1 means g fits f perfectly and R 2 = 0 (respectively R 2 &lt; 0) implies that g is as accurate as (resp. less accurate than) the best constant model. When a gold standard set F f (x) of important features is available, we can also calculate fidelity as the agreement between the explanation and the gold standard. This can be quantified via metrics such as recall <ref type="bibr" target="#b14">[15]</ref>, precision, or coverage <ref type="bibr" target="#b7">[8]</ref>. Assuming the surrogate and the original feature spaces are identical, if the explanation g for the target instance x reports features F g (x) as the most important, the recall and precision of g are respectively</p><formula xml:id="formula_2">|F f (x)∩Fg(x)| |F f (x)| and |F f (x)∩Fg(x)| |Fg(x)|</formula><p>. Coverage can be used for data types where segments, i.e., conglomerates of contiguous features, are more meaningful to humans than individual features. Examples are time series and images. For those cases, the coverage is the proportion of the gold standard regions that overlap with the regions reported by the surrogate. Further specialized metrics have been proposed to measure the fidelity of pixel attribution explanations for image classifiers <ref type="bibr" target="#b8">[9]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Locality vs. Fidelity</head><p>In this section we study the impact of two important elements of LIME on the fidelity and semantics of explanations, namely the bandwidth σ and the neighborhood X .  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">The Paradox of Small Bandwidth</head><p>We illustrate the impact of σ on the output of the tabular variant of LIME <ref type="foot" target="#foot_2">7</ref> , which we use to explain a random forest classifier trained on the UCI wine dataset <ref type="foot" target="#foot_3">8</ref> . Tabular LIME sets σ = 0.75 with no further explanation. Changing σ can, however, drastically change the resulting explanation as depicted in Figure <ref type="figure" target="#fig_0">1</ref>. In particular, LIME computes null attribution coefficients when σ = 0.1. Changing σ from 0.75 to 100 rearranges the attribution ranking of the features.</p><p>To investigate the cause of this instability, we measure the adherence of the surrogate in X as σ varies for all the test instances of the dataset. We plot the results for two instances in Figure <ref type="figure">2a</ref>, where instance 2 is the example explained in Figure <ref type="figure" target="#fig_0">1</ref>.</p><p>We recall that the R 2 score is calculated as 1 -vr(g) /v(f), where v r (g) is the residual sum of squares of the surrogate g and v r (f ) is the total sum of squares of f 's answers. This means that the surrogate accounts for no more than 60% of the variability of the black box in X . The dashed regions of the curves indicate that the surrogate model has degenerated into a set of zero weights. This points out a counter-intuitive phenomenon: higher locality -achieved by making σ small -yields poor explanations. We also observe that the R 2 may not increase monotonically with σ. Based on these observations, we devise two research questions that drive our contribution: (i) Why do seem locality and fidelity in opposition?, and (ii) what makes a good LIME explanation? The gradient of each of these functions at the target example (denoted by the * mark) is orthogonal to the border between white area and black area. The explanation in the middle captures the black box's gradient more faithfully.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Why do Seem Locality and Fidelity in Opposition?</head><p>We investigate the cause of this paradox by means of Figures <ref type="figure">2b</ref> and<ref type="figure">2c</ref> that depict the distribution of weights for the neighbors of instance 2 for σ = 0.1 and σ = 100. In the first case, the LIME surrogate is a degenerated model that predicts a constant as hinted by Figure <ref type="figure">2a</ref> and its corresponding explanation in Figure <ref type="figure" target="#fig_3">1a</ref>. Figure <ref type="figure">2b</ref> tells us that the bulk of the weights is concentrated on the target instance. Such a phenomenon leads to a trivial training set. Even though locality is defined in terms of the entire set of instances in X , almost all of them are dispensable because they do not have any influence when learning the surrogate. The situation is less skewed for σ = 100 (Figure <ref type="figure">2c</ref>), which yields the non-trivial explanation in Figure <ref type="figure" target="#fig_3">1c</ref>.</p><p>From this analysis we conclude that the selection of σ and the construction of X must go in hand. We thus propose a strategy to jointly select them in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">What Makes a Good LIME Explanation?</head><p>The human aspects of interpretability are beyond the scope of this paper; instead this study is concerned with the quality and meaningfulness of explanations from a mathematical point of view. As suggested by <ref type="bibr" target="#b5">[6]</ref>, LIME computes a scaled version of the gradient ∇f for linear black boxes f . The scaling arises because the surrogate is learned on a finite number of neighbors in a discrete space, and the scaling factor depends on x, σ, η x , and X . We argue that in the absence of a reference instance (as in <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19]</ref>), explanations based on instantaneous gradients are meaningful and desirable because their semantics are well-defined: the surrogate gradient ∇f (x) is the contribution of each surrogate feature to f 's change rate at point x. That said, LIME does not always estimate ∇f accurately as suggested by Figure <ref type="figure" target="#fig_2">3</ref>. The figures show that the weights associated to the neighbors may yield an estimation that differs largely from the black box's actual gradient in Figure <ref type="figure" target="#fig_2">3a</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">S-LIME</head><p>To tackle the locality-fidelity paradox explained in Section 3.1, we introduce an extension of LIME, called S-LIME (Smoothed LIME), that we detailed in the following.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Generic Algorithm</head><p>LIME may compute degenerated explanations due to two main factors: (i) the discreteness of the surrogate space, and (ii) the fact that instance generation and weighting are decoupled. Indeed, LIME first generates a discrete neighborhood X (independently of σ), and then weighs the instances in X using π σ x . In the extreme cases when σ tends to zero, the weighting is concentrated on x.</p><p>To prevent this skewed concentration of weights, we control the locality of the explanation in a single step (see Algorithm 1). Hence, we define the neighbors in the continuous space [0, 1] d and populate X with examples ẑ whose distance D to x is of the same magnitude as σ. Concretely, the neighborhood X = {ẑ (1) , . . . , ẑ(n) } consists of n equally-weighted instances drawn independently from a distribution ν σ . Such a design decision enables g to approximate ∇f when σ tends to zero, without hindering interpretability: g still combines the contributions of the surrogate features linearly, and we can still confer an interpretable meaning to the neighbors as later explained in Section 4.4. Moreover, this allows controlling locality via the bandwidth of the neighborhood distribution, and not anymore through an a-posteriori weighting.</p><p>Note that S-LIME also requires the definition of new conversion functions η x as X is now a subset of the continuous space [0, 1] d instead of the discrete space {0, 1} d. In Section 4.4 we provide examples of proper distributions ν σ and functions η x for images, time series, and tabular data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">S-LIME Subsumes LIME</head><p>Lemma 1. Let f be a function and x a target instance. There is a distribution ν σ over [0, 1] d such that LIME and S-LIME are minimizing the same expected loss function.</p><p>Proof. LIME outputs a function g that minimizes the loss L x (f, g) which is the residual sum of squares of the examples drawn from a distribution ν. The expectation of this loss function w.r.t. to a random neighborhood is</p><formula xml:id="formula_3">E ẑ∼ν π σ x (ẑ) (f (η x (ẑ)) -g(ẑ)) 2 . Re- mark that ν is a distribution on the finite space {0, 1} d, then ν = ẑ∈{0,1} d w ν (ẑ)δ(ẑ),</formula><p>where δ(ẑ) is the Dirac distribution at point ẑ, and w ν (ẑ) is a positive real number.</p><p>Similarly, S-LIME returns the linear surrogate g that minimizes a loss with expectation E ẑ∼νσ (f (η x (ẑ)) -g(ẑ))</p><p>2 . Let Z be ẑ∈{0,1} d π σ x (ẑ)w ν (ẑ). If we consider S-LIME with generating distribution ν σ = 1 /Z ẑ∈{0,1} d π σ</p><p>x (ẑ)w ν (ẑ)δ(ẑ), then</p><formula xml:id="formula_4">E ẑ∼νσ (f (η x (ẑ)) -g(ẑ)) 2 = ẑ∈{0,1} d π σ x (ẑ)w ν (ẑ) Z (f (η x (ẑ)) -g(ẑ)) 2 = 1 Z E ẑ∼ν π σ x (ẑ) (f (η x (ẑ)) -g(ẑ)) 2 ,</formula><p>which concludes the proof.</p><p>Remark 1. It follows from Lemma 1 that S-LIME may be used as a placeholder for LIME. Still, the proposed distribution ν σ is practical only when d is small, or when ν σ corresponds to a well-known distribution. Otherwise, storing the 2 d coefficients π σ x (ẑ)w ν (ẑ) is unpractical. Anyway, we demonstrate in Section 5 that S-LIME with a continuous distribution is more faithful than LIME.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">S-LIME and the Gradient of the Black-Box Function</head><p>Let us assume the surrogate function f • η x to be differentiable at x. Let us also denote by α the weights of the linear model returned by S-LIME when we drop the sparseness constraint. Then for any family of continuous distributions ν σ on [0, 1] d, such that their mass concentrates on x when σ tends to zero, α tends to the gradient ∇f (x) of f • η x at point x. An example of such family of distributions is the set {N x, σ 2 I I I , σ ∈ R + } of Gaussian distributions centered at x with variance σ 2 I I I, where I I I is the identity matrix.</p><p>This property has two main implications. First, while LIME degenerates as σ approaches zero, S-LIME remains well-defined for any value of σ. Secondly, we know what S-LIME is targeting when we look locally at x: ∇f (x).</p><p>Remark 2. There are settings for which surrogate gradients are meaningless: piece-wise constant functions such as random forests. In such a scenario, S-LIME outputs a zero gradient as soon as the bandwidth of the generating distribution is small enough. While the weights returned by S-LIME are mathematically consistent for such kinds of models, they are useless as they carry on information that is too local. If that is the case, users may pick a higher value for σ, or resort to a rule-based surrogate <ref type="bibr" target="#b15">[16]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">S-LIME Implementations</head><p>Let us now discuss examples of concrete distributions ν σ and functions η x . The generating distribution ν σ is the same for image and time series datasets: the uniform distribution on [1 -σ, 1] d, with σ ∈ (0, 1]. As needed, this distribution concentrates around the surrogate target x = 1 d when σ tends to zero.</p><p>In regards to the conversion function η x , we recall that for both images <ref type="bibr" target="#b14">[15]</ref> and time series <ref type="bibr" target="#b7">[8]</ref>, LIME splits the original instance into d contiguous regions, namely super-pixels for images or fragments of fixed size for time series. Those regions define the features of the surrogate space. Given a neighbor ẑ ∈ X and a surrogate feature j, we can project ẑ back to the original space by interpolating the original features of the target x with a baseline x 0 , i.e., η x (ẑ</p><formula xml:id="formula_5">)[i] = (1 -ẑ[j])x 0 + ẑ[j]x[i]</formula><p>for all the original features i, i.e., pixels or time measures, covered by segment j. We set x 0 = 0 in our experiments, i.e., the interpolation is done w.r.t. a black image and a null time series.</p><p>Finally, for tabular data we consider one surrogate feature per original feature. Therefore, the generating distribution ν σ is the centered multivariate Gaussian distribution with covariance σ 2 I I I, and the function η x (ẑ) = x + ẑ.</p><p>Remark 3. The design of a proper distribution ν σ and a proper function η x requires the black-box model to handle examples living in a continuous space. As a consequence, S-LIME cannot be defined for text data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We now show-case the impact of the bandwidth σ on the fidelity of LIME and S-LIME explanations. We first detail our experimental setup and then elaborate on our findings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experimental Settings</head><p>Datasets and Black Boxes. We conduct our experiments on a variety of datasets, comprising Cifar10 <ref type="bibr" target="#b9">[10]</ref> and MNIST <ref type="bibr" target="#b10">[11]</ref> for image data, the FordA and StarlightCurves time series datasets from the UEA &amp; UCR Time Series Classification Repository, and the Compas and Diabetes datasets from the UCI Machine Learning Repository for tabular data. We also consider a selection of black-box models, which may be smooth or piece-wise constant, simple or complex, interpretable or not.</p><p>Protocol and Metrics. For each combination of dataset, model, and explanation module, we compute the average value of the experimental metrics for different values of σ on the test instances of the dataset. The experimental metrics were introduced in Section 2: the R 2 score for all models, and the precision/recall or the coverage for the interpretable models, i.e., those for which a ground truth is available. All these metrics take values either in (-∞, 1] or in [0, 1], and higher values denote higher fidelity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Impact of σ</head><p>To study the impact of σ on the fidelity of the LIME and S-LIME explanations, we plot the surrogate's adherence on the StarlightCurves dataset for several black-box models all using 100 random shapelets as input features. The models include Learning Shapelets (LS) <ref type="bibr" target="#b6">[7]</ref>, RESNET <ref type="bibr" target="#b20">[21]</ref>, Fast Shapelets (FS) <ref type="bibr" target="#b13">[14]</ref>, and a sparse logistic regression (LR, with L 1 -regularization to enforce at most 10 features). The results are depicted in Figure <ref type="figure" target="#fig_5">4</ref>. We set k = 6 for the number of features in explanations <ref type="bibr" target="#b14">[15]</ref>.</p><p>We observe that very local S-LIME neighborhoods lead to higher adherence and coverage, except for FS. This translates into more faithful explanations as σ approaches zero, where LIME cannot deliver proper explanations. In contrast, LIME achieves higher   adherence and coverage for FS, because this model is a decision tree. Hence, the decision function is piece-wise constant and its gradient is zero almost every-where. When σ is small enough, S-LIME recovers this gradient and returns an explanation with null coefficients, which has little practical value. That said, a wider locality can still yield a more informative explanation. We also remark that, for complex models, the best value for σ may depend on the target instance. This is corroborated by Figure <ref type="figure" target="#fig_6">5</ref> that shows the disaggregated results for 3 instances on RESNET, a deep neural network. We can observe that the adherence is maximal when σ is equal 10 -4 , 3 × 10 -3 , and 2 × 10 -2 respectively. On the other hand, the same values of σ are optimal for all examples on a simpler LR model.</p><p>Finally, we highlight that the coverage peaks when the adherence is maximal both at the instance (Figure <ref type="figure" target="#fig_6">5b</ref>) and dataset level (Figures <ref type="figure" target="#fig_5">4(cdgh)</ref>). This shows the pertinence of the R 2 score as metric to select the right level of locality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Fidelity Analysis</head><p>Tables <ref type="table" target="#tab_0">1</ref> and<ref type="table" target="#tab_1">2</ref> show the average scores obtained by S-LIME and LIME when σ is selected to maximize the aggregated adherence (R 2 score) in the test instances of the ex-   <ref type="table" target="#tab_0">1</ref> shows recall, precision, and coverage for the interpretable models, whereas Table <ref type="table" target="#tab_1">2</ref> provides the R 2 score for all models. Firstly, we remark that S-LIME's explanations are strictly more faithful than LIME's except for piecewise constant models (FS, DT, and RF). That said, this does not prevent S-LIME from achieving higher adherence for such models on some datasets when we look at a larger vicinity.</p><p>Secondly, the R 2 score is a good proxy to predict the best neighborhood in terms of recall, precision, or coverage. This is a strong result from an application point of view.</p><p>Practitioners are mostly interested by the features that are actually used by the blackbox model. For cases where those actual features are unknown, the R 2 score enables the computation of faithful linear explanations that can identify the important features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head><p>Feature-attribution explanations. Methods such as DeepLIFT <ref type="bibr" target="#b17">[18]</ref>, Integrated Gradients (IG) <ref type="bibr" target="#b18">[19]</ref>, SHAP <ref type="bibr" target="#b11">[12]</ref>, or LIME <ref type="bibr" target="#b14">[15]</ref> compute importance local attribution scores for the features of a black-box ML model. Among those, SHAP and LIME are modelagnostic and compute linear surrogates learned from artificial neighbors. Despite these similarities, the semantics of their explanations are different as confirmed by existing studies <ref type="bibr" target="#b0">[1]</ref>. While LIME approximates -often coarsely -the instantaneous gradient of the black box w.r.t. the input features <ref type="bibr" target="#b5">[6]</ref>, SHAP computes -or rather approximatesthe Shapley values <ref type="bibr" target="#b11">[12]</ref>, which quantify the feature contributions to the difference between the model's answer on a baseline instance and the target. The baseline depends on the use case, e.g., a single-color image (represented by the vector 0 d in the surrogate space). This makes SHAP and LIME complementary methods rather than competitors. LIME Extensions. An important body of literature has studied the impact of the different components and parameters of LIME on the quality of the explanations. This has led to multiple extensions of the original LIME algorithm. As opposed to this work, some extensions <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b19">20]</ref> tackle the instability of LIME, i.e., the fact that two executions of the algorithm with the same input may not deliver the same explanation. This instability originates from the randomness in the different steps of the approach, e.g., sampling in the surrogate space, non-deterministic conversion functions, etc. On those grounds, the techniques to tackle instability are diverse. ALIME <ref type="bibr" target="#b16">[17]</ref>, for example, resorts to a denoising auto-encoder to create a surrogate space that characterizes the data manifold more accurately. DLIME <ref type="bibr" target="#b21">[22]</ref>, in contrast, applies hierarchical agglomerative clustering on the training instances to identify the closest neighbors of the target and use them to learn the surrogate. In another line of thought, the authors of OptiLIME <ref type="bibr" target="#b19">[20]</ref> study the relationship between the bandwidth σ, the adherence, and the instability of LIME. Similar to our work, the authors highlight the importance of choosing the right σ in a per-instance basis. Moreover, they show an inverse relationship between σ and explanation instability. This observation constitutes the basis of a method to select the bandwidth σ that yields the best trade-off between adherence and instability. We highlight that all these approaches have been proposed only for tabular data, and that none of them takes into account recall, precision, or coverage fidelity.</p><p>Other extensions of LIME have focused entirely on improving fidelity. ILIME <ref type="bibr" target="#b4">[5]</ref> proposes the use of influence functions in order to up-weight the neighbors that play a higher role in the linear fit of the surrogate. QLIME-A <ref type="bibr" target="#b2">[3]</ref> proposes to extend the local surrogate to report quadratic relationships for cases where a linear surrogate is still inaccurate. While quadratic functions do exhibit higher fit capabilities, their interpretability in general settings is debatable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this paper we have introduced S-LIME, an extension of LIME that reconciles locality and fidelity for linear explanations. We argue that LIME can produce degenerated explanations as locality -controlled through the bandwidth σ -increases. We solve this paradox by means of a new neighbor generation process on a continuous surrogate space. Our experiments on image, time series, and tabular data suggest that this strategy can provide even more faithful linear explanations with gradient-compliant semantics that are not affected by high locality. As a future work, we envision to investigate the fidelity of S-LIME explanations with other generating distributions and conversion functions, as well as to study the impact on the stability of the explanations.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: LIME explanations for three different bandwidths on the same instance of the wine dataset (k = 4).</figDesc><graphic coords="5,139.44,245.86,124.49,78.54" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>(a) R 2 Fig. 2 :</head><label>22</label><figDesc>Fig. 2: Left: Impact of the bandwidth σ on the R 2 score of LIME for two instances of the wine dataset. Right: Distribution of the neighborhood weights for instance 2.</figDesc><graphic coords="5,280.01,247.66,85.21,76.74" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>Fig. 3: Left: A logistic regression classifier and a neighborhood (denoted by + marks) generated on a 2D discrete surrogate space. Center and right: Two LIME explanations.The gradient of each of these functions at the target example (denoted by the * mark) is orthogonal to the border between white area and black area. The explanation in the middle captures the black box's gradient more faithfully.</figDesc><graphic coords="6,139.44,115.84,110.66,73.77" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Algorithm 1</head><label>1</label><figDesc>S-LIME applied to black-box function f at target instance xRequire: Conversion function ηx, distribution νσ on the surrogate space Require: Number k of features in the explanation, number n of local examples 1: X ← ẑ(i) : i = 1, . . . , n , where ẑ(i) ∼ νσ for i = 1, . . . , n 2: return argmin g∈G: α 0 k ẑ∈ X (f (ηx(ẑ)), g(ẑ))2   </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>S-LIME on RES.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 4 :</head><label>4</label><figDesc>Fig. 4: R 2 and coverage vs. σ on the StarlightCurves dataset. Each subplot corresponds to a couple (explainer, dataset). The plotted results are averaged on the instances of the test dataset. Recall that for S-LIME σ is defined in (0, 1].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 5 :</head><label>5</label><figDesc>Fig. 5: R 2 and coverage vs. σ on the StarlightCurves dataset. Each subplot corresponds to a couple (explainer, dataset). Each curve corresponds to one target instance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Best average recall and precision, or coverage (std. in parentheses) on different datasets and interpretable black-box classifiers.</figDesc><table><row><cell>Data type Dataset Model</cell><cell>S-LIME</cell><cell></cell><cell>LIME</cell></row><row><cell></cell><cell cols="4">Rec. or Cov. Precision Rec. or Cov. Precision</cell></row><row><cell cols="2">Timeseries FordA LR on shapelets 0.87 (0.15)</cell><cell>-(-)</cell><cell>0.73 (0.17)</cell><cell>-(-)</cell></row><row><cell cols="2">Fast Shapelets 0.51 (0.30)</cell><cell>-(-)</cell><cell>0.49 (0.27)</cell><cell>-(-)</cell></row><row><cell cols="2">Starlight-LR on shapelets 0.81 (0.17)</cell><cell>-(-)</cell><cell>0.75 (0.17)</cell><cell>-(-)</cell></row><row><cell cols="2">Curves Fast Shapelets 0.68 (0.19)</cell><cell>-(-)</cell><cell>0.45 (0.15)</cell><cell>-(-)</cell></row><row><cell>Tabular data Diabetes Logistic Reg.</cell><cell cols="4">1.00 (0.00) 1.00 (0.00) 0.88 (0.12) 0.88 (0.12)</cell></row><row><cell>Dec. Tree</cell><cell cols="4">0.95 (0.13) 0.81 (0.20) 0.94 (0.14) 0.80 (0.20)</cell></row><row><cell>Compas Logistic Reg.</cell><cell cols="4">1.00 (0.00) 1.00 (0.00) 0.52 (0.21) 0.52 (0.21)</cell></row><row><cell>Dec. Tree</cell><cell cols="4">0.66 (0.33) 0.25 (0.00) 0.65 (0.33) 0.33 (0.00)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Best average R 2 (std. in parentheses) on different datasets and black-box classifiers. MLP stands for a neural network with one hidden layer composed of 100 neurons and logistic sigmoid activation function. Column Int. indicates interpretable black-box models ( ). FS, DT and RF are put aside as they are piecewise constant models.</figDesc><table><row><cell>Data type Model</cell><cell>Int. k S-LIME</cell><cell cols="2">LIME k S-LIME</cell><cell>LIME</cell></row><row><cell>Images</cell><cell>MNIST</cell><cell></cell><cell>Cifar10</cell></row><row><cell>Alexnet</cell><cell cols="4">10 0.80 (0.28) 0.58 (0.20) 10 0.84 (0.10) 0.55 (0.25)</cell></row><row><cell>VGG16</cell><cell cols="4">10 0.56 (0.43) 0.57 (0.21) 10 0.69 (0.13) 0.50 (0.27)</cell></row><row><cell>Timeseries</cell><cell>FordA</cell><cell></cell><cell cols="2">StarlightCurves</cell></row><row><cell>Learning Shapelets</cell><cell cols="4">6 0.84 (0.08) 0.57 (0.15) 6 0.92 (0.07) 0.70 (0.07)</cell></row><row><cell>RESNET</cell><cell cols="4">6 0.73 (0.20) 0.10 (1.05) 6 0.87 (0.15) 0.44 (0.15)</cell></row><row><cell>LR on Shapelets</cell><cell cols="4">6 1.00 (0.01) 0.56 (0.13) 6 0.99 (0.02) 0.58 (0.12)</cell></row><row><cell>Fast Shapelets</cell><cell cols="4">6 0.15 (0.18) 0.19 (0.14) 6 0.25 (0.13) 0.19 (0.16)</cell></row><row><cell>Tabular data</cell><cell cols="2">Diabetes</cell><cell>Compas</cell></row><row><cell>Logistic Regression</cell><cell cols="4">4 1.00 (0.00) 0.99 (0.01) 11 1.00 (0.00) 0.42 (0.23)</cell></row><row><cell>MLP</cell><cell cols="4">4 0.97 (0.03) 0.72 (0.13) 6 0.79 (0.01) 0.31 (0.16)</cell></row><row><cell>Decision Tree</cell><cell cols="4">3 0.46 (0.09) 0.46 (0.10) 3 0.34 (0.00) 0.36 (0.00)</cell></row><row><cell>Random Forest</cell><cell cols="4">4 0.62 (0.03) 0.58 (0.12) 6 0.30 (0.01) 0.30 (0.02)</cell></row><row><cell>perimental datasets. Table</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_0"><p>By semantics of LIME, we mean the information carried by the feature attribution scores.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_1"><p>The exception to this rule is its implementation for tabular data.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_2"><p>The discretization is off, hence the classifier and the explanation operate in the same space.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_3"><p>https://archive.ics.uci.edu/ml/datasets/wine</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements This research was partially supported by the <rs type="funder">Inria Project Lab "</rs><rs type="projectName">Hybrid Approaches for Interpretable AI</rs><rs type="funder">" (HyAIAI)</rs>, the project "<rs type="projectName">Framework for Automatic Interpretability in Machine Learning</rs>" financed by the <rs type="funder">French National Research Agency (ANR JCJC FAbLe)</rs>, and the network on the foundations of trustworthy AI, integrating learning, optimisation, and reasoning (TAILOR) financed by the <rs type="funder">EU</rs>'s <rs type="programName">Horizon 2020 research and innovation program</rs> under agreement <rs type="grantNumber">952215</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_kwrvyJn">
					<orgName type="project" subtype="full">Hybrid Approaches for Interpretable AI</orgName>
				</org>
				<org type="funded-project" xml:id="_HKwUeBz">
					<orgName type="project" subtype="full">Framework for Automatic Interpretability in Machine Learning</orgName>
				</org>
				<org type="funding" xml:id="_jutsP6p">
					<idno type="grant-number">952215</idno>
					<orgName type="program" subtype="full">Horizon 2020 research and innovation program</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">To Trust or not to Trust an Explanation: Using LEAF to Evaluate Local Linear XAI Methods</title>
		<author>
			<persName><forename type="first">E</forename><surname>Amparore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Perotti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bajardi</surname></persName>
		</author>
		<idno type="DOI">10.7717/peerj-cs.479</idno>
		<ptr target="http://dx.doi.org/10.7717/peerj-cs.479" />
	</analytic>
	<monogr>
		<title level="j">PeerJ Computer Science</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Benchmarking and Survey of Explanation Methods for Black Box Models</title>
		<author>
			<persName><forename type="first">F</forename><surname>Bodria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Giannotti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Guidotti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Naretto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Pedreschi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rinzivillo</surname></persName>
		</author>
		<idno>CoRR abs/2102.13076</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">QLIME-A: Quadratic Local Interpretable Model-Agnostic Explanation Approach</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bramhall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Horn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tieu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Lohia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SMU Data Science Rev</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Accountability of AI Under the Law: The Role of Explanation</title>
		<author>
			<persName><forename type="first">F</forename><surname>Doshi-Velez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kortz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Budish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bavitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gershman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>O'brien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Schieber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Waldo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Wood</surname></persName>
		</author>
		<idno>CoRR abs/1711.01134</idno>
		<ptr target="http://arxiv.org/abs/1711.01134" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">ILIME: Local and Global Interpretable Model-Agnostic Explainer of Black-Box Decision</title>
		<author>
			<persName><forename type="first">R</forename><surname>Elshawi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sherif</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Al-Mallah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sakr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ADBIS</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Explaining the Explainer: A First Theoretical Analysis of LIME</title>
		<author>
			<persName><forename type="first">D</forename><surname>Garreau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Von Luxburg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AISTATS</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Learning Time-Series Shapelets</title>
		<author>
			<persName><forename type="first">J</forename><surname>Grabocka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Schilling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wistuba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Schmidt-Thieme</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>KDD</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Agnostic Local Explanation for Time Series Classification</title>
		<author>
			<persName><forename type="first">M</forename><surname>Guillemé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Masson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Rozé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Termier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICTAI</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Studying and Exploiting the Relationship Between Model Accuracy and Explanation Quality</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Pfahringer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bifet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Lim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECML/PKDD</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Learning Multiple Layers of Features from Tiny Images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Canadian Institute for Advanced Research</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Tech. rep</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">MNIST Handwritten Digit Database</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<ptr target="http://yann.lecun.com/exdb/mnist/" />
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">A Unified Approach to Interpreting Model Predictions</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Lundberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">The Bouncer Problem: Challenges to Remote Explainability</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">L</forename><surname>Merrer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Trédan</surname></persName>
		</author>
		<idno>CoRR abs/1910.01432</idno>
		<ptr target="http://arxiv.org/abs/1910.01432" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Fast Shapelets: A Scalable Algorithm for Discovering Time Series Shapelets</title>
		<author>
			<persName><forename type="first">T</forename><surname>Rakthanmanon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Keogh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>SDM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Why should I trust you?: Explaining the Predictions of Any Classifier</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Ribeiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>KDD</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Anchors: High-Precision Model-Agnostic Explanations</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Ribeiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
		<ptr target="https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16982" />
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">ALIME: Autoencoder Based Approach for Local Interpretability</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Shankaranarayana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Runje</surname></persName>
		</author>
		<idno>CoRR abs/1909.02437</idno>
		<ptr target="http://arxiv.org/abs/1909.02437" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning Important Features Through Propagating Activation Differences</title>
		<author>
			<persName><forename type="first">A</forename><surname>Shrikumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Greenside</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kundaje</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v70/shrikumar17a.html" />
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Axiomatic Attribution for Deep Networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sundararajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Taly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yan</surname></persName>
		</author>
		<idno>CoRR abs/1703.01365</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">OptiLIME: Optimized LIME Explanations for Diagnostic Computer Algorithms</title>
		<author>
			<persName><forename type="first">G</forename><surname>Visani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Bagli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Chesani</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-2699/paper03.pdf" />
	</analytic>
	<monogr>
		<title level="m">AIMLAI@CIKM</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Time Series Classification from Scratch with Deep Neural Networks: A Strong Baseline</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Oates</surname></persName>
		</author>
		<idno>CoRR abs/1611.06455</idno>
		<ptr target="http://arxiv.org/abs/1611.06455" />
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">DLIME: A Deterministic Local Interpretable Model-Agnostic Explanations Approach for Computer-Aided Diagnosis Systems</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Zafar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Khan</surname></persName>
		</author>
		<idno>CoRR abs/1906.10263</idno>
		<ptr target="http://arxiv.org/abs/1906.10263" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
