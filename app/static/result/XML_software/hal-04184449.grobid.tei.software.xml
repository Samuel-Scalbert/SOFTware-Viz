<TEI xmlns="http://www.tei-c.org/ns/1.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xml:space="preserve" xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Masking Strategies for Background Bias Removal in Computer Vision Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher />
				<availability status="unknown"><licence /></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Ananthu</forename><surname>Aniraj</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Inria</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">University of Montpellier</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">LIRMM</orgName>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="institution">UMR-Tetis</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Cassio</forename><forename type="middle">F</forename><surname>Dantas</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Inrae</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">University of Montpellier</orgName>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="institution">UMR-Tetis</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Dino</forename><surname>Ienco</surname></persName>
							<email>dino.ienco@inrae.fr</email>
							<affiliation key="aff1">
								<orgName type="institution">Inrae</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">University of Montpellier</orgName>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="institution">UMR-Tetis</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Diego</forename><surname>Marcos</surname></persName>
							<email>diego.marcos@inria.frcassio.fraga-dantas</email>
							<affiliation key="aff0">
								<orgName type="institution">Inria</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">University of Montpellier</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">LIRMM</orgName>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="institution">UMR-Tetis</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Masking Strategies for Background Bias Removal in Computer Vision Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date />
						</imprint>
					</monogr>
					<idno type="MD5">491347CDC797B3874DE8FBF7DA715E87</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-04-12T14:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid" />
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div><p>Models for fine-grained image classification tasks, where the difference between some classes can be extremely subtle and the number of samples per class tends to be low, are particularly prone to picking up background-related biases and demand robust methods to handle potential examples with out-of-distribution (OOD) backgrounds. To gain deeper insights into this critical problem, our research investigates the impact of background-induced bias on finegrained image classification, evaluating standard backbone models such as Convolutional Neural Network (CNN) and Vision Transformers (ViT). We explore two masking strategies to mitigate background-induced bias: Early masking, which removes background information at the (input) image level, and late masking, which selectively masks high-level spatial features corresponding to the background. Extensive experiments assess the behavior of CNN and ViT models under different masking strategies, with a focus on their generalization to OOD backgrounds. The obtained findings demonstrate that both proposed strategies enhance OOD performance compared to the baseline models, with early masking consistently exhibiting the best OOD performance. Notably, a ViT variant employing GAP-Pooled Patch tokenbased classification combined with early masking achieves the highest OOD robustness.</p><p>Our code and models are available at: GitHub</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div><head n="1.">Introduction</head><p>The context in which an object is found is known to have a large influence in how it is perceived, both by humans <ref type="bibr" target="#b2">[3]</ref> and machines <ref type="bibr" target="#b6">[7]</ref>. When training deep learningbased computer vision models on any object-centric task, it is extremely challenging to ensure that the model does indeed only pay attention to the object of interest, since there exist often strong correlations between object characteristics and the background. Such a bias can lead to degraded Figure <ref type="figure">1</ref>: Background bias in fine-grained classification performances when deploying the models on new, Out-of-Distribution (OOD), background configurations <ref type="bibr" target="#b0">[1]</ref>. This is particularly harmful in fine-grained classification tasks such as species identification (Figure <ref type="figure">1</ref>), due to the large number of closely related categories and the strong correlations between the object (the specimen of interest) and the background, which tends to relate to the habitat of the species <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b27">28]</ref>.</p><p>The standard way to address this type of bias is to make the model detect the foreground and focus on it for classification, both when using Convolutional Neural Networks <ref type="bibr" target="#b18">[19]</ref> and Transformers <ref type="bibr" target="#b12">[13]</ref>. This reduces the influence of the background on the model's decision, thus reducing the background-induced bias of the model.</p><p>Masking the background areas at some CNN high level representations seem like a simple and straight forward strategy to obtain this effect, as long as some type of foreground-background (FG-BG) mask can be obtained. After all, the inductive biases that characterize CNNs will induce an association between a high-level feature at a certain tensor location (i.e. before any global pooling) and the image patch at the corresponding image location. However, the Visual Transformers' architecture does not encode for such biases, although they could be learned via supervised or self-supervised learning <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b21">22]</ref>.</p><p>We aim to analyse the impact of dataset bias induced by background regions in the image on the performance and generalization capabilities of image classification models  on fine-grained classification tasks using standard CNN and Vision Transformer (ViT) <ref type="bibr" target="#b10">[11]</ref> backbones. Specifically, we study the effect of background masking, both at the image level and at high level features, on both CNN and ViT, on a fine-grained classification task. Our main motivation is to understand the difference in behaviour, in terms of generalization to OOD backgrounds, of ViT with respect to CNN under different strategies of background masking.</p></div>
<div><head n="2.">Related Work</head><p>The influence of context The ability of using context for object recognition is often considered an asset, since the context may help disambiguate between otherwise indistinguishable categories, and the explicit modeling of context played an important role in pre-deep learning computer vision systems <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref>. With the emergence of deep learning approaches, which are capable of automatically extracting features without leveraging user knowledge, the extraction of spurious correlations induced by contextual bias has started to be perceived as a potential issue <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b27">28]</ref>. Such effect can happen to the extent that CNN models may perform reasonably well when given only the background, without the object of interest <ref type="bibr" target="#b32">[33]</ref>.</p><p>Mitigating context-induced bias In early computer vision systems for object recognition, performing a preprocessing step of foreground segmentation was identified as an effective solution against background bias <ref type="bibr" target="#b22">[23]</ref>. When the context classes are known, another approach is to make sure that the learned attention maps for each class do not overlap <ref type="bibr" target="#b25">[26]</ref>. Alternatively, the manifold of learned class features can be explored in order to manually identify spurious ones that can then be suppressed <ref type="bibr" target="#b20">[21]</ref>. With modern Transformer-based architectures, it has been shown that allowing the model to refocus its attention in a top-down manner (thus based on the results after a first pass) helps in a variety of computer vision tasks <ref type="bibr" target="#b24">[25]</ref> by reducing the effect of background elements.</p><p>In this work, we study the effect on OOD background generalization of masking the image background regions, at either the image level or at the level of an intermediate representation within the model.</p></div>
<div><head n="3.">Methodology</head><p>We aim at studying the effect of dataset bias introduced by the image background regions and to what extent it can be corrected via background masking. We propose to investigate the following three training settings:</p><p>• Baseline: Fine-tune a standard image classification model from the literature on a fine-grained image classification dataset.</p><p>• Early masking: Apply background masking at the image level using foreground-background (FG-BG) masks. We then fine-tune image classification models to classify the masked out image.</p><p>• Late masking: Mask out high-level representations that spatially correspond to background regions at an intermediate stage of an image classification model us-ing the FG-BG masks. Subsequently, we fine-tune the resulting model.</p><p>The FG-BG masks are obtained by applying a semantic segmentation network trained on ground-truth foregroundbackground masks.</p><p>In order to investigate the generalization capabilities of the models under the different settings, we propose to evaluate them on a test set that has a different background distribution than the train set.</p></div>
<div><head n="3.1.">Binary FG-BG Segmentation</head><p>For binary segmentation, we trained a deep neural network using the segmentation labels provided by the CUB-200-2011 dataset <ref type="bibr" target="#b26">[27]</ref>. The objective was to classify each pixel in the image as either bird or background.</p><p>Given an image x ∈ R 3×M ×N , the segmentation network was trained to produce a binary mask m ∈ {0, 1} 1×M ×N , where 0 represents the background, and 1 represents the foreground.</p></div>
<div><head n="3.2.">Early Masking</head><p>For this strategy (Figure <ref type="figure" target="#fig_1">2</ref> left), we first mask out the background image regions using the binary segmentation network and then pass the masked image to a standard CNN/Transformer-based image classifier model. The CNN/ViT model is composed of two stages: a backbone h θ1 (•) (CNN or ViT) and a classification head g θ2 (•). More formally, we can define it as:</p><formula xml:id="formula_0">y = g θ2 (h θ1 (x ⊙ m))<label>(1)</label></formula><p>where each channel of the image (x) is multiplied elementwise by the FG-GB binary mask (m) and y ∈ R C is the vector of class logits.</p><p>The idea here is to mask out background features at the image level, forcing the model to only look at the salient object in the image for the classification task, ideally learning representations independent of bias caused by the image background.</p></div>
<div><head n="3.3.">Late Masking</head><p>In this strategy (Figure <ref type="figure" target="#fig_1">2</ref> right), the high-level spatial features corresponding to the background are masked out, similar to the feature masking layer proposed in <ref type="bibr" target="#b9">[10]</ref>. More formally, this strategy can be defined as:</p><formula xml:id="formula_1">y = g θ2 (h θ1 (x) ⊙ m ′ )<label>(2)</label></formula><p>Here, y represents the final classification logits, x is the input image, and m ′ is the subsampled version of the binary FG-BG mask.</p><p>The backbone h θ1 (•) outputs a spatial tensor z.</p><formula xml:id="formula_2">z = h θ1 (x) ∈ R D×M ′ ×N ′</formula><p>where M ′ = M/k and N ′ = N/k, with k being the model's spatial downsampling ratio. In the case of ViT, this spatial tensor results from the spatial rearrangement of the patch tokens' features, each a vector of dimension D.</p><p>The classification head g θ2 (•) performs a global average pooling followed by at least one learnable linear layer.</p><p>The late masking strategy allows selectively masking out background-related features at a later stage in the model, enabling the model to learn representations based solely on foreground image features.</p></div>
<div><head n="4.">Experiments</head></div>
<div><head n="4.1.">Dataset</head><p>All our models are trained on the training set of CUB <ref type="bibr" target="#b26">[27]</ref>. This dataset contains images of 200 bird species, with 5,994 images in the training set and 5,794 images in the testing set.</p><p>The trained models are evaluated on two test sets: (i) the in-distribution real-world images of the CUB test set <ref type="bibr" target="#b26">[27]</ref> and (ii) an Out-Of-Distribution (OOD) set of images with an adversarial background, specifically the Waterbirds dataset from <ref type="bibr" target="#b23">[24]</ref>.</p><p>The Waterbirds dataset <ref type="bibr" target="#b23">[24]</ref> was constructed by replacing the background regions in the CUB images with ones coming from the Places dataset <ref type="bibr" target="#b31">[32]</ref>. Therefore, this dataset contains the exact same bird species as CUB, with the only difference being the background.</p></div>
<div><head n="4.2.">Implementation Details</head><p>All models were implemented in <software ContextAttributes="used">PyTorch</software>. For the sake of comparison, we used the ImageNet-pretrained Con-vNeXt <ref type="bibr" target="#b15">[16]</ref> and DinoV2-pretrained ViT <ref type="bibr" target="#b21">[22]</ref> backbones as starting weights for all the experiments.</p><p>We utilized the Small (S), Base (B), and Large (L) variants of the ViT and ConvNeXt models for our experiments. These variants allowed us to explore the impact of model size on the performance of our masking strategies.</p><p>To perform binary segmentation, we fine-tuned a pretrained Mask2Former <ref type="bibr" target="#b4">[5]</ref> model with a Swin-Tiny backbone <ref type="bibr" target="#b14">[15]</ref> using the FG-BG masks provided along with the CUB dataset. We use only the images from the CUB training dataset to train the segmentation model. The Mask2Former model was chosen for its effectiveness in image segmentation tasks. However, in theory, any semantic segmentation model can be used in its place. Detailed information regarding the training settings for the binary segmentation model, as well as the evaluation results, can be found in the supplementary material. (9, 0.5) (9, 0.5) Mixup <ref type="bibr" target="#b29">[30]</ref> None None Cutmix <ref type="bibr" target="#b28">[29]</ref> None None Random Erasing <ref type="bibr" target="#b30">[31]</ref> 0.25 0.25 Label Smoothing <ref type="bibr" target="#b19">[20]</ref> 0.1 0.1 Stochastic Depth <ref type="bibr" target="#b13">[14]</ref> 0.  <ref type="bibr" target="#b8">[9]</ref> (9, 0.5) (9, 0.5) Mixup <ref type="bibr" target="#b29">[30]</ref> 0.8 0.8 Cutmix <ref type="bibr" target="#b28">[29]</ref> 1.0 1.0 Random Erasing <ref type="bibr" target="#b30">[31]</ref> 0.25 0.25 Label Smoothing <ref type="bibr" target="#b19">[20]</ref> 0.1 0.1 Stochastic Depth <ref type="bibr" target="#b13">[14]</ref> 0.1/ 0.2/ 0.3 0.1 Frozen Backbone ✗ ✗ Table <ref type="table">2</ref>: Training Settings for fine-tuning (i) For models with a frozen backbone, we utilized the settings described in Table <ref type="table" target="#tab_1">1</ref>. In this case, only the final classification layer was trained, and we employed a relatively short training schedule of 90 epochs. (ii) For models with fine-tuning, we employed the settings described in Table <ref type="table">2</ref>. In this case, all layers in the network were trained for a longer schedule of 300 epochs, with additional regularization techniques like <software ContextAttributes="used">MixUp</software> <ref type="bibr" target="#b29">[30]</ref> and <software ContextAttributes="used">CutMix</software> <ref type="bibr" target="#b28">[29]</ref>.</p></div>
<div><head n="4.3.">Training Settings</head></div>
<div><head n="4.4.">The Effect of Early Masking and Model Size</head><p>The main aims of this experiment are to investigate: (i) The impact of adopting our early masking strategy. (ii) The interplay between this masking strategy and varying model sizes. For this experiment, we train classifiers while keeping the ViT and ConvNeXt backbones of three different sizes frozen. The training is performed on the CUB dataset, with two scenarios: one involving input images masked using the predicted FG-BG masks and the other without any masking. To extract features for classification in the ViT model, we concatenate the class token with the global average-pooled (GAP) patch token features, following a similar implementation as described in <ref type="bibr" target="#b21">[22]</ref>.</p><p>To ensure a comprehensive evaluation, we also assess the model trained on masked images with the original images and vice versa. This provides insights into the effectiveness of the early masking strategy and its sensitivity to different model sizes.</p></div>
<div><head n="4.5.">Baseline vs Early Masking vs Late Masking</head><p>The aims of this experiment are to study: (i) The effects of applying both of our proposed masking strategies. (ii) The performance with a frozen and a fine-tuned backbone For this experiment, we chose two models of approximately the same size: the ViT-Base and the ConvNeXt-Base, due to their balanced compromise between performance and computational cost.</p></div>
<div><head n="4.6.">Feature masking at different model stages</head><p>The main aim of this experiment is to study the effect of applying late feature masking at different stages in the network. In this scenario, early masking would be equivalent to applying masking at stage L = 0.</p><p>Concretely, we experimented with performing the feature masking after the second-to-last stage of the model (L -1) and compared the OOD and in-distribution performance with the masking after the last stage (L) and the early masked model (L = 0).</p><p>The training settings for this experiment are similar to the fine-tuning settings given in Table <ref type="table">2</ref>. Here, the model training requires two learning rates: one before and one after the feature masking, to ensure convergence.</p><p>We were only able to perform this experiment for the ConvNeXt models, as we encountered challenges in finding stable training hyperparameters for masking the ViT at the (L -1) th stage. ablations when the backbone is frozen versus when the entire model is fine-tuned. More precisely, we conduct experiments and evaluate the performance using three different inputs for the linear classification layer: (i) using the CLS token, (ii) using the Global Average Pooled (GAP) Patch token, and (iii) concatenating them together (CLS+Patch).To manage computational constraints, we focus solely on the ViT-Base model for this study.</p></div>
<div><head n="4.7.">Varying the ViT representation</head></div>
<div><head n="5.">Results and Discussion</head></div>
<div><head n="5.1.">The Effect of Early Masking and Model Size</head><p>The performance of models trained with a frozen backbone, both with and without the early masking strategy, is evaluated using all combinations of test settings. The results of these evaluations are reported in Table <ref type="table" target="#tab_2">3</ref>.</p><p>From the table, it is evident that the models trained with the early masking technique consistently outperform their counterparts trained without the masking strategy on the OOD Waterbirds test set, regardless of the backbone and model size. This improvement holds true when the earlymasked models are tested on both the original and masked images. For instance, with ConvNeXt-B, the OOD performance improves from 65.96% to 80.10% when the images are masked at both training and testing time, and for ViT-B, it increases from 76.65% to 86.93%.</p><p>These results suggest that models trained on the origi-nal images are influenced by biases from the image backgrounds, which leads to lower generalization on the Waterbird test set when the background is not masked. We also observe a performance boost for the baseline models when tested on the early masked version of the Waterbirds Dataset. This suggests that these models might have been relying on background cues to make decisions, which the early masking strategy helps to mitigate.</p><p>Furthermore, the best overall performances on the original CUB dataset are obtained by ViT-L and ViT-B when early masking is performed both at train and test stage. In contrast, when using convolutional models, this setting performs less well than the original one. This suggests that: (i) ViT models are less sensitive than CNNs to potential artifacts induced by masking and (ii) the original CUB dataset is negatively affected by background-induced biases.</p></div>
<div><head n="5.2.">Baseline vs Early Masking vs Late Masking Effect of freezing the model backbone</head><p>The results are given in Table <ref type="table" target="#tab_3">4</ref>. Upon analyzing the table, it is evident that the early masking strategy shows the highest generalization performance on the OOD Waterbird test set for both backbones.</p><p>For the ConvNeXt model with a frozen backbone, we observe a substantial performance improvement when employing both the early and late masking training strategies. This suggests that the high-level features spatially corresponding to the foreground are highly localized <ref type="bibr" target="#b17">[18]</ref>, likely due to the inductive biases in the CNN architecture, resulting in high spatial correlation of features even at the later stages of the network.</p><p>In </p></div>
<div><head>Effect of fine-tuning</head><p>Fine-tuning consistently improves in-distribution performance on the CUB test set for both ConvNeXt and ViT models (Table <ref type="table" target="#tab_3">4</ref>). However, we do not observe a similar behavior on OOD data.</p><p>The early-masked model, for both ViT and CNN, demonstrates the best generalization, consistent with the findings from previous experiments.</p><p>For ConvNeXt, fine-tuning the baseline and early masked models leads to improvements in both indistribution performance and significant advances in OOD generalization. However, the late-masked model shows only marginal performance gains.</p><p>In the case of the ViT, we see a reduction in robustness for both the baseline and late-masking variant on the OOD Waterbirds test set, with the baseline experiencing a more evident performance drop-off. This indicates that, with longer training, the ViT model becomes more susceptible to overfitting when background correlations are present in the training data, unless the background is masked out at the image level.</p></div>
<div><head n="5.3.">Feature masking at different model stages</head><p>The results of adopting feature masking at different stages in the network are presented in Table <ref type="table" target="#tab_4">5</ref>. It shows that both the in-distribution and OOD performance systematically improve when background masking is applied at an earlier stage in the network. This observation suggests that earlier layers of the CNN are more spatially correlated with the image patches at their corresponding locations, possibly due to the comparatively smaller receptive fields.</p><p>Furthermore, we can note that the improvement in robustness of the models tends to decrease as the <ref type="bibr">CNN</ref>  Overall, these results suggest that feature masking at an earlier layer of the network has a positive impact on both in-distribution and OOD performances.</p></div>
<div><head n="5.4.">Varying the ViT representation</head><p>The results of the experiment are reported in Table <ref type="table" target="#tab_5">6</ref>. Fine-tuning the models generally improves in-distribution performance on the CUB dataset. Notably, the model using only Patch tokens exhibits a substantial accuracy increase from 62% to 90% for the baseline, 72% to 92% for the early masked, and 84% to 91% for the late-masked variants.</p><p>On the OOD Waterbird dataset, fine-tuning the baseline model using both CLS and Patch tokens leads to reduced performance. This pattern also repeats for the model using just the CLS token. However, we see a notable improvement for the model using just the Patch tokens, where the accuracy increases from 25% to 68%.</p><p>In contrast, the early and late-masked models benefit from fine-tuning, showing improved OOD generalization with either CLS or Patch token input. Particularly, the ViT The experiment highlights the importance of studying which representations are utilized for the classifier layer in ViT models to enhance performance in fine-grained image classification tasks, particularly in the presence of background-induced biases. The use of Patch tokens, especially in conjunction with early or late masking, emerges as a promising strategy. This approach allows the model to focus on foreground-containing tokens and enhances model generalization with respect to background-induced biases.</p></div>
<div><head n="6.">Limitations and Future Work</head><p>Our study provides valuable insights and lays the foundation for potential solutions; however, certain limitations point to avenues for future exploration:</p><p>Dataset Scope and Pre-trained Models: The research study was mainly restricted to the CUB dataset, and we utilized publicly available pre-trained models due to time and computational constraints. Expanding our assessment to encompass larger datasets featuring diverse species and backgrounds, while also maintaining better control over pretraining methodologies, will enhance the generalizability and robustness of our conclusions.</p><p>Real-World Robustness: Our reliance on FG-BG ground truth annotations during segmentation model training may not align with scenarios where such privileged information is unavailable. Deploying our proposed models on real-world datasets without access to FG-BG annotations could yield further insights into its robustness.</p><p>Computational Overhead: Our proposed masking strategies involving the segmentation model introduce computational overhead, which could be infeasible in scenarios with tight time constraints. To this end, future research could explore techniques that seamlessly integrate back-ground suppression as a training objective, aiming for enhanced out-of-distribution generalization. Notably, recent work by <ref type="bibr">Chou et al. [8]</ref> also emphasizes similar directions in the context of fine-grained image classification.</p><p>These limitations underscore the potential for future research endeavors aimed at refining and expanding the effectiveness of our proposed methods.</p></div>
<div><head n="7.">Conclusion</head><p>In this study, we conducted extensive experiments to investigate the impact of background-induced bias on the generalization capabilities of CNN and ViT models for finegrained image classification on out-of-distribution (OOD) data and explored various background masking strategies to mitigate such biases and enhance model generalization.</p><p>Overall, our results revealed that the proposed masking strategies improve OOD performance, with the early masking strategy exhibiting the best generalization capability on both CNN and ViT. Early masking is particularly advantageous when fine-tuning the model backbones, with late masking not benefiting from fine-tuning and even loosing some OOD generalization capacity in the case of ViT. Notably, a ViT variant employing GAP-Pooled Patch tokenbased classification, combined with early masking, exhibits the highest OOD robustness of all tested settings.</p><p>While our study provides valuable insights and solutions, certain limitations remind us of unexplored frontiers. Expanding investigations to larger datasets, controlled pretraining methodologies, and considering real-world scenarios lacking FG-BG annotations are essential steps toward refining and extending the impact of our methods.</p><p>In conclusion, our study emphasizes the significance of considering background information in object-centric tasks and demonstrates effective solutions to mitigate biases and enhance model generalization.</p></div><figure xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Early and late masking strategies</figDesc><graphic coords="3,204.85,202.80,79.27,79.27" type="bitmap" /></figure>
<figure type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Training Settings for frozen backbone</figDesc><table><row><cell /><cell>1/ 0.2/ 0.3</cell><cell>0.0</cell></row><row><cell>Frozen Backbone</cell><cell>✓</cell><cell>✓</cell></row><row><cell>Models</cell><cell cols="2">ConvNeXt-S/B/L ViT-S/B/L</cell></row><row><cell>Input Size</cell><cell>(224, 224)</cell><cell>(518, 518)</cell></row><row><cell>Optimizer</cell><cell>AdamW [17]</cell><cell>AdamW [17]</cell></row><row><cell>LR(Backbone)</cell><cell>4e-6</cell><cell>4e-6</cell></row><row><cell>LR(Classifier)</cell><cell>4e-3</cell><cell>4e-3</cell></row><row><cell>Weight Decay</cell><cell>5e-2</cell><cell>5e-2</cell></row><row><cell>Optimizer momentum</cell><cell>(0.9, 0,999)</cell><cell>(0.9, 0,999)</cell></row><row><cell>Batch Size</cell><cell>64</cell><cell>64</cell></row><row><cell>Training Epochs</cell><cell>300</cell><cell>300</cell></row><row><cell cols="2">Learning Rate Schedule Cosine Decay</cell><cell>Cosine Decay</cell></row><row><cell>Warmup Epochs</cell><cell>20</cell><cell>20</cell></row><row><cell>Warmup Schedule</cell><cell>Linear</cell><cell>Linear</cell></row><row><cell>RandAugment</cell><cell /><cell /></row></table></figure>
<figure type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Results: Early Masking vs Baseline (i) Vary the input to the ViT classification head to explore different ablations of the information extracted by the ViT model. (ii) Investigate the performance differences between these</figDesc><table><row><cell>In this experiment, we aim to:</cell></row></table></figure>
<figure type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>the case of the ViT model with a frozen backbone, Results of 4.5 -Frozen backbone vs Fine-tuning the late-masked and baseline models perform similarly on both the in-distribution CUB dataset and the OOD Waterbird dataset. This indicates that the early masking strategy helps prevent the ViT model, with frozen backbone parameters, from being influenced by background-related biases.</figDesc><table><row><cell /><cell /><cell cols="2">CUB(%)</cell><cell cols="2">Waterbird(%)</cell></row><row><cell>Backbone</cell><cell cols="5">Training on CUB Frozen Fine-tuned Frozen Fine-tuned</cell></row><row><cell /><cell>Baseline</cell><cell>88.00</cell><cell>89.62</cell><cell>65.96</cell><cell>76.20</cell></row><row><cell>ConvNeXt-B</cell><cell>Early-Masked</cell><cell>85.69</cell><cell>90.31</cell><cell>80.10</cell><cell>87.01</cell></row><row><cell /><cell>Late-Masked</cell><cell>87.66</cell><cell>88.76</cell><cell>77.95</cell><cell>78.42</cell></row><row><cell /><cell>Baseline</cell><cell>89.20</cell><cell>89.38</cell><cell>76.55</cell><cell>68.36</cell></row><row><cell>ViT-B</cell><cell>Early-Masked</cell><cell>90.10</cell><cell>91.37</cell><cell>86.93</cell><cell>88.81</cell></row><row><cell /><cell>Late-Masked</cell><cell>88.61</cell><cell>90.73</cell><cell>76.55</cell><cell>74.76</cell></row></table></figure>
<figure type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Results of adopting feature masking at different stages. In this table, L: after last stage, (L-1): after second last stage. 0: early masking/at image level size increases, unless the masking is done at the image level itself. For instance, the ConvNeXt-Large model performs approximately the same regardless of whether the masking was applied after the last stage or the second last stage.</figDesc><table><row><cell>model</cell></row></table></figure>
<figure type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Results of varying the representation of the ViT.</figDesc><table><row><cell cols="2">Backbone Training on CUB</cell><cell>ViT representation</cell><cell cols="4">CUB(%) Frozen Fine-tuned Frozen Fine-tuned Waterbird(%)</cell></row><row><cell /><cell /><cell>CLS</cell><cell>90.04</cell><cell>90.47</cell><cell>80.35</cell><cell>71.35</cell></row><row><cell /><cell>Baseline</cell><cell>Patch</cell><cell>62.24</cell><cell>90.05</cell><cell>24.71</cell><cell>67.74</cell></row><row><cell /><cell /><cell>CLS+Patch</cell><cell>89.20</cell><cell>89.38</cell><cell>76.65</cell><cell>68.36</cell></row><row><cell>ViT-B</cell><cell>Early-Masked</cell><cell>CLS Patch</cell><cell>90.33 72.17</cell><cell>91.73 91.51</cell><cell>86.81 66.37</cell><cell>88.60 89.22</cell></row><row><cell /><cell /><cell>CLS+Patch</cell><cell>90.10</cell><cell>91.37</cell><cell>86.93</cell><cell>88.81</cell></row><row><cell /><cell /><cell>CLS</cell><cell>89.16</cell><cell>90.78</cell><cell>75.10</cell><cell>80.54</cell></row><row><cell /><cell>Late-Masked</cell><cell>Patch</cell><cell>84.15</cell><cell>90.85</cell><cell>71.63</cell><cell>84.50</cell></row><row><cell /><cell /><cell>CLS+Patch</cell><cell>88.61</cell><cell>90.73</cell><cell>76.55</cell><cell>74.76</cell></row><row><cell cols="3">model employing GAP-Pooled Patch token classification,</cell><cell /><cell /><cell /><cell /></row><row><cell cols="3">considering both early and late masking strategies, clearly</cell><cell /><cell /><cell /><cell /></row><row><cell cols="2">outperforms all the other variants.</cell><cell /><cell /><cell /><cell /><cell /></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Objectnet: A large-scale bias-controlled dataset for pushing the limits of object recognition models</title>
		<author>
			<persName><forename type="first">A</forename><surname>Barbu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mayo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Alverio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gutfreund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Katz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Recognition in terra incognita</title>
		<author>
			<persName><forename type="first">S</forename><surname>Beery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Van Horn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="456" to="473" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Scene perception: Detecting and judging objects undergoing relational violations</title>
		<author>
			<persName><forename type="first">I</forename><surname>Biederman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Mezzanotte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Rabinowitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive psychology</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="143" to="177" />
			<date type="published" when="1982">1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Emerging Properties in Self-Supervised Vision Transformers</title>
		<author>
			<persName><forename type="first">M</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jegou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="9630" to="9640" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Masked-attention Mask Transformer for Universal Image Segmentation</title>
		<author>
			<persName><forename type="first">B</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girdhar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A tree-based context model for object recognition</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Willsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="240" to="252" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Context models and out-of-context objects</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Willsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="853" to="862" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Fine-grained Visual Classification with High-temperature Refinement and Background Suppression</title>
		<author>
			<persName><forename type="first">P.-Y</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-Y</forename><surname>Kao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-H</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023-03">March. 2023</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">RandAugment: Practical automated data augmentation with a reduced search space</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Decem</publisher>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Convolutional feature masking for joint object and stuff segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015-06">June. 2015</date>
			<biblScope unit="page" from="3992" to="4000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">An Image Is Worth 16X16 Words: Transformers for Image Recognition At Scale</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR 2021 -9th International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Shortcut learning in deep neural networks</title>
		<author>
			<persName><forename type="first">R</forename><surname>Geirhos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-H</forename><surname>Jacobsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Michaelis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Brendel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">A</forename><surname>Wichmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="665" to="673" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Rethinking generalization in few-shot classification</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hiller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Harandi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Drummond</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="3582" to="3595" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep Networks with Stochastic Depth</title>
		<author>
			<persName><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sedra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2016</title>
		<editor>
			<persName><forename type="first">B</forename><surname>Leibe</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Matas</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Sebe</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="646" to="661" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Swin Transformer: Hierarchical Vision Transformer using Shifted Windows</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="9992" to="10002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A ConvNet for the 2020s</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022-06">2022-June. 2022</date>
			<biblScope unit="page" from="11966" to="11976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<author>
			<persName><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Decoupled weight decay regularization. 7th International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Understanding the effective receptive field in deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<meeting><address><addrLine>Nips</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="4905" to="4913" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Rectifying the shortcut learning of background for few-shot learning</title>
		<author>
			<persName><forename type="first">X</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="13073" to="13085" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">When does label smoothing help?</title>
		<author>
			<persName><forename type="first">R</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Neuhaus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Augustin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Boreiko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2212.04871</idno>
		<title level="m">Spurious features everywhere-large-scale detection of harmful spurious features in imagenet</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Oquab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darcet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Moutakanni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Vo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Szafraniec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Khalidov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fernandez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Haziza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>El-Nouby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Assran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Galuba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Howes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rabbat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jegou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Labatut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.07193</idno>
		<title level="m">DINOv2: Learning Robust Visual Features without Supervision</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Extracting foreground masks towards object recognition</title>
		<author>
			<persName><forename type="first">A</forename><surname>Rosenfeld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Weinshall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 International Conference on Computer Vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1371" to="1378" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Distributionally Robust Neural Networks for Group Shifts: on the Importance of Regularization for Worst-Case Generalization</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sagawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">W</forename><surname>Koh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">B</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Learning Representations</title>
		<meeting><address><addrLine>ICLR</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Top-down visual attention from analysis by synthesis</title>
		<author>
			<persName><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="2102" to="2112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Don't judge an object by its context: Learning to overcome contextual bias</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">K</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Feiszli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ghadiyaram</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="11070" to="11078" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title />
		<author>
			<persName><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<idno>CNS-TR-2010-001</idno>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>California Institute of Technology</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Noise or signal: The role of image backgrounds in object recogni-tion</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Y</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Engstrom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ilyas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Madry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Cut-Mix: Regularization strategy to train strong classifiers with localizable features</title>
		<author>
			<persName><forename type="first">S</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019-05">2019-Octob. 5 2019</date>
			<biblScope unit="page" from="6022" to="6031" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Beyond empirical risk minimization</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<author>
			<persName><surname>Mixup</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">6th International Conference on Learning Representations, ICLR 2018 -Conference Track Proceedings</title>
		<imprint>
			<date type="published" when="2018">10 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Random erasing data augmentation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI 2020 -34th AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2020">8 2020</date>
			<biblScope unit="page" from="13001" to="13008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Places: A 10 Million Image Database for Scene Recognition</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1452" to="1464" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Object recognition with and without objects</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Joint Conference on Artificial Intelligence</title>
		<meeting>the 26th International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3609" to="3615" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>