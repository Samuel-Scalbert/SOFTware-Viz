<TEI xmlns="http://www.tei-c.org/ns/1.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xml:space="preserve" xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A 2 Million Commercial Song Interactive Navigator</title>
			</titleStmt>
			<publicationStmt>
				<publisher />
				<availability status="unknown"><licence /></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Michel</forename><surname>Buffa</surname></persName>
							<email>buffa@i3s.unice.fr</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Université Côte d'Azur CNRS</orgName>
								<orgName type="institution" key="instit2">INRIA</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jerome</forename><surname>Lebrun</surname></persName>
							<email>lebrun@i3s.unice.fr</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Université Côte d'Azur CNRS</orgName>
								<orgName type="institution" key="instit2">INRIA</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Johan</forename><surname>Pauwels</surname></persName>
							<email>j.pauwels@qmul.ac.uk</email>
							<affiliation key="aff1">
								<orgName type="institution">Queen Mary University of London</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Guillaume</forename><surname>Pellerin</surname></persName>
							<email>guillaume.pellerin@ircam.fr</email>
							<affiliation key="aff2">
								<orgName type="institution">IRCAM</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">A 2 Million Commercial Song Interactive Navigator</title>
					</analytic>
					<monogr>
						<imprint>
							<date />
						</imprint>
					</monogr>
					<idno type="MD5">18A7B952A88FAA4C2F929A7AE3F66396</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-04-12T14:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid" />
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div><p>In this paper, we present a web-based interactive tool for exploring a collection of two million commercially released songs. It gathers song information from a large number of heterogeneous sources, web-based and audio-based, and integrates work from multiple research groups. The resulting tool can be used to request information about a specific song such as lyrics, metadata and chords; to navigate further on to linked external resources such as Discogs, AllMusic, Mu-sicBrainz or a number of streaming providers; or to browse the collection by artist's discographies or band membership.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div><head n="1.">INTRODUCTION</head><p>Figure <ref type="figure">2</ref>: the metadata of the WASABI database is complemented by computational analysis of lyrics and audio, both on-demand and precomputed.</p><p>Since 2017, a 2M song database consisting of metadata collected from the Web of Data <ref type="bibr" target="#b3">[4]</ref> has been constructed in the context of the WASABI research project <ref type="foot" target="#foot_0">1</ref> . The heterogeneous sources that have been consulted are displayed on the right of Figure <ref type="figure" target="#fig_0">1</ref>.</p><p>These metadata include the identifiers of the corresponding audio on a variety of audio platforms, which allowed to enrich the database with computational analyses of the lyrics and audio data (see Figure <ref type="figure">2</ref>). Several research groups have contributed to the analysis and have built interactive Web Audio applications on top of the output. For example, IR-CAM linked their <software ContextAttributes="created">TimeSide</software> analysis and annotation tool to make on-demand audio analysis possible. Queen Mary University of London, through the FAST project<ref type="foot" target="#foot_1">2</ref> , performed an offline chord analysis of 442k songs, and built an online enhanced audio player <ref type="bibr" target="#b14">[15]</ref> and chord search engine <ref type="bibr" target="#b15">[16]</ref> around it. The I3S laboratory, responsible for the design and implementation of the database, extracted song structure based on lyrics analysis <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b11">12]</ref>. Furthermore, they have developed a rich set of Web Audio applications and plugins <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7</ref>] that allow, for example, songs to be played along with sounds similar to those used by artists.</p><p>These metadata, computational analyses and Web Audio applications have now been gathered in one easy-to-use web interface, the <software>WASABI Interactive Navigator</software>, which is presented in this paper.</p></div>
<div><head n="2.">RELATED WORKS</head><p>Other research projects aimed to collect metadata on a large set of commercial songs, such as The Million Song Dataset project from 2011, which is mainly based on audio data <ref type="bibr" target="#b2">[3]</ref> and did not exploit the availability of large structured data sources from the Web of Data to the full extent.</p><p>MusicWeb and its successor MusicLynx <ref type="bibr" target="#b0">[1]</ref> link music artists within a Web-based application for discovering connections between them and provides a browsing experience using extra-musical relations. The project shares some ideas with WASABI, but works on the artist level, and does not perform analyses on the audio and lyrics content itself. It reuses, for example, MIR metadata from AcousticBrainz.</p><p>The WASABI project has been built on a broader scope than these projects and mixes a wider set of metadata, including ones from audio and natural language processing of lyrics. In addition, as presented in this paper, it comes with a large set of Web Audio enhanced applications.</p></div>
<div><head n="3.">THE WASABI INTERACTIVE NAVIGA-TOR HOMEPAGE</head><p>The primary starting point for the interactive navigator is the home screen as seen in Figure <ref type="figure" target="#fig_1">3</ref>. From here, a textual search can be performed for one of the 2 million songs, 200k albums or 77k artists included in the database <ref type="bibr" target="#b3">[4]</ref>. The resulting page displays information aggregated from a huge set of online sources, as shown in Figure <ref type="figure" target="#fig_2">4</ref>.</p><p>A set of tabs at the top link directly to the tools based on the computational analysis of audio and lyrics, which will be discussed next.</p></div>
<div><head n="4.">SONG STRUCTURE DERIVED FROM LYRICS AND AUDIO</head><p>On the pages for individual songs, the musical structure is indicated by grouping the lyrics in blocks, as shown in Figure <ref type="figure">5</ref>. Song lyrics contain repeated patterns that facilitate automated segmentation, with the detection of constitutive elements of a song text (e.g., intro, chorus) as goal.</p><p>We proposed to segment lyrics by applying a convolutional neural network to a synchronized audio-text representation of a song. First, we created a corpus projecting the segmentation of the lyrics of the WASABI corpus onto a synchronized lyric-audio corpus (DALI corpus 3 ). We have shown that the information in the text enriched with the characteristics of the audio signal allows our segmentation model to surpass the state of the art method, which is based solely on textual characteristics <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b11">12]</ref>.</p></div>
<div><head n="5.">CHORD PLAYER AND SEARCH</head><p>Chord transcriptions can be requested from the song pages, and are presented as an interactive player showing the chords in sync with the music, such that musicians can play along with them (see Figure <ref type="figure">6</ref>, top). As an alternative entrypoint to the WASABI database, a chord searching interface is available to find specific songs that contain a certain set of chords (bottom of Figure <ref type="figure">6</ref>). This type of search interface has been previously tested with the Jamendo music collection <ref type="bibr" target="#b15">[16]</ref>. For the search interface to work, the songs need to be indexed offline. So far, 442k files have been analysed with the chord analysis algorithm proposed in <ref type="bibr" target="#b13">[14]</ref>. In order to comply with copyright requirements, a remote processing toolchain has been set-up in which algorithms packaged as a Docker container are sent to Deezer for processing on their servers. The output is then returned to us and stored in the WASABI database. Calculation of further music descriptors such as tempo and key is ongoing.</p></div>
<div><head n="6.">ON-DEMAND AUDIO ANALYSIS</head><p>On-demand audio analysis is complementary to precomputed analysis in the sense that it avoids large, upfront computational costs and scales easily to changes in data and algorithms. It comes with its own challenges, however. Building a web platform that depends on various external services requires that the underlying software architecture and data model must be robust against disappearing sources. For example, if a YouTube video gets removed, we still want its computational analysis to remain available in case any of the web services built on top refer to the track and its related metadata. The <software ContextAttributes="created">TimeSide</software> framework<ref type="foot" target="#foot_2">4</ref> has been designed to provide a RESTful API as well as plugin based core library dedicated to audio processing that can provide this resilience.</p><p>In the context of the WASABI project, we demonstrate how the service (see Figure <ref type="figure" target="#fig_0">1</ref>) can be used on demand by a master semantical application that will feed <software ContextAttributes="created">TimeSide</software> with URLs coming from various providers (YouTube, Deezer, etc.) and then dynamically return the analysis data to the client application to feed Web Audio based tools. A number of audio-based characteristics can be requested from a song's page in this way.</p></div>
<div><head n="7.">MULTITRACK PLAYER AND EFFECTS</head><p>In order to assist music schools, which is one of the target audiences of the WASABI project, some Web Audio tools Figure <ref type="figure">5</ref>: song lyric structure detection <ref type="bibr" target="#b9">[10]</ref>.</p><p>have been integrated into the Navigator. One possible scenario is that music teachers pick a song from the database for their students to learn. An enhanced multi-track player (Figure <ref type="figure" target="#fig_4">7</ref>) is presented to the users, which displays the song's sheet music and allows to play back or selectively mute the different instruments in the song. Furthermore, a simplified DAW is available in the browser for recording and playing back student or teacher performances. It includes real-time audio effects, with ready to use presets, to attain a realistic and attractive studio sound.</p><p>The in-browser audio effects have been implemented using a Web Audio plugin standard <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref> (including SDK, online validation tools and examples of host applications), for which a large set of plugins 5 already exists. An online IDE for designing these plugins <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b12">13]</ref> and a guitar tube amp simulator designer <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref> are also available for developers.</p></div>
<div><head n="8.">FUTURE ENHANCEMENTS</head><p>Fine-tuning the parameters in many of our Web Audio applications (tube amplifiers, pedals, etc. . . ) has been quite tedious and time-consuming with a high level of expertise  required <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref>. To ease this process of presetting for music school students, we are currently exploring machine-learning based approaches (with scattering wavelet based convolutional neural networks <ref type="bibr" target="#b1">[2]</ref>) to learn and extract the relevant features from large datasets of songs and to match these with presets leading to similar subjective timbre in simulated instruments and also to automatically classify songs in the WASABI database.</p><p>Furthermore, offline indexation of more audio-based characteristics is ongoing.  </p></div>
<div><head n="9.">ACKNOWLEDGMENTS</head></div><figure xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: a schema of the WASABI network architecture and workflow.</figDesc><graphic coords="2,53.80,359.20,239.10,251.50" type="bitmap" /></figure>
<figure xml:id="fig_1"><head>3Figure 3 :</head><label>3</label><figDesc>Figure 3: main page of the WASABI navigator interface.</figDesc><graphic coords="3,316.81,176.30,239.09,211.84" type="bitmap" /></figure>
<figure xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: a large set of online databases has been exploited for building the metadata part of the WASABI database.</figDesc><graphic coords="4,53.80,53.80,239.10,240.34" type="bitmap" /></figure>
<figure xml:id="fig_3"><head>5</head><label /><figDesc>Figure 6: augmented player and search engine, integrated in the navigator interface.</figDesc><graphic coords="4,316.81,492.69,239.10,191.99" type="bitmap" /></figure>
<figure xml:id="fig_4"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: a multitrack player capable of displaying Guitar Pro music tabs in sync with the audio [5].</figDesc><graphic coords="5,53.80,53.80,239.10,255.31" type="bitmap" /></figure>
<figure xml:id="fig_5"><head /><label /><figDesc>This work was supported by the French Research National Agency (ANR) and the WASABI team (contract ANR-16-CE23-0017-01). Part of this work has been funded by UK EPSRC grant EP/L019981/1.</figDesc></figure>
<figure xml:id="fig_6"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Web Audio embedded DAW and effect chain for recording and playback.</figDesc><graphic coords="5,53.80,554.08,239.08,130.60" type="bitmap" /></figure>
			<note place="foot" n="1" xml:id="foot_0"><p>http://wasabihome.i3s.unice.fr/</p></note>
			<note place="foot" n="2" xml:id="foot_1"><p>http://www.semanticaudio.ac.uk</p></note>
			<note place="foot" n="4" xml:id="foot_2"><p>https://github.com/Parisson/<software>TimeSide</software>/</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">MusicLynx: Exploring music through artist similarity graphs</title>
		<author>
			<persName><forename type="first">A</forename><surname>Allik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Thalmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Companion Proc. (Dev. Track) The Web Conf</title>
		<meeting><address><addrLine>WWW</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep scattering spectrum</title>
		<author>
			<persName><forename type="first">J</forename><surname>Anden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mallat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Sig. Proc</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">16</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The Million Song Dataset</title>
		<author>
			<persName><forename type="first">T</forename><surname>Bertin-Mahieux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Whitman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lamere</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 12th Int. Soc. Music Information Retrieval (ISMIR 2011)</title>
		<meeting>12th Int. Soc. Music Information Retrieval (ISMIR 2011)</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">WASABI: a two million song database project with audio and cultural metadata plus WebAudio enhanced client applications</title>
		<author>
			<persName><forename type="first">M</forename><surname>Buffa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 3rd Web Audio Conf. (WAC 2017)</title>
		<meeting>3rd Web Audio Conf. (WAC 2017)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">MT5: a HTML5 multitrack player for musicians</title>
		<author>
			<persName><forename type="first">M</forename><surname>Buffa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hallili</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Renevier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 1st Web Audio Conf. (WAC2015)</title>
		<meeting>1st Web Audio Conf. (WAC2015)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Real time tube guitar amplifier simulation using webaudio</title>
		<author>
			<persName><forename type="first">M</forename><surname>Buffa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lebrun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 3rd Web Audio Conference (WAC 2017)</title>
		<meeting>3rd Web Audio Conference (WAC 2017)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Web audio guitar tube amplifier vs native simulations</title>
		<author>
			<persName><forename type="first">M</forename><surname>Buffa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lebrun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 3rd Web Audio Conf. (WAC 2017)</title>
		<meeting>3rd Web Audio Conf. (WAC 2017)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Towards an open web audio plugin standard</title>
		<author>
			<persName><forename type="first">M</forename><surname>Buffa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lebrun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kleimola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Larkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Letz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Companion Proc. (Dev. Track) The Web Conf</title>
		<meeting><address><addrLine>WWW</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">WAP: Ideas for a Web Audio plug-in standard</title>
		<author>
			<persName><forename type="first">M</forename><surname>Buffa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lebrun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kleimola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Larkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Letz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Pellerin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 4th Web Audio Conf. (WAC 2018)</title>
		<meeting>4th Web Audio Conf. (WAC 2018)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Lyrics segmentation: Textual macrostructure detection using convolutions</title>
		<author>
			<persName><forename type="first">M</forename><surname>Fell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Nechaev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Cabrio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Gandon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 27th Int. Conf. on Computational Linguistics (COLING2018)</title>
		<meeting>27th Int. Conf. on Computational Linguistics (COLING2018)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Compiling Faust audio DSP code to WebAssembly</title>
		<author>
			<persName><forename type="first">S</forename><surname>Letz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Orlarey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Fober</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 3rd Web Audio Conf. (WAC 2017)</title>
		<meeting>3rd Web Audio Conf. (WAC 2017)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">DALI: A large dataset of synchronized audio, lyrics and notes, automatically created using teacher-student machine learning paradigm</title>
		<author>
			<persName><forename type="first">G</forename><surname>Meseguer-Brocal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cohen-Hadria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Peeters</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 19th Int. Soc. of Music Information Retrieval (ISMIR 2018)</title>
		<meeting>19th Int. Soc. of Music Information Retrieval (ISMIR 2018)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Syntactical and semantical aspects of Faust</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Orlarey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Fober</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Letz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Soft Computing</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">9</biblScope>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Confidence measures and their applications in music labelling systems based on hidden Markov models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pauwels</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>O'hanlon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Fazekas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 18th Int. Soc. Music Information Retrieval (ISMIR 2017)</title>
		<meeting>18th Int. Soc. Music Information Retrieval (ISMIR 2017)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A web-based system for suggesting new practice material to music learners based on chord content</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pauwels</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint Proc. 24th ACM IUI Workshops</title>
		<imprint>
			<date type="published" when="2019">IUI2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Exploring real-time visualisations to support chord learning with a large music collection</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pauwels</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Xambó</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Roma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Barthet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Fazekas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 4th Web Audio Conf. (WAC 2018)</title>
		<meeting>4th Web Audio Conf. (WAC 2018)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>