<TEI xmlns="http://www.tei-c.org/ns/1.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xml:space="preserve" xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Trade-offs in Large-Scale Distributed Tuplewise Estimation and Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher />
				<availability status="unknown"><licence /></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Robin</forename><surname>Vogel</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Telecom Paris</orgName>
								<orgName type="laboratory">LTCI</orgName>
								<orgName type="institution">Institut Polytechnique de Paris</orgName>
							</affiliation>
							<affiliation key="aff1">
								<address>
									<settlement>IDEMIA</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Aurélien</forename><surname>Bellet</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">INRIA</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Stephan</forename><surname>Clémençon</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Telecom Paris</orgName>
								<orgName type="laboratory">LTCI</orgName>
								<orgName type="institution">Institut Polytechnique de Paris</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ons</forename><surname>Jelassi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Telecom Paris</orgName>
								<orgName type="laboratory">LTCI</orgName>
								<orgName type="institution">Institut Polytechnique de Paris</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Guillaume</forename><surname>Papa</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Telecom Paris</orgName>
								<orgName type="laboratory">LTCI</orgName>
								<orgName type="institution">Institut Polytechnique de Paris</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Trade-offs in Large-Scale Distributed Tuplewise Estimation and Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date />
						</imprint>
					</monogr>
					<idno type="MD5">08E10124E98AD9DF1F1038CE241FFB74</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-04-12T14:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid" />
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Distributed Machine Learning</term>
					<term>Distributed Data Processing</term>
					<term>U -Statistics</term>
					<term>Stochastic Gradient Descent</term>
					<term>AUC Optimization</term>
				</keywords>
			</textClass>
			<abstract>
<div><p>The development of cluster computing frameworks has allowed practitioners to scale out various statistical estimation and machine learning algorithms with minimal programming effort. This is especially true for machine learning problems whose objective function is nicely separable across individual data points, such as classification and regression. In contrast, statistical learning tasks involving pairs (or more generally tuples) of data points -such as metric learning, clustering or ranking -do not lend themselves as easily to data-parallelism and in-memory computing. In this paper, we investigate how to balance between statistical performance and computational efficiency in such distributed tuplewise statistical problems. We first propose a simple strategy based on occasionally repartitioning data across workers between parallel computation stages, where the number of repartitioning steps rules the trade-off between accuracy and runtime. We then present some theoretical results highlighting the benefits brought by the proposed method in terms of variance reduction, and extend our results to design distributed stochastic gradient descent algorithms for tuplewise empirical risk minimization. Our results are supported by numerical experiments in pairwise statistical estimation and learning on synthetic and real-world datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div><head n="1">Introduction</head><p>Statistical machine learning has seen dramatic development over the last decades. The availability of massive datasets combined with the increasing need to perform predictive/inference/optimization tasks in a wide variety of domains has given a considerable boost to the field and led to successful applications. In parallel, there has been an ongoing technological progress in the architecture of data repositories and distributed systems, allowing to process ever larger (and possibly complex, high-dimensional) data sets gathered on distributed storage platforms. This trend is illustrated by the development of many easy-to-use cluster computing frameworks for large-scale distributed data processing. These frameworks implement the data-parallel setting, in which data points are partitioned across different machines which operate on their partition in parallel. Some striking examples are <software ContextAttributes="created">Apache Spark</software> <ref type="bibr" target="#b25">[26]</ref> and <software ContextAttributes="created">Petuum</software> <ref type="bibr" target="#b24">[25]</ref>, the latter being fully targeted to machine learning. The goal of such frameworks is to abstract away the network and communication aspects in order to ease the deployment of distributed algorithms on large computing clusters and on the cloud, at the cost of some restrictions in the types of operations and parallelism that can be efficiently achieved. However, these limitations as well as those arising from network latencies or the nature of certain memory-intensive operations are often ignored or incorporated in a stylized manner in the mathematical description and analysis of statistical learning algorithms (see e.g., <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b0">1]</ref>). The implementation of statistical methods proved to be theoretically sound may thus be hardly feasible in a practical distributed system, and seemingly minor adjustments to scale-up these procedures can turn out to be disastrous in terms of statistical performance, see e.g. the discussion in <ref type="bibr" target="#b17">[18]</ref>. This greatly restricts their practical interest in some applications and urges the statistics and machine learning communities to get involved with distributed computation more deeply <ref type="bibr" target="#b2">[3]</ref>.</p><p>In this paper, we propose to study these issues in the context of tuplewise estimation and learning problems, where the statistical quantities of interest are not basic sample means but come in the form of averages over all pairs (or more generally, d-tuples) of data points. Such data functionals are known as U -statistics <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b20">21]</ref>, and many empirical quantities describing global properties of a probability distribution fall in this category (e.g., the sample variance, the Gini mean difference, Kendall's tau coefficient). U -statistics are also natural empirical risk measures in several learning problems such as ranking <ref type="bibr" target="#b12">[13]</ref>, metric learning <ref type="bibr" target="#b23">[24]</ref>, cluster analysis <ref type="bibr" target="#b10">[11]</ref> and risk assessment <ref type="bibr" target="#b4">[5]</ref>. The behavior of these statistics is well-understood and a sound theory for empirical risk minimization based on U -statistics is now documented in the machine learning literature <ref type="bibr" target="#b12">[13]</ref>, but the computation of a U -statistic poses a serious scalability challenge as it involves a summation over an exploding number of pairs (or d-tuples) as the dataset grows in size. In the centralized (single machine) setting, this can be addressed by appropriate subsampling methods, which have been shown to achieve a nearly optimal balance between computational cost and statistical accuracy <ref type="bibr" target="#b11">[12]</ref>. Unfortunately, naive implementations in the case of a massive distributed dataset either greatly damage the accuracy or are inefficient due to a lot of network communication (or disk I/O). This is due to the fact that, unlike basic sample means, a U -statistic is not separable across the data partitions.</p><p>Our main contribution is to design and analyze distributed methods for statistical estimation and learning with U -statistics that guarantee a good trade-off between accuracy and scalability. Our approach incorporates an occasional data repartitioning step between parallel computing stages in order to circumvent the limitations induced by data partitioning over the cluster nodes. The number of repartitioning steps allows to trade-off between statistical accuracy and computational efficiency. To shed light on this phenomenon, we first study the setting of statistical estimation, precisely quantifying the variance of estimates corresponding to several strategies. Thanks to the use of Hoeffding's decomposition <ref type="bibr" target="#b16">[17]</ref>, our analysis reveals the role played by each component of the variance in the effect of repartitioning. We then discuss the extension of these results to statistical learning and design efficient and scalable stochastic gradient descent algorithms for distributed empirical risk minimization. Finally, we carry out some numerical experiments on pairwise estimation and learning tasks on synthetic and real-world datasets to support our results from an empirical perspective.</p><p>The paper is structured as follows. Section 2 reviews background on Ustatistics and their use in statistical estimation and learning, and discuss the common practices in distributed data processing. Section 3 deals with statistical tuplewise estimation: we introduce our general approach for the distributed setting and derive (non-)asymptotic results describing its accuracy. Section 4 extends our approach to statistical tuplewise learning. We provide experiments supporting our results in Section 5, and we conclude in Section 6. Proofs, technical details and additional results can be found in the supplementary material.</p></div>
<div><head n="2">Background</head><p>In this section, we first review the definition and properties of U -statistics, and discuss some popular applications in statistical estimation and learning. We then discuss the recent randomized methods designed to scale up tuplewise statistical inference to large datasets stored on a single machine. Finally, we describe the main features of cluster computing frameworks.</p></div>
<div><head n="2.1">U -Statistics: Definition and Applications</head><p>U -statistics are the natural generalization of i.i.d. sample means to tuples of points. We state the definition of U -statistics in their generalized form, where points can come from K ≥ 1 independent samples. Note that we recover classic sample mean statistics in the case where</p><formula xml:id="formula_0">K = d 1 = 1. Definition 1. (Generalized U -statistic) Let K ≥ 1 and (d 1 , . . . , d K ) ∈ N * K . For each k ∈ {1, . . . , K}, let X {1, ..., n k } = (X (k) 1 , . . . , X (k) n k ) be an inde- pendent sample of size n k ≥ d k composed of i.i.d. random variables with values in some measurable space X k with distribution F k (dx). Let h : X d1 1 ×• • •×X d K K → R be a measurable function, square integrable with respect to the probability dis- tribution µ = F ⊗d1 1 ⊗ • • • ⊗ F ⊗d K K . Assume w.l.o.g. that h(x (1) , . . . , x (K) ) is symmetric within each block of arguments x (k) (valued in X d k k ). The generalized (or K-sample) U -statistic of degrees (d 1 , . . . , d K ) with kernel H is defined as U n (h) = 1 K k=1 n k d k I1</formula><p>. . .</p><formula xml:id="formula_1">I K h(X<label>(1)</label></formula><p>I1 , X</p><p>I2 , . . . , X (K)</p><formula xml:id="formula_3">I K ),<label>(1)</label></formula><p>where I k denotes the sum over all n k d k subsets X (k)</p><formula xml:id="formula_4">I k = (X (k) i1 , . . . , X (k) i d k ) related to a set I k of d k indexes 1 ≤ i 1 &lt; . . . &lt; i d k ≤ n k and n = (n 1 , . . . , n K ).</formula><p>The U -statistic U n (h) is known to have minimum variance among all unbiased estimators of the parameter µ(h) = E h(X</p><formula xml:id="formula_5">(1) 1 , . . . , X (1) d1 , . . . , X (K) 1 , . . . , X (K) d K ) .</formula><p>The price to pay for this low variance is a complex dependence structure exhibited by the terms involved in the average (1), as each data point appears in multiple tuples. The (non)asymptotic behavior of U -statistics and U -processes (i.e., collections of U -statistics indexed by classes of kernels) can be investigated by means of linearization techniques <ref type="bibr" target="#b16">[17]</ref> combined with decoupling methods <ref type="bibr" target="#b20">[21]</ref>, reducing somehow their analysis to that of basic i.i.d. averages or empirical processes. One may refer to <ref type="bibr" target="#b18">[19]</ref> for an account of the asymptotic theory of U -statistics, and to <ref type="bibr" target="#b22">[23]</ref> (Chapter 12 therein) and <ref type="bibr" target="#b20">[21]</ref> for nonasymptotic results.</p><p>U -statistics are commonly used as point estimators for inferring certain global properties of a probability distribution as well as in statistical hypothesis testing. Popular examples include the (debiased) sample variance, obtained by setting</p><formula xml:id="formula_6">K = 1, d 1 = 2 and h(x 1 , x 2 ) = (x 1 -x 2 ) 2 , the Gini mean difference, where K = 1, d 1 = 2 and h(x 1 , x 2 ) = |x 1 -x 2 |, and Kendall's tau rank correlation, where K = 2, d 1 = d 2 = 1 and h((x 1 , y 1 ), (x 2 , y 1 )) = I{(x 1 -x 2 ) • (y 1 -y 2 ) &gt; 0}.</formula><p>U -statistics also correspond to empirical risk measures in statistical learning problems such as clustering <ref type="bibr" target="#b10">[11]</ref>, metric learning <ref type="bibr" target="#b23">[24]</ref> and multipartite ranking <ref type="bibr" target="#b13">[14]</ref>. The generalization ability of minimizers of such criteria over a class H of kernels can be derived from probabilistic upper bounds for the maximal deviation of collections of centered U -statistics under appropriate complexity conditions on H (e.g., finite VC dimension) <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b11">12]</ref>. Below, we describe the example of multipartite ranking used in our numerical experiments (Section 5). We refer to <ref type="bibr" target="#b11">[12]</ref> for details on more learning problems involving U -statistics.</p><p>Example 2 (Multipartite Ranking). Consider items described by a random vector of features X ∈ X with associated ordinal labels Y ∈ {1, . . . , K}, where K ≥ 2. The goal of multipartite ranking is to learn to rank items in the same preorder as that defined by the labels, based on a training set of labeled examples. Rankings are generally defined through a scoring function s : X → R transporting the natural order on the real line onto X . Given K independent samples, the empirical ranking performance of s(x) is evaluated by means of the empirical VUS (Volume Under the ROC Surface) criterion <ref type="bibr" target="#b13">[14]</ref>:</p><formula xml:id="formula_7">V U S(s) = 1 K k=1 n k n1 i1=1 . . . n K i K =1 I{s(X (1) i1 ) &lt; . . . &lt; s(X (K) i K )},<label>(2)</label></formula><p>which is a K-sample U -statistic of degree (1, . . . , 1) with kernel h s (x 1 , . . . , x K ) = I{s(x 1 ) &lt; . . . &lt; s(x K )}.</p></div>
<div><head n="2.2">Large-Scale Tuplewise Inference with Incomplete U -statistics</head><p>The cost related to the computation of the U -statistic (1) rapidly explodes as the sizes of the samples increase. Precisely, the number of terms involved in the</p><formula xml:id="formula_8">summation is n1 d1 ×• • •× n K d K ,</formula><p>which is of order O(n d1+...+d K ) when the n k 's are all asymptotically equivalent. Whereas computing U -statistics based on subsamples of smaller size would severely increase the variance of the estimation, the notion of incomplete generalized U -statistic <ref type="bibr" target="#b5">[6]</ref> enables to significantly mitigate this computational problem while maintaining a good level of accuracy.</p><formula xml:id="formula_9">Definition 3. (Incomplete generalized U -statistic) Let B ≥ 1.</formula><p>The incomplete version of the U -statistic (1) based on B terms is defined by:</p><formula xml:id="formula_10">U B (H) = 1 B I=(I1, ..., I K )∈D B h(X<label>(1)</label></formula><p>I1 , . . . , X (K)</p><formula xml:id="formula_11">I K ) (3)</formula><p>where D B is a set of cardinality B built by sampling uniformly with replacement in the set Λ of vectors of tuples ((i</p><formula xml:id="formula_12">(1) 1 , . . . , i<label>(1)</label></formula><p>d1 ), . . . , (i</p><formula xml:id="formula_13">(K) 1 , . . . , i (K) d K ))</formula><p>, where</p><formula xml:id="formula_14">1 ≤ i (k) 1 &lt; . . . &lt; i (k) d k ≤ n k and 1 ≤ k ≤ K.</formula><p>Note incidentally that the subsets of indices can be selected by means of other sampling schemes <ref type="bibr" target="#b11">[12]</ref>, but sampling with replacement is often preferred due to its simplicity. In practice, the parameter B should be picked much smaller than the total number of tuples to reduce the computational cost. Like (1), the quantity ( <ref type="formula">3</ref>) is an unbiased estimator of µ(H) but its variance is naturally larger:</p><formula xml:id="formula_15">Var( U B (h)) = 1 - 1 B Var(U n (h)) + 1 B Var(h(X<label>(1) 1</label></formula><formula xml:id="formula_16">, . . . , X (K) d K )).<label>(4)</label></formula><p>The recent work in <ref type="bibr" target="#b11">[12]</ref> has shown that the maximal deviations between (1) and ( <ref type="formula">3</ref>) over a class of kernels H of controlled complexity decrease at a rate of order O(1/ √ B) as B increases. An important consequence of this result is that sampling B = O(n) terms is sufficient to preserve the learning rate of order O P ( log n/n) of the minimizer of the complete risk (1), whose computation requires to average O(n d1+...+d K ) terms. In contrast, the distribution of a complete U -statistic built from subsamples of reduced sizes n k drawn uniformly at random is quite different from that of an incomplete U -statistic based on B = K k=1 n k d k terms sampled with replacement in Λ, although they involve the summation of the same number of terms. Empirical minimizers of such a complete U -statistic based on subsamples achieve a much slower learning rate of O P ( log(n)/n 1/(d1+...+d K ) ). We refer to <ref type="bibr" target="#b11">[12]</ref> for details and additional results.</p><p>We have seen that approximating complete U -statistics by incomplete ones is a theoretically and practically sound approach to tackle large-scale tuplewise estimation and learning problems. However, as we shall see later, the implementation is far from straightforward when data is stored and processed in standard distributed computing frameworks, whose key features are recalled below.</p></div>
<div><head n="2.3">Practices in Distributed Data Processing</head><p>Data-parallelism, i.e. partitioning the data across different machines which operate in parallel, is a natural approach to store and efficiently process massive datasets. This strategy is especially appealing when the key stages of the computation to be executed can be run in parallel on each partition of the data. As a matter of fact, many estimation and learning problems can be reduced to (a sequence of) local computations on each machine followed by a simple aggregation step. This is the case of gradient descent-based algorithms applied to standard empirical risk minimization problems, as the objective function is nicely separable across individual data points. Optimization algorithms operating in the data-parallel setting have indeed been largely investigated in the machine learning community, see <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b21">22]</ref> and references therein for some recent work.</p><p>Because of the prevalence of data-parallel applications in large-scale machine learning, data analytics and other fields, the past few years have seen a sustained development of distributed data processing frameworks designed to facilitate the implementation and the deployment on computing clusters. Besides the seminal <software ContextAttributes="created">MapReduce</software> framework <ref type="bibr" target="#b15">[16]</ref>, which is not suitable for iterative computations on the same data, one can mention <software ContextAttributes="created">Apache Spark</software> <ref type="bibr" target="#b25">[26]</ref>, <software ContextAttributes="created">Apache Flink</software> <ref type="bibr" target="#b9">[10]</ref> and the machine learning-oriented <software ContextAttributes="created">Petuum</software> <ref type="bibr" target="#b24">[25]</ref>. In these frameworks, the data is typically first read from a distributed file system (such as HDFS, <software ContextAttributes="created">Hadoop Distributed File System</software>) and partitioned across the memory of each machine in the form of an appropriate distributed data structure. The user can then easily specify a sequence of distributed computations to be performed on this data structure (map, filter, reduce, etc.) through a simple API which hides low-level distributed primitives (such as message passing between machines). Importantly, these frameworks natively implement fault-tolerance (allowing efficient recovery from node failures) in a way that is also completely transparent to the user.</p><p>While such distributed data processing frameworks come with a lot of benefits for the user, they also restrict the type of computations that can be performed efficiently on the data. In the rest of this paper, we investigate these limitations in the context of tuplewise estimation and learning problems, and propose solutions to achieve a good trade-off between accuracy and scalability.</p></div>
<div><head n="3">Distributed Tuplewise Statistical Estimation</head><p>In this section, we focus on the problem of tuplewise statistical estimation in the distributed setting (an extension to statistical learning is presented in Section 4). We consider a set of N ≥ 1 workers in a complete network graph (i.e., any pair of workers can exchange messages). For convenience, we assume the presence of a master node, which can be one of the workers and whose role is to aggregate estimates computed by all workers.</p><p>For ease of presentation, we restrict our attention to the case of two sample U -statistics of degree (1, 1) (K = 2 and d 1 = d 2 = 1), see Remark 7 in Section 3.3 for extensions to the general case. We denote by D n = {X 1 , . . . , X n } the first sample and by Q m = {Z 1 , . . . , Z m } the second sample (of sizes n and m respectively). These samples are distributed across the N workers. For i ∈ {1, . . . , N }, we denote by R i the subset of data points held by worker i and, unless otherwise noted, we assume for simplicity that all subsets are of equal size</p><formula xml:id="formula_17">|R i | = n+m N ∈ N.</formula><p>The notations R X i and R Z i respectively denote the subset of data points held by worker i from D n and Q m , with R X i ∪ R Z i = R i . We denote their (possibly random) cardinality by</p><formula xml:id="formula_18">n i = |R X i | and m i = |R Z i |.</formula><p>Given a kernel h, the goal is to compute a good estimate of the parameter U (h) = E[h(X 1 , Z 1 )] while meeting some computational and communication constraints.</p></div>
<div><head n="3.1">Naive Strategies</head><p>Before presenting our approach, we start by introducing two simple (but ineffective) strategies to compute an estimate of U (h). The first one is to compute the complete two-sample U -statistic associated with the full samples D n and Q m :</p><formula xml:id="formula_19">U n (h) = 1 nm n k=1 m l=1 h(X k , Z l ),<label>(5)</label></formula><p>with n = (n, m). While U n (h) has the lowest variance among all unbiased estimates that can be computed from (D n , Q m ), computing it is a highly undesirable solution in the distributed setting where each worker only has access to a subset of the dataset. Indeed, ensuring that each possible pair is seen by at least one worker would require massive data communication over the network. Note that a similar limitation holds for incomplete versions of (5) as defined in Definition 3.</p><p>A feasible strategy to go around this problem is for each worker to compute the complete U -statistic associated with its local subsample R i , and to send it to the master node who averages all contributions. This leads to the estimate</p><formula xml:id="formula_20">U n,N (h) = 1 N N i=1 U Ri (h) where U Ri (h) = 1 n i m i k∈R X i l∈R Z i h(X k , Z l ). (6)</formula><p>Note that if min(n i , m i ) = 0, we simply set U Ri (h) = 0.</p><p>Alternatively, as the R i 's may be large, each worker can compute an incomplete U -statistic U B,Ri (h) with B terms instead of U Ri , leading to the estimate</p><formula xml:id="formula_21">U n,N,B (h) = 1 N N i=1 U B,Ri (h) where U B,Ri (h) = 1 B (k,l)∈R i,B h(X k , Z l ),<label>(7)</label></formula><p>with R i,B a set of B pairs built by sampling uniformly with replacement from the local subsample R X i × R Z i . As shown in Section 3.3, strategies ( <ref type="formula">6</ref>) and ( <ref type="formula" target="#formula_21">7</ref>) have the undesirable property that their accuracy decreases as the number of workers N increases. This motivates our proposed approach, introduced in the following section.</p></div>
<div><head n="3.2">Proposed Approach</head><p>The naive strategies presented above are either accurate but very expensive (requiring a lot of communication across the network), or scalable but potentially inaccurate. The approach we promote here is of disarming simplicity and aims at finding a sweet spot between these two extremes. The idea is based on repartitioning the dataset a few times across workers (we keep the repartitioning scheme abstract for now and postpone the discussion of concrete choices to subsequent sections). By alternating between parallel computation and repartitioning steps, one considers several estimates based on the same data points. This allows to observe a greater diversity of pairs and thereby refine the quality of our final estimate, at the cost of some additional communication.</p><p>Formally, let T be the number of repartitioning steps. We denote by R t i the subsample of worker i after the t-th repartitioning step, and by U R t i (h) the complete U -statistic associated with R t i . At each step t ∈ {1, . . . , T }, each worker i computes U R t i (h) and sends it to the master node. After T steps, the master node has access to the following estimate:</p><formula xml:id="formula_22">U n,N,T (h) = 1 T T t=1 U t n,N (h),<label>(8)</label></formula><p>where</p><formula xml:id="formula_23">U t n,N (h) = 1 N N i=1 U R t i (h).</formula><p>Similarly as before, workers may alternatively compute incomplete U -statistics U B,R t i (h) with B terms. The estimate is then:</p><formula xml:id="formula_24">U n,N,B,T (h) = 1 T T t=1 U t n,N,B (h),<label>(9)</label></formula><p>where</p><formula xml:id="formula_25">U t n,N,B (h) = 1 N N i=1 U B,R t i (h).</formula><p>These statistics, and those introduced in Section 3.1 which do not rely on repartition, are summarized in Figure <ref type="figure" target="#fig_0">1</ref>.</p><p>Of course, the repartitioning operation is rather costly in terms of runtime so T should be kept to a reasonably small value. We illustrate this trade-off by the analysis presented in the next section.</p></div>
<div><head n="3.3">Analysis</head><p>In this section, we analyze the statistical properties of the various estimators introduced above. We focus here on repartitioning by proportional sampling with-out replacement (prop-SWOR). Prop-SWOR creates partitions that contain the same proportion of elements of each sample: specifically, it ensures that at any step t and for any worker i,</p><formula xml:id="formula_26">|R t i | = n+m N with |R t,X i | = n N and |R t,Z i | = m N .</formula><p>We discuss the practical implementation of this repartitioning scheme as well as some alternative choices in Section 3.4.</p><p>All estimators are unbiased when repartitioning is done with prop-SWOR. We will thus compare their variance. Our main technical tool is a linearization technique for U -statistics known as Hoeffding's Decomposition (see <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b11">12]</ref>).</p></div>
<div><head>Definition 4. (Hoeffding's decomposition)</head><formula xml:id="formula_27">Let h 1 (x) = E[h(x, Z 1 )], h 2 (z) = E[h(X 1 , z)] and h 0 (x, z) = h(x, z) -h 1 (x) -h 2 (z) + U (h). U n (h) -U (h)</formula><p>can be written as a sum of three orthogonal terms:</p><formula xml:id="formula_28">U n (h) -U (h) = T n (h) + T m (h) + W n (h), where T n (h) = 1 n n k=1 h 1 (X k ) -U (h) and T m (h) = 1 m n l=1 h 2 (Z l ) -U (h) are sums of independent r.v, while W n (h) = 1 nm n k=1 m l=1 h 0 (X k , Z l ) is a degen- erate U -statistic (i.e., E[h(X 1 , Z 1 )|X 1 ] = U (h) and E[h(X 1 , Z 1 )|Z 1 ] = U (h)).</formula><p>This decomposition is very convenient as the two terms T n (h) and T m (h) are decorrelated and the analysis of W n (h) (a degenerate U -statistic) is well documented <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b11">12]</ref>. It will allow us to decompose the variance of the estimators of interest into single-sample components σ 2 1 = Var(h 1 (X)) and σ 2 2 = Var(h 2 (Z)) on the one hand, and a pairwise component σ 2 0 = Var(h 0 (X 1 , Z 1 )) on the other hand. Denoting σ 2 = Var(h(X 1 , Z 1 )), we have</p><formula xml:id="formula_29">σ 2 = σ 2 0 + σ 2 1 + σ 2 2 .</formula><p>It is well-known that the variance of the complete U -statistic U n (h) can be written as Var(U n (h)) =</p><formula xml:id="formula_30">σ 2 1 n + σ 2 2 m + σ 2 0</formula><p>nm (see supplementary material for details). Our first result gives the variance of the estimators which do not rely on a repartitioning of the data with respect to the variance of U n (h).</p><p>Theorem 5. If the data is distributed over workers using prop-SWOR, we have:</p><formula xml:id="formula_31">Var(U n,N (h)) = Var(U n (h)) + (N -1) σ 2 0 nm , Var( U n,N,B (h)) = 1 - 1 B Var(U n,N (h)) + σ 2 N B .</formula><p>Theorem 5 precisely quantifies the excess variance due to the distributed setting if one does not use repartitioning. Two important observations are in order. First, the variance increase is proportional to the number of workers N , which clearly defeats the purpose of distributed processing. Second, this increase only depends on the pairwise component σ 2 0 of the variance. In other words, the average of U -statistics computed independently over the local partitions contains all the information useful to estimate the single-sample contributions, but fails to accurately estimate the pairwise contributions. The resulting estimates thus lead to significantly larger variance when the choice of kernel and the data distributions imply that σ 2 0 is large compared to σ 1 2 and/or σ 2 1 . The extreme case happens when U n (h) is a degenerate U -statistic, i.e. σ 2 1 = σ 2 2 = 0 and σ 2 0 &gt; 0, which is verified for example when h(x, z) = x • z and X, Z are both centered random variables.</p><p>We now characterize the variance of the estimators that leverage data repartitioning steps. Theorem 6. If the data is distributed and repartitioned between workers using prop-SWOR, we have:</p><formula xml:id="formula_32">Var( U n,N,T (h)) = Var(U n (h)) + (N -1) σ 2 0 nmT , Var( U n,N,B,T (h)) = Var( U n,N,T (h)) - 1 T B Var(U n,N (h)) + σ 2 N T B .</formula><p>Theorem 6 shows that the value of repartitioning arises from the fact that the term accounting for the pairwise variance in U n,N,T (h) is T times lower than that of U n,N (h). This validates the fact that repartitioning is beneficial when the pairwise variance term is significant in front of the other terms. Interestingly, Theorem 6 also implies that for a fixed budget of evaluated pairs, using all pairs on each worker is always a dominant strategy over using incomplete approximations. Specifically, we can show that under the constraint N BT = nmT 0 /N , Var( U n,N,T0 (h)) is always smaller than Var( U n,N,B,T (h)), see supplementary material for details. Note that computing complete U -statistics also require fewer repartitioning steps to evaluate the same number of pairs (i.e., T 0 ≤ T ).</p><p>We conclude the analysis with a visual illustration of the variance of various estimators with respect to the number of pairs they evaluate. We consider the imbalanced setting where n m, which is commonly encountered in applications such as imbalanced classification, bipartite ranking and anomaly detection. In this case, it suffices that σ 2 2 be small for the influence of the pairwise component of the variance to be significant, see Fig. <ref type="figure" target="#fig_1">2</ref> (left). The figure also confirms that complete estimators dominate their incomplete counterparts. On the other hand, when σ 2 2 is not small, the variance of U n mostly originates from the rarity of the minority sample, hence repartitioning does not provide estimates that are significantly more accurate (see Fig. <ref type="figure" target="#fig_1">2</ref>, right). We refer to Section 5 for experiments on concrete tasks with synthetic and real data.</p><p>Remark 7 (Extension to high-order U -statistics). The extension of our analysis to general U -statistics is straightforward and left to the reader (see <ref type="bibr" target="#b11">[12]</ref> for a review of the relevant technical tools). We stress the fact that the benefits of repartitioning are even stronger for higher-order U -statistics (K &gt; 2 and/or larger degrees) because higher-order components of the variance are also affected.</p></div>
<div><head n="3.4">Practical Considerations and Other Repartitioning Schemes</head><p>The analysis above assumes that repartitioning is done using prop-SWOR, which has the advantage of exactly preserving the proportion of points from the two samples D n and Q m even in the event of significant imbalance in their size. However, a naive implementation of prop-SWOR requires some coordination between workers at each repartitioning step. To avoid exchanging many messages, we propose that the workers agree at the beginning of the protocol on a numbering of the workers, a numbering of the points in each sample, and a random seed to use in a pseudorandom number generator. This allows the workers to implement prop-SWOR without any further coordination: at each repartitioning step, they independently draw the same two random permutations over {1, . . . , n} and {1, . . . , m} using the common random seed and use these permutations to assign each point to a single worker.</p><p>Of course, other repartitioning schemes can be used instead of prop-SWOR. A natural choice is sampling without replacement (SWOR), which does not require any coordination between workers. However, the partition sizes generated by SWOR are random. This is a concern in the case of imbalanced samples, where the probability that a worker i does not get any point from the minority sample (and thus no pair to compute a local estimate) is non-negligible. For these reasons, it is difficult to obtain exact and concise theoretical variances for the SWOR case, but we show in the supplementary material that the results with SWOR should not deviate too much from those obtained with prop-SWOR. For completeness, in the supplementary material we also analyze the case of proportional sampling with replacement (prop-SWR): results are quantitatively similar, aside from the fact that redistribution also corrects for the loss of information that occurs because of sampling with replacement.</p><p>Finally, we note that deterministic repartitioning schemes may be used in practice for simplicity. For instance, the repartition method in <software>Apache Spark</software> relies on a deterministic shuffle which preserves the size of the partitions.</p></div>
<div><head n="4">Extensions to Stochastic Gradient Descent for ERM</head><p>The results of Section 3 can be extended to statistical learning in the empirical risk minimization framework. In such problems, given a class of kernels H, one seeks the minimizer of ( <ref type="formula">6</ref>) or (8) depending on whether repartition is used. <ref type="foot" target="#foot_1">4</ref> Under appropriate complexity assumptions on H (e.g., of finite VC dimension), excess risk bounds for such minimizers can be obtained by combining our variance analysis of Section 3 with the control of maximal deviations based on Bernstein-type concentration inequalities as done in <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b11">12]</ref>. Due to the lack of space, we leave the details of such analysis to the readers and focus on the more practical scenario where the ERM problem is solved by gradient-based optimization algorithms.</p></div>
<div><head n="4.1">Gradient-based Empirical Minimization of U -statistics</head><p>In the setting of interest, the class of kernels to optimize over is indexed by a real-valued parameter θ ∈ R q representing the model. Adapting the notations of Section 3, the kernel h : X 1 × X 2 × R q → R then measures the performance of a model θ ∈ R q on a given pair, and is assumed to be convex and smooth in θ. Empirical Risk Minimization (ERM) aims at finding θ ∈ R q minimizing</p><formula xml:id="formula_33">U n (θ) = 1 nm n k=1 m l=1 h(X k , Z l ; θ).<label>(10)</label></formula><p>The minimizer can be found by means of Gradient Descent (GD) techniques. 5  Starting at iteration s = 1 from an initial model θ 1 ∈ R q and given a learning rate γ &gt; 0, GD consists in iterating over the following update:</p><formula xml:id="formula_34">θ s+1 = θ s -γ∇ θ U n (θ s ).<label>(11)</label></formula><p>Note that the gradient ∇ θ U n (θ) is itself a U -statistic with kernel given by ∇ θ H, and its computation is very expensive in the large-scale setting. In this regime, Stochastic Gradient Descent (SGD) is a natural alternative to GD which is known to provide a better trade-off between the amount of computation and the performance of the resulting model <ref type="bibr" target="#b6">[7]</ref>. Following the discussion of Section 2.2, a natural idea to implement SGD is to replace the gradient ∇ θ U n (θ) in <ref type="bibr" target="#b10">(11)</ref> by an unbiased estimate given by an incomplete U -statistic. The work of <ref type="bibr" target="#b19">[20]</ref> shows that SGD converges much faster than if the gradient is estimated using a complete U -statistic based on subsamples with the same number of terms. However, as in the case of estimation, the use of standard complete or incomplete U -statistics turns out to be impractical in the distributed setting. Building upon the arguments of Section 3, we propose a more suitable strategy.</p></div>
<div><head n="4.2">Repartitioning for Stochastic Gradient Descent</head><p>The approach we propose is to alternate between SGD steps using withinpartition pairs and repartitioning the data across workers. We introduce a parameter n r ∈ Z + corresponding to the number of iterations of SGD between each redistribution of the data. For notational convenience, we let r(s) := s/n r so that for any worker i, R r(s) i denotes its data partition at iteration s ≥ 1 of SGD. 5 When H is nonsmooth in θ, a subgradient may be used instead of the gradient.</p><p>Given a local batch size B, at each iteration s of SGD, we propose to adapt the strategy (9) by having each worker i compute a local gradient estimate using a set R s i,B of B randomly sampled pairs in its current local partition R r(s) i</p><p>:</p><formula xml:id="formula_35">∇ θ U B,R r(s) i (θ s ) = 1 B (k,l)∈R s i,B ∇ θ h(X k , Z l ; θ s ).</formula><p>This local estimate is then sent to the master node who averages all contributions, leading to the following global gradient estimate:</p><formula xml:id="formula_36">∇ θ U n,N,B (θ s ) = 1 N N i=1 ∇ θ U B,R r(s) i (θ s ).<label>(12)</label></formula><p>The master node then takes a gradient descent step as in <ref type="bibr" target="#b10">(11)</ref> and broadcasts the updated model θ s+1 to the workers. Following our analysis in Section 3, repartitioning the data allows to reduce the variance of the gradient estimates, which is known to greatly impact the convergence rate of SGD (see e.g. <ref type="bibr" target="#b8">[9]</ref>, Theorem 6.3 therein). When n r = +∞, data is never repartitioned and the algorithm minimizes an average of local U -statistics, leading to suboptimal performance. On the other hand, n r = 1 corresponds to repartitioning at each iteration of SGD, which minimizes the variance but is very costly and makes SGD pointless. We expect the sweet spot to lie between these two extremes: the dominance of U n,N,T over U n,N,B,T established in Section 3.3, combined with the common use of small batch size B in SGD, suggests that occasional redistributions are sufficient to correct for the loss of information incurred by the partitioning of data. We illustrate these trade-offs experimentally in the next section.</p></div>
<div><head n="5">Numerical Results</head><p>In this section, we illustrate the importance of repartitioning for estimating and optimizing the Area Under the ROC Curve (AUC) through a series of numerical experiments. The corresponding U -statistic is the two-sample version of the multipartite ranking VUS introduced in Example 2 (Section 2.1). The first experiment focuses on the estimation setting considered in Section 3. The second experiment shows that redistributing the data across workers, as proposed in Section 4, allows for more efficient mini-batch SGD. All experiments use prop-SWOR and are conducted in a simulated environment.</p><p>Estimation experiment. We seek to illustrate the importance of redistribution for estimating two-sample U -statistics with the concrete example of the AUC. The AUC is obtained by choosing the kernel h(x, z) = I{z &lt; x}, and is widely used as a performance measure in bipartite ranking and binary classification with class imbalance. Recall that our results of Section 3.3 highlighted the key role of the pairwise component of the variance σ 2 0 being large compared to the single-sample  components. In the case of the AUC, this happens when the data distributions are such that the expected outcome using single-sample information is far from the truth, e.g. in the presence of hard pairs. We illustrate this on simple discrete distributions for which we can compute σ 2 0 , σ 2 1 and σ 2 2 in closed form. Consider positive points X ∈ {0, 2}, negative points Z ∈ {-1, +1} and P (X = 2) = q, P (Z = +1) = p. It follows that:</p><formula xml:id="formula_37">σ 2 1 = p 2 q(1 -q), σ 2 2 = (1 -q) 2 p(1 -p), and σ 2 = p(1 -p + pq)(1 -q).<label>(13)</label></formula><p>Assume that the scoring function has a small probability to assign a low score to a positive instance or a large score to a negative instance. In our formal setting, this translates into letting p = 1 -q = for a small &gt; 0, which implies that</p><formula xml:id="formula_38">σ 2 0 σ 2 1 +σ 2 2 = 1- 2 → →0</formula><p>∞. We thus expect that as the true AUC U (h) = 1 -2 gets closer to 1, repartitioning the dataset becomes more critical to achieve good relative precision. This is confirmed numerically, as shown in Fig. <ref type="figure" target="#fig_2">3</ref>. Note that in practice, settings where the AUC is very close to 1 are very common as they correspond to well-functioning systems, such as face recognition systems.</p><p>Learning experiment. We now turn to AUC optimization, which is the task of learning a scoring function s : X → R that optimizes the VUS criterion (2) with K = 2 in order to discriminate between a negative and a positive class. We learn a linear scoring function s w,b (x) = w x + b, and optimize a continuous and convex surrogate of (2) based on the hinge loss. The resulting loss function to minimize is a two-sample U-statistic with kernel g w,b (x, z) = max(0, 1 + s w,b (x) -s w,b (z)) indexed by the parameters (w, b) of the scoring function, to which we add a small L2 regularization term of 0.05 w 2 2 . We use the shuttle dataset, a classic dataset for anomaly detection. <ref type="foot" target="#foot_2">6</ref> It contains roughly 49,000 points in dimension 9, among which only 7% (approx. 3,500) are anomalies. A high accuracy is expected for this dataset. To monitor the generalization performance, we keep 20% of the data as our test set, corresponding to 700 points of the minority class and approx. 9,000 points of the majority class. The test performance is measured with complete statistics over the 6.3 million pairs. The training set consists of the remaining data points, which we distribute over N = 100 workers. This leads to approx. 10, 200 pairs per worker. The gradient estimates are calculated following <ref type="bibr" target="#b11">(12)</ref> with batch size B = 100. We use an initial learning rate of 0.01 with a momentum of 0.9. As there are more than 100 million possible pairs in the training dataset, we monitor the training loss and accuracy on a fixed subset of 4.5 × 10 5 randomly sampled pairs. Fig. <ref type="figure" target="#fig_3">4</ref> shows the evolution of the continuous loss and the true AUC on the training and test sets along the iteration for different values of n r , from n r = 1 (repartition at each iteration) to n r = +∞ (no repartition). The lines are the median at each iteration over 100 runs, and the shaded area correspond to confidence intervals for the AUC and loss value of the testing dataset. We can clearly see the benefits of repartition: without it, the median performance is significantly lower and the variance across runs is very large. The results also show that occasional repartitions (e.g., every 25 iterations) are sufficient to mitigate these issues significantly.</p></div>
<div><head n="6">Future Work</head><p>We envision several further research questions on the topic of distributed tuplewise learning. We would like to provide a rigorous convergence rate analysis of the general distributed SGD algorithm introduced in Section 4. This is a challenging task because each series of iterations executed between two repartition steps can be seen as optimizing a slightly different objective function. It would also be interesting to investigate settings where the workers hold sensitive data that they do not want to share in the clear due to privacy concerns.</p></div><figure xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig.1. Graphical summary of the statistics that we compare: with/without repartition and with/without subsampling. Note that {(σt, πt)} T t=1 denotes a set of T independent couples of random permutations in Sn × Sm.</figDesc><graphic coords="9,152.06,116.83,311.23,93.96" type="bitmap" /></figure>
<figure xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Theoretical variance as a function of the number of evaluated pairs for different estimators under prop-SWOR, with n = 100, 000, m = 200 and N = 100.</figDesc></figure>
<figure xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Relative variance estimated over 5000 runs, n = 5000, m = 50, N = 10 and T = 4. Results are divided by the true variance of Un deduced from (13) and Theorem 5.</figDesc></figure>
<figure xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Learning dynamics for different repartition frequencies computed over 100 runs.</figDesc></figure>
			<note place="foot" xml:id="foot_0"><p>Trade-offs in Large-Scale Distributed Tuplewise Estimation and Learning</p></note>
			<note place="foot" n="4" xml:id="foot_1"><p>Alternatively, for scalability purposes, one may instead work with their incomplete counterparts, namely<ref type="bibr" target="#b6">(7)</ref> and (9) respectively.</p></note>
			<note place="foot" n="6" xml:id="foot_2"><p>http://odds.cs.stonybrook.edu/shuttle-dataset/</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Communication complexity of distributed convex learning and optimization</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Arjevani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Shamir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Distributed Learning, Communication Complexity and Privacy</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Balcan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Mansour</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>COLT</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Bekkerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bilenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Langford</surname></persName>
		</author>
		<title level="m">Scaling Up Machine Learning: Parallel and Distributed Approaches</title>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">A Distributed Frank-Wolfe Algorithm for Communication-Efficient Sparse Learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bellet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Garakani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Balcan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Sha</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>SDM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Incomplete generalized U -statistics for food risk assessment</title>
		<author>
			<persName><forename type="first">P</forename><surname>Bertail</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tressou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrics</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="66" to="74" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Some properties of incomplete U -statistics</title>
		<author>
			<persName><forename type="first">G</forename><surname>Blom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="573" to="580" />
			<date type="published" when="1976">1976</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The Tradeoffs of Large Scale Learning</title>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Bousquet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Distributed Optimization and Statistical Learning via the Alternating Direction Method of Multipliers</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Peleato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Eckstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends in Machine Learning</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="122" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Convex Optimization: Algorithms and Complexity</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bubeck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends in Machine Learning</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="231" to="357" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Apache Flink TM : Stream and Batch Processing in a Single Engine</title>
		<author>
			<persName><forename type="first">P</forename><surname>Carbone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Katsifodimos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ewen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Markl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Haridi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Tzoumas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Data Engineering Bulletin</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="28" to="38" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A statistical view of clustering performance through the theory of U-processes</title>
		<author>
			<persName><forename type="first">S</forename><surname>Clémençon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Multivariate Analysis</title>
		<imprint>
			<biblScope unit="volume">124</biblScope>
			<biblScope unit="page" from="42" to="56" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Scaling-up Empirical Risk Minimization: Optimization of Incomplete U-statistics</title>
		<author>
			<persName><forename type="first">S</forename><surname>Clémençon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bellet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Colin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="165" to="202" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Ranking and empirical risk minimization of U -statistics</title>
		<author>
			<persName><forename type="first">S</forename><surname>Clémençon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lugosi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Vayatis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="844" to="874" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Building confidence regions for the ROC surface</title>
		<author>
			<persName><forename type="first">S</forename><surname>Clémençon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Robbiano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="67" to="74" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Protocols for Learning Classifiers on Distributed Data</title>
		<author>
			<persName><forename type="first">Iii</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Saha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Venkatasubramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AISTATS</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Mapreduce: simplified data processing on large clusters</title>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="107" to="113" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A class of statistics with asymptotically normal distribution</title>
		<author>
			<persName><forename type="first">W</forename><surname>Hoeffding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Mathematics and Statistics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="293" to="325" />
			<date type="published" when="1948">1948</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">On statistics, computation and scalability</title>
		<author>
			<persName><forename type="first">M</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bernoulli</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1378" to="1390" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Lee</surname></persName>
		</author>
		<title level="m">U -statistics: Theory and practice</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Marcel Dekker, Inc</publisher>
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">SGD Algorithms based on Incomplete Ustatistics: Large-Scale Minimization of Empirical Risk</title>
		<author>
			<persName><forename type="first">G</forename><surname>Papa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bellet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Clémençon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">V</forename><surname>De La Pena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Giné</surname></persName>
		</author>
		<title level="m">Decoupling: from Dependence to Independence</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">CoCoA: A General Framework for Communication-Efficient Distributed Optimization</title>
		<author>
			<persName><forename type="first">V</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Forte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Takác</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jaggi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">230</biblScope>
			<biblScope unit="page" from="1" to="49" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Van Der Vaart</surname></persName>
		</author>
		<title level="m">Asymptotic Statistics</title>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A Probabilistic Theory of Supervised Similarity Learning for Pointwise ROC Curve Optimization</title>
		<author>
			<persName><forename type="first">R</forename><surname>Vogel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bellet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Clémençon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Petuum: A New Platform for Distributed Machine Learning on Big Data</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Big Data</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="49" to="67" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Spark : Cluster Computing with Working Sets</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zaharia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chowdhury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Franklin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shenker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Stoica</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>HotCloud</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>