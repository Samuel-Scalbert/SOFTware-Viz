<?xml version='1.0' encoding='utf-8'?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" version="1.1" xsi:schemaLocation="http://www.tei-c.org/ns/1.0 http://api.archives-ouvertes.fr/documents/aofr-sword.xsd">
  <teiHeader>
    <fileDesc>
      <titleStmt>
        <title>HAL TEI export of hal-03279850</title>
      </titleStmt>
      <publicationStmt>
        <distributor>CCSD</distributor>
        <availability status="restricted">
          <licence target="http://creativecommons.org/licenses/by/4.0/">Distributed under a Creative Commons Attribution 4.0 International License</licence>
        </availability>
        <date when="2024-04-22T04:09:58+02:00" />
      </publicationStmt>
      <sourceDesc>
        <p part="N">HAL API platform</p>
      </sourceDesc>
    </fileDesc>
  </teiHeader>
  <text>
    <body>
      <listBibl>
        <biblFull>
          <titleStmt>
            <title xml:lang="en">Techniques de traitement automatique du langage naturel appliquées aux représentations symboliques musicales</title>
            <author role="aut">
              <persName>
                <forename type="first">Mikaela</forename>
                <surname>Keller</surname>
              </persName>
              <email type="md5">03abb26c2593bc94277fcd3494e2a644</email>
              <email type="domain">inria.fr</email>
              <idno type="idhal" notation="string">mikaela-keller</idno>
              <idno type="idhal" notation="numeric">21206</idno>
              <idno type="halauthorid" notation="string">32975-21206</idno>
              <idno type="ORCID">https://orcid.org/0000-0003-2447-0122</idno>
              <affiliation ref="#struct-432650" />
            </author>
            <author role="aut">
              <persName>
                <forename type="first">Kamil</forename>
                <surname>Akesbi</surname>
              </persName>
              <idno type="halauthorid">2244416-0</idno>
              <affiliation ref="#struct-410272" />
            </author>
            <author role="aut">
              <persName>
                <forename type="first">Lorenzo</forename>
                <surname>Moreira</surname>
              </persName>
              <idno type="halauthorid">2244417-0</idno>
              <affiliation ref="#struct-410272" />
            </author>
            <author role="aut">
              <persName>
                <forename type="first">Louis</forename>
                <surname>Bigo</surname>
              </persName>
              <email type="md5">b5d6dfa264982b06e625c649c3fc7477</email>
              <email type="domain">univ-lille.fr</email>
              <idno type="idhal" notation="string">louis-bigo</idno>
              <idno type="idhal" notation="numeric">19602</idno>
              <idno type="halauthorid" notation="string">15569-19602</idno>
              <idno type="ORCID">https://orcid.org/0000-0002-9865-2861</idno>
              <idno type="IDREF">https://www.idref.fr/193516691</idno>
              <affiliation ref="#struct-469193" />
            </author>
            <editor role="depositor">
              <persName>
                <forename>Mathieu</forename>
                <surname>Giraud</surname>
              </persName>
              <email type="md5">8cc1ead528c598df5131657d8c9b7c67</email>
              <email type="domain">univ-lille.fr</email>
            </editor>
          </titleStmt>
          <editionStmt>
            <edition n="v1" type="current">
              <date type="whenSubmitted">2021-08-05 16:06:38</date>
              <date type="whenModified">2024-01-24 09:54:23</date>
              <date type="whenReleased">2021-08-05 16:34:06</date>
              <date type="whenProduced">2021-07-08</date>
              <date type="whenEndEmbargoed">2021-08-05</date>
              <ref type="file" target="https://hal.science/hal-03279850/document">
                <date notBefore="2021-08-05" />
              </ref>
              <ref type="file" subtype="author" n="1" target="https://hal.science/hal-03279850/file/keller_et_al.pdf">
                <date notBefore="2021-08-05" />
              </ref>
            </edition>
            <respStmt>
              <resp>contributor</resp>
              <name key="103858">
                <persName>
                  <forename>Mathieu</forename>
                  <surname>Giraud</surname>
                </persName>
                <email type="md5">8cc1ead528c598df5131657d8c9b7c67</email>
                <email type="domain">univ-lille.fr</email>
              </name>
            </respStmt>
          </editionStmt>
          <publicationStmt>
            <distributor>CCSD</distributor>
            <idno type="halId">hal-03279850</idno>
            <idno type="halUri">https://hal.science/hal-03279850</idno>
            <idno type="halBibtex">keller:hal-03279850</idno>
            <idno type="halRefHtml">&lt;i&gt;JIM 2021 - Journées d'Informatique Musicale&lt;/i&gt;, Jul 2021, Virtual, France</idno>
            <idno type="halRef">JIM 2021 - Journées d'Informatique Musicale, Jul 2021, Virtual, France</idno>
          </publicationStmt>
          <seriesStmt>
            <idno type="stamp" n="SHS">Sciences de l'Homme et de la Société</idno>
            <idno type="stamp" n="CNRS">CNRS - Centre national de la recherche scientifique</idno>
            <idno type="stamp" n="INRIA">INRIA - Institut National de Recherche en Informatique et en Automatique</idno>
            <idno type="stamp" n="UNIV-PICARDIE">Université de Picardie Jules Verne</idno>
            <idno type="stamp" n="INRIA-LILLE">INRIA Lille - Nord Europe</idno>
            <idno type="stamp" n="AO-MUSICOLOGIE" corresp="SHS">AO-MUSICOLOGIE</idno>
            <idno type="stamp" n="INRIA_TEST">INRIA - Institut National de Recherche en Informatique et en Automatique</idno>
            <idno type="stamp" n="TESTALAIN1">TESTALAIN1</idno>
            <idno type="stamp" n="CRISTAL-ALGOMUS">CRIStAL - Algorithmic Musicology</idno>
            <idno type="stamp" n="CRISTAL">Centre de Recherche en Informatique, Signal et Automatique de Lille (CRISTAL)</idno>
            <idno type="stamp" n="INRIA2">INRIA 2</idno>
            <idno type="stamp" n="CRISTAL-MAGNET" corresp="CRISTAL">CRISTAL-MAGNET</idno>
            <idno type="stamp" n="UNIV-LILLE">Université de Lille</idno>
            <idno type="stamp" n="TEST-HALCNRS">Collection test HAL CNRS</idno>
            <idno type="stamp" n="AFIM">Association Francophone d'Informatique Musicale</idno>
            <idno type="stamp" n="JIM" corresp="AFIM">Journées d'Informatique Musicale</idno>
            <idno type="stamp" n="U-PICARDIE">Université de Picardie Jules Verne</idno>
            <idno type="stamp" n="MIS" corresp="U-PICARDIE">Modélisation, Information et Systèmes - UR UPJV 4290</idno>
          </seriesStmt>
          <notesStmt>
            <note type="audience" n="3">National</note>
            <note type="invited" n="0">No</note>
            <note type="popular" n="0">No</note>
            <note type="peer" n="1">Yes</note>
            <note type="proceedings" n="1">Yes</note>
          </notesStmt>
          <sourceDesc>
            <biblStruct>
              <analytic>
                <title xml:lang="en">Techniques de traitement automatique du langage naturel appliquées aux représentations symboliques musicales</title>
                <author role="aut">
                  <persName>
                    <forename type="first">Mikaela</forename>
                    <surname>Keller</surname>
                  </persName>
                  <email type="md5">03abb26c2593bc94277fcd3494e2a644</email>
                  <email type="domain">inria.fr</email>
                  <idno type="idhal" notation="string">mikaela-keller</idno>
                  <idno type="idhal" notation="numeric">21206</idno>
                  <idno type="halauthorid" notation="string">32975-21206</idno>
                  <idno type="ORCID">https://orcid.org/0000-0003-2447-0122</idno>
                  <affiliation ref="#struct-432650" />
                </author>
                <author role="aut">
                  <persName>
                    <forename type="first">Kamil</forename>
                    <surname>Akesbi</surname>
                  </persName>
                  <idno type="halauthorid">2244416-0</idno>
                  <affiliation ref="#struct-410272" />
                </author>
                <author role="aut">
                  <persName>
                    <forename type="first">Lorenzo</forename>
                    <surname>Moreira</surname>
                  </persName>
                  <idno type="halauthorid">2244417-0</idno>
                  <affiliation ref="#struct-410272" />
                </author>
                <author role="aut">
                  <persName>
                    <forename type="first">Louis</forename>
                    <surname>Bigo</surname>
                  </persName>
                  <email type="md5">b5d6dfa264982b06e625c649c3fc7477</email>
                  <email type="domain">univ-lille.fr</email>
                  <idno type="idhal" notation="string">louis-bigo</idno>
                  <idno type="idhal" notation="numeric">19602</idno>
                  <idno type="halauthorid" notation="string">15569-19602</idno>
                  <idno type="ORCID">https://orcid.org/0000-0002-9865-2861</idno>
                  <idno type="IDREF">https://www.idref.fr/193516691</idno>
                  <affiliation ref="#struct-469193" />
                </author>
              </analytic>
              <monogr>
                <meeting>
                  <title>JIM 2021 - Journées d'Informatique Musicale</title>
                  <date type="start">2021-07-08</date>
                  <date type="end">2021-07-09</date>
                  <settlement>Virtual</settlement>
                  <country key="FR">France</country>
                </meeting>
                <imprint />
              </monogr>
              <ref type="publisher">https://jim2021.sciencesconf.org/</ref>
            </biblStruct>
          </sourceDesc>
          <profileDesc>
            <langUsage>
              <language ident="fr">French</language>
            </langUsage>
            <textClass>
              <classCode scheme="halDomain" n="shs.musiq">Humanities and Social Sciences/Musicology and performing arts</classCode>
              <classCode scheme="halDomain" n="info.info-sd">Computer Science [cs]/Sound [cs.SD]</classCode>
              <classCode scheme="halDomain" n="info.info-tt">Computer Science [cs]/Document and Text Processing</classCode>
              <classCode scheme="halTypology" n="COMM">Conference papers</classCode>
              <classCode scheme="halOldTypology" n="COMM">Conference papers</classCode>
              <classCode scheme="halTreeTypology" n="COMM">Conference papers</classCode>
            </textClass>
            <abstract xml:lang="fr">
              <p>La discipline du Traitement Automatique du Langage Naturel (TALN) a connu d’importants progrès ces dix dernières années grâce aux avancées de l’intelligence artificielle et en particulier des réseaux de neurones profonds. Ces techniques sont aujourd’hui largement utilisées pour l’extraction de connaissances, l’analyse de sentiments ou encore la traduction automatique de textes. Les similarités structurelles et conceptuelles entre le langage naturel et la musique ont motivé de nombreuses initiatives de recherche visant à adapter les outils du TALN pour le traite- ment de données musicales symboliques ou audio. Ces démarches ont fourni des résultats prometteurs, notamment dans les domaines de l’analyse et la génération automatique de musique. Au-delà de leur performance, le présent projet vise à étudier le fonctionnement interne de deux de ces modèles, les &lt;i&gt;plongements de mots&lt;/i&gt; et les &lt;i&gt;transformeurs&lt;/i&gt;, ainsi que leur aptitude à s’adapter à des données musicales plutôt que textuelles. Ces expériences bénéficient à notre maîtrise de ces outils adaptés à la musique et contribuent à clarifier de nombreux parallèles entre le langage naturel et le langage musical.</p>
            </abstract>
          </profileDesc>
        </biblFull>
      </listBibl>
    </body>
    <back>
      <listOrg type="structures">
        <org type="researchteam" xml:id="struct-432650" status="VALID">
          <idno type="RNSR">201321079K</idno>
          <orgName>Machine Learning in Information Networks</orgName>
          <orgName type="acronym">MAGNET</orgName>
          <date type="start">2015-01-01</date>
          <desc>
            <address>
              <country key="FR" />
            </address>
            <ref type="url">http://www.inria.fr/equipes/magnet</ref>
          </desc>
          <listRelation>
            <relation active="#struct-104752" type="direct" />
            <relation active="#struct-300009" type="indirect" />
            <relation active="#struct-410272" type="direct" />
            <relation name="UMR9189" active="#struct-120930" type="indirect" />
            <relation name="UMR9189" active="#struct-374570" type="indirect" />
            <relation name="UMR9189" active="#struct-441569" type="indirect" />
          </listRelation>
        </org>
        <org type="laboratory" xml:id="struct-410272" status="VALID">
          <idno type="IdRef">18388695X</idno>
          <idno type="RNSR">201521249L</idno>
          <idno type="ROR">https://ror.org/05vrs3189</idno>
          <orgName>Centre de Recherche en Informatique, Signal et Automatique de Lille - UMR 9189</orgName>
          <orgName type="acronym">CRIStAL</orgName>
          <date type="start">2015-01-01</date>
          <desc>
            <address>
              <addrLine>Université de Lille - Campus scientifique - Bâtiment ESPRIT - Avenue Henri Poincaré - 59655 Villeneuve d’Ascq</addrLine>
              <country key="FR" />
            </address>
            <ref type="url">https://www.cristal.univ-lille.fr/</ref>
          </desc>
          <listRelation>
            <relation name="UMR9189" active="#struct-120930" type="direct" />
            <relation name="UMR9189" active="#struct-374570" type="direct" />
            <relation name="UMR9189" active="#struct-441569" type="direct" />
          </listRelation>
        </org>
        <org type="researchteam" xml:id="struct-469193" status="VALID">
          <orgName>Algomus</orgName>
          <desc>
            <address>
              <country key="FR" />
            </address>
            <ref type="url">http://www.algomus.fr/</ref>
          </desc>
          <listRelation>
            <relation active="#struct-59714" type="direct" />
            <relation name="UR4290" active="#struct-300258" type="indirect" />
            <relation active="#struct-410272" type="direct" />
            <relation name="UMR9189" active="#struct-120930" type="indirect" />
            <relation name="UMR9189" active="#struct-374570" type="indirect" />
            <relation name="UMR9189" active="#struct-441569" type="indirect" />
          </listRelation>
        </org>
        <org type="laboratory" xml:id="struct-104752" status="VALID">
          <idno type="RNSR">200818245B</idno>
          <idno type="ROR">https://ror.org/04eej9726</idno>
          <orgName>Inria Lille - Nord Europe</orgName>
          <desc>
            <address>
              <addrLine>Parc Scientifique de la Haute Borne 40, avenue Halley Bât.A, Park Plaza 59650 Villeneuve d'Ascq</addrLine>
              <country key="FR" />
            </address>
            <ref type="url">http://www.inria.fr/lille/</ref>
          </desc>
          <listRelation>
            <relation active="#struct-300009" type="direct" />
          </listRelation>
        </org>
        <org type="institution" xml:id="struct-300009" status="VALID">
          <idno type="ROR">https://ror.org/02kvxyf05</idno>
          <orgName>Institut National de Recherche en Informatique et en Automatique</orgName>
          <orgName type="acronym">Inria</orgName>
          <desc>
            <address>
              <addrLine>Domaine de VoluceauRocquencourt - BP 10578153 Le Chesnay Cedex</addrLine>
              <country key="FR" />
            </address>
            <ref type="url">http://www.inria.fr/en/</ref>
          </desc>
        </org>
        <org type="institution" xml:id="struct-120930" status="VALID">
          <idno type="IdRef">256304629</idno>
          <idno type="ISNI">0000000122034461</idno>
          <idno type="ROR">https://ror.org/01x441g73</idno>
          <orgName>Centrale Lille</orgName>
          <desc>
            <address>
              <addrLine>École Centrale de Lille - Cité Scientifique - CS 20048 59651 Villeneuve d'Ascq Cedex</addrLine>
              <country key="FR" />
            </address>
            <ref type="url">https://centralelille.fr/</ref>
          </desc>
        </org>
        <org type="regroupinstitution" xml:id="struct-374570" status="VALID">
          <idno type="IdRef">223446556</idno>
          <idno type="ISNI">0000 0001 2242 6780</idno>
          <idno type="ROR">https://ror.org/02kzqn938</idno>
          <idno type="Wikidata">Q3551621</idno>
          <orgName>Université de Lille</orgName>
          <desc>
            <address>
              <addrLine>EPE Université de Lille. -- 42 rue Paul Duez, 59000 Lille</addrLine>
              <country key="FR" />
            </address>
            <ref type="url">https://www.univ-lille.fr/</ref>
          </desc>
        </org>
        <org type="regroupinstitution" xml:id="struct-441569" status="VALID">
          <idno type="IdRef">02636817X</idno>
          <idno type="ISNI">0000000122597504</idno>
          <idno type="ROR">https://ror.org/02feahw73</idno>
          <orgName>Centre National de la Recherche Scientifique</orgName>
          <orgName type="acronym">CNRS</orgName>
          <date type="start">1939-10-19</date>
          <desc>
            <address>
              <country key="FR" />
            </address>
            <ref type="url">https://www.cnrs.fr/</ref>
          </desc>
        </org>
        <org type="laboratory" xml:id="struct-59714" status="VALID">
          <idno type="IdRef">197029876</idno>
          <idno type="RNSR">200815526W</idno>
          <orgName>Modélisation, Information et Systèmes - UR UPJV 4290</orgName>
          <orgName type="acronym">MIS</orgName>
          <date type="start">2008-01-01</date>
          <desc>
            <address>
              <addrLine>Université de Picardie Jules Verne - UFR des Sciences - 33, rue Saint Leu - 80039 AMIENS CEDEX 1</addrLine>
              <country key="FR" />
            </address>
            <ref type="url">https://www.mis.u-picardie.fr/</ref>
          </desc>
          <listRelation>
            <relation name="UR4290" active="#struct-300258" type="direct" />
          </listRelation>
        </org>
        <org type="institution" xml:id="struct-300258" status="VALID">
          <idno type="IdRef">026403714</idno>
          <idno type="ISNI">0000 0001 0789 1385</idno>
          <idno type="ROR">https://ror.org/01gyxrk03</idno>
          <orgName>Université de Picardie Jules Verne</orgName>
          <orgName type="acronym">UPJV</orgName>
          <desc>
            <address>
              <addrLine>Chemin du Thil - 80000 Amiens</addrLine>
              <country key="FR" />
            </address>
            <ref type="url">https://www.u-picardie.fr/</ref>
          </desc>
        </org>
      </listOrg>
    </back>
  <teiCorpus>
	<teiHeader xml:lang="fr">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">TECHNIQUES DE TRAITEMENT AUTOMATIQUE DU LANGAGE</title>
			</titleStmt>
			<publicationStmt>
				<publisher />
				<availability status="unknown"><licence /></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Naturel</forename><surname>Appliquées</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Aux</forename><surname>Représentations</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Symboliques</forename><surname>Musicales</surname></persName>
						</author>
						<author role="corresp">
							<persName><forename type="first">Mikaela</forename><surname>Keller</surname></persName>
							<email>mikaela.keller@univ-lille.fr</email>
						</author>
						<author>
							<persName><forename type="first">Kamil</forename><surname>Akesbi</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Lorenzo</forename><surname>Moreira</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="laboratory">UMR 9189</orgName>
								<orgName type="institution" key="instit1">Univ. Lille</orgName>
								<orgName type="institution" key="instit2">Inria CNRS</orgName>
								<orgName type="institution" key="instit3">Centrale Lille</orgName>
								<address>
									<postCode>F-59000</postCode>
									<settlement>CRIStAL, Lille</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="laboratory">UMR 9189</orgName>
								<orgName type="institution" key="instit1">Univ. Lille CNRS</orgName>
								<orgName type="institution" key="instit2">Centrale Lille</orgName>
								<address>
									<postCode>F-59000</postCode>
									<settlement>CRIStAL, Lille</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="laboratory">UMR</orgName>
								<orgName type="institution" key="instit1">Louis Bigo Univ. Lille CNRS</orgName>
								<orgName type="institution" key="instit2">Centrale Lille</orgName>
								<address>
									<addrLine>9189</addrLine>
									<postCode>F-59000</postCode>
									<settlement>CRIStAL, Lille</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">TECHNIQUES DE TRAITEMENT AUTOMATIQUE DU LANGAGE</title>
					</analytic>
					<monogr>
						<imprint>
							<date />
						</imprint>
					</monogr>
					<idno type="MD5">9E4B1BB9D404EE9F4EA64546A3D84DD7</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-04-12T14:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid" />
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div><p>La discipline du Traitement Automatique du Langage Naturel (TALN) a connu d'importants progrès ces dix dernières années grâce aux avancées de l'intelligence artificielle et en particulier des réseaux de neurones profonds. Ces techniques sont aujourd'hui largement utilisées pour l'extraction de connaissances, l'analyse de sentiments ou encore la traduction automatique de textes. Les similarités structurelles et conceptuelles entre le langage naturel et la musique ont motivé de nombreuses initiatives de recherche visant à adapter les outils du TALN pour le traitement de données musicales symboliques ou audio. Ces démarches ont fourni des résultats prometteurs, notamment dans les domaines de l'analyse et la génération automatique de musique. Au-delà de leur performance, le présent projet vise à étudier le fonctionnement interne de deux de ces modèles, les plongements de mots et les transformeurs, ainsi que leur aptitude à s'adapter à des données musicales plutôt que textuelles. Ces expériences bénéficient à notre maîtrise de ces outils adaptés à la musique et contribuent à clarifier de nombreux parallèles entre le langage naturel et le langage musical.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="fr">
		<body>
<div><head n="1.">INTRODUCTION 1.Traitement Automatique du Langage Naturel et informatique musicale</head><p>Un parallèle existe entre le texte et la partition vus comme une séquence d'éléments constituant un tout expressif. Une mélodie constituée de notes, ou une séquence d'accords de jazz, peuvent être comparés à une phrase constituée de mots. Il est courant en musique de parler de langage tonal, de narration, de fin de phrase, par exemple en comparant les différents types de cadences aux éléments de ponctuation dans le texte, notamment la virgule et le point <ref type="bibr" target="#b0">[1]</ref>. Concevoir la musique comme un langage suggère des apports mutuels potentiels entre les recherches en Traitement Automatique du Langage Naturel (TALN ou NLP pour Natural Language Processing) et en informatique musicale. Les domaines du TALN et de l'informatique musicale ont d'ailleurs des applications communes, comme la génération automatique de contenu ou encore la classification stylistique par époque, genre, auteur/compositeur. Le transfert de style (transformer un contenu dans le style d'un autre) a suscité des travaux en TALN <ref type="bibr" target="#b1">[2]</ref> comme en musique <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>.</p><p>En TALN comme en musique, ces applications se distinguent par une prise en compte variable de la complexité des relations entre les éléments dans les séquences de données. Par exemple, la catégorisation de textes par thèmes est souvent abordée à l'aide de modèles qui prennent simplement en compte la co-occurrence des mots alors que la détection de sentiments nécessite des abstractions plus profondes. De même en musique, la détection de style peut généralement s'étudier sur des fenêtres temporelles moins large <ref type="bibr" target="#b4">[5]</ref> que la détection de la structure qui nécessite le recours à des modèles capturant les dépendances à plus long terme entre les notes <ref type="bibr" target="#b5">[6]</ref>.</p><p>Certains domaines d'applications du TALN, comme la traduction automatique, la génération de résumé, ou la détection d'humour et d'ironie sont plus difficilement interprétable dans le domaine musical. La recherche de ces parallèles ouvrent toutefois la voie à d'intéressantes réflexions.</p><p>Une grand nombre de techniques modernes en TALN visent à représenter une phrase dans un espace abstrait rendant compte de sa sémantique. Ce type d'espace a un intérêt pour la traduction automatique étant donné qu'une phrase et sa traduction dans une autre langue sont censées y être représentées par un même point. Ces espaces font aussi apparaître le principe d'analogie, avec par exemple un même vecteur permettant de se déplacer du mot reine au mot roi et du mot femme au mot homme. Ces principes s'appliquent moins naturellement dans le domaine musical, où la notion de sens est plus subjective. Si le terme de langage est largement utilisé pour qualifier la musique, la question de savoir si deux phrases musicales distinctes ont un sens similaire ouvre elle aussi de nombreuses questions.</p><p>En texte comme en musique, le contexte d'un élément d'une séquence, c'est à dire les éléments qui l'environnent, contribuent généralement au sens de cet élément. Cette propriété est particulièrement vrai en musique où la fonction d'une note ou d'un accord pris de manière isolée change radicalement selon la tonalité dans laquelle il se trouve.</p></div>
<div><head n="1.2.">Représentations séquentielles</head><p>L'application de techniques dédiées au texte sur des données musicales ne se fait toutefois pas de manière directe. En effet, les algorithmes de TALN sont adaptés en premier lieu à des représentations textuelles consistant en des séquences de mots. Malgré sa nature temporelle, la musique ne se représente pas systématiquement sous la forme de séquences. La polyphonie de la musique fait en effet apparaître des notes qui sont simultanées et qui se chevauchent. L'utilisation de méthodes de TALN sur des données musicales nécessite donc soit une adaptation des modèles afin de leur permettre de considérer des structures de données plus complexes que des séquences, soit une approximation de l'information musicale afin de la rendre strictement séquentielle.</p><p>Une partition pour plusieurs instruments peut par exemple être réduite en une séquence de symboles d'accords qui fait abstraction du rythme et de la conduite des voix, ou en une séquence de tranches de notes qui fait abstraction des notes tenues. Il est intéressant de remarquer que dans le domaine du TALN, les linguistes effectuent euxaussi des abstractions en regroupant les différentes formes d'un mot (genre, nombre, temps) par une forme prototypique appelé lemme.</p><p>Cet article décrit deux expériences visant à analyser le comportement de techniques de TALN lorsqu'on les utilise sur des représentations symboliques musicales. La première expérience porte sur les plongements de mots et la seconde sur le mécanisme d'attention dans les réseaux de neurones.</p></div>
<div><head n="2.">TRAVAUX EXISTANTS</head><p>L'utilisation de techniques d'algorithmique du texte, et plus généralement de traitement du langage, sur des données musicales a fait l'objet de nombreux travaux en informatique musicale. L'algorithme de Mongeau Sankoff <ref type="bibr" target="#b6">[7]</ref> propose une mesure de similarité musicale se basant sur un calcul de distance entre séquences d'éléments élaboré à l'origine pour des séquences textuelles. Récemment, les scores TF-IDF (term frequency-inverse document frequency) permettant d'évaluer l'importance des mots contenus dans un document, ont par exemple été utilisés pour l'analyse des modes dans le plain-chant <ref type="bibr" target="#b7">[8]</ref> et pour l'étude du style musical arabo-andalou <ref type="bibr" target="#b8">[9]</ref>. D'importants progrès ont récemment été réalisés dans le domaine du TALN grâce à l'élaboration de méthodes d'intelligence artificielle faisant notamment intervenir des réseaux de neurones profonds. L'adaptation de ces techniques dans un cadre musical fait partie des sujets très attractifs du moment dans la communauté de l'informatique musicale. Cette discipline fait l'objet d'un workshop dé-dié (NLP4MusA : Natural Language Processing for Music and Audio<ref type="foot" target="#foot_0">1</ref> ) dont la première édition<ref type="foot" target="#foot_1">2</ref> a eu lieu en 2020 en évènement satellite de la conférence internationale ISMIR (International Symposium on Music Information Retrieval).</p><p>Le principe de plongement de mots (word embedding) qui est à l'origine du modèle word2vec <ref type="bibr" target="#b9">[10]</ref> a été appliqué sur des séquences d'accords <ref type="bibr" target="#b10">[11]</ref>, de tranches de notes <ref type="bibr" target="#b11">[12]</ref> et expérimenté dans le cadre de substitutions musicales <ref type="bibr" target="#b12">[13]</ref>.</p><p>Des architectures de réseaux de neurones profonds dédiés au traitement de données séquentielles et initialement élaborés pour le TALN, comme les réseaux transformeurs <ref type="bibr" target="#b13">[14]</ref>, ont largement été adaptées ces dernières années pour la modélisation de musique. Ces travaux ont entre autres été appliqués à l'analyse harmonique <ref type="bibr" target="#b14">[15]</ref> et à la génération de musique <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18]</ref>. La performance de ces modèles, permettant souvent de dépasser les résultats de l'état de l'art, attire cependant généralement plus l'attention que leur fonctionnement interne et leur aptitude à s'adapter spécifiquement au langage musical. Certaines initiatives semblent cependant montrer un intérêt pour l'ouverture de ces modèles complexes, souvent comparés à des "boites noires", permettant de visualiser la manière dont ils s'adaptent au langage musical <ref type="bibr" target="#b18">[19]</ref>. Le présent projet de recherche s'inscrit dans cette démarche.</p></div>
<div><head n="3.">EXPÉRIENCE SUR PLONGEMENTS STATIQUES</head></div>
<div><head n="3.1.">Representations vectorielles des mots</head><p>Effectuer des tâches de TALN nécessite généralement de passer par des représentations numériques spécifiques des mots afin de faciliter leur manipulation par des algorithmes informatiques. Une première représentation primitive, appelée sac-de-mots (bag-of-words), consiste à considérer un espace multi-dimensionnel dont les dimensions sont respectivement associées à chacun des mots du vocabulaire V considéré), et à voir le contenu lexical d'un texte comme un vecteur dans cet espace. Le désavantage de cette représentation naïve est que la distance euclidienne entre deux mots quelconques y est toujours la même : contre toute intuition mer et océan sont à la même distance que mer et radis.</p><p>Une hypothèse datant des années 50, dite hypothèse distributionnelle <ref type="bibr" target="#b19">[20]</ref>, suppose que des mots dans des contextes similaires devraient avoir des "sens" similaires, et donc des représentations avec des distances moindres que des mots apparaissant dans des contextes différents. Il existe diverses façons étant donné un corpus C de textes utilisant un vocabulaire V de construire des representations dite distributionnelles des mots. Parmi les plus récentes certaines, comme word2vec <ref type="bibr" target="#b9">[10]</ref>, font appel à l'entraîne-ment d'un réseau de neurones pour modéliser le langage utilisé dans un corpus. Cet entraînement ne nécessite pas des données annotées mais s'apparente plutôt à l'apprentissage d'un modèle de langue, c'est à dire que la supervision provient des occurrences de langage observées. Une fois le modèle entrainé, la sortie d'une des couches intermédiaires du réseau sera utilisée comme vecteur de représentation du mot passé en entrée, avec l'idée que la représentation du mot plongé (embedded) dans cet espace sera similaire aux représentations des mots apparaissant dans des contextes semblables dans le corpus d'apprentissage.</p><p>D'autres approches <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22]</ref> accumulent les fréquences de co-apparition des mots et construisent des représentations parfois appelées sac-de-contextes (bag-of-contexts) ( <ref type="bibr" target="#b22">[23]</ref>). Dans ces approches, un vocabulaire représentatif V c ⊂ V est sélectionné, et tout mot w est représenté par un vecteur de taille |V c | dont les composantes rendent compte de la fréquence de co-occurrence de w avec les mots de V c . Une co-occurence de deux mots a lieu chaque fois que l'un apparait dans le contexte de l'autre. Le contexte d'un mot w peut désigner de multiples choses, le plus souvent il est définit comme étant les k mots précédents et succédants w (c.à.d. 2k mots, sans l'information de leur distance à w) dans la séquence de mots. On obtient ainsi une matrice S de dimensions</p><formula xml:id="formula_0">|V | × |V c |, où S ij représente la fréquence de co-occurrence des mots w i ∈ V et c j ∈ V c .</formula><p>Il y a différentes façons de caractériser ce degré d'association. Parmi celles-là, nous nous sommes intéressés à l'information mutuelle ponctuelle positive (PPMI : positive pointwise mutual information) <ref type="bibr" target="#b22">[23]</ref> </p><formula xml:id="formula_1">S ij = PPMI(w i , c j ) définie par : PPMI(w, c) = PMI(w, c) si PMI(w, c) ≥ 0 0 sinon et PMI(w, c) = log N wc N N w N c</formula><p>où N est le nombre total d'occurrences dans le corpus, N wc est le nombre de fois que le mot c apparait dans le contexte du mot w, N w et respectivement N c sont le nombre de fois que les mots w et c apparaissent dans le corpus.</p><p>Que ce soit en collectant des statistiques sur un corpus comme on vient de le voir où en entraînant un réseau de neurones comme expliqué plus haut, les représentations obtenues sont des représentation dites statiques des mots. Lors de l'étude d'une phrase, chaque mot possède une représentation unique calculée à partir du corpus d'apprentissage et indépendante de son contexte dans la phrase étudiée. On s'intéressera dans la section 4 à des représentations dites contextuelles qui, au contraire, varient en fonction du contexte dans lequel apparaissent les mots.</p></div>
<div><head n="3.2.">Plongement de tranches musicales</head><p>L'approche des plongements lexicaux en musique renouvelle la question de similarité musicale en l'abordant du point de vue du contexte des objets musicaux plutôt que des objets eux-mêmes. Pour être appliquée de manière analogue au TALN, cette approche fait toutefois l'hypothèse d'une représentation exclusivement séquentielle de la musique. Cela peut se justifier dans le cas d'une mélodie ou d'une séquence de symboles d'accords, mais nécessite des approximations dans le cas de la musique polyphonique.</p><p>L'expérience décrite dans cette section a pour motivation d'évaluer le principe de plongement d'accords, plus spécifiquement le plongement de tranches de notes, qui s'extraient automatiquement des partitions numériques, et permettent donc une étude systématique sur de larges corpus. Nous appliquons cette approche à travers une expérience de transformation de partition par substitution de tranches.</p><p>La représentation sous forme de tranches musicales (parfois appelée salami slicing <ref type="bibr" target="#b23">[24]</ref>) consiste à réduire un extrait polyphonique sous la forme d'une séquence d'ensembles de hauteurs simultanées, chacun associé à une durée. Une nouvelle tranche commence chaque fois qu'une nouvelle hauteur apparait ou disparait. Si cette représentation permet de manipuler de manière systématique un extrait musical, même polyphonique, sous la forme d'une séquence, il ne permet que difficilement de conserver l'information des notes tenues entre tranches successives <ref type="foot" target="#foot_2">3</ref> . L'utilisation de cette représentation pour des tâches de génération ou de substitution a ainsi tendance à produire un excès indésirable de notes répétées.</p></div>
<div><head n="3.3.">Co-occurrence de tranches et hypothèses de substitutions</head><p>La représentation d'un corpus de partitions sous la forme de séquences de tranches de notes permet d'appliquer la méthode présentée dans la section 3.1 et de calculer systématiquement la fréquence de co-occurrence de toute paire de tranches de notes apparaissant dans ce corpus. Cette approche est ici utilisée pour effectuer des transformations de partition par substitution de tranche.</p><p>À partir d'un corpus de référence C, on calcule en premier lieu la matrice S C qui rend compte de la co-occurrence de toute paire de tranches de notes dans ce corpus. On choisit par ailleurs une séquence de tranches T dans laquelle on désire effectuer une substitution. À chaque position i de T , il est possible de calculer un plongement rendant compte des tranches constituant le contexte de la position i dans la séquence T . La matrice S C permet ensuite d'identifier les tranches de C qui apparaissent dans un contexte similaire au contexte de la position i dans T . Ces tranches constituent des candidats pour substituer la tranche à la position i dans T . Deux types de substitutions apparaissent alors : les substitutions que nous appelleront conventionnalisantes consistent à remplacer dans T la tranche à la position i par une tranche dont le contexte moyen, calculé à partir du corpus, est d'avantage similaire au contexte de la position i dans T . À l'inverse, les substitutions originalisantes consistent à remplacer dans T la tranche à la position i par une tranche dont le contexte moyen est moins similaire au contexte de la position i dans T .</p></div>
<div><head n="3.4.">Expériences de substitution</head><p>Cette section présente une expérience<ref type="foot" target="#foot_3">4</ref> visant à évaluer la méthode de substitution présentée dans la section 3.3. Un ensemble de 355 chorals de J.-S. Bach<ref type="foot" target="#foot_4">5</ref> a été utilisé comme corpus de références. Chaque choral a été représenté sous la forme d'une séquence de tranches de notes comme indiqué dans la section 3.2 afin de produire la matrice de co-occurrence S. Un total de 20 tranches issues de séquences du corpus ont été substituées par des tranches de contexte similaire. La figure <ref type="figure" target="#fig_1">1</ref> illustre un exemple de substitution sur le choral BWV 353 de J.-S. Bach. La partition de gauche fait apparaitre l'extrait avec la tranche originale (Sib3, Ré4, Sol4, Sib4), la partie droite l'extrait avec la tranche substituée (Ré4, Ré4, Fa#4, La4). Cet exemple illustre la tendance de la réduction en tranches à briser les notes tenues évoquée dans la section 3.2.</p><p>Une étude regroupant 54 participants a été menée pour étudier les effets perceptifs de ces substitutions. 12 participants déclarent être familiers avec les chorals de Bach. Pour chaque substitution, un extrait audio de quelques secondes a été créé afin de faire entendre la tranche dans son contexte. Les extraits ont une durée moyenne <ref type="bibr">de</ref>   </p></div>
<div><head n="4.">EXPÉRIENCE SUR PLONGEMENTS CONTEXTUELS</head><p>Cette section présente une série d'expériences visant à analyser le fonctionnement d'un réseau de neurones transformeur sur des données musicales. Les transformeurs constituent un type de réseaux de neurones profonds utilisant le principe d'attention mutuelle <ref type="bibr" target="#b13">[14]</ref>. Ils sont souvent considérés comme une alternative aux réseaux de neurones récurrents, permettant une prise en compte plus fine des relations liant les éléments constituant une séquence. L'objet de cette expérience est d'"ouvrir la boîte noire" d'un transformeur entraîné sur des données musicales, et d'apporter des interprétations musicales aux valeurs de ses coefficients internes.</p></div>
<div><head n="4.1.">Le mécanisme d'attention</head><p>Dans un phrase textuelle, les relations des mots ne se réduisent pas à leurs contiguïtés, les mots qui se suivent ne sont pas nécessairement ceux qui ont le plus d'influence les uns sur les autres). Par exemple dans la phrase :</p><p>"Il allait cueillir cette pomme à la belle couleur rouge quand il a vu qu'elle avait un ver" le pronom (elle) et l'entité (la pomme) à laquelle il se rapporte se trouvent à distance dans la séquence.</p><p>Certains mots ont plus d'influence sur le sens global de la phrase que d'autres. C'est le cas par exemple de la négation : le simple mot "pas" inverse à lui seul le sens de la phrase.</p><p>La transposition de ce principe dans le domaine musical consiste à considérer que les notes et accords constituant une séquence musicale contribuent différemment au sens que l'on en perçoit. L'attribution d'un sens à une phrase musicale relève cependant d'un processus plus subjectif que pour une phrase textuelle. Même si ce sens n'est pas clairement définit, il semble toutefois raisonnable de considérer que les notes peuvent jouer un rôle différent, et même avoir une importance variable au sein d'une phrase musicale. Il n'est pas rare de qualifier les notes de musique d'après leurs relations mutuelles ou d'après une fonction qu'on leur assigne. C'est le cas pas exemple des notes fondamentales et réelles des accords qui sont le signe de progressions harmoniques sous-jacentes, ou encore à une échelle plus fine des notes modulantes qui annoncent une transition vers une nouvelle tonalité.</p><p>Parmi les multiples types de réseaux de neurones utilisés en TALN, les transformeurs <ref type="bibr" target="#b13">[14]</ref> ont fourni des résultats particulièrement prometteurs en modélisation du langage, par exemple pour la conception de systèmes de questions-réponses <ref type="bibr" target="#b25">[26]</ref> ou pour la traduction automatique <ref type="bibr" target="#b13">[14]</ref>. Le fonctionnement des transformeurs repose sur le principe d'attention mutuelle, qui incite un modèle, lors de son entraînement, à évaluer l'influence mutuelle des termes successifs d'une séquence, d'où le terme original de self-attention. Le modèle de transformeur décrit dans <ref type="bibr" target="#b13">[14]</ref> est entraîné comme un modèle de langue : le début d'une phrase est donné au réseau de neurone qui doit prédire la suite de la séquence (ou la séquence dans une autre langue dans le cas de la traduction automatique). Le mécanisme d'attention est utilisé une première fois pour "encoder" les relations entre les éléments de la séquence, puis de nouveau pour "décoder" les relations avec les élements à prédire.</p><p>Les réseaux de neurones transformeurs encodent l'attention mutuelle sous la forme de matrices de coefficients qui sont ajustés itérativement par rétro-propagation au cours d'une phase d'entrainement sur un ensemble de séquences.</p><p>Transposé dans le domaine des représentations musicales, le mécanisme d'attention offre une approche originale pour comparer la contribution de chacune des notes pour la modélisation statistique d'une séquence musicale.</p><p>Malgré leur performance, les réseaux de neurones profonds sont souvent critiqués pour leur opacité qui limite l'identification des abstractions qu'a re-constitué le modèle au cours de son entraînement et qui lui permettent de prendre des décisions correctes. Le travail présenté dans cette section vise à examiner les représentations internes d'un transformeur entraîné sur un corpus de musique classique pour piano. L'objectif est d'analyser les représentations internes au modèle, apparaissant à travers les coefficients des unités d'attention regroupées en matrices désignées sous le terme de têtes d'attention, elles-même regroupées dans les différentes couches du réseau de neurones. L'observation de ces valeurs a pour but d'identifier les éléments du langage musical sur lesquels a tendance à se focaliser le mécanisme d'attention, puis de les comparer avec des règles issues de la théorie musicale.</p></div>
<div><head n="4.2.">Expériences</head><p>L'expérience décrite dans cette section se base sur le modèle Music Transformer <ref type="bibr" target="#b5">[6]</ref> élaboré par l'équipe Magenta de Google Brain. Une contribution majeure de ce système est l'élaboration d'un mécanisme d'attention relative privilégiant la position relatives de deux éléments d'une séquence musicale plutôt que leurs positions absolues. Ce modèle a été élaboré dans le but de générer de la musique faisant apparaitre une structure long terme cohérente. L'expérience présenté dans cette section a pour objet de ré-utiliser le code du modèle Music Transformer non pas dans un but de génération, mais pour l'étude du mécanisme d'attention via l'observation des représentations internes du modèle. Le modèle a été entrainé à l'aide de l'ensemble de données Maestro <ref type="bibr" target="#b16">[17]</ref> qui inclue près de 200 heures de piano au format MIDI dans un répertoire majoritairement classique incluant des compositeurs du XVII ème siècle au XX ème siècle. Les messages MIDI constituant les fichiers de l'ensemble de données sont converties sous la forme d'entiers n ∈ [0, 387] à leur tour représentés sous la forme de vecteurs one-hot communément utilisés pour l'entrainement des réseaux de neurones.</p><p>Le transformeur utilisé pour cette expérience est constitué de 6 couches comprenant chacune 4 têtes d'attention. Il prend en entrée des séquences de L (≤ 2048) événements MIDI représentés par des vecteurs one-hot en 388 dimensions. Chaque évenement MIDI subis un plongement de position en 256 dimensions qui sont ensuite passées en parallèle par paquets de 64 à chacune des 4 têtes d'attention de la première couche. Le réseau a été entraîné par descente de gradient stochastique avec des minibatch de 2 séquences sur un GPU du service de Google colab.</p><p>Lorsqu'une séquence de taille L est présentée au modèle, chacune des 24 têtes d'attention calcule une matrice d'attention de taille L × L rendant chacune compte d'un aspect de l'attention mutuelle des éléments de la séquence. L'analyse du mécanisme d'attention passe donc par une observation conjointe de l'ensemble de ces matrices. Une matrice d'attention fait apparaitre la séquence d'éléments fourni en entrée sur l'axe des abscisses et sur l'axe des ordonnées. L'élément d'abscisse x et d'ordonnée y indique l'attention que l'élément y porte à l'élément x, qu'il soit dans son passé ou dans son futur. Par conséquent, le triangle inférieur gauche (resp. supérieur droit) correspond à des attentions portées vers le passé (resp. vers le futur).</p></div>
<div><head n="4.2.1.">Gamme chromatique</head><p>Afin d'observer le comportement des têtes d'attention dans un cadre simple, une séquence de valeurs correspondant à une progression chromatique sur plusieurs octaves consécutives est fournie en entrée du modèle entrainé. Afin de faciliter la visualisation de l'influence mutuelle des différentes hauteurs de la gamme, seuls les éléments correspondant à des messages MIDI Note On sont conservés dans la séquence d'entrée.</p><p>La figure <ref type="figure" target="#fig_3">2</ref>   La figure <ref type="figure" target="#fig_4">3</ref> illustre la matrice d'attention produite par une tête d'attention de la quatrième couche. Cette matrice fait apparaitre une importante attention entre une note et les 2 ou 3 notes qui la précèdent. La séquence consistant en une gamme chromatique, les notes situées les unes après les autres s'avèrent aussi être des notes séparées par de petits intervalles (de un à quatre demi-tons). La séquence d'entrée utilisée dans cette expérience ne permet toutefois pas de distinguer si l'attention capturée par cette tête résulte d'une proximité dans le temps et/ou dans les hauteurs des notes.</p><p>La figure <ref type="figure" target="#fig_5">4</ref> illustre les matrices d'attentions produites par les 4 têtes d'attention de la sixième couche du transformeur. Il est connu dans le domaine de l'apprentissage profond que les couches les plus élevées ont tendance à capturer les notions les plus abstraites. Il n'est donc pas surprenant d'observer des valeurs d'attention réparties de manière plus uniforme dans cette couche. Malgré cette montée en abstraction, des comportements distincts semblent se dessiner pour chaque tête d'attention. Dans la tête 0, les notes portent leur attention sur les notes qui les précèdent. Au contraire, la tête 3 porte l'attention des notes sur les notes qui lui succèdent. On remarque dans cette dernière matrice une attention forte portée par les dernières notes sur les premières notes de la séquence. La complexité du réseau de neurones rend naturellement difficile d'interpréter ce phénomène.</p><p>Cette première expérience illustre la faculté des transformeurs à assigner des rôles distincts aux différentes têtes   </p></div>
<div><head n="4.2.2.">Relations Note On et Note Off</head><p>On concatène à la séquence précédente les messages Note Off qui indiquent originalement dans le fichier MIDI qu'une hauteur particulière cesse d'être jouée. La figure <ref type="figure" target="#fig_6">5</ref> montre une tête d'attention de la troisième couche à l'intérieur de laquelle chaque message Note On porte son attention sur l'élément Note Off futur correspondant à sa hauteur. Par exemple, le message Note On sur la hauteur Do porte son attention sur le message Note Off de cette même hauteur, qui survient 12 éléments plus tard dans la séquence. Cette constatation montre la capacité des transformeurs à assimiler non seulement les relations musicales qui lient les notes, mais aussi les artefacts résultants de nos outils de représentations informatiques de la musique.</p></div>
<div><head n="4.2.3.">Durées et volumes</head><p>La figure <ref type="figure" target="#fig_7">6</ref> illustre l'attention mutuelle calculée par une tête de la première couche, à partir des onze premières notes de la mélodie Au Clair de la Lune. Aux messages Note On et Note Off ont été ajoutés les messages de durée et de volumes des notes. Les éléments de la séquence sont ainsi groupés par quatre. Cette matrice fait apparaitre un lien fort entre les messages de volumes 6 et de durées. Ce phénomène peut s'expliquer par le fait que les notes longues dans le répertoire classique ont en moyenne tendance à être jouée avec un volume plus élevé que les notes courtes. Ce type d'hypothèses peut se vérifier de manière calculatoire sur le corpus, ce qui permettrait de valider la capacité du transformeur à capturer ce phénomène. Cela fait partie des perspectives de ce travail.</p></div>
<div><head n="4.2.4.">Accords et notes étrangères</head><p>La table 1 illustre l'attention totale portée à chaque note de l'ensemble {Do4, Mi4, Fa#4, Sol4}. L'objectif de cette expérience est de visualiser l'effet en terme d'attention d'une note étrangère dans un accord parfait (ici la note Fa# dans l'accord de Do majeur). Afin de lever toute ambiguïté sur l'influence de la position des notes dans la séquence de valeurs passée en entrée, l'expérience est effectuée en ordonnant de manière différente les notes de l'ensemble. L'attention plus faible portée à la note Fa#, et ce quel que soit l'ordre, traduit le fait que cette note est moins associée aux autres. La sur-représentation des accords parfaits dans ce corpus de musique de style majoritairement classique 6 . Les informations de volume constituent une composante essentielles du dataset MAESTRO étant donné que les pièces qui le constituent ont été enregistrées lors de performance pianistiques professionnelles. Ce dataset se distingue ainsi de ceux constitués à partir de partitions qui regroupent des notes dont le volume ne résulte que des annotations de la partition et fait donc apparaitre beaucoup moins de variété qu'à l'issue d'une performance réelle. explique probablement cette attention mutuelle prédominante entre les notes de l'accord de do majeur dans cet exemple.</p></div>
<div><head n="4.3.">Bilan et perspectives</head><p>Ces expérience nous aident à visualiser la manière dont les transformeurs, initialement imaginés pour la modélisation du langage naturel, apprennent le concept d'attention mutuelle entre les éléments d'une séquence de notes. Ces expériences préliminaires ouvrent la voie à de nombreuses perspectives pour améliorer notre compréhension de ces algorithmes souvent comparés à des boites noires. La confirmation des tendances observées dans les matrices d'attentions par des tests de corrélation sur les données d'apprentissage constitue une des perspectives majeures de ce projet.</p><p>Les capacités d'attention d'un transformeur prennent la forme d'un ensemble complexe de matrices réparties en différentes couches. Ces expériences nous offrent des intuitions sur le potentiel de ces matrices, mais leur étude systématique requiert l'élaboration d'approches formelles qui sont centrales dans les perspectives de ce projet. Nous nous pencherons en particulier sur la notion de vecteur d'attention <ref type="bibr" target="#b26">[27]</ref> qui consiste à résumer l'attention mutuelle entre deux éléments arbitraires d'une séquence à travers un vecteur prenant en compte l'ensemble des têtes d'attention. Les vecteurs d'attentions permettent d'évaluer de manière commode la capacité d'un transformeur à identifier des relations entres des couples d'éléments spécifiés à priori. Une perspective de ce projet consiste à évaluer cette approche pour l'identification d'éléments déterminants dans le déroulement d'une cadence, en particulier le point de préparation et le point de cadence <ref type="bibr" target="#b27">[28]</ref>.</p></div>
<div><head n="5.">CONCLUSIONS ET PERSPECTIVES</head><p>À travers les plongements de mots et le mécanisme d'attention, ce projet de recherche exploratoire vise à améliorer notre compréhension du potentiel des techniques de TALN pour la modélisation de données musicales. Ces expériences ouvrent de nombreuses questions sur la pertinence de l'analogie entre le langage naturel et le langage musical. La question du sens d'une phrase musicale apparait à la fois commune limite et un défi majeur pour l'étude de cette analogie. Des concepts abstraits propres au langage naturel, tels que l'humour et l'ironie nous interrogent particulièrement lorsqu'on tente de les interpréter dans le domaine musical. Les représentations internes d'un modèle mettant en oeuvre le mécanisme d'attention ouvrent la voie à des approches originales pour la classification stylistique musicale et pour notre compréhension d'éléments du langage tonal tels que les cadences et les notes modulantes. Ces techniques du TALN font actuellement l'objet de nombreuses recherches et améliorations. Leur utilisation en musique est amenée à renouveler ces questions dans les années à venir. Les outils élaborés en TALN visent à manipuler des séquences d'éléments. La musique est cependant généralement structurée de manière plus complexe. L'adaptation de ces outils à des représentation multi-sequences s'annonce prometteuse pour la modélisation de musique.</p></div><figure xml:id="fig_0"><head /><label /><figDesc>10 secondes. La tranche substituée est généralement située au milieu de l'extrait, sauf dans les cas où la substitution a lieu au début ou à la fin d'une phrase. Les extraits sont présentés aux participant dans leur version originale et dans leur version transformée. Pour chaque paire, il est demandé au participant d'indiquer dans un premier temps si il a perçu une différence. Si oui, il lui est demandé d'indiquer si un des deux extraits semble sonner le mieux, ou si il n'a pas d'avis sur la question. Les 20 substitutions ayant été présentées aux 54 participants, un total de 1080 comparaisons ont été effectuées. Dans 950 cas (88%), la différence entre l'extrait original et l'extrait transformé a été perçue. Parmi ces cas, 906 ont fait l'objet d'un classement des deux extraits et 34% de ces classements privilégient l'extrait transformé. Il n'est pas surprenant que le choix des participants se porte majoritairement sur les extraits originaux. Toutefois, le fait que la proportion d'extraits transformés dépasse un tiers des comparaisons semble confirmer la capacité du modèle à effectuer des substitutions raisonnables en ne se basant que sur la similarité de leur contexte.</figDesc></figure>
<figure xml:id="fig_1"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Exemple de substitution présentée aux participants, extraite du choral BWV 353 de J.-S. Bach (1ère mesure).</figDesc><graphic coords="5,310.11,54.39,236.13,126.43" type="bitmap" /></figure>
<figure xml:id="fig_2"><head /><label /><figDesc>illustre l'attention mutuelle entre ces éléments (du bleu foncé pour une faible attention, au jaune clair pour une forte attention), calculée par une des quatre têtes d'attention de la troisième couche. Cette figure fait apparaitre d'importantes valeurs d'attention entre les notes ayant une relation d'octave. Par exemple, le do2 porte une attention élevée aux notes do1 et do3. Cette observation indique la capacité du modèle à apprendre l'importance de la relation d'octave en musique. En effet, la représentation utilisée en entrée du modèle attribue un entier à chaque hauteur sans apporter aucune connaissance musicale supplémentaire à priori.</figDesc></figure>
<figure xml:id="fig_3"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Attentions calculées par la tête 4 de la couche 3 pour une séquence chromatique ascendante. Les axes des abscisses et des ordonnées font apparaitre les messages MIDI Note On de 36 demi-tons constituant 3 octaves contiguës (de do 1 à si 3). Cette tête d'attention fait apparaître une attention importante entre les hauteurs ayant une relation d'octave.</figDesc><graphic coords="7,57.83,54.39,236.13,233.92" type="bitmap" /></figure>
<figure xml:id="fig_4"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Attentions calculées par la tête 2 de la couche 4 pour une séquence chromatique ascendante sur deux octaves contiguës. Cette tête d'attention fait apparaitre une attention importante entre notes consécutives.</figDesc><graphic coords="7,310.11,80.08,236.13,240.83" type="bitmap" /></figure>
<figure xml:id="fig_5"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Attentions calculées par les têtes de la couche 6 pour une séquence chromatique ascendante sur deux octaves contiguës. Les têtes d'attention des couches élevées font apparaitre des attention réparties de manière plus uniforme.</figDesc><graphic coords="7,310.11,440.21,236.13,218.80" type="bitmap" /></figure>
<figure xml:id="fig_6"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Attentions calculées par la tête 3 de la couche 2 pour une séquence chromatique ascendante constituée de 12 messages Note On et 12 messages Note Off. La matrice d'attention fait apparaitre une attention importante entre les messages Note Off et les messages Note On de la même hauteur.</figDesc><graphic coords="8,57.83,54.39,236.13,230.44" type="bitmap" /></figure>
<figure xml:id="fig_7"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Attentions calculées par une tête de la première couche pour les 11 premières notes de Au Clair de la Lune. 4 messages MIDI sont associés à chaque note : Volume, Note On, Time shift (reflétant la durée de la note) et Note Off. Cette tête d'attention fait apparaitre un lien fort entre les informations de volume et de durée d'une même note.</figDesc><graphic coords="8,310.11,54.39,236.13,242.91" type="bitmap" /></figure>
<figure type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Attention cumulée reçue par chacune des notes constituant l'ensemble de notes {Do4, Mi4, Fa#4, Sol4}. Les notes sont successivement présentées au modèle suivant deux ordonnancements différents.</figDesc><table><row><cell>Attention reçue</cell><cell>Do4</cell><cell>Mi4</cell><cell>Fa#4</cell><cell>Sol4</cell></row><row><cell>Do4 -Mi4 -Fa#4 -Sol4</cell><cell>2.7</cell><cell cols="3">2.9 1.26 2.3</cell></row><row><cell cols="5">Mi4 -Sol4 -Do4 -Fa#4 3.89 2.97 1.17 1.69</cell></row></table></figure>
			<note place="foot" n="1" xml:id="foot_0"><p>. https://sites.google.com/view/nlp4musa</p></note>
			<note place="foot" n="2" xml:id="foot_1"><p>. Il est à noter toutefois que la majeure partie des travaux présentés lors de ce workshop rendent comptent de l'utilisation de méthodes de TALN pour l'extraction de connaissances sur des textes portant sur des sujets liés à la musique et non sur des représentations séquentielles musicales comme c'est l'objet du présent projet.</p></note>
			<note place="foot" n="3" xml:id="foot_2"><p>. Une méthode consiste à ajouter dans chaque tranche un ensemble des notes tenues. Cette approche est notamment utilisée pour l'encodage vectoriel de corpus afin de faciliter leur utilisation par des algorithmes d'apprentissage automatique<ref type="bibr" target="#b24">[25]</ref>. Cette méthode a toutefois l'inconvénient d'élargir l'espace des valeurs prises par les tranches et donc de compliquer leur identification.</p></note>
			<note place="foot" n="4" xml:id="foot_3"><p>. https://enquetes.univ-lille.fr/index.php/ 178616?lang=fr</p></note>
			<note place="foot" n="5" xml:id="foot_4"><p>. téléchargé au format MIDI sur le site http://kern.ccarh. org/</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">W</forename><surname>Piston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Devoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jannery</surname></persName>
		</author>
		<title level="m">Harmony</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>WW Norton</publisher>
			<date type="published" when="1978">1978</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Multiple-attribute text rewriting</title>
		<author>
			<persName><forename type="first">G</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Denoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-L</forename><surname>Boureau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Music style transfer : A position paper</title>
		<author>
			<persName><forename type="first">S</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">G</forename><surname>Xia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.06841</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><surname>Brunner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Konrad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wattenhofer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.07600</idno>
		<title level="m">Midi-vae : Modeling dynamics and instrumentation of music with applications to style transfer</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Maximum entropy models capture melodic styles</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sakellariou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Tria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Loreto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Pachet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific reports</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="9" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">C.-Z</forename><forename type="middle">A</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hawthorne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dinculescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Eck</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.04281</idno>
		<title level="m">Music transformer</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Comparison of musical sequences</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mongeau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sankoff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers and the Humanities</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="161" to="175" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Mode classification and natural units in plainchant</title>
		<author>
			<persName><forename type="first">B</forename><surname>Cornelissen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zuidema</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Burgoyne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21th International Conference on Music Information Retrieval (ISMIR 2020)</title>
		<meeting>the 21th International Conference on Music Information Retrieval (ISMIR 2020)<address><addrLine>Montréal, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Contributing to new musicological theories with computational methods : the case of centonization in arab-andalusian music</title>
		<author>
			<persName><forename type="first">T</forename><surname>Nuttall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>García Casado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Núñez Tarifa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">Caro</forename><surname>Repetto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Serra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">20th Conference of the International Society for Music Information Retrieval (ISMIR 2019)</title>
		<meeting><address><addrLine>Delft, The Netherlands; Canada</addrLine></address></meeting>
		<imprint>
			<publisher>ISMIR</publisher>
			<date type="published" when="2019-08">2019 Nov 4-8. 2019. 2019</date>
			<biblScope unit="page" from="223" to="228" />
		</imprint>
	</monogr>
	<note>International Society for Music Information Retrieval (ISMIR)</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1310.4546</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Chord2vec : Learning musical chord embeddings</title>
		<author>
			<persName><forename type="first">S</forename><surname>Madjiheurem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Walder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the constructive machine learning workshop at 30th conference on neural information processing systems (NIPS2016)</title>
		<meeting>the constructive machine learning workshop at 30th conference on neural information processing systems (NIPS2016)<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Herremans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-H</forename><surname>Chuan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.09088</idno>
		<title level="m">Modeling musical context with word2vec</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">From context to concept : exploring semantic relationships in music with word2vec</title>
		<author>
			<persName><forename type="first">C.-H</forename><surname>Chuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Agres</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Herremans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computing and Applications</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1023" to="1036" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.03762</idno>
		<title level="m">Attention is all you need</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Attend to chords : Improving harmonic analysis of symbolic music using transformer-based models</title>
		<author>
			<persName><forename type="first">T.-P</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the International Society for Music Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Pop music transformer : Generating music with rhythm and harmony</title>
		<author>
			<persName><forename type="first">Y.-S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-H</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.00212</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Enabling factorized piano music modeling and generation with the maestro dataset</title>
		<author>
			<persName><forename type="first">C</forename><surname>Hawthorne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Stasyuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-Z</forename><forename type="middle">A</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Engel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Eck</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.12247</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Automatic composition of guitar tabs by transformers and groove modeling</title>
		<author>
			<persName><forename type="first">Y.-H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-Y</forename><surname>Hsiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-H</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.01431</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dinculescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Eck</surname></persName>
		</author>
		<title level="m">Visualizing music self-attention</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Distributional structure</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Harris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Word</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="146" to="162" />
			<date type="published" when="1954">1954</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Glove : Global vectors for word representation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/D14-1162" />
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">From frequency to meaning : Vector space models of semantics</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">D</forename><surname>Turney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Pantel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Artif. Int. Res</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="141" to="188" />
			<date type="published" when="2010-01">Jan. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Linguistic regularities in sparse and explicit word representations</title>
		<author>
			<persName><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the eighteenth conference on computational natural language learning</title>
		<meeting>the eighteenth conference on computational natural language learning</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="171" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The yale-classical archives corpus</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">W</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Quinn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Empirical Musicology Review</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deepbach : a steerable model for bach chorales generation</title>
		<author>
			<persName><forename type="first">G</forename><surname>Hadjeres</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Pachet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Nielsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1362" to="1371" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Bert : Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Visualizing and measuring the geometry of bert</title>
		<author>
			<persName><forename type="first">A</forename><surname>Coenen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Reif</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pearce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Viégas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wattenberg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.02715</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Relevance of musical features for cadence detection</title>
		<author>
			<persName><forename type="first">L</forename><surname>Bigo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Feisthauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Giraud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Levé</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Society for Music Information Retrieval Conference (ISMIR 2018)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</teiCorpus></text>
</TEI>