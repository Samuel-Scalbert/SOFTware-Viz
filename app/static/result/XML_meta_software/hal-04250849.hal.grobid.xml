<?xml version='1.0' encoding='utf-8'?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" version="1.1" xsi:schemaLocation="http://www.tei-c.org/ns/1.0 http://api.archives-ouvertes.fr/documents/aofr-sword.xsd">
  <teiHeader>
    <fileDesc>
      <titleStmt>
        <title>HAL TEI export of hal-04250849</title>
      </titleStmt>
      <publicationStmt>
        <distributor>CCSD</distributor>
        <availability status="restricted">
          <licence target="http://creativecommons.org/licenses/by/">Attribution</licence>
        </availability>
        <date when="2024-04-22T04:06:08+02:00" />
      </publicationStmt>
      <sourceDesc>
        <p part="N">HAL API platform</p>
      </sourceDesc>
    </fileDesc>
  </teiHeader>
  <text>
    <body>
      <listBibl>
        <biblFull>
          <titleStmt>
            <title xml:lang="en">WordNet Is All You Need: A Surprisingly Effective Unsupervised Method for Graded Lexical Entailment</title>
            <author role="aut">
              <persName>
                <forename type="first">Joseph</forename>
                <surname>Renner</surname>
              </persName>
              <email type="md5">eb9b275b9041ae48367c5b7076337256</email>
              <email type="domain">inria.fr</email>
              <idno type="idhal" notation="numeric">1116880</idno>
              <idno type="halauthorid" notation="string">2338569-1116880</idno>
              <affiliation ref="#struct-410272" />
              <affiliation ref="#struct-432650" />
            </author>
            <author role="aut">
              <persName>
                <forename type="first">Pascal</forename>
                <surname>Denis</surname>
              </persName>
              <email type="md5">89fd4466c5a169e94f8fab315f79441d</email>
              <email type="domain">inria.fr</email>
              <idno type="idhal" notation="string">pascal-denis</idno>
              <idno type="idhal" notation="numeric">1744</idno>
              <idno type="halauthorid" notation="string">1313-1744</idno>
              <idno type="IDREF">https://www.idref.fr/031934684</idno>
              <affiliation ref="#struct-410272" />
              <affiliation ref="#struct-432650" />
            </author>
            <author role="aut">
              <persName>
                <forename type="first">Rémi</forename>
                <surname>Gilleron</surname>
              </persName>
              <email type="md5">77f58dc0a0e655f1cb3e4cc92b270cdc</email>
              <email type="domain">univ-lille3.fr</email>
              <idno type="idhal" notation="string">remi-gilleron</idno>
              <idno type="idhal" notation="numeric">184332</idno>
              <idno type="halauthorid" notation="string">2711-184332</idno>
              <idno type="ORCID">https://orcid.org/0000-0002-1583-5938</idno>
              <idno type="IDREF">https://www.idref.fr/061168718</idno>
              <affiliation ref="#struct-410272" />
              <affiliation ref="#struct-432650" />
            </author>
            <editor role="depositor">
              <persName>
                <forename>Pascal</forename>
                <surname>Denis</surname>
              </persName>
              <email type="md5">89fd4466c5a169e94f8fab315f79441d</email>
              <email type="domain">inria.fr</email>
            </editor>
          </titleStmt>
          <editionStmt>
            <edition n="v1" type="current">
              <date type="whenSubmitted">2023-10-20 09:04:44</date>
              <date type="whenModified">2024-01-24 09:54:23</date>
              <date type="whenReleased">2023-10-23 11:38:35</date>
              <date type="whenProduced">2023</date>
              <date type="whenEndEmbargoed">2023-10-20</date>
              <ref type="file" target="https://hal.science/hal-04250849/document">
                <date notBefore="2023-10-20" />
              </ref>
              <ref type="file" subtype="author" n="1" target="https://hal.science/hal-04250849/file/emnlp23-GLE.pdf">
                <date notBefore="2023-10-20" />
              </ref>
            </edition>
            <respStmt>
              <resp>contributor</resp>
              <name key="144964">
                <persName>
                  <forename>Pascal</forename>
                  <surname>Denis</surname>
                </persName>
                <email type="md5">89fd4466c5a169e94f8fab315f79441d</email>
                <email type="domain">inria.fr</email>
              </name>
            </respStmt>
          </editionStmt>
          <publicationStmt>
            <distributor>CCSD</distributor>
            <idno type="halId">hal-04250849</idno>
            <idno type="halUri">https://hal.science/hal-04250849</idno>
            <idno type="halBibtex">renner:hal-04250849</idno>
            <idno type="halRefHtml">&lt;i&gt;Findings of the Association for Computational Linguistics: EMNLP 2023&lt;/i&gt;, 2023, Singapore, France</idno>
            <idno type="halRef">Findings of the Association for Computational Linguistics: EMNLP 2023, 2023, Singapore, France</idno>
            <availability status="restricted">
              <licence target="http://creativecommons.org/licenses/by/">Attribution</licence>
            </availability>
          </publicationStmt>
          <seriesStmt>
            <idno type="stamp" n="SHS">Sciences de l'Homme et de la Société</idno>
            <idno type="stamp" n="CNRS">CNRS - Centre national de la recherche scientifique</idno>
            <idno type="stamp" n="INRIA">INRIA - Institut National de Recherche en Informatique et en Automatique</idno>
            <idno type="stamp" n="INRIA-LILLE">INRIA Lille - Nord Europe</idno>
            <idno type="stamp" n="INRIA_TEST">INRIA - Institut National de Recherche en Informatique et en Automatique</idno>
            <idno type="stamp" n="TESTALAIN1">TESTALAIN1</idno>
            <idno type="stamp" n="CRISTAL">Centre de Recherche en Informatique, Signal et Automatique de Lille (CRISTAL)</idno>
            <idno type="stamp" n="INRIA2">INRIA 2</idno>
            <idno type="stamp" n="CRISTAL-MAGNET" corresp="CRISTAL">CRISTAL-MAGNET</idno>
            <idno type="stamp" n="UNIV-LILLE">Université de Lille</idno>
          </seriesStmt>
          <notesStmt>
            <note type="audience" n="2">International</note>
            <note type="invited" n="0">No</note>
            <note type="popular" n="0">No</note>
            <note type="peer" n="1">Yes</note>
            <note type="proceedings" n="1">Yes</note>
          </notesStmt>
          <sourceDesc>
            <biblStruct>
              <analytic>
                <title xml:lang="en">WordNet Is All You Need: A Surprisingly Effective Unsupervised Method for Graded Lexical Entailment</title>
                <author role="aut">
                  <persName>
                    <forename type="first">Joseph</forename>
                    <surname>Renner</surname>
                  </persName>
                  <email type="md5">eb9b275b9041ae48367c5b7076337256</email>
                  <email type="domain">inria.fr</email>
                  <idno type="idhal" notation="numeric">1116880</idno>
                  <idno type="halauthorid" notation="string">2338569-1116880</idno>
                  <affiliation ref="#struct-410272" />
                  <affiliation ref="#struct-432650" />
                </author>
                <author role="aut">
                  <persName>
                    <forename type="first">Pascal</forename>
                    <surname>Denis</surname>
                  </persName>
                  <email type="md5">89fd4466c5a169e94f8fab315f79441d</email>
                  <email type="domain">inria.fr</email>
                  <idno type="idhal" notation="string">pascal-denis</idno>
                  <idno type="idhal" notation="numeric">1744</idno>
                  <idno type="halauthorid" notation="string">1313-1744</idno>
                  <idno type="IDREF">https://www.idref.fr/031934684</idno>
                  <affiliation ref="#struct-410272" />
                  <affiliation ref="#struct-432650" />
                </author>
                <author role="aut">
                  <persName>
                    <forename type="first">Rémi</forename>
                    <surname>Gilleron</surname>
                  </persName>
                  <email type="md5">77f58dc0a0e655f1cb3e4cc92b270cdc</email>
                  <email type="domain">univ-lille3.fr</email>
                  <idno type="idhal" notation="string">remi-gilleron</idno>
                  <idno type="idhal" notation="numeric">184332</idno>
                  <idno type="halauthorid" notation="string">2711-184332</idno>
                  <idno type="ORCID">https://orcid.org/0000-0002-1583-5938</idno>
                  <idno type="IDREF">https://www.idref.fr/061168718</idno>
                  <affiliation ref="#struct-410272" />
                  <affiliation ref="#struct-432650" />
                </author>
              </analytic>
              <monogr>
                <meeting>
                  <title>Findings of the Association for Computational Linguistics: EMNLP 2023</title>
                  <date type="start">2023</date>
                  <settlement>Singapore</settlement>
                  <country key="FR">France</country>
                </meeting>
                <imprint>
                  <date type="datePub">2023</date>
                </imprint>
              </monogr>
            </biblStruct>
          </sourceDesc>
          <profileDesc>
            <langUsage>
              <language ident="en">English</language>
            </langUsage>
            <textClass>
              <classCode scheme="halDomain" n="info">Computer Science [cs]</classCode>
              <classCode scheme="halDomain" n="scco">Cognitive science</classCode>
              <classCode scheme="halDomain" n="shs">Humanities and Social Sciences</classCode>
              <classCode scheme="halTypology" n="COMM">Conference papers</classCode>
              <classCode scheme="halOldTypology" n="COMM">Conference papers</classCode>
              <classCode scheme="halTreeTypology" n="COMM">Conference papers</classCode>
            </textClass>
            <abstract xml:lang="en">
              <p>We propose a simple unsupervised approach which exclusively relies on WordNet (Miller, 1995) for predicting graded lexical entailment (GLE) in English. Inspired by the seminal work of Resnik (1995), our method models GLE as the sum of two information-theoretic scores: a symmetric semantic similarity score and an asymmetric specicity loss score, both exploiting the hierarchical synset structure of Word-Net. Our approach also includes a simple disambiguation mechanism to handle polysemy in a given word pair. Despite its simplicity, our method achieves performance above the state of the art (Spearman ρ = 0.75) on HyperLex (Vulic et al., 2017), the largest GLE dataset, outperforming all previous methods, including specialized word embeddings approaches that use WordNet as weak supervision.</p>
            </abstract>
          </profileDesc>
        </biblFull>
      </listBibl>
    </body>
    <back>
      <listOrg type="structures">
        <org type="laboratory" xml:id="struct-410272" status="VALID">
          <idno type="IdRef">18388695X</idno>
          <idno type="RNSR">201521249L</idno>
          <idno type="ROR">https://ror.org/05vrs3189</idno>
          <orgName>Centre de Recherche en Informatique, Signal et Automatique de Lille - UMR 9189</orgName>
          <orgName type="acronym">CRIStAL</orgName>
          <date type="start">2015-01-01</date>
          <desc>
            <address>
              <addrLine>Université de Lille - Campus scientifique - Bâtiment ESPRIT - Avenue Henri Poincaré - 59655 Villeneuve d’Ascq</addrLine>
              <country key="FR" />
            </address>
            <ref type="url">https://www.cristal.univ-lille.fr/</ref>
          </desc>
          <listRelation>
            <relation name="UMR9189" active="#struct-120930" type="direct" />
            <relation name="UMR9189" active="#struct-374570" type="direct" />
            <relation name="UMR9189" active="#struct-441569" type="direct" />
          </listRelation>
        </org>
        <org type="researchteam" xml:id="struct-432650" status="VALID">
          <idno type="RNSR">201321079K</idno>
          <orgName>Machine Learning in Information Networks</orgName>
          <orgName type="acronym">MAGNET</orgName>
          <date type="start">2015-01-01</date>
          <desc>
            <address>
              <country key="FR" />
            </address>
            <ref type="url">http://www.inria.fr/equipes/magnet</ref>
          </desc>
          <listRelation>
            <relation active="#struct-104752" type="direct" />
            <relation active="#struct-300009" type="indirect" />
            <relation active="#struct-410272" type="direct" />
            <relation name="UMR9189" active="#struct-120930" type="indirect" />
            <relation name="UMR9189" active="#struct-374570" type="indirect" />
            <relation name="UMR9189" active="#struct-441569" type="indirect" />
          </listRelation>
        </org>
        <org type="institution" xml:id="struct-120930" status="VALID">
          <idno type="IdRef">256304629</idno>
          <idno type="ISNI">0000000122034461</idno>
          <idno type="ROR">https://ror.org/01x441g73</idno>
          <orgName>Centrale Lille</orgName>
          <desc>
            <address>
              <addrLine>École Centrale de Lille - Cité Scientifique - CS 20048 59651 Villeneuve d'Ascq Cedex</addrLine>
              <country key="FR" />
            </address>
            <ref type="url">https://centralelille.fr/</ref>
          </desc>
        </org>
        <org type="regroupinstitution" xml:id="struct-374570" status="VALID">
          <idno type="IdRef">223446556</idno>
          <idno type="ISNI">0000 0001 2242 6780</idno>
          <idno type="ROR">https://ror.org/02kzqn938</idno>
          <idno type="Wikidata">Q3551621</idno>
          <orgName>Université de Lille</orgName>
          <desc>
            <address>
              <addrLine>EPE Université de Lille. -- 42 rue Paul Duez, 59000 Lille</addrLine>
              <country key="FR" />
            </address>
            <ref type="url">https://www.univ-lille.fr/</ref>
          </desc>
        </org>
        <org type="regroupinstitution" xml:id="struct-441569" status="VALID">
          <idno type="IdRef">02636817X</idno>
          <idno type="ISNI">0000000122597504</idno>
          <idno type="ROR">https://ror.org/02feahw73</idno>
          <orgName>Centre National de la Recherche Scientifique</orgName>
          <orgName type="acronym">CNRS</orgName>
          <date type="start">1939-10-19</date>
          <desc>
            <address>
              <country key="FR" />
            </address>
            <ref type="url">https://www.cnrs.fr/</ref>
          </desc>
        </org>
        <org type="laboratory" xml:id="struct-104752" status="VALID">
          <idno type="RNSR">200818245B</idno>
          <idno type="ROR">https://ror.org/04eej9726</idno>
          <orgName>Inria Lille - Nord Europe</orgName>
          <desc>
            <address>
              <addrLine>Parc Scientifique de la Haute Borne 40, avenue Halley Bât.A, Park Plaza 59650 Villeneuve d'Ascq</addrLine>
              <country key="FR" />
            </address>
            <ref type="url">http://www.inria.fr/lille/</ref>
          </desc>
          <listRelation>
            <relation active="#struct-300009" type="direct" />
          </listRelation>
        </org>
        <org type="institution" xml:id="struct-300009" status="VALID">
          <idno type="ROR">https://ror.org/02kvxyf05</idno>
          <orgName>Institut National de Recherche en Informatique et en Automatique</orgName>
          <orgName type="acronym">Inria</orgName>
          <desc>
            <address>
              <addrLine>Domaine de VoluceauRocquencourt - BP 10578153 Le Chesnay Cedex</addrLine>
              <country key="FR" />
            </address>
            <ref type="url">http://www.inria.fr/en/</ref>
          </desc>
        </org>
      </listOrg>
    </back>
  <teiCorpus>
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">WordNet Is All You Need: A Surprisingly Effective Unsupervised Method for Graded Lexical Entailment</title>
			</titleStmt>
			<publicationStmt>
				<publisher />
				<availability status="unknown"><licence /></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Joseph</forename><surname>Renner</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">UMR 9189</orgName>
								<orgName type="institution" key="instit1">Univ. Lille</orgName>
								<orgName type="institution" key="instit2">Inria</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<orgName type="institution" key="instit4">Centrale Lille</orgName>
								<address>
									<addrLine>-CRIStAL</addrLine>
									<postCode>F-59000</postCode>
									<settlement>Lille</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Pascal</forename><surname>Denis</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">UMR 9189</orgName>
								<orgName type="institution" key="instit1">Univ. Lille</orgName>
								<orgName type="institution" key="instit2">Inria</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<orgName type="institution" key="instit4">Centrale Lille</orgName>
								<address>
									<addrLine>-CRIStAL</addrLine>
									<postCode>F-59000</postCode>
									<settlement>Lille</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Rémi</forename><surname>Gilleron</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">UMR 9189</orgName>
								<orgName type="institution" key="instit1">Univ. Lille</orgName>
								<orgName type="institution" key="instit2">Inria</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<orgName type="institution" key="instit4">Centrale Lille</orgName>
								<address>
									<addrLine>-CRIStAL</addrLine>
									<postCode>F-59000</postCode>
									<settlement>Lille</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">WordNet Is All You Need: A Surprisingly Effective Unsupervised Method for Graded Lexical Entailment</title>
					</analytic>
					<monogr>
						<imprint>
							<date />
						</imprint>
					</monogr>
					<idno type="MD5">A22536BDF22AE1C27297380CFB9954A8</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-04-12T14:50+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid" />
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div><p>We propose a simple unsupervised approach which exclusively relies on WordNet (Miller,  1995)  for predicting graded lexical entailment (GLE) in English. Inspired by the seminal work of Resnik (1995), our method models GLE as the sum of two information-theoretic scores: a symmetric semantic similarity score and an asymmetric specicity loss score, both exploiting the hierarchical synset structure of Word-Net. Our approach also includes a simple disambiguation mechanism to handle polysemy in a given word pair. Despite its simplicity, our method achieves performance above the state of the art (Spearman ρ = 0.75) on HyperLex (Vulic et al., 2017), the largest GLE dataset, outperforming all previous methods, including specialized word embeddings approaches that use WordNet as weak supervision.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div><head n="1">Introduction</head><p>A crucial aspect of language understanding is the ability to draw inferences between sentences. In many cases, these inferences are directly licensed by the semantics of words: e.g., the sentence a duck is in the room entails an animal is in the room simply because the concept of duck entails that of animal. These cases of (taxonomic) Lexical Entailment (LE) hold for words whose extensional denotations form a partial order: that is, the set of ducks is included in the set of birds which is itself included in the set of animals.</p><p>The taxonomic structure of lexical concepts is a dening aspect of human semantic memory and has been extensively studied in cognitive science as well as in NLP due to its multiple related applications. Initial research milestones include the construction of the WordNet lexical database <ref type="bibr">(Beckwith et al., 2021;</ref><ref type="bibr">Miller, 1995)</ref>, and the rst distributional approaches for automatically detecting hypernym-hyponym pairs <ref type="bibr">(Hearst (1992)</ref>; <ref type="bibr">Snow et al. (2004)</ref>; <ref type="bibr">Baroni et al. (2012)</ref>  <ref type="bibr">(1975)</ref> and <ref type="bibr">Kamp and</ref><ref type="bibr">Partee (1995), Vulic et al. (2017)</ref> have challenged the traditional view that LE is a binary relation, showing that it is instead a graded relation, based on human judgements (i.e., X entails Y to a certain degree). The concomitant release of Hyperlex,<ref type="foot" target="#foot_0">1</ref> a data set of English word pairs scored by humans for the LE relation, has spurred new research into developing models for predicting Graded Lexical Entailment (GLE). A small subset of the dataset is presented in Table <ref type="table">1</ref>.</p><p>An intriguing research question is whether existing hand-crafted lexical hierarchies like WordNet are indeed able to capture GLE. Preliminary experiments by <ref type="bibr">Vulic et al. (2017)</ref>; Vulic and Mrksic (2017) report largely negative results: their best WordNet-only based system achieves a mere 0.234 Spearman correlation score with human judgments from Hyperlex. These poor performance results are blamed on the binary coding of the hypernymhyponym relation in WordNet. Yet LEAR <ref type="bibr">(Vulic and Mrksic, 2017)</ref>, the best GLE system to date achieving a 0.682 Spearman correlation score, uses WordNet as a source of constraints for specializing static word embedding models to the task. As static word embeddings alone achieve poor performance for GLE, the question of the contribution of Word-Net in the LEAR improved performance remains open.</p><p>In this paper, we propose a simple method that directly and solely exploits the internal structure of WordNet to predict GLE. Our approach relies on Information Content (IC), a continuous informationtheoretic measure introduced in Resnik (1995) to model semantic similarity in WordNet. Specically, we propose to model GLE as a trade-off between a symmetric semantic similarity score and an asymmetric specicity loss score, both of which are dened in terms of IC. Our method is completed with a disambiguation mechanism to address the fact that (G)LE is sense, rather than word specic, and is therefore sensitive to polysemy, an issue that has been largely overlooked in previous work: e.g., the noun plant entails building only in its working plant sense, and not in its botanical sense. This simple method achieves a 0.744 Spearman correlation score with human judgements, outperforming all previous systems, including specialized word embeddings methods and supervised models, as well as systems based on contextual language models.</p><p>To sum-up, our main contributions are threefold. First, we show that the internal structure of Word-Net, as revealed by information-theoretic measures and completed by a disambiguation mechanism, is a reliable predictor of the graded nature of LE. Second, our simple WordNet-only based approach provides a new state-of-the-art for GLE, outperforming previous methods that specialize word embeddings to the LE task using WordNet as weak supervision. Third, we provide a detailed analysis of our method showing the role of the two information-theoretic terms and the importance of sense disambiguation. We also present a simplied version of our scoring function without any frequency information in the computation of IC, which further improves the correlation score (0.753), thus emphasizing the singular importance of Wordnet hierarchical structure for GLE.</p></div>
<div><head n="2">Proposed Method</head><p>Given a(n) (ordered) pair of words (X, Y ), instantiating a pair of latent (i.e., unknown) concepts (s X , s Y ), we aim to predict a score gle(X, Y ) indicating to what degree s X entails s Y . Specically, we propose to compute the score gle(X, Y ) as the sum of two terms: gle(X, Y ) = Sim(ŝ X , ŝY ) + SpecLoss(ŝ X , ŝY )</p><p>(1) where ŝX and ŝY are estimations of the latent concepts s X and s Y . The rst term Sim(ŝ X , ŝY ) stands for a (symmetric) semantic similarity function, capturing the fact that LE requires concepts to be semantically close. The second (asymmetric) term SpecLoss(ŝ X , ŝY ) encodes another important aspect of LE, namely the fact that there is generally a loss of specicity incurred by using ŝX (e.g., dog) instead of ŝY (e.g., animal), as the set denotation of ŝX is included in that of ŝY . <ref type="foot" target="#foot_1">2</ref>While the general idea of modeling GLE as a trade-off between a similarity term and a speci-city term is already present in Vulic et al. ( <ref type="formula">2017</ref>), the originality of our approach is to exclusively dene these terms using the hierarchical structure of WordNet, a lexical semantic graph made up of word senses (aka synsets) and relations between these synsets. This structure is accessed through information-theoretic measures that we dene now.</p><p>Information Content (IC) At rst glance, Word-Net might appear inadequate to model GLE because it encodes the hypernym-hyponym relation as a binary relation. But this claim is oblivious of two main facts. First, WordNet has some built-in gradedness as it models the hypernym-hyponym relation as a transitive relation. Second, the binary nature of the taxonomic links in WordNet can be easily bypassed by resorting to the notion of IC. This information-theoretical notion provides a continuous value for synsets by fully exploiting the tree structure associated with the hypernym-hyponym relation. Following <ref type="bibr">Shannon (1951</ref><ref type="bibr">), Resnik (1995)</ref> proposes to quantify the information content (aka self-entropy) of each lexical concept s as the log of its inverse probability by IC(s) = log(1/P (s)). While one can simply estimate P (s) via the word frequencies associated with s in a large text corpus, the crucial innovation of Resnik (1995) was to use the taxonomic tree structure of WordNet in this estimation. Specically, P (s) is estimated as</p><formula xml:id="formula_0">P (s) =  h∈Hypo(s) wc(h)  k wc(k)<label>(2)</label></formula><p>where wc(s) is the word count for synset s in a large corpus 3 (in our case, Wikipedia), Hypo(s) denotes the set of all hyponym descendants of s (s included), and k stands over all synsets in Word-Net. By fully exploiting the hierarchical structure of WordNet, the notion of IC intuitively captures the monotonic relation between the generality (resp. specicity) of concepts, as measured by their height (resp. depth) in the taxonomy, and their informativeness.</p><p>Similarity We dene Sim as the IC-based similarity measure introduced in Lin (1998). The similarity between two synsets ŝX and ŝY is dened as the ratio between the information shared by the two concepts, modeled by the IC value of their least common subsumer node (denoted as lcs below), and the information needed to fully describe the two concepts, modeled as the sum of their ICs, leading to</p><formula xml:id="formula_1">Sim(ŝ X , ŝY ) = 2 IC(lcs(ŝ X , ŝY )) IC(ŝ X ) + IC(ŝ Y ) .<label>(3)</label></formula></div>
<div><head>Specicity Loss</head><p>The above similarity measure is arguably a poor predictor of GLE if used alone. This measure will assign high scores to co-hyponyms (e.g., cat and dog) and equal scores to the same hypernym-hyponym pair whatever the order. We therefore need to complement this measure with another, asymmetric measure that is able to quantify the fact that the entailed concept is typically less informative. For this, we dene the specity loss by</p><formula xml:id="formula_2">SpecLoss(ŝ X , ŝY ) = 1 - IC(ŝ Y ) IC(ŝ X ) . (<label>4</label></formula><formula xml:id="formula_3">)</formula><p>This function returns values closer to 1.0 when the ŝX is more specic than ŝY and lower (possibly negative) values when ŝY is more specic than ŝX . The example of co-hyponyms shows the importance of the trade-off between the two scores. Indeed, while the similarity is maximized, the speci-city loss is minimized as both synsets have similar 3 wc(s) is the occurrence count of all words associated with s in WordNet, where a word count is normalized by its total number of synsets.</p><p>IC values, resulting in a sum that indicates relatively low GLE strength. Similarly, when ŝX is a hypernym of ŝY , the similarity score will be high but the specicity score will be low (even negative) and reduce the sum to a more appropriate score.</p><p>Synset Disambiguation Turning to the issue of estimating the latent synsets s X and s Y , we propose to jointly select a pair of synsets with ŝX , ŝY = argmax</p><formula xml:id="formula_4">s X ∈S(X),s Y ∈S(Y ) Sim(s X , s Y ) (5)</formula><p>where S(X) and S(Y ) denote the set of possible synsets for X and Y , respectively. That is, we select the pair of synsets with the maximum similarity value. For example, given the words plant and building, this method should hopefully select the synset corresponding to plant as a working plant, not the synset corresponding its botanical sense. We hypothesize that humans implicitly perform such joint sense selection when asked to score the relation between plant and building.</p></div>
<div><head n="3">Experiments</head><p>This section presents our experimental framework and results of our approach against various baselines and competing systems.<ref type="foot" target="#foot_2">4</ref> </p></div>
<div><head n="3.1">Dataset and Settings</head><p>Our evaluation dataset is the Hyperlex dataset <ref type="bibr">(Vulic et al., 2017)</ref>, which contains 2616 English word pairs (2163 noun pairs and 453 verb pairs). Extracted from WordNet, the pairs from the dataset were scored on a 0-6 scale by human subjects based on the prompt "To what degree is X a type of Y ?".<ref type="foot" target="#foot_3">5</ref> Scores of the different systems are compared using Spearman's ρ correlation <ref type="bibr">(Spearman, 1904)</ref>. As our method is fully unsupervised, we can evaluate it and other competing unsupervised methods and baselines on the entire Hyperlex dataset.</p><p>For ensuring fair comparison with supervised competitors, we also report the performance of our method on specic test subsets of Hyperlex. Specifically, we rely on the two test subsets provided by the Hyperlex authors: a random subset (25% of the pairs) and a train/validation/test split without any lexical overlap (see <ref type="bibr">Vulic et al. (2017)</ref> for more details). Finally, note that we use a text dump of Wikipedia for counting word occurrences for IC calculation and frequency baselines.</p></div>
<div><head n="3.2">Unsupervised Systems</head><p>Static Word Embeddings and WordNet Baselines Our baseline systems are taken or inspired from <ref type="bibr">Vulic et al. (2017)</ref>. These include a cosine similarity function based on Word2Vec <ref type="bibr">(Mikolov et al., 2013)</ref> and the best WordNet-only method reported in <ref type="bibr">Vulic et al. (2017)</ref>, using the Wu-Palmer similarity <ref type="bibr">(Wu and Palmer, 1994)</ref>. <ref type="bibr">Finally, Vulic et al. (2017)</ref> introduce a strong baseline (ρ score of 0.279) that combines a specicity term, dened in terms of a concept frequency ratio (i.e., 1 -wc(X) wc(Y ) for a word pair (X, Y )), and a Word2Vec cosine similarity term acting as a threshold. <ref type="foot" target="#foot_4">6</ref> We propose a variation of this approach, by instead summing the Word2Vec vector cosine similarity and the concept frequency ratio. Recall that static embeddings collapse all word senses, thus prevent the use of disambiguation technique in these methods.</p></div>
<div><head>CLM-based Methods</head><p>The success of contextual language models (CLM) on many tasks led us to study their usage for GLE. We tested several techniques of deriving static representations from contextual representations following <ref type="bibr">Apidianaki (2023)</ref>. We found that the best performing one was the method introduced by Misra et al. (2021) (called taxonomic verication) for the related task of graded typicality; in this case, the method uses a GPT-2-XL (Radford et al., 2019)  pretrained model. 7 In this approach, taxonomic sentences of the form "A(n) X is a(n) Y" are scored by the model, calculating P (Y |A(n) X is a(n)). Notice that such contextual prompts allow for some implicit joint disambiguation of the two words.</p></div>
<div><head>Specialized Static Embeddings</head><p>The last competitor is the current state-of-the-art LEAR system (Vulic and Mrksic, 2017), which is based on static embeddings specialized for LE through WordNet-derived constraints. Other systems which also use WordNet constraints are Hyper-Vec <ref type="bibr">(Nguyen et al., 2017)</ref> and Poincaré Embeddings (Nickel and Kiela, 2017) but their reported performance is lower on the GLE task.</p><p>Comparing Unsupervised Methods As shown in Table <ref type="table">2</ref>, all three baseline systems from Vulic et al. ( <ref type="formula">2017</ref>) achieve a ρ score below 0.3. Our baseline combining the concept frequency ratio and a Word2Vec cosine similarity achieves a 0.314 ρ score. Our CLM-based method achieves a 0.425 ρ score which is the best score achieved so far on the GLE task using CLMs. The best competitor to date is the LEAR system with a 0.686 ρ score (taken from Vulic and Mrksic ( <ref type="formula">2017</ref>)). <ref type="foot" target="#foot_6">8</ref>Our WordNet-based method, denoted by WordNet-SSD, reaches a 0.744 ρ score. To our knowledge, this is the best correlation score reported so far on Hyperlex. And it is indeed quite close to the human inter-annotator agreement correlation score of 0.854, which we can take as an upper bound on this task. These results strongly suggest that the hierarchical structure of WordNet provide enough information to accurately model graded LE, and that previous WordNet-based approaches have so far failed at properly leveraging this information.</p></div>
<div><head n="3.3">Supervised Baselines and Competing Systems</head><p>We also compare our method's performance to that of the supervised approach presented in <ref type="bibr">Vulic et al. (2017)</ref>. This method trains a supervised linear regression model on Word2Vec embeddings. As another baseline, we also train a supervised linear regression model using BERT token embeddings, instead of Word2Vec embeddings. Results on the two test splits of Hyperlex are presented in Table <ref type="table">3</ref>. The regression model with static embeddings achieves a Spearman's ρ of 0.53 and 0.45 for the random and lexical test splits, respectively, and of 0.420 and 0.257 when using BERT embeddings. On the same splits, our unsupervised method signicantly outperforms these supervised models, reaching ρ scores of 0.605 and 0.636, respectively.</p></div>
<div><head n="4">Analysis</head><p>This section analyses the different components of our approach via several targeted ablation studies.</p></div><figure type="table" xml:id="tab_0"><head /><label /><figDesc>; Dagan et al.</figDesc><table><row><cell>X</cell><cell>Y</cell><cell>LE Score</cell></row><row><cell>duck</cell><cell>animal</cell><cell>5.92</cell></row><row><cell>duck</cell><cell>bird</cell><cell>5.75</cell></row><row><cell>conict</cell><cell>disagreement</cell><cell>5.20</cell></row><row><cell cols="2">competence ability</cell><cell>4.64</cell></row><row><cell>aura</cell><cell>light</cell><cell>3.69</cell></row><row><cell>sofa</cell><cell>chair</cell><cell>3.38</cell></row><row><cell>butter</cell><cell>cream</cell><cell>2.69</cell></row><row><cell>noun</cell><cell>adjective</cell><cell>0.50</cell></row><row><cell>rhyme</cell><cell>dinner</cell><cell>0.00</cell></row></table><note><p><p><p><p><p><p><p><p><p>Table</p>1</p>: The human lexical entailment scores (0-6) for a small subset of the Hyperlex</p>(Vulic et al., 2017)   </p>dataset. Each row should be read as: X entails Y to a degree of LE score.</p>(2013) inter alia). More recently, an important strand of research has led to the development of word representation models that are able to geometrically express asymmetric relations like LE in the embedding space</p>(Roller and Erk, 2016; Vilnis and  McCallum, 2015; Nickel and Kiela, 2017)</p>.</p>Inspired by the pioneering works of Rosch</p></note></figure>
			<note place="foot" n="1" xml:id="foot_0"><p>https://github.com/cambridgeltl/hyperlex</p></note>
			<note place="foot" n="2" xml:id="foot_1"><p>Synonyms are an obvious exception, as they trivially entail each other while having the same denotation hence speci-city.</p></note>
			<note place="foot" n="4" xml:id="foot_2"><p>Our code and data are publicly available at: https:// gitlab.inria.fr/magnet/GLE_emnlp.</p></note>
			<note place="foot" n="5" xml:id="foot_3"><p>It is important to note the use of WordNet in creating Hyperlex was restricted to word pair selection, so no structural information from WordNet has inuenced the human scores.</p></note>
			<note place="foot" n="6" xml:id="foot_4"><p>See Equation (13) inVulic et al. (2017).</p></note>
			<note place="foot" n="7" xml:id="foot_5"><p>The other pretrained models we tested were bert-base and bert-large(Devlin et al., 2018), roberta-large(Liu  et al., 2019), deberta-v3-large (He et al., 2021), and  pythia-1b (Biderman et al., 2023).</p></note>
			<note place="foot" n="8" xml:id="foot_6"><p>Note that the system in Wang et al. (2020) is based on the LEAR system, but evaluated the SemEval 2020 English task 2, which is a different (fourth) subset of Hyperlex, achieving a Spearman's ρ of 0.696. We evaluated our method on this subset as well, achieving a Spearman's rho of 0.741.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl />
			</div>
		</back>
	</text>
</teiCorpus></text>
</TEI>