<?xml version='1.0' encoding='utf-8'?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" version="1.1" xsi:schemaLocation="http://www.tei-c.org/ns/1.0 http://api.archives-ouvertes.fr/documents/aofr-sword.xsd">
  <teiHeader>
    <fileDesc>
      <titleStmt>
        <title>HAL TEI export of hal-04411802</title>
      </titleStmt>
      <publicationStmt>
        <distributor>CCSD</distributor>
        <availability status="restricted">
          <licence target="http://creativecommons.org/licenses/by/4.0/">Distributed under a Creative Commons Attribution 4.0 International License</licence>
        </availability>
        <date when="2024-04-22T04:05:41+02:00" />
      </publicationStmt>
      <sourceDesc>
        <p part="N">HAL API platform</p>
      </sourceDesc>
    </fileDesc>
  </teiHeader>
  <text>
    <body>
      <listBibl>
        <biblFull>
          <titleStmt>
            <title xml:lang="en">Spatiotemporal self-supervised pre-training on satellite imagery improves food insecurity prediction</title>
            <author role="aut">
              <persName>
                <forename type="first">Ruben</forename>
                <surname>Cartuyvels</surname>
              </persName>
              <email type="md5">7583ee37f53f067e93d01596d602e29c</email>
              <email type="domain">kuleuven.be</email>
              <idno type="idhal" notation="numeric">1340101</idno>
              <idno type="halauthorid" notation="string">3026490-1340101</idno>
              <idno type="ORCID">https://orcid.org/0000-0003-1063-4659</idno>
              <affiliation ref="#struct-177790" />
            </author>
            <author role="aut">
              <persName>
                <forename type="first">Tom</forename>
                <surname>Fierens</surname>
              </persName>
              <idno type="halauthorid">3026491-0</idno>
              <affiliation ref="#struct-177790" />
            </author>
            <author role="aut">
              <persName>
                <forename type="first">Emiel</forename>
                <surname>Coppieters</surname>
              </persName>
              <idno type="halauthorid">3026492-0</idno>
              <affiliation ref="#struct-177790" />
            </author>
            <author role="aut">
              <persName>
                <forename type="first">Marie-Francine</forename>
                <surname>Moens</surname>
              </persName>
              <idno type="halauthorid">356552-0</idno>
              <affiliation ref="#struct-177790" />
            </author>
            <author role="aut">
              <persName>
                <forename type="first">Damien</forename>
                <surname>Sileo</surname>
              </persName>
              <email type="md5">dc6dee38e4ab31991c4d61b5d54d453a</email>
              <email type="domain">inria.fr</email>
              <idno type="idhal" notation="string">damien-sileo</idno>
              <idno type="idhal" notation="numeric">1247304</idno>
              <idno type="halauthorid" notation="string">1053014-1247304</idno>
              <idno type="ARXIV">https://arxiv.org/a/sileo_d_1</idno>
              <idno type="GOOGLE SCHOLAR">SIJPeoYAAAAJ</idno>
              <affiliation ref="#struct-104752" />
              <affiliation ref="#struct-120930" />
              <affiliation ref="#struct-374570" />
              <affiliation ref="#struct-432650" />
              <affiliation ref="#struct-410272" />
            </author>
            <editor role="depositor">
              <persName>
                <forename>Damien</forename>
                <surname>SILEO</surname>
              </persName>
              <email type="md5">dc6dee38e4ab31991c4d61b5d54d453a</email>
              <email type="domain">inria.fr</email>
            </editor>
          </titleStmt>
          <editionStmt>
            <edition n="v1" type="current">
              <date type="whenSubmitted">2024-01-23 11:52:31</date>
              <date type="whenModified">2024-02-16 11:12:07</date>
              <date type="whenReleased">2024-01-23 18:51:19</date>
              <date type="whenProduced">2023-12-18</date>
              <date type="whenEndEmbargoed">2024-01-23</date>
              <ref type="file" target="https://hal.science/hal-04411802/document">
                <date notBefore="2024-01-23" />
              </ref>
              <ref type="file" subtype="author" n="1" target="https://hal.science/hal-04411802/file/spatiotemporal_selfsupervised_pretraining_on_satellite_imagery_improves_food_insecurity_prediction.pdf">
                <date notBefore="2024-01-23" />
              </ref>
            </edition>
            <respStmt>
              <resp>contributor</resp>
              <name key="1409753">
                <persName>
                  <forename>Damien</forename>
                  <surname>SILEO</surname>
                </persName>
                <email type="md5">dc6dee38e4ab31991c4d61b5d54d453a</email>
                <email type="domain">inria.fr</email>
              </name>
            </respStmt>
          </editionStmt>
          <publicationStmt>
            <distributor>CCSD</distributor>
            <idno type="halId">hal-04411802</idno>
            <idno type="halUri">https://hal.science/hal-04411802</idno>
            <idno type="halBibtex">cartuyvels:hal-04411802</idno>
            <idno type="halRefHtml">&lt;i&gt;Environmental Data Science&lt;/i&gt;, 2023, 2, pp.e48. &lt;a target="_blank" href="https://dx.doi.org/10.1017/eds.2023.42"&gt;&amp;#x27E8;10.1017/eds.2023.42&amp;#x27E9;&lt;/a&gt;</idno>
            <idno type="halRef">Environmental Data Science, 2023, 2, pp.e48. &amp;#x27E8;10.1017/eds.2023.42&amp;#x27E9;</idno>
          </publicationStmt>
          <seriesStmt>
            <idno type="stamp" n="CNRS">CNRS - Centre national de la recherche scientifique</idno>
            <idno type="stamp" n="INRIA">INRIA - Institut National de Recherche en Informatique et en Automatique</idno>
            <idno type="stamp" n="INRIA-LILLE">INRIA Lille - Nord Europe</idno>
            <idno type="stamp" n="INRIA_TEST">INRIA - Institut National de Recherche en Informatique et en Automatique</idno>
            <idno type="stamp" n="TESTALAIN1">TESTALAIN1</idno>
            <idno type="stamp" n="CRISTAL">Centre de Recherche en Informatique, Signal et Automatique de Lille (CRISTAL)</idno>
            <idno type="stamp" n="INRIA2">INRIA 2</idno>
            <idno type="stamp" n="CRISTAL-MAGNET" corresp="CRISTAL">CRISTAL-MAGNET</idno>
            <idno type="stamp" n="UNIV-LILLE">Université de Lille</idno>
          </seriesStmt>
          <notesStmt>
            <note type="audience" n="2">International</note>
            <note type="popular" n="0">No</note>
            <note type="peer" n="1">Yes</note>
          </notesStmt>
          <sourceDesc>
            <biblStruct>
              <analytic>
                <title xml:lang="en">Spatiotemporal self-supervised pre-training on satellite imagery improves food insecurity prediction</title>
                <author role="aut">
                  <persName>
                    <forename type="first">Ruben</forename>
                    <surname>Cartuyvels</surname>
                  </persName>
                  <email type="md5">7583ee37f53f067e93d01596d602e29c</email>
                  <email type="domain">kuleuven.be</email>
                  <idno type="idhal" notation="numeric">1340101</idno>
                  <idno type="halauthorid" notation="string">3026490-1340101</idno>
                  <idno type="ORCID">https://orcid.org/0000-0003-1063-4659</idno>
                  <affiliation ref="#struct-177790" />
                </author>
                <author role="aut">
                  <persName>
                    <forename type="first">Tom</forename>
                    <surname>Fierens</surname>
                  </persName>
                  <idno type="halauthorid">3026491-0</idno>
                  <affiliation ref="#struct-177790" />
                </author>
                <author role="aut">
                  <persName>
                    <forename type="first">Emiel</forename>
                    <surname>Coppieters</surname>
                  </persName>
                  <idno type="halauthorid">3026492-0</idno>
                  <affiliation ref="#struct-177790" />
                </author>
                <author role="aut">
                  <persName>
                    <forename type="first">Marie-Francine</forename>
                    <surname>Moens</surname>
                  </persName>
                  <idno type="halauthorid">356552-0</idno>
                  <affiliation ref="#struct-177790" />
                </author>
                <author role="aut">
                  <persName>
                    <forename type="first">Damien</forename>
                    <surname>Sileo</surname>
                  </persName>
                  <email type="md5">dc6dee38e4ab31991c4d61b5d54d453a</email>
                  <email type="domain">inria.fr</email>
                  <idno type="idhal" notation="string">damien-sileo</idno>
                  <idno type="idhal" notation="numeric">1247304</idno>
                  <idno type="halauthorid" notation="string">1053014-1247304</idno>
                  <idno type="ARXIV">https://arxiv.org/a/sileo_d_1</idno>
                  <idno type="GOOGLE SCHOLAR">SIJPeoYAAAAJ</idno>
                  <affiliation ref="#struct-104752" />
                  <affiliation ref="#struct-120930" />
                  <affiliation ref="#struct-374570" />
                  <affiliation ref="#struct-432650" />
                  <affiliation ref="#struct-410272" />
                </author>
              </analytic>
              <monogr>
                <idno type="halJournalId" status="VALID">338915</idno>
                <idno type="eissn">2634-4602</idno>
                <title level="j">Environmental Data Science</title>
                <imprint>
                  <publisher>Cambridge University Press</publisher>
                  <biblScope unit="volume">2</biblScope>
                  <biblScope unit="pp">e48</biblScope>
                  <date type="datePub">2023-12-18</date>
                </imprint>
              </monogr>
              <idno type="doi">10.1017/eds.2023.42</idno>
            </biblStruct>
          </sourceDesc>
          <profileDesc>
            <langUsage>
              <language ident="en">English</language>
            </langUsage>
            <textClass>
              <keywords scheme="author">
                <term xml:lang="en">deep learning food insecurity remote sensing unsupervised pre-training</term>
                <term xml:lang="en">deep learning</term>
                <term xml:lang="en">food insecurity</term>
                <term xml:lang="en">remote sensing</term>
                <term xml:lang="en">unsupervised pre-training</term>
              </keywords>
              <classCode scheme="halDomain" n="info">Computer Science [cs]</classCode>
              <classCode scheme="halTypology" n="ART">Journal articles</classCode>
              <classCode scheme="halOldTypology" n="ART">Journal articles</classCode>
              <classCode scheme="halTreeTypology" n="ART">Journal articles</classCode>
            </textClass>
            <abstract xml:lang="en">
              <p>Abstract Global warming will cause unprecedented changes to the world. Predicting events such as food insecurities in specific earth regions is a valuable way to face them with adequate policies. Existing food insecurity prediction models are based on handcrafted features such as population counts, food prices, or rainfall measurements. However, finding useful features is a challenging task, and data scarcity hinders accuracy. We leverage unsupervised pre-training of neural networks to automatically learn useful features from widely available L andsat -8 satellite images. We train neural feature extractors to predict whether pairs of images are coming from spatially close or distant regions on the assumption that close regions should have similar features. We also integrate a temporal dimension to our pre-training to capture the temporal trends of satellite images with improved accuracy. We show that with unsupervised pre-training on a large set of satellite images, neural feature extractors achieve a macro F1 of 65.4% on the Famine Early Warning Systems network dataset—a 24% improvement over handcrafted features. We further show that our pre-training method leads to better features than supervised learning and previous unsupervised pre-training techniques. We demonstrate the importance of the proposed time-aware pre-training and show that the pre-trained networks can predict food insecurity with limited availability of labeled data.</p>
            </abstract>
          </profileDesc>
        </biblFull>
      </listBibl>
    </body>
    <back>
      <listOrg type="structures">
        <org type="laboratory" xml:id="struct-177790" status="VALID">
          <orgName>Department of Computer Science</orgName>
          <orgName type="acronym">KU Leuven - CS</orgName>
          <desc>
            <address>
              <addrLine>Departement ComputerwetenschappenCelestijnenlaan 200A B-3001 Leuven</addrLine>
              <country key="BE" />
            </address>
            <ref type="url">https://wms.cs.kuleuven.be/cs/english</ref>
          </desc>
          <listRelation>
            <relation active="#struct-300656" type="direct" />
          </listRelation>
        </org>
        <org type="laboratory" xml:id="struct-104752" status="VALID">
          <idno type="RNSR">200818245B</idno>
          <idno type="ROR">https://ror.org/04eej9726</idno>
          <orgName>Inria Lille - Nord Europe</orgName>
          <desc>
            <address>
              <addrLine>Parc Scientifique de la Haute Borne 40, avenue Halley Bât.A, Park Plaza 59650 Villeneuve d'Ascq</addrLine>
              <country key="FR" />
            </address>
            <ref type="url">http://www.inria.fr/lille/</ref>
          </desc>
          <listRelation>
            <relation active="#struct-300009" type="direct" />
          </listRelation>
        </org>
        <org type="institution" xml:id="struct-120930" status="VALID">
          <idno type="IdRef">256304629</idno>
          <idno type="ISNI">0000000122034461</idno>
          <idno type="ROR">https://ror.org/01x441g73</idno>
          <orgName>Centrale Lille</orgName>
          <desc>
            <address>
              <addrLine>École Centrale de Lille - Cité Scientifique - CS 20048 59651 Villeneuve d'Ascq Cedex</addrLine>
              <country key="FR" />
            </address>
            <ref type="url">https://centralelille.fr/</ref>
          </desc>
        </org>
        <org type="regroupinstitution" xml:id="struct-374570" status="VALID">
          <idno type="IdRef">223446556</idno>
          <idno type="ISNI">0000 0001 2242 6780</idno>
          <idno type="ROR">https://ror.org/02kzqn938</idno>
          <idno type="Wikidata">Q3551621</idno>
          <orgName>Université de Lille</orgName>
          <desc>
            <address>
              <addrLine>EPE Université de Lille. -- 42 rue Paul Duez, 59000 Lille</addrLine>
              <country key="FR" />
            </address>
            <ref type="url">https://www.univ-lille.fr/</ref>
          </desc>
        </org>
        <org type="researchteam" xml:id="struct-432650" status="VALID">
          <idno type="RNSR">201321079K</idno>
          <orgName>Machine Learning in Information Networks</orgName>
          <orgName type="acronym">MAGNET</orgName>
          <date type="start">2015-01-01</date>
          <desc>
            <address>
              <country key="FR" />
            </address>
            <ref type="url">http://www.inria.fr/equipes/magnet</ref>
          </desc>
          <listRelation>
            <relation active="#struct-104752" type="direct" />
            <relation active="#struct-300009" type="indirect" />
            <relation active="#struct-410272" type="direct" />
            <relation name="UMR9189" active="#struct-120930" type="indirect" />
            <relation name="UMR9189" active="#struct-374570" type="indirect" />
            <relation name="UMR9189" active="#struct-441569" type="indirect" />
          </listRelation>
        </org>
        <org type="laboratory" xml:id="struct-410272" status="VALID">
          <idno type="IdRef">18388695X</idno>
          <idno type="RNSR">201521249L</idno>
          <idno type="ROR">https://ror.org/05vrs3189</idno>
          <orgName>Centre de Recherche en Informatique, Signal et Automatique de Lille - UMR 9189</orgName>
          <orgName type="acronym">CRIStAL</orgName>
          <date type="start">2015-01-01</date>
          <desc>
            <address>
              <addrLine>Université de Lille - Campus scientifique - Bâtiment ESPRIT - Avenue Henri Poincaré - 59655 Villeneuve d’Ascq</addrLine>
              <country key="FR" />
            </address>
            <ref type="url">https://www.cristal.univ-lille.fr/</ref>
          </desc>
          <listRelation>
            <relation name="UMR9189" active="#struct-120930" type="direct" />
            <relation name="UMR9189" active="#struct-374570" type="direct" />
            <relation name="UMR9189" active="#struct-441569" type="direct" />
          </listRelation>
        </org>
        <org type="institution" xml:id="struct-300656" status="VALID">
          <idno type="ROR">https://ror.org/05f950310</idno>
          <orgName>Catholic University of Leuven = Katholieke Universiteit Leuven</orgName>
          <orgName type="acronym">KU Leuven</orgName>
          <desc>
            <address>
              <addrLine>Oude Markt 13 - bus 5005, 3000 Leuven</addrLine>
              <country key="BE" />
            </address>
            <ref type="url">http://www.kuleuven.be/english/</ref>
          </desc>
        </org>
        <org type="institution" xml:id="struct-300009" status="VALID">
          <idno type="ROR">https://ror.org/02kvxyf05</idno>
          <orgName>Institut National de Recherche en Informatique et en Automatique</orgName>
          <orgName type="acronym">Inria</orgName>
          <desc>
            <address>
              <addrLine>Domaine de VoluceauRocquencourt - BP 10578153 Le Chesnay Cedex</addrLine>
              <country key="FR" />
            </address>
            <ref type="url">http://www.inria.fr/en/</ref>
          </desc>
        </org>
        <org type="regroupinstitution" xml:id="struct-441569" status="VALID">
          <idno type="IdRef">02636817X</idno>
          <idno type="ISNI">0000000122597504</idno>
          <idno type="ROR">https://ror.org/02feahw73</idno>
          <orgName>Centre National de la Recherche Scientifique</orgName>
          <orgName type="acronym">CNRS</orgName>
          <date type="start">1939-10-19</date>
          <desc>
            <address>
              <country key="FR" />
            </address>
            <ref type="url">https://www.cnrs.fr/</ref>
          </desc>
        </org>
      </listOrg>
    </back>
  <teiCorpus>
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Spatiotemporal self-supervised pre-training on satellite imagery improves food insecurity prediction</title>
				<funder>
					<orgName type="full">Research Foundation-Flanders (FWO)</orgName>
				</funder>
				<funder>
					<orgName type="full">VSC (Flemish Supercomputer Center)</orgName>
				</funder>
				<funder>
					<orgName type="full">Flemish government</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher />
				<availability status="unknown"><licence /></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Ruben</forename><surname>Cartuyvels</surname></persName>
							<email>ruben.cartuyvels@kuleuven.be</email>
							<idno type="ORCID">0000-0003-1063-4659</idno>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">KU Leuven</orgName>
								<address>
									<settlement>Leuven</settlement>
									<country key="BE">Belgium</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">KU Leuven</orgName>
								<address>
									<settlement>Leuven</settlement>
									<country key="BE">Belgium</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tom</forename><surname>Fierens</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">KU Leuven</orgName>
								<address>
									<settlement>Leuven</settlement>
									<country key="BE">Belgium</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Emiel</forename><surname>Coppieters</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">KU Leuven</orgName>
								<address>
									<settlement>Leuven</settlement>
									<country key="BE">Belgium</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Marie-Francine</forename><surname>Moens</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">KU Leuven</orgName>
								<address>
									<settlement>Leuven</settlement>
									<country key="BE">Belgium</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Damien</forename><surname>Sileo</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">KU Leuven</orgName>
								<address>
									<settlement>Leuven</settlement>
									<country key="BE">Belgium</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Spatiotemporal self-supervised pre-training on satellite imagery improves food insecurity prediction</title>
					</analytic>
					<monogr>
						<imprint>
							<date />
						</imprint>
					</monogr>
					<idno type="MD5">AC21D38063892B10B34D148F0AB0EA3C</idno>
					<idno type="DOI">10.1017/eds.2023.42</idno>
					<note type="submission">Received: 01 February 2022; Revised: 25 September 2023; Accepted: 06 November 2023</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-04-12T14:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid" />
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>deep learning</term>
					<term>food insecurity</term>
					<term>remote sensing</term>
					<term>unsupervised pre-training</term>
				</keywords>
			</textClass>
			<abstract>
<div><p>Global warming will cause unprecedented changes to the world. Predicting events such as food insecurities in specific earth regions is a valuable way to face them with adequate policies. Existing food insecurity prediction models are based on handcrafted features such as population counts, food prices, or rainfall measurements. However, finding useful features is a challenging task, and data scarcity hinders accuracy. We leverage unsupervised pre-training of neural networks to automatically learn useful features from widely available Landsat-8 satellite images. We train neural feature extractors to predict whether pairs of images are coming from spatially close or distant regions on the assumption that close regions should have similar features. We also integrate a temporal dimension to our pre-training to capture the temporal trends of satellite images with improved accuracy. We show that with unsupervised pretraining on a large set of satellite images, neural feature extractors achieve a macro F1 of 65.4% on the Famine Early Warning Systems network dataset-a 24% improvement over handcrafted features. We further show that our pretraining method leads to better features than supervised learning and previous unsupervised pre-training techniques. We demonstrate the importance of the proposed time-aware pre-training and show that the pre-trained networks can predict food insecurity with limited availability of labeled data.</p></div>
<div><head>Impact Statement</head><p>This study shows that satellite images and deep learning can be used to drastically improve predictions of food insecurity compared to existing predictors in countries or regions where food insecurity is mainly caused by agricultural or weather-related factors. Vast amounts of unlabeled and publicly available satellite image data can be used to pre-train a neural network using the method proposed in this study. This further improves predictions, but also decreases the amount of labeled food insecurity data needed for training in order to obtain accurate predictions. This is useful since accurate food insecurity data to train models might be hard or costly to obtain. To increase the impact of this work, it would be valuable to research how to improve forecasts of food insecurity in the future, which remains hard.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div><head n="1.">Introduction</head><p>Satellite imagery has been a precious source of information for many different fields and for many years. Satellite images are, for instance, essential for weather prediction, agricultural observations, oceanography, cartography, biodiversity monitoring, and many more. Since the first orbital satellite images obtained in 1959, there are now over 150 earth observation satellites in orbit. With an abundance of satellite imagery available both across time and space, many studies <ref type="bibr" target="#b43">(Mohanty et al., 2020)</ref> have searched for efficient ways to process this data to gain useful insights. In recent years, deep convolutional neural networks (CNNs) have increasingly been used to analyze such imagery <ref type="bibr" target="#b27">(Jean et al., 2016;</ref><ref type="bibr" target="#b33">Kussul et al., 2017;</ref><ref type="bibr" target="#b44">Nevavuori et al., 2019;</ref><ref type="bibr" target="#b66">Yeh et al., 2020)</ref>. However, training deep neural networks from scratch in a supervised way requires a large amount of labeled data, which is costly to obtain.</p><p>A variety of contrastive self-supervised pre-training methods has been proposed to deal with this problem <ref type="bibr" target="#b28">(Jean et al., 2019;</ref><ref type="bibr">Ayush et al., 2021a;</ref><ref type="bibr" target="#b29">Kang et al., 2021;</ref><ref type="bibr">Manas et al., 2021)</ref>. These methods pre-train neural networks on large amounts of unlabeled satellite imagery so they learn useful parameters. They typically are contrastive, which means they maximize the mutual information between pairs of similar samples (tiles of satellite imagery) while minimizing mutual information between dissimilar pairs. The learned parameters can then be used as a starting point for the supervised training of different downstream tasks, for which little labeled data might be available, and often prove to be more effective than using randomly initialized neural networks. Yet, existing methods completely ignore the temporal dimension of satellite imagery <ref type="bibr" target="#b28">(Jean et al., 2019;</ref><ref type="bibr" target="#b29">Kang et al., 2021)</ref> or learn only highly time-invariant and highly spatially variant representations in a non-flexible manner <ref type="bibr">(Ayush et al., 2021a;</ref><ref type="bibr">Manas et al., 2021)</ref>. This is problematic since downstream tasks may range from being highly variant to highly invariant against spatial, or independently, temporal distance. For instance, models for weather or rainfall forecasting might benefit from sensitivity to changes that typically occur on a timescale of days, while for land cover classification, it might be beneficial to abstract those exact same changes away and focus on changes occurring over years. This study explores the use of relational pre-training <ref type="bibr" target="#b46">(Patacchiola and Storkey, 2020)</ref>, a state-of-the-art contrastive pre-training method for satellite imagery. During pretraining, both similarities between the same satellite image tile over time and similarities between geographically neighboring image tiles are taken into account. Importantly, this framework allows the implementer to easily and independently specify the degree of temporal and spatial sensitivity needed for a certain downstream task by choosing thresholds that determine which pairs in the contrastive pretraining are considered similar and which pairs dissimilar.</p><p>We use freely available LANDSAT-8 imagery, from which we construct representations that serve as an input to predict food insecurity in Somalia. Although several studies explore the use of satellite imagery for predicting poverty, food insecurity is a relatively unexplored topic. Yet, in 2019, as much as 8.9% of the world's population was undernourished, and 10.10% lived in severe food insecurity <ref type="bibr" target="#b49">(Roser and Itchie, 2019)</ref>. Existing early-warning systems, as <ref type="bibr" target="#b0">Andree et al. (2020)</ref> note, suffer from high falsenegative rates. Therefore, automating and improving warning systems can be of great humanitarian value.</p><p>Our hypothesis is that useful information can be drawn from satellite imagery to predict Famine Early Warning Systems (FEWS) Integrated Phase Classification (IPC) food insecurity scores <ref type="bibr" target="#b31">(Korpi-Salmela et al., 2012)</ref> due to, for instance, environmental changes and increasing droughts.</p><p>Our research questions are:</p><p>1. Can pre-trained representations of satellite images improve food insecurity prediction accuracy? 2. How do different temporal and spatial relationship prediction settings as pre-training influence downstream task performance?</p><p>We analyze the effect of relational pre-training on satellite imagery representations by comparing different temporal and spatial similarity thresholds. We compare the performance of our pre-trained model with a pre-trained baseline and with fully supervised networks for a range of training set sizes. We include the predictions of our model in the input for an existing food crises predictor <ref type="bibr" target="#b0">(Andree et al., 2020)</ref> to test if this improves performance. We test out-of-domain food insecurity prediction in regions that weren't included in pre-training data.</p><p>Our findings suggest that using spatially and temporally linked images as positive pairs for relational pre-training can outperform (1) a randomly initialized network without pre-training, (2) pre-training on standard data augmentations as in <ref type="bibr" target="#b46">Patacchiola and Storkey (2020)</ref>, (3) a network that has been pre-trained on ImageNet <ref type="bibr" target="#b13">(Deng et al., 2009)</ref>, and (4) a strong contrastive baseline pre-trained on the same satellite imagery <ref type="bibr" target="#b28">(Jean et al., 2019)</ref>. Our pre-trained model also outperforms a random forest classifier based on previously used manually selected features <ref type="bibr" target="#b0">(Andree et al., 2020)</ref>. We show that our pre-trained model needs little labeled data to learn to make good predictions and that the model's predictions are not reducible to predicting the season of the acquisition of a satellite image. We compare the importance of the input LANDSAT-8 bands. We find that forecasting future food insecurity remains hard for all of the considered methods.</p></div>
<div><head n="2.">Related work</head><p>2.1. Self-supervised image representation learning Large amounts of labeled data are needed for training neural networks in a supervised way. Since labeled data are scarce and expensive to collect compared to unlabeled data, specific methods have been developed to leverage unlabeled data. A model can be trained in two stages.</p><p>First, the model is trained on a large, unlabeled dataset in a self-supervised manner. In self-supervised learning (SSL), pseudo-labels are constructed automatically from the unlabeled data, which reframes unsupervised learning as supervised learning and allows the use of standard learning techniques like gradient descent <ref type="bibr" target="#b15">(Dosovitskiy et al., 2016;</ref><ref type="bibr" target="#b67">Zhang et al., 2017)</ref>. The goal of this first stage is to obtain a neural network that produces informative, general-purpose representations for input data <ref type="bibr" target="#b4">(Bengio et al., 2013)</ref>.</p><p>In a second stage, models are finetuned for specific downstream tasks, for which (often little) labeled data are available, in a supervised way. By leveraging large amounts of unlabeled data, pre-trained models often outperform their counterparts that have only been trained on the smaller labeled dataset in a supervised manner <ref type="bibr" target="#b54">(Schmarje et al., 2021)</ref>. Such techniques are used in many machine learning (ML) application domains, like in natural language processing, image recognition, video-based tasks, or control tasks <ref type="bibr" target="#b41">(Mikolov et al., 2013;</ref><ref type="bibr" target="#b14">Devlin et al., 2019;</ref><ref type="bibr" target="#b16">Florensa et al., 2019;</ref><ref type="bibr" target="#b50">Rouditchenko et al., 2019;</ref><ref type="bibr" target="#b21">Han et al., 2020;</ref><ref type="bibr" target="#b38">Liu et al., 2023;</ref><ref type="bibr" target="#b48">Qian et al., 2021)</ref>.</p><p>Our work uses contrastive learning for learning image representations in a self-supervised way <ref type="bibr" target="#b10">(Chopra et al., 2005;</ref><ref type="bibr" target="#b34">Le-Khac et al., 2020;</ref><ref type="bibr" target="#b26">Jaiswal et al., 2021)</ref>. We train a model to project samples into a feature space where positive pairs are close to and negative pairs are far from each other. Contrastive pre-training has been used to learn image representations with great success recently, for instance, by van den Oord et al. ( <ref type="formula">2018</ref> Our study builds upon the relational reasoning framework for contrastive pre-training proposed by <ref type="bibr" target="#b46">Patacchiola and Storkey (2020)</ref>, but adapts it to satellite images by using spatial and temporal information to define similar and dissimilar image pairs, instead of images and data augmentations. <ref type="foot" target="#foot_1">1</ref> We chose this approach because of its state-of-the-art results and interpretability. We are not the first to use spatial and temporal information for contrastive pre-training: for instance, <ref type="bibr" target="#b48">Qian et al. (2021)</ref> proposed a method for pre-training video representations, but their application domain of videos of daily human actions was quite different from our setting, their contrastive samples were video fragments, and they did not use spatial neighborhoods for defining positive or negative pairs.</p></div>
<div><head n="2.2.">Learning representations of satellite imagery</head><p>The large amounts of publicly available remote-sensing data from programs such as LANDSAT <ref type="bibr" target="#b64">(Williams et al., 2006)</ref> and SENTINEL (The European Space Agency, 2021) make this an interesting area of application for self-supervised pre-training techniques. Additionally, metadata like the spatial location or timestamps of images can be used to construct the distributions from which positive and negative pairs for contrastive learning are sampled.</p><p>Deep learning has, for instance, been used on satellite imagery for land cover and vegetation type classification <ref type="bibr" target="#b33">(Kussul et al., 2017;</ref><ref type="bibr" target="#b53">Rustowicz et al., 2019;</ref><ref type="bibr" target="#b59">Vali et al., 2020)</ref>, various types of scene classification <ref type="bibr" target="#b9">(Cheng et al., 2017)</ref>, object or infrastructure recognition <ref type="bibr" target="#b37">(Li et al., 2017</ref><ref type="bibr" target="#b36">(Li et al., , 2020))</ref>, and change detection <ref type="bibr" target="#b32">(Kotkar and Jadhav, 2015;</ref><ref type="bibr" target="#b11">Chu et al., 2016;</ref><ref type="bibr" target="#b18">Gong et al., 2016;</ref><ref type="bibr" target="#b12">de Jong and Bosman, 2019)</ref>.</p><p>Several studies have proposed the use of self-supervised pre-training on satellite images. <ref type="bibr" target="#b28">Jean et al. (2019)</ref> proposed a triplet loss that pulls representations of spatially close tiles toward each other and pushes representations of distant tiles away from each other. <ref type="bibr">Wang et al. (2020b)</ref> additionally used language embeddings from geotagged customer reviews. <ref type="bibr" target="#b29">Kang et al. (2021)</ref> and <ref type="bibr">Ayush et al. (2021a)</ref> also defined positive pairs based on geographical proximity and used momentum contrast <ref type="bibr" target="#b22">(He et al., 2020)</ref> for a larger set of negative samples.</p><p>However, most recent works ignore the additional information that could be obtained from the temporal dimension of satellite images: satellites usually gather images of the same locations across multiple points in time. <ref type="bibr">Ayush et al. (2021a)</ref> take images of the same location from distinct points in time as positive pairs, which causes their representations to be inevitably time-invariant. Manas et al. ( <ref type="formula">2021</ref>) also proposed a contrastive pre-training method for satellite imagery using the temporal dimension. Both studies obtained representations that are maximally spatially variant, in the sense that only tiles of exactly the same location are considered similar. Neither method allowed to flexibly set different thresholds and consequently obtain different degrees of temporal and spatial variance.</p><p>In this work, we apply the state-of-the-art relational reasoning method of <ref type="bibr" target="#b46">Patacchiola and Storkey (2020)</ref> to satellite images for the first time. This allows us to flexibly define and test several rules for positive/negative pair sampling, including rules that define images of the same location from distinct moments as dissimilar, and images from the same moments of nearby but not exactly the same locations as similar, which could result in relatively more space-invariant but time-variant representations, compared to <ref type="bibr">Ayush et al. (2021a)</ref> and <ref type="bibr">Manas et al. (2021)</ref>.</p></div>
<div><head n="2.3.">Food insecurity prediction</head><p>Numerous studies have attempted to predict socioeconomic variables from satellite images. The predicted variables most often concern poverty, economic activity, welfare, or population density <ref type="bibr" target="#b57">(Townsend and Bruce, 2010;</ref><ref type="bibr" target="#b27">Jean et al., 2016;</ref><ref type="bibr" target="#b17">Goldblatt et al., 2019;</ref><ref type="bibr" target="#b25">Hu et al., 2019;</ref><ref type="bibr" target="#b3">Bansal et al., 2020;</ref><ref type="bibr" target="#b66">Yeh et al., 2020;</ref><ref type="bibr">Ayush et al., 2021b;</ref><ref type="bibr" target="#b6">Burke et al., 2021)</ref>. Some studies used additional data sources such as geotagged Wikipedia articles <ref type="bibr" target="#b55">(Sheehan et al., 2019;</ref><ref type="bibr" target="#b58">Uzkent et al., 2019)</ref>. Researchers have also predicted crop yields from satellite images <ref type="bibr" target="#b61">(Wang et al., 2018;</ref><ref type="bibr" target="#b44">Nevavuori et al., 2019)</ref>, which is closer to food insecurity prediction, with the main difference being that food insecurity might also be caused by different factors such as political instability.</p><p>Other studies also predicted food insecurity using ML, but none used satellite imagery directly. <ref type="foot" target="#foot_2">2</ref> The World Bank has published two studies that predicted food insecurity from data in addition to defining a food insecurity score. <ref type="bibr">Wang et al. (2020a)</ref> used a panel vector-autoregression (PVAR) model to model food insecurity distributions of 15 Sub-Saharan African countries on longer time horizons. <ref type="bibr" target="#b0">Andree et al. (2020)</ref>, on the other hand, used a random forest <ref type="bibr" target="#b5">(Breiman, 2001)</ref> to model food insecurity on a shorter time horizon, with multiple handcrafted features as input: (1) structural factors such as spatial and population trends, ruggedness, and land use shares, (2) environmental factors such as the normalized difference vegetation index (NDVI), rainfall, and water balance equation, (3) violent conflict information, and (4) food price inflation. We mainly compare with the shorter-term predictions of <ref type="bibr" target="#b0">Andree et al. (2020)</ref>. <ref type="bibr" target="#b35">Lentz et al. (2019)</ref> predicted different food insecurity scores for Malawi from various input variables using linear and log-linear regression models.</p></div>
<div><head n="3.">Spatiotemporal SSL</head><p>Contrastive learning methods enable representation learning without annotated data. Instead, they rely on the intrinsic structure of data. For example, different patches from the same image are likely to be similar to each other and dissimilar to patches from other images. Training image representations to enable this discrimination should lead to useful image features.</p><p>Here we leverage the contrastive framework proposed by <ref type="bibr" target="#b46">Patacchiola and Storkey (2020)</ref>, who formulated this principle with an explicit relation prediction. They mapped each image I to augmentations A I ð Þ (e.g., patches) and jointly trained an image encoder ϕ and a relation prediction network ρ to predict whether augmentations come from the same image:</p><formula xml:id="formula_0">ŷ ¼ ρ ϕ A I i ð Þ ð Þ, ϕ A I j À Á À Á À Á , (<label>1</label></formula><formula xml:id="formula_1">)</formula><p>where ŷ should be close to 1 when i ¼ j and close to 0 otherwise. We use the same loss as Patacchiola and Storkey (2020):</p><formula xml:id="formula_2">L ŷ, y ð Þ¼À 1 N X N i¼1 w i CrossEntropy y i ,ŷ i ð Þ,<label>(2)</label></formula><p>where w i is the focal factor that modulates the loss according to the prediction confidence through a hyperparameter γ:</p><formula xml:id="formula_3">w i ¼ 1 2 1 À y i ð Þŷ i þ y i 1 À ŷi ð Þ ½ γ :<label>(3)</label></formula><p>If γ &gt; 1, uncertain predictions have a greater effect on the training loss. <ref type="bibr" target="#b46">Patacchiola and Storkey (2020)</ref> only rely on standard spatial image augmentations (horizontal flip, random crop-resize, conversion to grayscale, and color jitter). Where for natural images it makes sense to assume different images will be semantically different, since they are likely to depict different objects or scenes, satellite image tiles could be seen as a patchwork that forms a single large image, evolving over time, from the same object, that is, the Earth. The division of satellite imagery into smaller image tiles follows arbitrary boundaries determined by, for example, latitude/longitude coordinates, and not actual semantic boundaries, hence the resulting neighboring tiles are not necessarily likely to vary semantically. However, as spatial distance between satellite image tiles or time between when satellite images are taken increases, so does the likelihood that what is depicted changes semantically. Therefore, we define new augmentations based on temporal and spatial distances, and we consider far-away patches as if they came from a different image. In the next section, we evaluate this idea and compare different similarity criteria. We call our pre-training method spatiotemporal SSL (SSSL).</p><p>For this purpose, we introduce different thresholds D g and D t of respectively geographic distance and temporal distance in order to define positive pairs. D g is measured in degrees of longitude/latitude, and D t in months. Let x i be a sampled anchor observation characterized by time t i , latitude lat i , and longitude lon i . Positive (similar) pairs include images x j for which the following constraints apply:</p><formula xml:id="formula_4">t i À D t &lt; t j &lt; t i þ D t , (<label>4a</label></formula><formula xml:id="formula_5">)</formula><formula xml:id="formula_6">lat i À D g &lt; lat j &lt; lat i þ D g , (<label>4b</label></formula><formula xml:id="formula_7">)</formula><p>Environmental Data Science e48-5</p><p>https://doi.org/10.1017/eds.2023.42 Published online by Cambridge University Press</p><formula xml:id="formula_8">lon i À D g &lt; lon j &lt; lon i þ D g :<label>(4c)</label></formula><p>Figure <ref type="figure" target="#fig_1">1</ref> illustrates this constraint. When D t is arbitrarily high, and D g ≈ 0, the positive pairs are similar to the positive pairs defined by <ref type="bibr">Ayush et al. (2021a)</ref>. This would result in spatially variant but timeinvariant representations and reduce the effect of seasonality or other temporal trends on the representation. When D t &lt; f , on the other hand, with f the temporal frequency of the imagery, positive pairs are purely location-based. This is similar to the strategy of Tile2Vec <ref type="bibr" target="#b28">(Jean et al., 2019.</ref> In addition to fixed thresholds of D g as in Eqs. ( <ref type="formula" target="#formula_4">4b</ref>)-(4c), we also define spatial positive pairs as images that correspond to the same predefined area (administrative unit, AU).</p></div>
<div><head n="4.">IPC score prediction</head><p>We approach the downstream task of predicting food insecurity as a classification problem of satellite tiles I (or a collection thereof) into one out of five possible IPC scores s corresponding to different levels of food insecurity. We chose to approach IPC score prediction as a classification problem following <ref type="bibr" target="#b0">Andree et al. (2020)</ref>.</p><p>An image encoder ϕ (cf. Eq. ( <ref type="formula" target="#formula_0">1</ref>)), possibly pre-trained as described in the previous section, projects a tile onto a tile embedding ϕ I ð Þ. A multilayer perceptron (MLP) with 1 hidden layer then projects the tile embedding to a probability distribution over the IPC scores:</p><formula xml:id="formula_9">ŝtile $ MLP ϕ I ð Þ ð Þ.</formula><p>We train the classification MLP and potentially the image encoder ϕ with a cross-entropy loss to assign the highest probability to the IPC score of the AU to which the location of a tile belongs to, on the date of the satellite image. Unless otherwise stated, we predict the IPC score gathered by <software>FEWS NET</software> for the same date as the satellite image was taken. In one experiment, we will forecast future IPC scores, gathered for dates up to 12 months after the date of the satellite image that was used as input.</p></div>
<div><head n="4.1.">Score aggregation</head><p>Since IPC scores are defined in AUs, and since one AU contains many different locations and hence satellite image tiles, we need a way to aggregate our network's predictions per tile into one prediction per AU. If M tiles I i f g i¼1,…,M ∈ AU k , where AU k is an AU, we need a single predicted IPC score ŝAU k for the whole unit, based on the predicted IPC score ŝtile per tile I i :</p><formula xml:id="formula_10">ŝAU k ¼ Agg ŝtile i È É i¼1,…,M ,</formula><p>where ŝtile</p><formula xml:id="formula_11">i ¼ argmaxðMLP ϕ I i ð Þ ð Þ,<label>(7)</label></formula><p>and where Agg :</p><formula xml:id="formula_12">s tile 1 ,…,s tile M È É</formula><p>↦ŝ AU is an aggregation function. We consider three aggregation methods:</p><p>1. Majority voting: the predicted score for the AU is the score that has been predicted most often for tiles within that AU. 2. Maximum voting: the predicted score for the AU is the maximum of the predicted tile scores. 3. Individual tiles: predicting and evaluating the IPC scores on a per-tile basis, which is arguably harder since a tile's IPC score may be determined by another location in the AU.</p></div>
<div><head n="5.">Experiments</head></div>
<div><head n="5.1.">Data</head></div>
<div><head n="5.1.1.">Pre-training</head><p>We make use of publicly available imagery from the LANDSAT-8 satellite<ref type="foot" target="#foot_3">3</ref>  <ref type="bibr">(Roy et al., 2014)</ref>. LANDSAT is the longest-running satellite photography program and is a collaboration between the US Geological Service (USGS) and the National Aeronautics and Space Administration (NASA). The satellite captures landscapes from all over the world with a spatial resolution of 30 m per pixel and a temporal resolution of 16 days. To reduce the impact of clouds on the satellite images, we use <software ContextAttributes="used">Google Earth Engine</software><ref type="foot" target="#foot_4">4</ref> (GEE; <ref type="bibr" target="#b19">Gorelick et al., 2017)</ref> to generate composite images comprised of individual images spanning 3-4 months, matching the temporal frequency of the downstream food insecurity samples. We use all seven available surface reflectance spectral bands: one ultra-blue, three visible (RGB), one near-infrared, and two shortwave infrared. We use images of the entire surface area of Somalia (640K km 2 ), which were captured between May 2013 (earliest LANDSAT-8 data availability in GEE) and March 2020 (latest available IPC score), resulting in 10 three-month and 13 four-month composites. We divide the images into tiles of 145×145 pixels so they can be processed by a CNN. Figure <ref type="figure" target="#fig_2">2</ref> shows the visible RGB bands of three such tiles. One tile corresponds to almost 19 km 2 . We end up with 800K tiles, consisting of 35K locations across 23 moments in time.</p></div>
<div><head n="5.1.2.">Food insecurity prediction</head><p>We use the data on food insecurity in 21 developing countries made available by <ref type="bibr" target="#b0">Andree et al. (2020)</ref> to finetune our results. <software ContextAttributes="used">FEWS NET</software><ref type="foot" target="#foot_5">5</ref> , an information provider that monitors and publishes data on food insecurity events, defines the target variable: the IPC score. The IPC score has five possible values:</p><p>(1) minimal, (2) stressed, (3) crisis, (4) emergency, and (5) famine. The scores are measured using the IPC system, which is an analytical framework to qualitatively assess the severity of a food crisis and consecutively recommend policies to mitigate and avoid crises <ref type="bibr" target="#b24">(Hillbruner and Moloney, 2012)</ref>. IPC scores are given per AU, of which the boundaries are set by the UN Food and Agriculture Organization.</p><p>Figure <ref type="figure" target="#fig_3">3</ref> shows the IPC score distribution per country and for Somalia per year. The classes are heavily imbalanced: the relative frequencies of IPC scores 1, 2, 3, and 4 are 14%, 71%, 13%, and 1.6%, respectively.</p><p>To limit resource usage, we chose to focus on IPC score prediction for Somalia, since four out of five possible IPC scores occur in Somalia between 2013 and 2020, and because food insecurity in Somalia is mainly caused by agricultural and rainfall factors <ref type="bibr" target="#b0">(Andree et al., 2020)</ref>. We also experimented with predicting IPC scores for South Sudan, but results were far worse. This can be explained by the fact that food insecurity in South Sudan in 2013-20 was caused by non-environmental factors such as markets and conflicts <ref type="bibr" target="#b0">(Andree et al., 2020)</ref>. The timeframe of the IPC score extends from August 2009 to February 2020. The score is reported at quarterly frequency from 2009 to 2016, and three times per year from 2016 to 2020. We limit our timeframe to August 2013 until March 2020 as for the pre-training images. The 3-or 4-month satellite image composite start and end dates (e.g., May 2013 to August 2013) are chosen so that  </p></div>
<div><head n="5.1.3.">Data splits</head><p>We take 4 of the 74 AUs as out of domain: all 44K tiles belonging to 1.9K locations within these AUs, together with these regions' 92 IPC scores (one per region per date), form the out-of-domain test set D ipc ood . These tiles are not included in pre-training data.</p><p>For pre-training, we divide all locations in the 70 remaining AUs over two data splits: the training set D pre train (31K locations, 712K tiles) and the validation set D pre val (1.9K locations, 43K tiles, or little more than 5%). All tiles (timestamps) belonging to one location are always in the same split, but the train-val split does not necessarily respect AU boundaries. The validation split consists of a number of contiguous square areas randomly spread over Somalia in order to make sure that every location has a sufficiently large spatial neighborhood to sample positives from (for contrastive learning). Figure <ref type="figure" target="#fig_4">4a</ref> shows the spatial division of the pre-training data.</p><p>For the downstream task (IPC score prediction) of training and evaluation, we take 7 out of the 70 AUs (74 minus 4 for the out-of-domain split) for the validation set D ipc val (3.1K locations, 72K tiles, 161 IPC scores) and another 7 for the in-domain test set D pre test (4.6K locations, 105K paths, 161 IPC scores). The remaining 56 AUs make up the training set D ipc train (25K locations, 578K tiles, 1.3K IPC scores). Figure <ref type="figure" target="#fig_4">4b</ref> shows the geography of the downstream task splits. To test performance when the amount of available labeled data for supervised downstream task training decreases, we also construct training sets with a decreasing number of AUs: 70%, 50%, 20%, 5%, and 1% of the full training set D ipc train . Table <ref type="table" target="#tab_0">1</ref> compares the total number of pixels of our data splits with those used by other self-supervised pre-training for (satellite) image studies and shows that we match the order of magnitude of the most largescale study of <ref type="bibr">Ayush et al. (2021a)</ref>.</p></div>
<div><head n="5.2.">Experimental setup and methodology</head></div>
<div><head n="5.2.1.">SSSL pre-training</head><p>To define positive and negative pairs of patches for SSSL pre-training, we explore spatial resolutions D g of 0.15°, 0.4°, and entire AUs, and temporal resolutions D t of 1 (meaning only tiles from the same date are considered similar), 4, 12, 36, or 84 months (the length of our entire timeframe, which means spatially nearby tiles are considered similar regardless of their date). When D t equals 84 months and D g is small enough, our positive and negative pairs are similar to those used by <ref type="bibr">Ayush et al. (2021a)</ref> (their spatial threshold is actually so small that only the exact same location is considered similar, while our smallest spatial threshold of 0.15°still considers for nearby but not identical locations to be similar). <ref type="bibr" target="#b28">Jean et al. (2019)</ref> used positive pairs determined by spatial locality (with a small spatial threshold), which resemble our pairs when D t equals 1 month and D g is small but large enough to include more than a single location.</p><p>Baselines. We compare SSSL with the following pre-training baselines. The best pre-training checkpoints are chosen based on IPC score prediction performance from the frozen checkpoint weights, but we perform further evaluations involving both frozen and finetuned pre-trained weights.</p><p>1. The relational reasoning method of <ref type="bibr" target="#b46">Patacchiola and Storkey (2020)</ref>, which uses image augmentations of the anchor images like random flips, random crops, etc., to define positive instead of spatial and temporal thresholds. 2. The Tile2Vec contrastive pre-training method for satellite imagery, which uses a triplet loss to pull an anchor tile's representation closer to a nearby positive tile's representation in feature space while pushing it away from a far-away negative tile <ref type="bibr" target="#b28">(Jean et al., 2019)</ref>. We adjust the algorithm to work with a configurable spatial threshold instead of a fixed one. We add a configurable temporal threshold, so we can directly compare this baseline to our SSSL pre-training for different spatial and temporal thresholds D g and D t .</p><p>We chose Tile2Vec as baseline that uses contrastive pre-training specifically designed for satellite imagery since it is more easily extendable to configurable spatial and temporal thresholds and to limit the resources required for our study. rely on fixed thresholds that only consider tiles of the exact same location across time to be similar to arrive at temporally invariant and spatially variant representations, so they cannot be as naturally extended to our flexible threshold setting. Hyperparameters and settings. We use a total number of positive (and negative) pairs K ¼ 8 for SSSL, and batch size N ¼ 50 for both SSSL and Tile2Vec pre-training. Minibatches are constructed by sampling N anchor samples from the dataset, and adding K À 1 ¼ 7 for SSSL and 1 for Tile2Vec positives per anchor to the batch. For each anchor, random other anchors or other anchors' positives from the same minibatch are used as negatives.</p><p>We pre-train all CNN backbones on D pre train for a fixed number of epochs and save all intermediate checkpoints for later evaluation (one per epoch). We stopped SSSL pre-training after 10 epochs, and Tile2Vec pre-training after 20 (40 would have been in some sense more fair since Tile2Vec only sees one positive and one negative per sampled anchor tile, while SSSL sees K À 1 ¼ 7 positives and negatives per anchor tile, so SSSL batches are 4 × larger than Tile2Vec batches, but we noticed that downstream task performance steadily decreased after 10 epochs, while the needed training time for 20 epochs of Tile2Vec training was already significantly more than 10 epochs of SSSL pre-training, due to increased overhead).</p><p>We use the ResNet-18 architecture as CNN backbone for all experiments to balance performance with resource usage <ref type="bibr" target="#b23">(He et al., 2016)</ref>. We also tested the Conv4 network that Patacchiola and Storkey (2020) used, but results were much worse. We use the Adam optimizer <ref type="bibr" target="#b30">(Kingma and Ba, 2015)</ref> with learning rate 1e À 4 and β 1 , β 2 ,ε ð Þ¼ 0:9,0:999,1e À 6 ð Þ . We set γ ¼ 2:0 in the focal factor (Eq. ( <ref type="formula" target="#formula_3">3</ref>)) and use a weight decay factor of 1e À 4 for SSSL. For Tile2Vec, we set the margin for the triplet loss to 1:0 and the L2 regularization weight to 0:01.</p><p>We used a 16 GB NVIDIA Tesla P100 GPU for all pre-training and downstream task runs. SSSL pretraining on D pre train took on average 36 h. Tile2Vec pre-training took on average 51 h. Neural network training and evaluation was implemented with <software ContextAttributes="used">PyTorch</software> <ref type="bibr" target="#b45">(Paszke et al., 2019</ref>).<ref type="foot" target="#foot_6">6</ref> </p></div>
<div><head n="5.2.2.">Downstream task: IPC score prediction</head><p>After pre-training, we train a single-layer MLP on D ipc train to predict IPC scores from the frozen image features of the CNN backbone of each pre-training checkpoint until macro F1 on the validation set D ipc val converges. We use these validation scores to choose the best pre-training checkpoint for every pre-training run and to choose the best performing spatial and temporal thresholds and the best performing score aggregation method. We then use the best checkpoints (best validation performance) of the pre-training runs with the best spatial and temporal thresholds for further evaluations.</p><p>We report macro F1 scores since higher IPC scores (indicating a higher degree of food insecurity) occur much less frequently than lower scores, but are at least as important (if not more important) to detect. The macro F1 weighs all IPC scores equally, as opposed to micro averaged metrics that would give more weight to more frequent IPC scores.</p><p>Baselines. In addition to the pre-training baselines, for which we used the checkpoints to initialize an IPC score predictor as described in the beginning of this section, we consider the following IPC prediction baselines that don't need manual pre-training.</p><p>1. A randomly initialized CNN backbone, without any pre-training. 2. A CNN pre-trained on ImageNet classification <ref type="bibr" target="#b13">(Deng et al., 2009;</ref><ref type="bibr" target="#b23">He et al., 2016)</ref>. Since ImageNet images consist of three RGB channels instead of seven like our LANDSAT-8 images, we copy the convolution weights of the RGB channels from the pre-trained checkpoint but add randomly initialized weights for four additional channels. 3. A random forest, like the one proposed by <ref type="bibr" target="#b0">Andree et al. (2020)</ref>.</p><p>Random forest. We compare our neural network's food crisis predictions to those of a random forest classifier, as used by <ref type="bibr" target="#b0">Andree et al. (2020)</ref>. <ref type="bibr" target="#b0">Andree et al. (2020)</ref> merged the five IPC score categories into two-food crisis or not-and trained a binary classifier. They used the following input variables for 20 developing countries from 2009 until 2020:<ref type="foot" target="#foot_7">7</ref> </p><p>• the coordinates of the central points;</p><p>• the district size; • the population; • the terrain ruggedness;</p><p>• the cropland and pastures area shares;</p><p>• the NDVI-a measure of the "greenness," relative density, and health of vegetation of the earth's surface; • the rainfall;</p><p>• the evapo-transpiration (ET); • conflict events; • food prices.</p><p>For a fair comparison, we only use the data for Somalia and the 2013-20 timeframe. We perform food insecurity prediction under two setups: binary classification, following <ref type="bibr" target="#b0">Andree et al. (2020)</ref>, and multiclass classification with the five possible IPC scores, as described thus far. Our random forests consist of 50 decision trees. A leaf node needs to contain at least three samples to be considered during training, and it needs to contain at least 10 samples to be split into new leaf nodes. Out of the 11 features a sample has, three are considered per split point. Trees have a maximum depth of six nodes. The class weights for random forest training are inversely proportional to their frequency.</p><p>After comparing the learned image encoder features with handcrafted features, we also combine both by adding the neural network predictions as additional input features to assess their complementarity.</p><p>Hyperparameters and settings. Again, we use the Adam optimizer, now with the weight decay factor set to 0:01. If the pre-trained CNN backbone is not frozen during downstream task training, but finetuned, its weights are updated with a lower learning rate of 1e À 5 than the classification MLP (which is updated with learning rate 1e À 4). We use early stopping on the validation macro F1 score of predictions that use majority voting as aggregation method and reduce the learning rate when the validation macro F1 reaches a plateau. To counteract class imbalance, we weigh the IPC classes in the cross-entropy loss inversely proportional to their frequency in the training data.</p><p>Training for the downstream task D ipc train took approximately 4 h when freezing the pre-trained CNN backbone, and 8 h when finetuning it. We used <software ContextAttributes="used">Scikit-learn</software> to implement the random forest <ref type="bibr" target="#b47">(Pedregosa et al., 2011)</ref>.</p></div>
<div><head n="6.">Results</head></div>
<div><head n="6.1.">Spatial and temporal thresholds</head><p>Figure <ref type="figure">5</ref> shows the validation macro F1 on the downstream task for different combinations of spatial and temporal threshold values for positive pair selection, as well as for different aggregation methods, after pre-training with SSSL on images in D pre train . It is clear that the best performing configurations use a small temporal threshold, with by far the best performance when using D t ¼ 1 month (so only spatially nearby tiles of the same 3-or 4-month composite are considered similar). This makes the representations time-variant by minimizing mutual information between image representations of the same location at different times. Since our downstream task is time-dependent as well (regions might be food-insecure during certain time periods but not during others), this is not surprising.</p><p>Using a fixed spatial threshold D g of either 0:15 ∘ or 0:4 ∘ usually gave better results than defining spatial positive pairs based on AUs. This means it is desirable to maximize mutual information between image representations of locations that share a medium-sized vicinity, but not when this vicinity's size increases or decreases too much. This is somewhat surprising because the granularity of one AU corresponds exactly to the granularity of the IPC scores, and one might thus expect maximizing information between image representations of locations that share an IPC score to work best. But while patches in one AU share one IPC score, AUs can be quite large (&gt;10K km 2 ), and patches might thus be quite different. If the patches are too different, or if they do not share the properties informative to IPC score prediction, the network might thus be forced to ignore important properties.</p></div>
<div><head n="6.2.">Score aggregation</head><p>"Individual tiles" in Figure <ref type="figure">5</ref> means predicting an entire AU's IPC score from a single patch, which is inherently difficult since the IPC score might be determined by much more information than a single patch contains.</p><p>Majority voting almost always performed best. Maximum voting performed much worse, which could be caused by <software>FEWS NET</software> not giving an AU the worst IPC score of its subregions, and by the fact that a single incorrect patch prediction is more likely to change the entire AU prediction with maximum voting than with majority voting.</p><p>The rest of the experiments use SSSL with one configuration of positive pairs: a temporal threshold D t of 1 month and a spatial threshold D g of 0.4°, with majority voting as aggregation method. Conclusions for different spatial and temporal thresholds for Tile2Vec pre-training are largely similar, with the best performing setting D t ¼ 1 and D g ¼ 0:15 ∘ (see Figure <ref type="figure" target="#fig_1">A1</ref> in Appendix A).</p></div>
<div><head n="6.3.">SSSL vs. baselines</head><p>Table <ref type="table" target="#tab_1">2</ref> compares the macro F1 on the in-domain and out-of-domain test sets of the best pre-training settings for SSSL and Tile2Vec to the original pre-training proposed by <ref type="bibr" target="#b46">Patacchiola and Storkey (2020)</ref> 5. Macro F1 on validation set D ipc val using different configurations of positive and negative pairs (determined by temporal threshold D t and spatial threshold D g ) for SSSL pre-training, with D t and D g denoted on the x-axis. The baseline in this plot always predicts the majority class. "admin" means using administrative units instead of longitude/latitude to define spatial positive pairs. that uses data augmentations (color jitter, resized crop, etc.) instead of our spatiotemporal model. It also shows the results of a model that does not integrate pre-training (i.e., starting from randomly initialized weights and training these only during the supervised training of food insecurity prediction), of pretraining on ImageNet <ref type="bibr" target="#b13">(Deng et al., 2009;</ref><ref type="bibr" target="#b23">He et al., 2016)</ref> (i.e., starting the supervised training with the convolutional weights initialized to publicly available weights that were trained on the ImageNet classification dataset), and of a random forest that uses handcrafted features <ref type="bibr" target="#b0">(Andree et al., 2020)</ref>. We consider both freezing and not freezing the CNN backbone's weights in the supervised stage.</p><p>SSSL significantly outperforms all baselines in all settings, with 21-39% relative improvement over the second best model. All neural network-based models (bottom five rows) scored better than the randomly initialized neural network baseline across all settings, although in some cases only marginally, especially on the out-of-domain test set. Tile2Vec and data augmentations showed comparable performance, and only outperformed the random forest baseline when their CNN weights were finetuned. Surprisingly, ImageNet outperformed Tile2Vec and data augmentations with frozen backbone weights on the in-domain test set D ipc test , even though the CNN backbone had only seen images of daily scenes like cats and dogs during ImageNet pre-training, while the latter two baselines were pre-trained on satellite images in the D pre train dataset. Finetuning the CNN weights improved performance compared to freezing them, often significantly.</p></div>
<div><head n="6.4.">Transferability</head><p>Performance generally drops on the out-of-domain test set, but stays well above the random and majority baselines. This shows that it is harder but still feasible to make good IPC score predictions for locations for which no imagery was included in the pre-training data. Note that none of the images and IPC scores in D ipc test or D ipc ood were included in the downstream task training set D ipc train , but images in D ipc test might have been included in the pre-training data D pre train , while images in D ipc ood were definitely not. Also note that this only makes a difference for SSSL, Tile2Vec, and data augmentations, since the other models were not pretrained on D pre train anyway. Therefore, our model could be used not only to predict food insecurity for locations for which no labeled data are available, but also for locations on which it has not been pre-trained (although it is preferable to pre-train on all locations for which IPC predictions need to be made). The out-of-domain locations in D ipc ood are in separate AUs, but of course still in the same country as and adjacent to AUs in pretraining and downstream training data. Some degree of similarity can thus still be expected. It would be interesting to test how performance degrades when distance or dissimilarity between out-of-domain test data and training data increases, for example, on locations in different countries or climates.</p></div>
<div><head n="6.5.">Decreasing labeled dataset size</head><p>Figure <ref type="figure" target="#fig_5">6</ref> shows the macro F1 on the in-domain test set D ipc test for models with different weight initializations for different amounts of labeled data used to train for the downstream task. As expected, macro F1 decreases with decreasing training set sizes, but it does so gradually, not disproportionately. This is the case both for SSSL and most baselines, except for Tile2Vec when freezing its CNN's weights, for which performance drops rapidly to the majority baseline. Performance starts falling sharply when decreasing the training set size further than 20% of its original size, but up until a decrease to 5% of available data, all models perform better than the majority baseline. SSSL pre-training outperforms the baselines for training set sizes above 5%, both with finetuned and frozen weights.</p><p>The random forest performed better than neural baselines (but not SSSL) with frozen weights and performed equally well or better than neural baselines (but not SSSL) with unfrozen weights, for 50-70% of training data, meaning that it is more robust to slight decreases in training set size. Surprisingly, its performance when trained on all data drops compared to when trained on 50-70%, which might be explained by some samples being excluded from training data that are "harder" or more dissimilar to test data.</p><p>Although overall performance with unfrozen CNN weights is better than with frozen weights, the latter is more robust to decreasing training set size. This could be explained by the fact that not updating the representations reduces the number of trainable parameters vastly, and hence the risk to overfit a small labeled training set. We noticed some training instability on the smaller training sets when freezing the CNN weights (shown, e.g., by the unexpected bump in Random init. performance for 20% of the training data).</p><p>Figure <ref type="figure" target="#fig_1">B1</ref> in Appendix B shows the same plots but for the out-of-domain test set D ipc ood instead of the in-domain test set (of which the satellite image tiles were not included in pre-training data). The gap between SSSL and baseline is smaller with frozen weights. For unfrozen weights, SSSL performance drops below baselines when trained on 50% or less of labeled data.</p><p>We can conclude from these experiments that (1) contrastive SSSL pre-training and (2) defining rules for positive/negative pair selection that are tailored to satellite images, by making use of their spatial and temporal dimensions instead of data augmentations, will improve results for varying amounts of labeled training data. Little labeled data are needed for finetuning to a downstream task. Performance with decreasing training set size is better retained when the model has seen the locations during its pre-training stage.</p><p>6.6. Forecasting food insecurity in the future Figure <ref type="figure" target="#fig_6">7</ref> shows the macro F1 on a different, temporally separated test set D ipcÀtemp test for different models (with frozen (7a) and unfrozen (7b) CNN weights), when forecasting food insecurity in the future. Here the future means a later relative point in time than the date the input satellite image was acquired. To allow time for preventive political measures or timely humanitarian action, a system that warns about food insecurity more than 3-4 months before it actually occurs would be useful. Hence we train and evaluate models for predicting the next gathered IPC score for every location (N ¼ 1, which corresponds to 3-4 months later), the one after that (N ¼ 2, 6-8 months later), and the one after that (N ¼ 3, 9-12 months later). While pre-training remains the same, we no longer use the geographically separated D ipc train ,D ipc val , D ipc test for finetuning. Instead we separate all the in-domain data temporally (exclusively for the experiments in this section): we use the IPC scores from March 2020 for validation (i.e., the last available IPC scores at the time of the start of this study, corresponding to the date of the last LANDSAT-8 tiles that were included in pre-training). We train on the IPC scores up to November 2019 (up until one step before the validation scores). The first IPC scores used for training are chosen so that all of the training sets for this experiment (i.e., corresponding to a different number of steps into the future) are of equal size. The test IPC scores are from June 2020 and have been published by <software ContextAttributes="used">FEWS NET</software> since the start of this study. This means that no satellite imagery corresponding to the time of the test IPC scores has been used for pretraining. Note that for the experiments in this section, predicting 0 steps into the future (N ¼ 0) still uses the temporally instead of geographically separated splits, and is therefore not identical to previously discussed runs. The same temporal splits are used to obtain the train, validation, and test set for the random forest.</p><p>Figure <ref type="figure" target="#fig_6">7</ref> shows that forecasting into the future is difficult for any of the considered methods, and that generally performance decreases when forecasting further into the future. We consider the sudden increase in macro F1 when forecasting three steps into the future with the random forest an anomaly: since only one IPC score and corresponding covariates have been collected per AU per timestep, the data sets to train and evaluate the random forest are relatively small. With frozen weights, SSSL pre-training performs comparable to the random forest as used by <ref type="bibr" target="#b0">Andree et al. (2020)</ref>, better than the majority baseline (although barely for N ≥ 2) and better than the other methods (which drop below the majority baseline for more than one or two timesteps into the future). With unfrozen weights, SSSL performs best when forecasting N ¼ 1 steps into the future, and slightly worse than ImageNet for N ¼ 2.</p><p>To verify to what extent models are able to predict future IPC scores that do not change over time, we compute the macro F1 on the subset of AUs whose IPC scores actually changed since the acquisition of the image. Performance dropped significantly: for N ¼ 1, SSSL performance dropped from 0.351/0.361 to 0.217/0.262 with frozen/unfrozen weights, but stayed above the baselines' performance (e.g., the random forest scored 0.075 on this subset).</p></div>
<div><head n="6.7.">Seasons</head><p>To rule out the possibility that IPC scores correlate heavily with seasons, and that the model relies on this correlation to predict IPC scores by predicting which season an image was taken. Figure <ref type="figure" target="#fig_7">8</ref> shows the macro F1 of the best SSSL model per season, as well as the distribution of both ground truth and predicted IPC scores in the geographically separated test set D ipc test during that season. Note that there are far fewer available IPC labels during spring, since these were only collected every 3 months between 2013 and 2015, and after that every 4 months, hence skipping spring. The figure shows that different IPC labels occur in different seasons, so that making accurate IPC predictions cannot be reduced to predicting a satellite tile's season. It also shows that the model does predict different IPC scores during different seasons, hence the model does not attempt to shortcut IPC score prediction by predicting a tile's season.</p></div>
<div><head n="6.8.">Feature importance</head><p>We compute the importance of input features with the SHAP framework's version of <software ContextAttributes="used">DeepLIFT</software> <ref type="bibr" target="#b39">(Lundberg and Lee, 2017;</ref><ref type="bibr" target="#b56">Shrikumar et al., 2017)</ref>, a method that attributes the output of a neural network to its individual input features by backpropagating the activations of neurons to the input, and comparing each neuron's activation to a reference activation for that neuron. The reference activations are computed Figure <ref type="figure" target="#fig_8">9</ref> shows the importance values per LANDSAT-8 band, where the SHAP values are averaged across the pixels and across all 400 (100 × 4) tiles. It shows that the neural network learns physically sensible patterns: activated infrared bands (NIR and SW-IR wavelengths are reflected by healthy vegetation) contribute positively to lower predicted IPC scores and negatively to higher predicted IPC scores. It further shows that the network does not only look at vegetation greenness: for example, the blue and ultra-blue bands have high SHAP values. Figure <ref type="figure" target="#fig_1">C1</ref> in Appendix C shows examples of image tiles and the magnitude and direction of each pixel's contribution toward an IPC score prediction. It shows that pixels portraying vegetation or a river contribute positively toward lower IPC scores. 6.9. SSSL vs. random forest food insecurity predictor Table <ref type="table" target="#tab_3">3</ref> reports the performance of SSSL and ImageNet pre-training versus the random forest models based on <ref type="bibr" target="#b0">Andree et al. (2020)</ref>, both on multiclass IPC prediction (with five IPC scores) and on binary IPC prediction (where five IPC scores are mapped into two classes: risk or no risk). The first row represents the random forest using only the handcrafted input features as input. As shown already, the neural networks outperformed the random forest significantly, the unfrozen SSSL model giving relative improvements of 64% (multiclass) and 46% (binary) in macro F1. This is a striking result: in absolute percentage points 25% more accurate results can be obtained by only analyzing widely available raw satellite images, instead of a set of handcrafted features from different sources.</p><p>The last two rows represent the same random forest, now using the majority voted IPC prediction per AU per date by the frozen or unfrozen SSSL model as extra input feature. This combination improves the random forest's performance up to more or less the level of the neural network, but not more. The handcrafted features from <ref type="bibr" target="#b0">Andree et al. (2020)</ref> and the LANDSAT-8 tiles do not appear to be complementary in this setting. We noticed that when using IPC prediction by the unfrozen SSSL pre-trained neural network as a feature for the random forest, the random forest often copies the neural network prediction, resulting in very similar scores.</p><p>Note that the random forests get the pre-computed NDVI feature as input, and yet are significantly outperformed by the neural networks, which means the neural networks manage to extract more useful information than simply an NDVI proxy from the seven satellite image bands. </p></div>
<div><head>e48-18</head><p>Ruben Cartuyvels et al. </p></div>
<div><head n="7.">Conclusion</head><p>Several conclusions can be drawn from this study. We showed that the remote-sensing data and neural networks improve predictions vastly compared to using handcrafted input variables. One important remark is that this conclusion is only valid for regions in which food insecurity is actually linked to phenomena that are observable from satellite images. Next, we showed that compared to not pre-training or using different weight initialization or pretraining paradigms, the relational reasoning framework of <ref type="bibr" target="#b46">Patacchiola and Storkey (2020)</ref> for contrastive pre-training improves predictions significantly, especially when using the spatial and temporal dimensions that are inherent parts of satellite imagery. Self-supervised pre-training fits the domain of satellite imagery especially well, as vast amounts of unlabeled data are (publicly) available. We showed that using spatial and temporal thresholds is preferred over using data augmentations as in <ref type="bibr" target="#b46">Patacchiola and Storkey (2020)</ref>. The study also found that, unlike <ref type="bibr">Ayush et al. (2021a)</ref>, using a non-zero spatial threshold and a small temporal threshold would work best on food insecurity prediction. These conclusions remain valid for varying amounts of available labeled data, and in fact, we found the required amount of labels to be low. Our model generalizes to locations that it has not seen during pre-training and/or finetuning, but performance was better and fewer labeled data were needed for locations the model was pre-trained on.</p><p>We found that forecasting future food insecurity is difficult, but our proposed model is competitive with baselines. We analyzed whether ground truth and predicted IPC score distributions follow seasons and conclude that food insecurity prediction cannot be reduced to detecting the season. We also analyzed the importance of the satellite's bands and found that the model does not only look at vegetation greenness. We hope this work paves the way for further research into using satellite images to predict food insecurity (and potentially other socioeconomic indicators).</p></div>
<div><head n="7.1.">Future work</head><p>The first way in which future work could built upon our work is by using more data. Because of computational resource limitations, we focused our study on satellite images and IPC scores only of Somalia during 7 years. However, LANDSAT-8 images are available for the entire world and continuously since 2013, meaning pre-training data comparable to ours is available in greater quantities by several orders of magnitude. Besides, satellites other than LANDSAT-8 also provide publicly available images. The World Bank also made available much more IPC score data: for 21 developing countries since 2009, meaning much more data are available to test finetuning strategies and food insecurity prediction. These data would be interesting not only to potentially improve the model's performance but also to pinpoint the countries in which satellite images help food insecurity prediction. Although the number of pixels we use exceeds that of <ref type="bibr" target="#b46">Patacchiola and Storkey (2020)</ref>, as with all deep-learning applications, and even more so with self-supervised pre-training, it can be expected that the more data are used, the better the performance of the model.</p><p>A methodological limitation of this study is that we did not have the resources to do multiple runs for each configuration in each experiment, which would have canceled out some of the inherent stochasticity of training deep-learning models. It would be valuable to test other image encoders, like larger CNNs or different architectures, and more contrastive pre-training baselines, like the methods proposed by <ref type="bibr">Manas et al. (2021)</ref> and <ref type="bibr">Ayush et al. (2021a)</ref>.</p><p>The satellite images we used were derived from the LANDSAT-8 satellite and have a resolution of 30 m per pixel. More modern satellites provide images with much higher resolutions of &lt; 1 m per pixel, albeit often commercial and not producing publicly available data. 8 Whereas our experiments showed that it is possible to detect food insecurity from relatively low-resolution satellite images, if it is caused by agricultural factors (like in Somalia), presumably since these factors have a detectable effect on the images, experiments also showed that it was not possible to detect food insecurity in regions where it is caused by political or economical factors (like South Sudan), presumably because these factors do not have a detectable effect on the low-resolution images. However, it seems plausible that some effects of political or economic instability could be detectable from higher-resolution images, like the presence of military vehicles, large civil protests, abandoned factories, and so forth. Hence, future work could test whether food insecurity driven by other factors than agricultural or weather-related ones could be predicted from higher-resolution images.</p><p>It would also be interesting to further test the generalization capabilities of the method, like testing how different degrees of distance or dissimilarity to training regions impact performance. Future work could also apply SSSL to different downstream tasks. It would be particularly worthwhile to evaluate SSSL for tasks that require different degrees of temporal and spatial variance by matching the spatiotemporal thresholds accordingly.</p></div><figure xml:id="fig_0"><head /><label /><figDesc>), Wu et al. (2018), Chen et al. (2020), Grill et al. (2020), He et al. (2020), Misra and van der Maaten (2020), and Patacchiola and Storkey (2020), who used different notions of distance and different training objectives.</figDesc></figure>
<figure xml:id="fig_1"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. SSSL: for one sample, positive samples are those that are closer in time to the image than the temporal threshold, and are closer in space to the sample than the spatial threshold.</figDesc><graphic coords="7,126.65,61.40,239.92,201.43" type="bitmap" /></figure>
<figure xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Examples of 145 × 145 pixel tiles taken from composite LANDSAT-8 images of Somalia, exported from GEE (only RGB bands visualized), with corresponding IPC scores. Note that the difference between images with different IPC scores is not easily discernible.</figDesc><graphic coords="9,134.15,187.03,91.82,91.82" type="bitmap" /></figure>
<figure xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. IPC score distribution (a) for each country in the dataset from 2009 to 2020 and (b) for Somalia per year from 2013 until 2020. Note that IPC score 5 does only occur in 2011 in Somalia.</figDesc></figure>
<figure xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. (a) Geography of pre-train data splits: train data are used for SSSL pre-training, validation data are used to select the best checkpoint after pre-training, and out-of-domain data are set aside. (b) Geography of downstream IPC score prediction data splits: train data are used for IPC score classification, validation data are used for early stopping and selecting the best checkpoint, out-ofdomain and in-domain test data are used for evaluation.</figDesc></figure>
<figure xml:id="fig_5"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Test macro F1 on D ipc test with frozen (a) and unfrozen (b) CNN backbone weights for models with different weight initializations using increasing amounts of labeled training data.</figDesc></figure>
<figure xml:id="fig_6"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. Test macro F1 on D ipcÀtemp test with frozen (a) and unfrozen (b) CNN backbone weights for neural networks with different weight initializations and a random forest when predicting an increasing number of time steps into the future (one step corresponds to 3-4 months, two to 6-8 months, and three to 9-12 months).</figDesc></figure>
<figure xml:id="fig_7"><head>Figure 8 .</head><label>8</label><figDesc>Figure 8. Test macro F1 on D ipc test of the SSSL model with unfrozen CNN weights (magenta line, right vertical axis), and ground truth (red) and predicted (blue) IPC score distributions (violin plots, left vertical axis), both versus the season of the IPC score measurement (x-axis). Note that only four IPC scores are depicted, since only four out of five possible IPC scores occur in Somalia between 2013 and 2020.</figDesc></figure>
<figure xml:id="fig_8"><head>Figure 9 .</head><label>9</label><figDesc>Figure 9. Mean SHAP values per LANDSAT-8 band for 100 tiles per IPC score. A positive mean SHAP value for one band and one predicted IPC score means that strong activations for features in this band make the prediction of this IPC score more likely.</figDesc></figure>
<figure type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Comparison of (pre-training) dataset sizes in related work</figDesc><table><row><cell>e48-10</cell><cell cols="2">Ruben Cartuyvels et al.</cell><cell /><cell /></row><row><cell /><cell /><cell /><cell>Number of</cell><cell>Pixels per</cell><cell>Total number</cell></row><row><cell>Study</cell><cell /><cell>Dataset name</cell><cell>images</cell><cell>image</cell><cell>of pixels</cell></row><row><cell>This study</cell><cell /><cell>Total</cell><cell>799K</cell><cell>145 × 145</cell><cell>17B</cell></row><row><cell /><cell /><cell>D pre train</cell><cell>712K</cell><cell>145 × 145</cell><cell>15B</cell></row><row><cell cols="2">Relational reasoning</cell><cell>CIFAR-10 and CIFAR</cell><cell>60K</cell><cell>32 × 32</cell><cell>61M</cell></row><row><cell cols="2">(Patacchiola and Storkey,</cell><cell>100</cell><cell /><cell /></row><row><cell>2020)</cell><cell /><cell>Tiny-ImageNet</cell><cell>100K</cell><cell>64 × 64</cell><cell>409M</cell></row><row><cell cols="2">Tile2Vec (Jean et al., 2019)</cell><cell>NAIP</cell><cell /><cell /><cell>12B</cell></row><row><cell /><cell /><cell>LANDSAT-8: American</cell><cell /><cell /><cell>60M</cell></row><row><cell /><cell /><cell>cities</cell><cell /><cell /></row><row><cell /><cell /><cell>LANDSAT-7: Uganda</cell><cell>16K</cell><cell>145 × 145</cell><cell>344M</cell></row><row><cell /><cell /><cell>DigitalGlobe</cell><cell /><cell /><cell>2.9B</cell></row><row><cell cols="2">Geography-aware SSL</cell><cell>Functional map of the</cell><cell>417K</cell><cell>224 × 224</cell><cell>21B</cell></row><row><cell cols="2">(Ayush et al., 2021a)</cell><cell>world</cell><cell /><cell /></row></table><note><p>The methods proposed by Ayush et al. (2021a) and Manas et al. (2021) explicitly https://doi.org/10.1017/eds.2023.42 Published online by Cambridge University Press</p></note></figure>
<figure type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Macro F1 on the in-domain and out-of-domain test set of the SSSL model with spatial and temporal positive pairs vs. baselines: Tile2Vec (also with spatial and temporal pairs), the data augmentation-based model of<ref type="bibr" target="#b46">Patacchiola and Storkey (2020)</ref>, ImageNet random initialization, and the random forest (RF) of Andree et al. (2020). The best result per column is marked in bold. Results of CNN backbones with both frozen backbone weights and unfrozen backbone weights during supervised training are reported. Maj. baseline corresponds to always predicting the majority class.</figDesc><table><row><cell>D ipc test</cell></row></table><note><p>Note:</p></note></figure>
<figure type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Random forest performance for binary and multiclass predictions compared to pre-trained neural networks. The best result per column is marked in bold.</figDesc><table><row><cell>Model</cell></row></table></figure>
			<note place="foot" xml:id="foot_0"><p>https://doi.org/10.1017/eds.2023.42 Published online by Cambridge University Press</p></note>
			<note place="foot" n="1" xml:id="foot_1"><p>Data augmentations are generations of new samples from a base sample, with a transformation, such as a random crop or color distortion. Environmental Data Science e48-3 https://doi.org/10.1017/eds.2023.42 Published online by Cambridge University Press</p></note>
			<note place="foot" n="2" xml:id="foot_2"><p><ref type="bibr" target="#b0">Andree et al. (2020)</ref> use the normalized difference vegetation index (NDVI) as an input feature, which is computed from satellite images as the normalized difference of images in two different spectral bands, but this simple scalar value cannot convey as much information as an entire image.e48-4Ruben Cartuyvels et al. https://doi.org/10.1017/eds.2023.42 Published online by Cambridge University Press</p></note>
			<note place="foot" n="3" xml:id="foot_3"><p>https://www.usgs.gov/core-science-systems/nli/landsat/Landsat-8.</p></note>
			<note place="foot" n="4" xml:id="foot_4"><p>https://earthengine.google.com/.</p></note>
			<note place="foot" n="5" xml:id="foot_5"><p>https://fews.net/IPC. Environmental Data Science e48-7 https://doi.org/10.1017/eds.2023.42 Published online by Cambridge University Press</p></note>
			<note place="foot" n="6" xml:id="foot_6"><p>Upon acceptance, we will publish all training, evaluation and data preprocessing code, as well as the scripts used to export satellite images from GEE, and trained checkpoints of our models. Environmental Data Science e48-11 https://doi.org/10.1017/eds.2023.42 Published online by Cambridge University Press</p></note>
			<note place="foot" n="7" xml:id="foot_7"><p>We do not use time data like the month and the year of an IPC measurement as input for the random forest, since the test and validation sets are spatially but not temporally separated, and since the neural networks also do not have access to this information.e48-12Ruben Cartuyvels et al. https://doi.org/10.1017/eds.2023.42 Published online by Cambridge University Press</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgments. The resources and services used in this work were provided by the <rs type="funder">VSC (Flemish Supercomputer Center)</rs>, funded by the <rs type="funder">Research Foundation-Flanders (FWO)</rs> and the <rs type="funder">Flemish government</rs>.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>

			<div type="availability">
<div><p>Data availability statement. All LANDSAT-8 images used in this study are publicly available via a number of platforms, for instance, through <software ContextAttributes="used">Google Earth Engine</software>. We make available <software ContextAttributes="used">scripts</software> to export the images used for this study at https://github.com/ rubencart/SSSL-food-security. <software ContextAttributes="used">FEWS NET</software> data are available from https://fews.net/data/ <ref type="bibr" target="#b31">(Korpi-Salmela et al., 2012)</ref>. The handcrafted input features are also publicly available. <ref type="bibr" target="#b0">Andree et al. (2020)</ref> state their sources in the appendix and make their preprocessed data available at https://microdata.worldbank.org/index.php/catalog/3811/.</p></div>
			</div>

			<div type="annex">
<div><p>Competing interest. The authors declare no competing interests exist.</p><p>Ethics statement. The research meets all ethical guidelines, including adherence to the legal requirements of the study country.</p><p>Funding statement. This work is part of the CALCULUS project, funded by the ERC Advanced Grant H2020-ERC-2017 ADG 788506. 9 It also received funding from the Research Foundation-Flanders (FWO) under Grant Agreement No. G078618N.  </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Predicting food crises</title>
		<author>
			<persName><forename type="first">B</forename><surname>Andree</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chamorro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kraay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Spencer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wang</forename><forename type="middle">D</forename></persName>
		</author>
		<idno type="DOI">10.1596/1813-9450-9412</idno>
		<ptr target="https://doi.org/10.1596/1813-9450-9412" />
	</analytic>
	<monogr>
		<title level="j">World Bank Working Paper</title>
		<imprint>
			<date type="published" when="2020">2020. 9412</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Geography-aware self-supervised learning</title>
		<author>
			<persName><forename type="first">K</forename><surname>Ayush</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Uzkent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Tanmay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Burke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lobell</forename><surname>Db</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)<address><addrLine>Montreal, QC</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021-10">2021. October 2021</date>
			<biblScope unit="page" from="10181" to="10190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Efficient poverty mapping from high resolution remote sensing images</title>
		<author>
			<persName><forename type="first">K</forename><surname>Ayush</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Uzkent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Tanmay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Burke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lobell</forename><surname>Db</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
		<idno type="DOI">10.1017/eds.2023.42</idno>
		<ptr target="https://doi.org/10.1017/eds.2023.42" />
	</analytic>
	<monogr>
		<title level="m">Cambridge University Press Innovative Applications of Artificial Intelligence, IAAI 2021, The Eleventh Symposium on Educational Advances in Artificial Intelligence, EAAI 2021, Virtual Event</title>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="12" to="20" />
		</imprint>
	</monogr>
	<note>Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Thirty-Third Conference on 8 For example, the SkySat constellation owned by Planet Labs</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Temporal prediction of socio-economic indicators using satellite imagery</title>
		<author>
			<persName><forename type="first">C</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Barwaria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Choudhary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seth</forename><forename type="middle">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CoDS-COMAD 2020: 7th ACM IKDD CoDS and 25th COMAD</title>
		<editor>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Roy</surname></persName>
		</editor>
		<meeting><address><addrLine>Hyderabad</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="73" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Representation learning: A review and new perspectives</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1798" to="1828" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Random forests</title>
		<author>
			<persName><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="5" to="32" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Using satellite imagery to understand and promote sustainable development</title>
		<author>
			<persName><forename type="first">M</forename><surname>Burke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Driscoll</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lobell</forename><surname>Db</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">371</biblScope>
			<biblScope unit="issue">6535</biblScope>
			<biblScope unit="page">8628</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning, ICML 2020, Virtual Event</title>
		<title level="s">Proceedings of Machine Learning Research</title>
		<meeting>the 37th International Conference on Machine Learning, ICML 2020, Virtual Event</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="page" from="1597" to="1607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title />
		<author>
			<persName><surname>Pmlr</surname></persName>
		</author>
		<imprint />
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Remote sensing image scene classification: Benchmark and state of the art</title>
		<author>
			<persName><forename type="first">G</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><forename type="middle">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">105</biblScope>
			<biblScope unit="page" from="1865" to="1883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning a similarity metric discriminatively, with application to face verification</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR 2005)</title>
		<meeting><address><addrLine>San Diego, CA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="539" to="546" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Change detection of remote sensing image based on deep neural networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hayat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 2nd International Conference on Artificial Intelligence and Industrial Engineering (AIIE 2016)</title>
		<meeting>the 2016 2nd International Conference on Artificial Intelligence and Industrial Engineering (AIIE 2016)<address><addrLine>Beijing</addrLine></address></meeting>
		<imprint>
			<publisher>Atlantis Press</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="262" to="267" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Unsupervised change detection in satellite images using convolutional neural networks</title>
		<author>
			<persName><forename type="first">Jong</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">L</forename><surname>Bosman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Neural Networks, IJCNN 2019</title>
		<meeting><address><addrLine>Budapest</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei-Fei L</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2009">2009. 2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lee</forename><forename type="middle">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019<address><addrLine>Minneapolis, MN</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
	<note>Long and Short Papers</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Discriminative unsupervised feature learning with exemplar convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1734" to="1747" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Self-supervised learning of image embedding for continuous control</title>
		<author>
			<persName><forename type="first">C</forename><surname>Florensa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Degrave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
		<idno>CoRR, abs/1901.00943</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Can medium-resolution satellite imagery measure economic activity at small geographies? Evidence from landsat in Vietnam</title>
		<author>
			<persName><forename type="first">R</forename><surname>Goldblatt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Heilmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Vaizman</surname></persName>
		</author>
		<idno type="DOI">10.1093/wber/lhz001</idno>
		<ptr target="https://doi.org/10.1093/wber/lhz001" />
	</analytic>
	<monogr>
		<title level="j">The World Bank Economic Review</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="635" to="653" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Change detection in synthetic aperture radar images based on deep neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="125" to="138" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Google earth engine: Planetary-scale geospatial analysis for everyone</title>
		<author>
			<persName><forename type="first">N</forename><surname>Gorelick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hancher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dixon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ilyushchenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Thau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moore</forename><forename type="middle">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sensing of Environment</title>
		<imprint>
			<biblScope unit="volume">202</biblScope>
			<biblScope unit="page" from="18" to="27" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Bootstrap your own latent -A new approach to self-supervised learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Grill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Altché</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H</forename><surname>Richemond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">Á</forename><surname>Pires</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Azar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Piot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Valko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020</title>
		<meeting><address><addrLine>NeurIPS; Virtual</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Self-supervised co-training for video representation learning</title>
		<author>
			<persName><forename type="first">T</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2020</title>
		<meeting><address><addrLine>Seattle, WA</addrLine></address></meeting>
		<imprint>
			<publisher>Computer Vision Foundation/IEEE</publisher>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="page" from="9726" to="9735" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016</title>
		<meeting><address><addrLine>Las Vegas, NV</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">When early warning is not enough-Lessons learned from the 2011 Somalia famine</title>
		<author>
			<persName><forename type="first">C</forename><surname>Hillbruner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Moloney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Global Food Security</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="20" to="28" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Mapping missing population in rural India: A deep learning approach with satellite imagery</title>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Novosad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Asher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Burke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lobell</forename><surname>Db</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society, AIES 2019</title>
		<meeting>the 2019 AAAI/ACM Conference on AI, Ethics, and Society, AIES 2019<address><addrLine>Honolulu, HI</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="353" to="359" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A survey on contrastive self-supervised learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Jaiswal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Babu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Z</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Makedon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Technologies</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">2</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Combining satellite imagery and machine learning to predict poverty</title>
		<author>
			<persName><forename type="first">N</forename><surname>Jean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Burke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Davis</forename><forename type="middle">M</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">353</biblScope>
			<biblScope unit="issue">6301</biblScope>
			<biblScope unit="page" from="790" to="794" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Tile2vec: Unsupervised representation learning for spatially distributed data</title>
		<author>
			<persName><forename type="first">N</forename><surname>Jean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Samar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Azzari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lobell</forename><surname>Db</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Thirty-Third AAAI Conference on Artificial Intelligence, AAAI 2019, The Thirty-First Innovative Applications of Artificial Intelligence Conference, IAAI 2019, The Ninth AAAI Symposium on Educational Advances in Artificial Intelligence</title>
		<meeting><address><addrLine>EAAI; Honolulu, HI</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page" from="3967" to="3974" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deep unsupervised embedding for remotely sensed images based on spatially augmented momentum contrast</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fernández-Beltran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Plaza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="2598" to="2610" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations, ICLR 2015</title>
		<editor>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</editor>
		<meeting><address><addrLine>San Diego, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>Korpi-Salmela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Negre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Nkunzimana</surname></persName>
		</author>
		<title level="m">Integrated Food Security Phase Classification (IPC) Technical Manual Version 2.0</title>
		<meeting><address><addrLine>Rome</addrLine></address></meeting>
		<imprint>
			<publisher>Food and Agriculture Organization of the United Nations</publisher>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Analysis of various change detection techniques using satellite images</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Kotkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Jadhav</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 International Conference on Information Processing (ICIP)</title>
		<meeting><address><addrLine>Québec City, QC</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="664" to="668" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Deep learning classification of land cover and crop types using remote sensing data</title>
		<author>
			<persName><forename type="first">N</forename><surname>Kussul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lavreniuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Skakun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shelestov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geoscience and Remote Sensing Letters</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="778" to="782" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Contrastive representation learning: A framework and review</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H</forename><surname>Le-Khac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Healy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Smeaton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="193907" to="193934" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A data-driven approach improves food insecurity crisis prediction</title>
		<author>
			<persName><forename type="first">E</forename><surname>Lentz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Michelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Baylis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">World Development</title>
		<imprint>
			<biblScope unit="volume">122</biblScope>
			<biblScope unit="page" from="399" to="409" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Object detection in optical remote sensing images: A survey and a new benchmark</title>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS Journal of Photogrammetry and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">159</biblScope>
			<biblScope unit="page" from="296" to="307" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Deep learning based oil palm tree detection and counting for high-resolution remote sensing images</title>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cracknell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">22</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Self-supervised learning: Generative or contrastive</title>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Mian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge &amp; Data Engineering</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="857" to="876" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A unified approach to interpreting model predictions</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Lundberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2017/hash/8a20a8621978632d76c43dfd28b67767-Abstract.html" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017</title>
		<editor>
			<persName><forename type="first">I</forename><surname>Guyon</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Von</forename><surname>Luxburg</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">U</forename><surname>Bengio</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>Fergus</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Vishwanathan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Svn</forename><surname>Garnett</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename></persName>
		</editor>
		<meeting><address><addrLine>Long Beach, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-09">2017. December 4-9, 2017</date>
			<biblScope unit="page" from="4765" to="4774" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Seasonal contrast: Unsupervised pre-training from uncurated remote sensing data</title>
		<author>
			<persName><forename type="first">O</forename><surname>Mañas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lacoste</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Giró-I Nieto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Vázquez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Rodríguez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision, ICCV 2021</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision, ICCV 2021<address><addrLine>Montreal, QC</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="9414" to="9423" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">1st International Conference on Learning Representations, ICLR, ICLR 2013</title>
		<title level="s">Workshop Track Proceedings</title>
		<meeting><address><addrLine>Scottsdale, AZ</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Self-supervised learning of pretext-invariant representations</title>
		<author>
			<persName><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2020</title>
		<meeting><address><addrLine>Seattle, WA</addrLine></address></meeting>
		<imprint>
			<publisher>Computer Vision Foundation/IEEE</publisher>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="page" from="6706" to="6716" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Deep learning for understanding satellite imagery: An experimental survey</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Mohanty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Czakon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Kaczmarek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pyskir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Tarasiewicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kunwar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Prasad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fleer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Göpfert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tandon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Mollard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Rayaprolu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Salathe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schilling</surname></persName>
		</author>
		<idno type="DOI">10.3389/frai.2020.534696</idno>
		<ptr target="https://doi.org/10.3389/frai.2020.534696" />
	</analytic>
	<monogr>
		<title level="j">Frontiers in Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">534696</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Crop yield prediction with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Nevavuori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Narra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lipping</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.compag.2019.104859</idno>
		<ptr target="https://doi.org/10.1016/j.compag.2019.104859" />
	</analytic>
	<monogr>
		<title level="j">Computers and Electronics in Agriculture</title>
		<imprint>
			<biblScope unit="volume">163</biblScope>
			<biblScope unit="page">104859</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Köpf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019</title>
		<meeting><address><addrLine>NeurIPS; Vancouver, BC</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Self-supervised relational reasoning for representation learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Patacchiola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Storkey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020</title>
		<meeting><address><addrLine>NeurIPS; Virtual</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Scikit-learn: Machine learning in python</title>
		<author>
			<persName><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Dubourg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cournapeau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brucher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Perrot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Duchesnay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Spatiotemporal contrastive video representation learning</title>
		<author>
			<persName><forename type="first">R</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2021, Virtual</title>
		<imprint>
			<publisher>Computer Vision Foundation/IEEE</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="6964" to="6974" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Roser</surname></persName>
		</author>
		<ptr target="https://ourworldindata.org/hunger-andundernourishment" />
		<title level="m">Hunger and undernourishment</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>Our World in Data</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Self-supervised audio-visual co-segmentation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Rouditchenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mcdermott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICASSP.2019.8682467</idno>
		<ptr target="https://doi.org/10.1109/ICASSP.2019.8682467" />
	</analytic>
	<monogr>
		<title level="m">ICASSP 2019 -2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2357" to="2361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title />
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Wulder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">R</forename><surname>Loveland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Woodcock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">G</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Helder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Irons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Scambos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schaaf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Vermote</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Belward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bindschadler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hipple</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P ; -22</forename><surname>Hostert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruben</forename><surname>Cartuyvels</surname></persName>
		</author>
		<idno type="DOI">10.1017/eds.2023.42</idno>
		<ptr target="https://doi.org/10.1017/eds.2023.42" />
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<biblScope unit="page">48</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Landsat-8: Science and product vision for terrestrial global change research</title>
		<author>
			<persName><forename type="first">J</forename><surname>Huntington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Justice</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kilic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kovalskyy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lymburner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Masek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mccorkel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shuai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Trezza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vogelmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wynne</forename><forename type="middle">R</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sensing of Environment</title>
		<imprint>
			<biblScope unit="volume">145</biblScope>
			<biblScope unit="page" from="154" to="172" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Semantic segmentation of crop type in Africa: A novel dataset and analysis of deep learning methods</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Rustowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cheong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Burke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lobell</forename><surname>Db</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition Workshops, CVPR Workshops 2019</title>
		<meeting><address><addrLine>Long Beach, CA</addrLine></address></meeting>
		<imprint>
			<publisher>Computer Vision Foundation/IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="75" to="82" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">A survey on semi-, self-and unsupervised learning for image classification</title>
		<author>
			<persName><forename type="first">L</forename><surname>Schmarje</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Santarossa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S-M</forename><surname>Schröder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Koch</surname></persName>
		</author>
		<idno type="DOI">10.1109/ACCESS.2021.3084358</idno>
		<ptr target="https://doi.org/10.1109/ACCESS.2021.3084358" />
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="82146" to="82168" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Predicting economic development using geolocated wikipedia articles</title>
		<author>
			<persName><forename type="first">E</forename><surname>Sheehan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Uzkent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Burke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lobell</forename><surname>Db</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining, KDD 2019</title>
		<meeting>the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining, KDD 2019<address><addrLine>Anchorage, AK</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2698" to="2706" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Learning important features through propagating activation differences</title>
		<author>
			<persName><forename type="first">A</forename><surname>Shrikumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Greenside</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kundaje</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v70/shrikumar17a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning, ICML 2017</title>
		<title level="s">Proceedings of Machine Learning Research</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Precup</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</editor>
		<meeting>the 34th International Conference on Machine Learning, ICML 2017<address><addrLine>Sydney, NSW, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-06-11">2017. 6-11 August 2017</date>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="3145" to="3153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">The use of night-time lights satellite imagery as a measure of Australia's regional electricity consumption and population distribution</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Townsend</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bruce</forename><surname>Da</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">16</biblScope>
			<biblScope unit="page" from="4459" to="4480" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Learning to interpret satellite images using wikipedia</title>
		<author>
			<persName><forename type="first">B</forename><surname>Uzkent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Sheehan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Burke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lobell</forename><surname>Db</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Kraus</surname></persName>
		</editor>
		<meeting>the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI<address><addrLine>Macao</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3620" to="3626" />
		</imprint>
	</monogr>
	<note>ijcai.org</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Deep learning for land use and land cover classification based on hyperspectral and multispectral earth observation data: A review</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Comai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Matteucci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">15</biblScope>
			<biblScope unit="page">2495</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Representation learning with contrastive predictive coding</title>
		<author>
			<persName><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><forename type="middle">Y</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename></persName>
		</author>
		<idno>CoRR, abs/1807.03748</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Deep transfer learning for crop yield prediction with remote sensing data</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Desai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lobell</forename><surname>Db</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st ACM SIGCAS Conference on Computing and Sustainable Societies, COMPASS 2018</title>
		<meeting>the 1st ACM SIGCAS Conference on Computing and Sustainable Societies, COMPASS 2018<address><addrLine>Menlo Park and San Jose, CA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="1" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Stochastic modeling of food insecurity</title>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bpj</forename><surname>Andree</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Chamorro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">G</forename><surname>Spencer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>World Bank</publisher>
			<pubPlace>Washington, DC</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Policy Research Working Papers</note>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Urban2vec: Incorporating street view imagery and POIs for multi-modal urban neighborhood embedding</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rajagopal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence</title>
		<meeting><address><addrLine>EAAI; New York</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="page" from="1013" to="1020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Landsat</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Goward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Arvidson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Photogrammetric Engineering &amp; Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1171" to="1178" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Unsupervised feature learning via non-parametric instance-level discrimination</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><forename type="middle">S</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Lin</forename><forename type="middle">D</forename></persName>
		</author>
		<idno>CoRR, abs/1805.01978</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Using publicly available satellite imagery and deep learning to understand economic well-being in Africa</title>
		<author>
			<persName><forename type="first">C</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Driscoll</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Azzari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lobell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Burke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Communications</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Split-brain autoencoders: Unsupervised learning by cross-channel prediction</title>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Honolulu, HI</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="645" to="654" />
		</imprint>
	</monogr>
	<note>CVPR 2017</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</teiCorpus></text>
</TEI>