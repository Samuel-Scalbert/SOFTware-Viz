<?xml version='1.0' encoding='utf-8'?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" version="1.1" xsi:schemaLocation="http://www.tei-c.org/ns/1.0 http://api.archives-ouvertes.fr/documents/aofr-sword.xsd">
  <teiHeader>
    <fileDesc>
      <titleStmt>
        <title>HAL TEI export of hal-03150441v2</title>
      </titleStmt>
      <publicationStmt>
        <distributor>CCSD</distributor>
        <availability status="restricted">
          <licence target="http://creativecommons.org/licenses/by/4.0/">Distributed under a Creative Commons Attribution 4.0 International License</licence>
        </availability>
        <date when="2024-04-26T02:37:51+02:00" />
      </publicationStmt>
      <sourceDesc>
        <p part="N">HAL API platform</p>
      </sourceDesc>
    </fileDesc>
  </teiHeader>
  <text>
    <body>
      <listBibl>
        <biblFull>
          <titleStmt>
            <title xml:lang="en">Graph integration of structured, semistructured and unstructured data for data journalism</title>
            <author role="aut">
              <persName>
                <forename type="first">Angelos</forename>
                <forename type="middle">Christos</forename>
                <surname>Anadiotis</surname>
              </persName>
              <email type="md5">f5828c800e41f90d1d55a6c6055c44f0</email>
              <email type="domain">polytechnique.edu</email>
              <idno type="idhal" notation="string">angelos-christos-anadiotis</idno>
              <idno type="idhal" notation="numeric">742708</idno>
              <idno type="halauthorid" notation="string">50961-742708</idno>
              <idno type="ORCID">https://orcid.org/0000-0001-8101-6044</idno>
              <affiliation ref="#struct-451441" />
            </author>
            <author role="aut">
              <persName>
                <forename type="first">Oana</forename>
                <surname>Balalau</surname>
              </persName>
              <email type="md5">abc0cb3b3eb81d1f4607e9405d8eaaea</email>
              <email type="domain">inria.fr</email>
              <idno type="idhal" notation="string">oana-balalau</idno>
              <idno type="idhal" notation="numeric">742878</idno>
              <idno type="halauthorid" notation="string">38860-742878</idno>
              <idno type="ORCID">https://orcid.org/0000-0003-1469-3664</idno>
              <affiliation ref="#struct-451441" />
            </author>
            <author role="aut">
              <persName>
                <forename type="first">Catarina</forename>
                <surname>Conceicao</surname>
              </persName>
              <idno type="halauthorid">2157007-0</idno>
              <affiliation ref="#struct-300600" />
            </author>
            <author role="aut">
              <persName>
                <forename type="first">Helena</forename>
                <surname>Galhardas</surname>
              </persName>
              <idno type="halauthorid">144196-0</idno>
              <affiliation ref="#struct-300600" />
            </author>
            <author role="aut">
              <persName>
                <forename type="first">Mhd Yamen</forename>
                <surname>Haddad</surname>
              </persName>
              <email type="md5">6599dcaa1efe1a24c96d94d13e94c9b4</email>
              <email type="domain">inria.fr</email>
              <idno type="idhal" notation="string">mhd-yamen-haddad</idno>
              <idno type="idhal" notation="numeric">1091767</idno>
              <idno type="halauthorid" notation="string">2022698-1091767</idno>
              <orgName ref="#struct-300009" />
              <affiliation ref="#struct-451441" />
            </author>
            <author role="aut">
              <persName>
                <forename type="first">Ioana</forename>
                <surname>Manolescu</surname>
              </persName>
              <email type="md5">198b5d07a89578a2f2e563c17579cde7</email>
              <email type="domain">inria.fr</email>
              <idno type="idhal" notation="string">ioana-manolescu</idno>
              <idno type="idhal" notation="numeric">742652</idno>
              <idno type="halauthorid" notation="string">16724-742652</idno>
              <idno type="ORCID">https://orcid.org/0000-0002-0425-2462</idno>
              <idno type="GOOGLE SCHOLAR">https://scholar.google.com/citations?user=q6Ft35wAAAAJ&amp;hl=en</idno>
              <orgName ref="#struct-300009" />
              <affiliation ref="#struct-451441" />
            </author>
            <author role="aut">
              <persName>
                <forename type="first">Tayeb</forename>
                <surname>Merabti</surname>
              </persName>
              <idno type="halauthorid">438328-0</idno>
              <affiliation ref="#struct-451441" />
            </author>
            <author role="aut">
              <persName>
                <forename type="first">Jingmao</forename>
                <surname>You</surname>
              </persName>
              <idno type="halauthorid">2004256-0</idno>
              <affiliation ref="#struct-451441" />
            </author>
            <editor role="depositor">
              <persName>
                <forename>Ioana</forename>
                <surname>Manolescu</surname>
              </persName>
              <email type="md5">198b5d07a89578a2f2e563c17579cde7</email>
              <email type="domain">inria.fr</email>
            </editor>
            <funder ref="#projanr-51646" />
            <funder>ANR-20-CHIA-0015</funder>
          </titleStmt>
          <editionStmt>
            <edition n="v1">
              <date type="whenSubmitted">2021-02-23 18:27:07</date>
            </edition>
            <edition n="v2" type="current">
              <date type="whenSubmitted">2021-09-08 10:39:34</date>
              <date type="whenWritten">2021-06-28</date>
              <date type="whenModified">2024-02-01 10:04:36</date>
              <date type="whenReleased">2021-09-08 10:58:37</date>
              <date type="whenProduced">2022-10-01</date>
              <date type="whenEndEmbargoed">2021-09-08</date>
              <ref type="file" target="https://inria.hal.science/hal-03150441v2/document">
                <date notBefore="2021-09-08" />
              </ref>
              <ref type="file" subtype="author" n="1" target="https://inria.hal.science/hal-03150441v2/file/revised-hal.pdf">
                <date notBefore="2021-09-08" />
              </ref>
              <ref type="externalLink" target="http://arxiv.org/pdf/2007.12488" />
            </edition>
            <respStmt>
              <resp>contributor</resp>
              <name key="108450">
                <persName>
                  <forename>Ioana</forename>
                  <surname>Manolescu</surname>
                </persName>
                <email type="md5">198b5d07a89578a2f2e563c17579cde7</email>
                <email type="domain">inria.fr</email>
              </name>
            </respStmt>
          </editionStmt>
          <publicationStmt>
            <distributor>CCSD</distributor>
            <idno type="halId">hal-03150441</idno>
            <idno type="halUri">https://inria.hal.science/hal-03150441</idno>
            <idno type="halBibtex">anadiotis:hal-03150441</idno>
            <idno type="halRefHtml">&lt;i&gt;Information Systems&lt;/i&gt;, 2022, 104, pp.101846. &lt;a target="_blank" href="https://dx.doi.org/10.1016/j.is.2021.101846"&gt;&amp;#x27E8;10.1016/j.is.2021.101846&amp;#x27E9;&lt;/a&gt;</idno>
            <idno type="halRef">Information Systems, 2022, 104, pp.101846. &amp;#x27E8;10.1016/j.is.2021.101846&amp;#x27E9;</idno>
          </publicationStmt>
          <seriesStmt>
            <idno type="stamp" n="X">Ecole Polytechnique</idno>
            <idno type="stamp" n="UNIV-RENNES1">Université de Rennes 1</idno>
            <idno type="stamp" n="CNRS">CNRS - Centre national de la recherche scientifique</idno>
            <idno type="stamp" n="INRIA">INRIA - Institut National de Recherche en Informatique et en Automatique</idno>
            <idno type="stamp" n="IRISA">Irisa</idno>
            <idno type="stamp" n="LIX">Laboratoire d'informatique de l'école polytechnique</idno>
            <idno type="stamp" n="INRIA-SACLAY" corresp="INRIA">INRIA Saclay - Ile de France</idno>
            <idno type="stamp" n="X-LIX" corresp="X">Laboratoire d'informatique de l'X (LIX)</idno>
            <idno type="stamp" n="X-DEP" corresp="X">Polytechnique</idno>
            <idno type="stamp" n="X-DEP-INFO" corresp="X-DEP">Département d'informatique</idno>
            <idno type="stamp" n="INRIA_TEST">INRIA - Institut National de Recherche en Informatique et en Automatique</idno>
            <idno type="stamp" n="TESTALAIN1">TESTALAIN1</idno>
            <idno type="stamp" n="INRIA2">INRIA 2</idno>
            <idno type="stamp" n="UR1-HAL">Publications labos UR1 dans HAL-Rennes 1</idno>
            <idno type="stamp" n="UR1-MATH-STIC">UR1 - publications Maths-STIC</idno>
            <idno type="stamp" n="UR1-UFR-ISTIC">UFR ISTIC Informatique et électronique</idno>
            <idno type="stamp" n="TEST-UR-CSS">TEST Université de Rennes CSS</idno>
            <idno type="stamp" n="UNIV-RENNES">Université de Rennes</idno>
            <idno type="stamp" n="INRIA-300009">Inria 300009</idno>
            <idno type="stamp" n="IP_PARIS">Institut Polytechnique de Paris</idno>
            <idno type="stamp" n="ANR">ANR</idno>
            <idno type="stamp" n="UR1-MATH-NUM">Pôle UnivRennes - Mathématiques - Numérique </idno>
            <idno type="stamp" n="GS-COMPUTER-SCIENCE">Graduate School Computer Science</idno>
            <idno type="stamp" n="INRIAARTDOI">INRIAARTDOI</idno>
          </seriesStmt>
          <notesStmt>
            <note type="audience" n="2">International</note>
            <note type="popular" n="0">No</note>
            <note type="peer" n="1">Yes</note>
          </notesStmt>
          <sourceDesc>
            <biblStruct>
              <analytic>
                <title xml:lang="en">Graph integration of structured, semistructured and unstructured data for data journalism</title>
                <author role="aut">
                  <persName>
                    <forename type="first">Angelos</forename>
                    <forename type="middle">Christos</forename>
                    <surname>Anadiotis</surname>
                  </persName>
                  <email type="md5">f5828c800e41f90d1d55a6c6055c44f0</email>
                  <email type="domain">polytechnique.edu</email>
                  <idno type="idhal" notation="string">angelos-christos-anadiotis</idno>
                  <idno type="idhal" notation="numeric">742708</idno>
                  <idno type="halauthorid" notation="string">50961-742708</idno>
                  <idno type="ORCID">https://orcid.org/0000-0001-8101-6044</idno>
                  <affiliation ref="#struct-451441" />
                </author>
                <author role="aut">
                  <persName>
                    <forename type="first">Oana</forename>
                    <surname>Balalau</surname>
                  </persName>
                  <email type="md5">abc0cb3b3eb81d1f4607e9405d8eaaea</email>
                  <email type="domain">inria.fr</email>
                  <idno type="idhal" notation="string">oana-balalau</idno>
                  <idno type="idhal" notation="numeric">742878</idno>
                  <idno type="halauthorid" notation="string">38860-742878</idno>
                  <idno type="ORCID">https://orcid.org/0000-0003-1469-3664</idno>
                  <affiliation ref="#struct-451441" />
                </author>
                <author role="aut">
                  <persName>
                    <forename type="first">Catarina</forename>
                    <surname>Conceicao</surname>
                  </persName>
                  <idno type="halauthorid">2157007-0</idno>
                  <affiliation ref="#struct-300600" />
                </author>
                <author role="aut">
                  <persName>
                    <forename type="first">Helena</forename>
                    <surname>Galhardas</surname>
                  </persName>
                  <idno type="halauthorid">144196-0</idno>
                  <affiliation ref="#struct-300600" />
                </author>
                <author role="aut">
                  <persName>
                    <forename type="first">Mhd Yamen</forename>
                    <surname>Haddad</surname>
                  </persName>
                  <email type="md5">6599dcaa1efe1a24c96d94d13e94c9b4</email>
                  <email type="domain">inria.fr</email>
                  <idno type="idhal" notation="string">mhd-yamen-haddad</idno>
                  <idno type="idhal" notation="numeric">1091767</idno>
                  <idno type="halauthorid" notation="string">2022698-1091767</idno>
                  <orgName ref="#struct-300009" />
                  <affiliation ref="#struct-451441" />
                </author>
                <author role="aut">
                  <persName>
                    <forename type="first">Ioana</forename>
                    <surname>Manolescu</surname>
                  </persName>
                  <email type="md5">198b5d07a89578a2f2e563c17579cde7</email>
                  <email type="domain">inria.fr</email>
                  <idno type="idhal" notation="string">ioana-manolescu</idno>
                  <idno type="idhal" notation="numeric">742652</idno>
                  <idno type="halauthorid" notation="string">16724-742652</idno>
                  <idno type="ORCID">https://orcid.org/0000-0002-0425-2462</idno>
                  <idno type="GOOGLE SCHOLAR">https://scholar.google.com/citations?user=q6Ft35wAAAAJ&amp;hl=en</idno>
                  <orgName ref="#struct-300009" />
                  <affiliation ref="#struct-451441" />
                </author>
                <author role="aut">
                  <persName>
                    <forename type="first">Tayeb</forename>
                    <surname>Merabti</surname>
                  </persName>
                  <idno type="halauthorid">438328-0</idno>
                  <affiliation ref="#struct-451441" />
                </author>
                <author role="aut">
                  <persName>
                    <forename type="first">Jingmao</forename>
                    <surname>You</surname>
                  </persName>
                  <idno type="halauthorid">2004256-0</idno>
                  <affiliation ref="#struct-451441" />
                </author>
              </analytic>
              <monogr>
                <idno type="halJournalId" status="VALID">14192</idno>
                <idno type="issn">0306-4379</idno>
                <title level="j">Information Systems</title>
                <imprint>
                  <publisher>Elsevier</publisher>
                  <biblScope unit="volume">104</biblScope>
                  <biblScope unit="pp">101846</biblScope>
                  <date type="datePub">2022-10-01</date>
                  <date type="dateEpub">2021-07-18</date>
                </imprint>
              </monogr>
              <idno type="doi">10.1016/j.is.2021.101846</idno>
              <ref target="https://sourcessay.inria.fr" type="seeAlso" />
            </biblStruct>
          </sourceDesc>
          <profileDesc>
            <langUsage>
              <language ident="en">English</language>
            </langUsage>
            <textClass>
              <keywords scheme="author">
                <term xml:lang="en">Data journalism</term>
                <term xml:lang="en">heterogeneous data integration</term>
                <term xml:lang="en">information extraction</term>
              </keywords>
              <classCode scheme="halDomain" n="info.info-db">Computer Science [cs]/Databases [cs.DB]</classCode>
              <classCode scheme="halTypology" n="ART">Journal articles</classCode>
              <classCode scheme="halOldTypology" n="ART">Journal articles</classCode>
              <classCode scheme="halTreeTypology" n="ART">Journal articles</classCode>
            </textClass>
            <abstract xml:lang="en">
              <p>Digital data is a gold mine for modern journalism. However, datasets which interest journalists are extremely heterogeneous, ranging from highly structured (relational databases), semi-structured (JSON, XML, HTML), graphs (e.g., RDF), and text. Journalists (and other classes of users lacking advanced IT expertise, such as most non-governmental-organizations, or small public administrations) need to be able to make sense of such heterogeneous corpora, even if they lack the ability to define and deploy custom extract-transform-load workflows, especially for dynamically varying sets of data sources. We describe a complete approach for integrating dynamic sets of heterogeneous datasets along the lines described above: the challenges we faced to make such graphs useful, allow their integration to scale, and the solutions we proposed for these problems. Our approach is implemented within the ConnectionLens system; we validate it through a set of experiments.</p>
            </abstract>
          </profileDesc>
        </biblFull>
      </listBibl>
    </body>
    <back>
      <listOrg type="structures">
        <org type="researchteam" xml:id="struct-451441" status="VALID">
          <idno type="RNSR">201622056J</idno>
          <orgName>Rich Data Analytics at Cloud Scale</orgName>
          <orgName type="acronym">CEDAR</orgName>
          <date type="start">2016-01-01</date>
          <desc>
            <address>
              <addrLine>1 rue Honoré d'Estienne d'OrvesBâtiment Alan TuringCampus de l'École Polytechnique91120 Palaiseau</addrLine>
              <country key="FR" />
            </address>
          </desc>
          <listRelation>
            <relation active="#struct-2071" type="direct" />
            <relation active="#struct-300340" type="indirect" />
            <relation name="UMR7161" active="#struct-441569" type="indirect" />
            <relation active="#struct-118511" type="direct" />
            <relation active="#struct-300009" type="indirect" />
          </listRelation>
        </org>
        <org type="institution" xml:id="struct-300600" status="VALID">
          <orgName>Instituto Superior Técnico, Universidade Técnica de Lisboa</orgName>
          <orgName type="acronym">IST</orgName>
          <desc>
            <address>
              <addrLine>Av. Rovisco Pais, 11049-001 Lisboa</addrLine>
              <country key="PT" />
            </address>
            <ref type="url">http://tecnico.ulisboa.pt/</ref>
          </desc>
        </org>
        <org type="laboratory" xml:id="struct-2071" status="VALID">
          <idno type="IdRef">196509955</idno>
          <idno type="RNSR">200519331V</idno>
          <orgName>Laboratoire d'informatique de l'École polytechnique [Palaiseau]</orgName>
          <orgName type="acronym">LIX</orgName>
          <desc>
            <address>
              <addrLine>Route de Saclay 91128 PALAISEAU CEDEX</addrLine>
              <country key="FR" />
            </address>
            <ref type="url">https://www.lix.polytechnique.fr/</ref>
          </desc>
          <listRelation>
            <relation active="#struct-300340" type="direct" />
            <relation name="UMR7161" active="#struct-441569" type="direct" />
          </listRelation>
        </org>
        <org type="institution" xml:id="struct-300340" status="VALID">
          <idno type="IdRef">027309320</idno>
          <idno type="ROR">https://ror.org/05hy3tk52</idno>
          <orgName>École polytechnique</orgName>
          <orgName type="acronym">X</orgName>
          <date type="start">1794-03-11</date>
          <desc>
            <address>
              <addrLine>École polytechnique, 91128 Palaiseau Cedex</addrLine>
              <country key="FR" />
            </address>
            <ref type="url">https://www.polytechnique.edu/</ref>
          </desc>
        </org>
        <org type="regroupinstitution" xml:id="struct-441569" status="VALID">
          <idno type="IdRef">02636817X</idno>
          <idno type="ISNI">0000000122597504</idno>
          <idno type="ROR">https://ror.org/02feahw73</idno>
          <orgName>Centre National de la Recherche Scientifique</orgName>
          <orgName type="acronym">CNRS</orgName>
          <date type="start">1939-10-19</date>
          <desc>
            <address>
              <country key="FR" />
            </address>
            <ref type="url">https://www.cnrs.fr/</ref>
          </desc>
        </org>
        <org type="laboratory" xml:id="struct-118511" status="VALID">
          <idno type="RNSR">200818248E</idno>
          <idno type="ROR">https://ror.org/0315e5x55</idno>
          <orgName>Inria Saclay - Ile de France</orgName>
          <desc>
            <address>
              <addrLine>1 rue Honoré d'Estienne d'OrvesBâtiment Alan TuringCampus de l'École Polytechnique91120 Palaiseau</addrLine>
              <country key="FR" />
            </address>
            <ref type="url">http://www.inria.fr/centre/saclay</ref>
          </desc>
          <listRelation>
            <relation active="#struct-300009" type="direct" />
          </listRelation>
        </org>
        <org type="institution" xml:id="struct-300009" status="VALID">
          <idno type="ROR">https://ror.org/02kvxyf05</idno>
          <orgName>Institut National de Recherche en Informatique et en Automatique</orgName>
          <orgName type="acronym">Inria</orgName>
          <desc>
            <address>
              <addrLine>Domaine de VoluceauRocquencourt - BP 10578153 Le Chesnay Cedex</addrLine>
              <country key="FR" />
            </address>
            <ref type="url">http://www.inria.fr/en/</ref>
          </desc>
        </org>
      </listOrg>
      <listOrg type="projects">
        <org type="anrProject" xml:id="projanr-51646" status="VALID">
          <idno type="anr">ANR-20-CHIA-0015</idno>
          <orgName>SourcesSay</orgName>
          <desc>Analyse et Interconnexion Intelligente des Contenus Héterogènes dans des Arènes Numériques</desc>
          <date type="start">2020</date>
        </org>
      </listOrg>
    </back>
  <teiCorpus>
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Graph integration of structured, semistructured and unstructured data for data journalism</title>
				<funder ref="#_nQHx3Yv">
					<orgName type="full">Agence Nationale de la Recherche</orgName>
					<orgName type="abbreviated">ANR</orgName>
				</funder>
				<funder ref="#_8nGCncS #_wtrmbpP">
					<orgName type="full">unknown</orgName>
				</funder>
				<funder ref="#_ySfDteS">
					<orgName type="full">FCT</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher />
				<availability status="unknown"><licence /></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Christos</forename><surname>Angelos</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Oana</forename><surname>Anadiotis</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Catarina</forename><surname>Balalau</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Helena</forename><surname>Conceicao</surname></persName>
						</author>
						<author>
							<persName><roleName>Mhd</roleName><forename type="first">Yamen</forename><surname>Galhardas</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ioana</forename><surname>Haddad</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Tayeb</forename><surname>Manolescu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jingmao</forename><surname>Merabti</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Angelos</forename><forename type="middle">Christos</forename><surname>Anadiotis</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">École Polytechnique</orgName>
								<orgName type="department" key="dep2">EPFL b Inria</orgName>
								<orgName type="institution" key="instit1">Institut Polytechnique de Paris</orgName>
								<orgName type="institution" key="instit2">Institut Polytechnique de Paris</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Oana</forename><surname>Balalau</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Catarina</forename><surname>Conceição</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">INESC-ID and IST</orgName>
								<orgName type="institution">Univ. Lisboa</orgName>
								<address>
									<country key="PT">Portugal</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Helena</forename><surname>Galhardas</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">INESC-ID and IST</orgName>
								<orgName type="institution">Univ. Lisboa</orgName>
								<address>
									<country key="PT">Portugal</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><roleName>Mhd</roleName><forename type="first">Yamen</forename><surname>Haddad</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ioana</forename><surname>Manolescu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Tayeb</forename><surname>Merabti</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jingmao</forename><surname>You</surname></persName>
						</author>
						<title level="a" type="main">Graph integration of structured, semistructured and unstructured data for data journalism</title>
					</analytic>
					<monogr>
						<imprint>
							<date />
						</imprint>
					</monogr>
					<idno type="MD5">752DEAA26F5D22D7A1F5661D68E0B429</idno>
					<idno type="DOI">10.1016/j.is.2021.101846</idno>
					<note type="submission">Submitted on 8 Sep 2021</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-04-12T14:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid" />
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Data journalism</term>
					<term>heterogeneous data integration</term>
					<term>information extraction</term>
				</keywords>
			</textClass>
			<abstract>
<div><p>come from teaching and research institutions in France or abroad, or from public or private research centers. L'archive ouverte pluridisciplinaire</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div><head n="1.">Introduction</head><p>Data journalism <ref type="bibr" target="#b21">[22]</ref>, according to Wikipedia 1 , is "a brand of journalism reflecting the increased role that numerical data is used in the production and distribution of information in the digital era". Data journalists often have to analyze and exploit datasets that they obtain from official organizations or their sources, extract from social media, or create themselves (typically Excel or Word-style). For instance, journalists from the French Le Monde newspaper want to retrieve connections between elected people at Assemblée Nationale and companies that have subsidiaries outside of France. Such a query can be answered currently at a high human effort cost, by inspecting e.g., a JSON list of Assemblée elected officials (available from NosDeputes.fr) and manually connecting the names with those found in a national registry of companies. This considerable effort may still miss connections that could be found if one added information about politicians' and business people's spouses, information sometimes available in public knowledge bases such as DBPedia, or journalists' notes.</p><p>Since 2013 <ref type="bibr" target="#b20">[21]</ref>, and later in the ContentCheck (2015-2020) and then in the SourcesSay (2020-2024) projects, we have been investigating ways in which Computer Science research could benefit data journalism, as well as journalistic fact-checking, that is: the task of verifying claims contained in media articles, based on some trusted evidence. The platforms we initially developed and proposed to journalists <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b5">6]</ref> followed a data integration <ref type="bibr" target="#b13">[14]</ref> approach, specifically, a mediator (or polystore <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b2">3]</ref>) architecture. These systems allowed users to express queries using a mix of languages fitting every individual data source. Demonstrating these tools to journalists from Le Monde, a leading French newspaper and partner in the above two projects, as well as others working at Ouest France, the Financial Times etc. highlighted that installing and maintaining a mediator over several data sources, as well as querying them through a mixed language, appeared way too complex and required too many IT resources. Clearly, journalists need simpler systems and simpler, more intuitive methods for querying their varied data sources. From these exchanges, and learning how they work, we have drawn the following set of requirements and constraints:</p><p>R1. Integral source preservation and provenance: in journalistic work, it is crucial to be able to trace each node within the integrated graph back to the dataset from which it came. This enables adequately sourcing information, an important tenet of quality journalism.</p><p>R2. Little to no user effort : journalists often lack time and resources to set up IT tools or data processing pipelines. Even when they are able to use a tool supporting one or two data models (e.g., most relational databases provide some support for JSON data), handling other data models remains challenging. Thus, the construction of the integrated graph needs to be as automatic as possible.</p><p>C1. Little-known entities: interesting journalistic datasets feature some extremely well-known entities (e.g., highly visible National Assembly members) next to others of much smaller notoriety (e.g., the collaborators employed by the National Assembly to help organize each deputy's work; or a company in which a deputy had worked). From a journalistic perspective, such lesser-known entities may play a crucial role in making interesting connections among nodes in the graph.</p><p>C2. Controlled dataset ingestion: the confidence level in the data required for journalistic use excludes massive ingestion from uncontrolled data sources, e.g., through large-scale Web crawls.</p><p>C3. Language support: journalists are first and foremost concerned with the affairs surrounding them (at the local or national scale). This requires supporting datasets in the language(s) relevant for them -in our case, French.</p><p>R3. Performance on "off-the-shelf " hardware: Our algorithms' complexity in the data size should be low, and overall performance is a concern; the tool should run on general-purpose hardware, available to nonexpert users like the ones we consider.</p><p>For what concerns querying the integrated graph, we note: R4. Finding connections across heterogeneous datasets is a core need. In particular, our approach needs to be tolerant of inevitable differences in the organization of data across sources.</p><p>C4. Query algorithm orthogonal to answer quality scores After discussing several journalistic scenarios, no unique method (score) for de-ciding which are the best answers to a query has been identified. Instead: (i) it appears that "very large" answers (say, of more than 20 edges) are of limited interest; (ii) connections that "state the obvious", e.g., that any two actors from a French political scenario are connected through "France", are not of interest. Therefore, our algorithm must be orthogonal, and it should be possible to use it with any score function.</p><p>To address these requirements and constraints, we have started in 2018 developing a tool called ConnectionLens, described in a short demonstration paper <ref type="bibr" target="#b9">[10]</ref>. ConnectionLens' approach since its inception has been to (i) integrate arbitrary heterogeneous datasets into a unique graph and (ii) query them with keywords, such that an answer can span over arbitrary combinations of datasets and heterogeneous data models. However, this version of the tool <ref type="bibr" target="#b9">[10]</ref> did not meet (R2) (little to no effort required from users), in particular, because it still followed a mediator approach: the integrated graph was virtual, thus queries were answered by evaluating subqueries over individual data sources and stitching them into a complete answer. This method did not scale on large graphs, therefore it did not meet (R3). It also suffered from other shortcomings, particularly a relatively low quality of integrated graphs from French-language data. This paper reflects the result of the years of research and development that invested in the system.</p><p>To reach our goals under these requirements and constraints, in the following:</p><p>1. We formalize integrated graphs we target, and formalize the problem of constructing them from arbitrary sets of datasets. This follows the line of <ref type="bibr" target="#b9">[10]</ref> and extends it to novel data models, such as XML and PDF.</p><p>2. We introduce an approach, and an architecture for building the graphs, leveraging data integration, information extraction, knowledge bases, and data management techniques. Steering away from <ref type="bibr" target="#b9">[10]</ref>, we adopt a centralized graph warehouse architecture, easy to install, meeting simultaneously (R2) and (R3). Within this architecture, a significant part of our effort was invested in developing resources and tools for datasets in French. English is also supported, thanks to a (much) wider availability of linguistics resources.</p><p>3. Fully novel with respect to <ref type="bibr" target="#b9">[10]</ref>, we analyze the properties of the search space for our keyword search problem, and propose the first algorithm capable of finding matches across heterogeneous data sources while preserving each node within its original source. This algorithm vastly outperformed the previous one, which we discarded as a consequence of the change of architecture.</p><p>4. We have fully implemented our approach in an end-to-end tool; it currently supports text, CSV, JSON, XML, RDF, PDF datasets, and existing relational databases; the system has more than doubled in size and has little in common with its 2018 incarnation. Also, for the first time, we present: (i) a set of use cases with real journalistic datasets;</p><p>(ii) an experimental evaluation of its scalability and the quality of the extracted graph; (iii) query experiments confirming the practical interest of query algorithm.</p><p>Motivating example. To illustrate our approach, we rely on a set of four datasets, shown in Figure <ref type="figure" target="#fig_0">1</ref>. Starting from the top left, in clockwise order, we have: a table with assets of public officials, a JSON listing of France elected officials, an article from the newspaper Libération with entities highlighted, and a subset of the DBPedia RDF knowledge base. Our goal is to interconnect these datasets into a graph and to be able to answer, for example, the question: "What are the connections between Levallois-Perret and Africa?" One possible answer comes by noticing that P. Balkany was the mayor of Levallois-Perret (as stated in the JSON document), and he owned the "Dar Gyucy" villa in Marrakesh (as shown in the relational table), which is in Morocco, which is in Africa (stated by the DBPedia subset). Another interesting connection in this graph is that Levallois-Perret appears in the same sentence as the Centrafrican Republic in the Libération snippet at the bottom right, which (as stated in DBPedia) is in Africa.</p></div>
<div><head n="2.">Approach and outline</head><p>We describe here the main principles of our approach, guided by the requirements and constraints stated above.</p><p>From requirement R1 (integral source preservation), it follows that all the structure and content of each dataset is preserved in the integrated graph, thus every detail of any dataset is mapped to some of its nodes and edges. This requirement also leads us to preserve the provenance of each dataset, as well as the links that may exist within and across datasets before loading them (e.g. interconnected HTML pages, JSON tweets replying to one another, or RDF graphs referring to a shared resource). We term primary nodes the nodes created in our graph strictly based on the input dataset and their provenance; we detail their creation in Section 3.</p><p>From requirement R2 (ideally no user input), it follows that we must identify the opportunities to automatically link (interconnect) nodes, even when they were not interconnected in their original dataset, and even when they come from different datasets. We achieve this at several levels:</p><p>First, we leverage and extend information extraction techniques to extract (identify) entities occurring in every node's labels in every input dataset. For instance, "Levallois-Perret" is identified as a Location in the two datasets at right in Figure <ref type="figure" target="#fig_0">1</ref> (JSON and Text). Similarly, "P. Balkany", "Balkany", "I. Balkany" occurring in the relational, JSON, and Text datasets are extracted as Person entities. Our method of entity extraction, in particular for the French language, is described in Section 4.</p><p>Second, we compare (match) occurrences of entities extracted from the datasets to determine when they refer to the same entity and thus should be interconnected. (i) Some entity occurrences we encounter refer to entities such as Marrakech, Giverny etc. known in a trusted Knowledge Base (or KB, in short), such as DBPedia. Journalists may trust a KB for such general, non-disputed entities. We disambiguate each entity occurrence, i.e., try to find the URI (identifier) assigned in the KB to the entity referred to in this occurrence, and we connect the occurrence to the entity. Disambiguation enables, for instance, to connect an occurrence of "Hollande" to the country, and another to the former French president. A common entity found in two datasets interconnects them. We describe the module we built for entity disambiguation for the French language (language constraint C3), based on AIDA <ref type="bibr" target="#b25">[26]</ref>, in Section 5. It is of independent interest, as it can be used outside of our context. (ii) On little-known entities (constraint C1), disambiguation fails (no URI is found); this is the case, e.g., of "Moulin Cossy". Combined with constraint C2 (strict control on ingested sources) it leads to the lack of reliable IDs for many entities mentioned in the datasets. We strive to connect them as soon as several identical or at least strongly similar occurrences are found in the same or different datasets. We describe our approach for comparing (matching) occurrences to identify identical or similar pairs in Section 6. Section 7 describes our persistent graph storage.</p><p>For what concerns the keyword search problem, we formalize it in Section 8, showing in particular that requirement R4 leads to an explosion in the size of the search space compared to those studied in the literature. Section 9 discusses some favorable cost function properties exploited in the literature and why they do not apply to our case (constraint C5). Based on this, Section 10 introduces our query algorithm.</p><p>All the above modules are implemented within the ConnectionLens systems; the connections between its modules are outlined in Figure <ref type="figure" target="#fig_1">2</ref> and will become clear as we present each module below. We present experiments evaluating the quality and performance of our algorithms in Section 11. Our work pertains to several areas, most notably data integration, knowledge base construction, and keyword search; we detail our positioning in Section 12.</p></div>
<div><head n="3.">Primary graph construction from heterogeneous datasets</head><p>We consider the following data models: relational (including SQL databases, CSV files etc.), RDF, JSON, XML, or HTML, and text. A dataset DS = (db, prov) in our context is a pair whose first component is a concrete data object: a relational database, or an RDF graph, or a JSON, HTML, XML document, or a CSV, text, or PDF file. The second (optional) component prov is the dataset provenance; we consider here that it is the URI from which the dataset was obtained, but this could easily be generalized.</p><p>Let A be an alphabet of words. We define an integrated graph G = (N, E) where N is the set of nodes and E the set of edges. We have</p><formula xml:id="formula_0">E ⊆ N × N × A * × [0, 1]</formula><p>, where A * denotes the set of (possibly empty) sequences of words, and the value in [0, 1] is the confidence, reflecting the probability that the relationship between two nodes holds. Each node n ∈ N has a label λ(n) ∈ A * and similarly each edge e has λ(e) ∈ A * . We use to denote the empty label. We assign to each node and edge a unique ID, as well as a type. We introduce the supported node types as needed, and write them in bold font (e.g., dataset node, URI node) when they are first mentioned; node types are important as they determine the quality and performance of matching (see <ref type="bibr">Section 6)</ref>. Finally, we create unique dataset IDs and associate to each node its dataset's ID.</p><p>Let DS i = (db i , prov i ) be a dataset of any of the above models. The following two steps are taken regardless of db i 's data model: First, we introduce a dataset node n DS i ∈ N , which models the dataset itself (not its content). Second, if prov i is not null, we create an URI node n prov i ∈ N , whose value is the provenance URI prov i , and an edge n DS i cl:prov ---→ n prov i , where cl:prov is a special edge label denoting provenance (we do not show these edges in the Figure to avoid clutter).</p><p>Next, Section 3.1 explains how each type of dataset yields nodes and edges in G. For illustration, Figure <ref type="figure" target="#fig_2">3</ref> shows the integrated graph resulting from the datasets in Figure <ref type="figure" target="#fig_0">1</ref>. In Section 3.2, we describe a set of techniques that improve the informativeness and the connectedness and decrease the size of G. Section 3.3 discusses the complexity of the graph construction.</p></div>
<div><head n="3.1.">Mapping each dataset to the graph</head><p>All the edges whose creation is described in this section reflect the structure of the input datasets. Thus, their confidence is always 1.0.</p><p>Relational. Let db = R(a 1 , . . . , a m ) be a relation (table) (residing within an RDBMS, or ingested from a CSV file etc.) A table node n R is created to represent R (yellow node with label hatvp.csv on top left in Figure <ref type="figure" target="#fig_2">3</ref>). Let t ∈ R be a tuple of the form (v 1 , . . . , v m ) in R. A tuple node n t is created for t, with an empty label, e.g., t 1 and t 2 in Figure <ref type="figure" target="#fig_2">3</ref>. For each non-null attribute v i in t, a value node n v i is created, together with an edge from n t to n v i , labeled a i (for example, the edge labeled owner at the top left in Figure <ref type="figure" target="#fig_2">3</ref>). To keep the graph readable, confidence values of 1 are not shown. Moreover, for any two relations R, R for which we know that attribute a in R is a foreign key referencing b in R , and for any tuples t ∈ R, t ∈ R such that t.a = t .b, the graph comprises an edge from n t to n t with confidence 1. This graph modeling of relational data has been used for keyword search in relational databases <ref type="bibr" target="#b26">[27]</ref>. RDF. The mapping from an RDF graph to our graph is the most natural. Each node in the RDF graph becomes, respectively, a URI node or a value node in G, and each RDF triple becomes an edge in E. At the bottom left in Figure <ref type="figure" target="#fig_2">3</ref> appear some edges resulting from our DBPedia snippet. Text. We model a text document very simply, as a node having a sequence of children, where each child is a segment of the text (e.g., the four nodes connected by the libe.fr node at the bottom right of Figure <ref type="figure" target="#fig_2">3</ref>). Segmentation is optional, and the default segmentation unit is a phrase, which appeared convenient for users inspecting the graph. Other segmentations (e.g., by paragraph for longer texts) could also be used, without changing the set of answers to a given query. JSON. As customary, we view a JSON document as a tree, with nodes that are either map nodes, array nodes or value nodes. We map each node into a node of our graph and create an edge for each parent-child relation. Map and array nodes have the empty label . Attribute names within a map become edge labels in our graph. Figure <ref type="figure" target="#fig_2">3</ref> at the top right shows how a JSON document's nodes and edges are ingested in our graph. XML. The ingestion of an XML document is very similar to that of JSON ones. XML nodes are either element, or attribute, or values nodes. As customary when modeling XML, value nodes are either text children of elements or values of their attributes.</p><p>HTML. An HTML document is treated very similarly to an XML one. In particular, when an HTML document contains a hyperlink of the form &lt;a href="http://a.org"&gt;...&lt;/a&gt;, we create a node labeled "a" and another labeled "http://a.org", and connect them through an edge labeled "href"; this is the common treatment of element and attribute nodes. However, we detect that a child node satisfies a URI syntax, and recognize (convert) it into a URI node. This enables us to preserve links across HTML documents ingested together in the same graph, with the help of node comparisons (see Section 6). PDF. A trove of useful data is found in the PDF format. To take advantage of their content, we have developed a PDF scrapper which transforms a PDF file into: (i) a JSON file, typically containing all the text found in the PDF document; (ii) if the PDF file contains bidimensional (2d) tables, each table is extracted in a separate RDF file, following an approach introduced in <ref type="bibr" target="#b8">[9]</ref>, which preserves the logical connections of each data cell with its closest. The JSON and possibly the RDFs thus obtained are ingested as explained above; moreover, from the dataset node of each such dataset d i , we add an edge labeled cl:extractedFromPDF, whose value is the URI of the PDF file. Thus, the PDF-derived datasets are all interconnected through that URI.</p></div>
<div><head n="3.2.">Refinements and optimizations</head><p>Value node typing. The need to recognize particular types of values goes beyond identifying URIs in HTML. URIs also frequently occur in JSON (e.g., tweets), in CSV datasets etc. Thus, we examine each value to see if it follows the syntax of a URI and if so, convert the value node into a URI one, regardless of the nature of the dataset from which it comes. More generally, other categories of values can be recognized in order to make our graphs more meaningful. Currently, we similarly recognize numeric nodes, date nodes, email address nodes and hashtag nodes. Node factorization. The graph resulting from the ingestion of a JSON, XML, or HTML document, or one relational table, is a tree; any value (leaf) node is reachable by a finite set of label paths from the dataset node. For instance, in an XML document, two value nodes labeled "Paris" may be reachable on the paths employee.address.city, while another is on the path headquartersCity. Graph creation as described in Section 3.1 creates three value nodes labeled "Paris"; we call this per-occurrence value node creation. Instead, per-path creation leads to a single node for all occurrences of "Paris" on the paths employee.address.city and employee.headquartersCity. We have also experimented with per-dataset value node creation, which in the above example creates a single "Paris" node, and with per-graph, where a single "Paris" value node is created in a graph, regardless of how many times "Paris" appears across all the datasets. Per-graph is consistent with the RDF data model, where each literal denotes one node.</p><p>Factorization turns a tree-structured dataset into a directed acyclic graph (DAG); it reduces the number of nodes and increases the graph connectivity. Factorization may introduce erroneous connections. For instance, constants such as true and false appear in many contexts, yet this should not lead to connecting all nodes having an attribute whose value is true. Named entities are another example, and they should be firstly disambiguated.</p><p>To prevent such erroneous connections, we have heuristically identified a set of values which should not be factorized even with per-path, perdataset or per-graph value node creation. Beyond true and false and named entities, this currently includes integer numeric node labels written on less than 4 digits, the rationale being that small integers tend to be used for ordinals, while numbers on many digits could denote years, product codes, or other forms of identifiers. This simple heuristic could be refined. Null codes, or strings used to signal missing values, e.g., "N/A", "Unknown", should not be factorized, either. As is well-known from database theory, nulls should not lead to joins (or connections, in our case). Nodes with such labels will never lead to connections in the graph: they are not factorized, and they are not compared for similarity (see <ref type="bibr">Section 6)</ref>. This is why we currently require user input on the null codes, and assist them by showing the most frequent constants, as null codes, when they occur, tend to be more frequent than real values (as our experiments illustrate in Section 11.2.1). Devising an automated approach toward detecting null codes in the data is an interesting avenue for future work.</p></div>
<div><head n="3.3.">Complexity of the graph construction</head><p>Summing up the above processing stages, the worst case complexity of ingesting a set of datasets in a ConnectionLens graph G = (N, E) is of the form:</p><formula xml:id="formula_1">c 1 • |E| + c 2 • |N | + c 3 • |N e | + c 4 • |N | 2</formula><p>In the above, the constant factors are explicitly present (i.e., not wrapped in an O(. . .) notation) as the differences between them are high enough to significantly impact the overall construction time (see Section 11.2.2 and 11.2.3). Specifically: c 1 reflects the (negligible) cost of creating each edge using the respective data parser, and the (much higher) cost of storing it; c 2 reflects the cost to store a node in the database and to invoke the entity extractor on its label, if it is not ; N e is the number of entity nodes found in the graph, and c 3 is the cost to disambiguate each entity; finally, the last component reflects the worst-case complexity of node matching, which may be quadratic in the number of nodes compared. The constant c 4 reflects the cost of recording on disk that the two nodes are equivalent (one query and one update) or similar (one update).</p><p>Observe that the number of value nodes (thus N ) is impacted by the node creation policy; N e (and, as we show below, c 3 ) depend on the entity extractor module used.</p></div>
<div><head n="4.">Named-Entity Recognition</head><p>We enrich our graphs by leveraging Machine Learning (ML) tools for Information Extraction.</p><p>Named entities (NEs) <ref type="bibr" target="#b36">[37]</ref> are words or phrases which, together, designate certain real-world entities. Named entities include common concepts such as people, organizations, and locations. The Named-Entity Recognition (NER) task consists of (i) identifying NEs in a natural language text, and (ii) classifying them according to a pre-defined set of NE types. Let n t be a text node. We feed n t as input to a NER module and create, for each entity occurrence E in n t , an entity occurrence node (or entity node, in short) n E ; as explained below, we extract Person, Organization and Location entity nodes. Further, we add an edge from n t to n E whose label is cl:extractT , where T is the type of E, and whose confidence is c, the confidence of the extraction. In Figure <ref type="figure" target="#fig_2">3</ref>, the blue, round-corner rectangles Centrafrique, Areva, P. Balkany, Levallois-Perret correspond to the entities recognized from the text document, while the Marrakech entity is extracted from the identical-label value node originating from the CSV file.</p></div>
<div><head>Named-Entity Recognition</head><p>We describe here the NER approach we devised for our framework, for English and French. While we have used Stanford NER <ref type="bibr" target="#b16">[17]</ref> in <ref type="bibr" target="#b9">[10]</ref>, we have subsequently developed a more performant module based on the Deep Learning Flair NLP framework <ref type="bibr" target="#b0">[1]</ref>. Flair and similar frameworks rely on embedding words into vectors in a multi-dimensional space. Traditional word embeddings, e.g., Word2Vec <ref type="bibr" target="#b35">[36]</ref>, Glove <ref type="bibr" target="#b39">[40]</ref> and fast-Text <ref type="bibr" target="#b4">[5]</ref>, are static, meaning that a word's representation does not depend on the context where it occurs. New embedding techniques are dynamic, in the sense that the word's representation also depends on its context. In particular, the Flair dynamic embeddings <ref type="bibr" target="#b1">[2]</ref> achieve state-of-the-art NER performance. The latest Flair architecture <ref type="bibr" target="#b0">[1]</ref> facilitates combining different types of word embeddings, as a better performance might be achieved by combining dynamic with static word embeddings.</p><p>For English, we rely on a model<ref type="foot" target="#foot_2">2</ref> pre-trained using the English CoNLL-2003<ref type="foot" target="#foot_3">3</ref> news articles dataset. The model combines Glove embeddings <ref type="bibr" target="#b39">[40]</ref> and so-called forward and backward pooled Flair embeddings that evolve across subsequent extractions. As such a model was missing for French, we trained a Flair one on WikiNER <ref type="bibr" target="#b37">[38]</ref>, a multilingual NER dataset automatically created using the text and structure of Wikipedia. The dataset contains 132K sentences, 3.4M tokens and 216K named-entities, including 74K Person, 116K Location and 25K Organization entities. The model uses stacked forward and backward French Flair embeddings with French fastText <ref type="bibr" target="#b4">[5]</ref> embeddings. Entity node creation. Similarly to the discussion about value node factorization (Section 3.2), we have the choice of creating an entity node n E of type t once per occurrence, or (in hierarchical datasets) per-path, per-dataset or per-graph. We adopt the per graph method, with the mention that we will create one entity node for each disambiguated entity and one entity node for each non-disambiguated entity.</p></div>
<div><head n="5.">Entity disambiguation</head><p>Some (but not all) entity nodes extracted from a dataset as an entity of type T may correspond to an entity (resource) described in a trusted knowledge base (KB) such as DBPedia or Yago. When this is the case, this allows: (i) resolving ambiguity to make a more confident decision about the entity, e.g., whether the entity node "Hollande" refers to the former president or the country; (ii) tackling name variations, e.g., two Organization entities labeled "Paris Saint-Germain Football Club" and "PSG" are linked to the same KB identifier, and (iii) if this is desired, enriching the dataset with a certain number of facts the KB provides about the entity.</p><p>Named entity disambiguation (NED, in short, also known as entity linking) is the process of assigning a unique identifier (typically, a URI from a KB) to each named-entity present in a text. We built our NED module based on AIDA <ref type="bibr" target="#b25">[26]</ref>, part of the <software ContextAttributes="used">Ambiverse</software><ref type="foot" target="#foot_4">4</ref> framework; AIDA maps entities to resources in YAGO 3 <ref type="bibr" target="#b34">[35]</ref> and Wikidata <ref type="bibr" target="#b46">[47]</ref>. Our work consisted of (i) adding support for French (not present in <software ContextAttributes="used">Ambiverse</software>), and (ii) integrating our own NER module (Section 4) within the <software ContextAttributes="used">Ambiverse</software> framework.</p><p>For the first task, in collaboration with the maintainers of <software ContextAttributes="used">Ambiverse</software><ref type="foot" target="#foot_5">5</ref> , we built a new dataset for French, containing the information required for AIDA. The dataset consists of entity URIs, information about entity popularity (derived from the frequency of entity names in link anchor texts within Wikipedia), and entity context (a set of weighted words or phrases that co-occur with the entity), among others. This information is languagedependent and was computed from the French Wikipedia.</p><p>For what concerns the second task, <software ContextAttributes="used">Ambiverse</software> takes an input text and passes it through a text processing pipeline consisting of tokenization (separating words), part-of-speech (POS) tagging, which identifies nouns, verbs, etc., NER, and finally NED. Text and annotations are stored and processed in <software ContextAttributes="used">Ambiverse</software> using the UIMA standard <ref type="foot" target="#foot_6">6</ref> . A central UIMA concept is the Common Annotation Scheme (or CAS); in short, it encapsulates the document analyzed, together with all the annotations concerning it, e.g., token offsets, tokens types, etc. In each <software ContextAttributes="used">Ambiverse</software> module, the CAS object containing the document receives new annotations, which are used by the next module. For example, in the tokenization module, the CAS initially contains only the original text; after tokenization, the CAS also contains token offsets.</p><p>To integrate our Flair-based extractor (Section 4), we deployed a new <software>Ambiverse</software> processing pipeline, to which we pass as input both the input text and the extracted entities.</p></div>
<div><head n="6.">Node matching</head><p>This section presents our fourth and last method for identifying and materializing connections among nodes from the same or different datasets of the graph. Recall that (i) value nodes with identical labels can be fused (factorized) (Section 3.2); (ii) nodes become connected as parents of a common extracted entity (Section 4); (iii) entity nodes with different labels can be interconnected through a common reference entity when NED returns the same KB entity (Section 5). Other pairs of nodes with similar labels that may still need to be connected, are: entity pairs where disambiguation, which is context-dependent, returned no result for one or for both (entity, value) pairs where the value corresponds to a node from which no entity was extracted (the extraction is context-dependent and may also have some misses); and value pairs, such as numbers, dates, and (if desired) identical texts. Comparing texts is useful, e.g., to compare social media posts when their topics (short strings) and/or body (long string) are very similar.</p><p>When a comparison finds two nodes with very similar labels, we create an edge labeled cl:sameAs, whose confidence is the similarity between the two labels. In Figure <ref type="figure" target="#fig_2">3</ref>, a dotted red edge (part of the subtree highlighted in green) with confidence .85 connects the "Central African Republic" RDF literal node with the "Centrafrique" Location entity extracted from the text.</p><p>When a comparison finds two nodes with identical labels, one could unify them, but this raises some modeling issues, e.g., when a value node from a dataset d 1 is unified with an entity encountered in another dataset d 2 . Instead, we conceptually connect the nodes with sameAs edges whose confidence is 1.0. These edges are drawn in solid red lines in Figure <ref type="figure" target="#fig_2">3</ref>. Nodes connected by a 1.0 sameAs edge are also termed equivalent. Note that conceptual equivalence edges are not stored; instead, the information about k equivalent nodes is stored using O(k) space, as we explain in Section 7.</p><p>Inspired by the data cleaning literature, we start by normalizing node labels, e.g., person names are analyzed to identify first names, last names, civility prefixes such as "Mr.", "Dr.", and a normalized label is computed as "Firstname Lastname". Subsequently, our approach for matching is set-at-atime. More precisely, we form node group pairs (Gr 1 , Gr 2 ), and we compare pairs of labels using the similarity function known to give the best results (in terms of matching quality) for those groups. The Jaro measure <ref type="bibr" target="#b27">[28]</ref> gives good results for short strings <ref type="bibr" target="#b13">[14]</ref> and is applied to compute the similarity between pairs of entities of the same type recognized by the entity extractor (i.e., person names, locations, and organization names which are typically described by short strings).</p></div>
<div><head n="7.">Graph storage</head><p>The DBMS used to store our graphs (Figure <ref type="figure" target="#fig_1">2</ref>) is <software ContextAttributes="used">PostgreSQL</software>, accessed through JDBC. This is a mature, efficient and free tool, running on a variety of platforms, thus meeting our requirement R3 from the Introduction <ref type="foot" target="#foot_7">7</ref> .</p><p>The table Nodes(id, label, type, datasource, label, normaLabel, representative) stores the basic attributes of a node, the ID of its data source, its normalized label, and its representative's ID. For nodes not equivalent to any other, the representative is the ID of the node itself. As explained previously, the representative attribute allows encoding information about equivalent nodes. Table Edges(id, source, target, label, datasource, confidence) stores the graph edges derived from the data sources, as well as the extraction edges connecting entity nodes with the parent(s) from which they have been extracted. Finally, the Similar(source, target, similarity) table stores a tuple for each similarity comparison whose result is above the threshold but less than 1.0. The pairs of nodes to be compared for similarity are retrieved by means of SQL queries.</p><p>This relational store is encapsulated behind a Graph interface, which can be easily implemented differently to take advantage of other storage engines. When creating a graph, storage is far from being the bottleneck (as our experiments in Section 11.2.1 show); on the contrary, when querying the graph (see below), the relational store incurs a relatively high access cost for each edge. We currently mitigate this problem using a memory cache for nodes and edges within the ConnectionLens application.</p></div>
<div><head n="8.">Querying the graph</head><p>We formalize the keyword search problem over a graph built out of heterogeneous datasets as previously described.</p></div>
<div><head n="8.1.">Search problem</head><p>We consider a graph G = (N, E) and we denote by L the set of all the labels of G nodes, plus the empty label (see Figure <ref type="figure" target="#fig_2">3</ref>). Let W be the set of keywords, obtained by stemming the label set L; a search query is a set of keywords Q = {w 1 , ..., w m }, where w i ∈ W . We define an answer tree (AT, in short) as a set t of G edges which (i) together, form a tree (each node is reachable from any other through exactly one path), (ii) for each w i , contain at least one node whose label matches w i . Here, the edges are considered undirected, that is:</p><formula xml:id="formula_2">n 1 a - → n 2 b ← -n 3 c - → n 4 is a sample AT, such that for all w i ∈ Q, there is a node n i ∈ t such that w i ∈ λ(n i ).</formula><p>We treat the edges of G as undirected when defining the AT to allow more query results on a graph built out of heterogeneous content whose structure is not well-known to users. Further, we are interested in minimal answer trees, that is: (i) removing an edge from the tree should make it lack one or more of the query keywords w i ; (ii) if a query keyword w i matches the label of more than one node in the answer tree, then all these matching nodes must be equivalent.</p><p>Condition (ii) is specific to the graph we consider, built from several data sources connected by equivalence or similarity edges. In classical graph keyword search problems, each query keyword is matched exactly once in an answer (otherwise, the tree is considered non-minimal). In contrast, our answer trees may need to traverse equivalence edges, and if w i is matched by one node connected by such an edge, it is also matched by the other. For instance, consider the three-keyword query "Gyucy Balkany Levallois" in Figure <ref type="figure" target="#fig_2">3</ref>: the keyword Balkany is matched by the two nodes labeled "P. Balkany" which are part of the answer. As a counter-example, consider the query "Balkany Centrafrique" in Figure <ref type="figure" target="#fig_2">3</ref>, assuming the keyword Centrafrique is also matched in the label "Central African Republic" <ref type="foot" target="#foot_8">8</ref> . Consider the tree that connects a "P. Balkany" node with "Centrafrique", and also traverses the edge between "Centrafrique" and "Central African Republic": this tree is not minimal, thus it is not an answer. The intuition for rejecting it is that "Centrafrique" and "Central African Republic" are not necessarily equivalent (we have a similarity, not an equivalence edge). Therefore the query keyword "Centrafrique" is matched by two potentially different things in this answer, making it hard to interpret.</p><p>A direct consequence of minimality is that in an answer, each and every leaf matches a query keyword. A graph may hold several minimal answer trees for a given query. We consider available a scoring function which assigns a higher value to more interesting answer trees (see <ref type="bibr">Section 9)</ref>.</p><p>Problem Statement. Given the graph G built out of the datasets D and a query Q, return the k highest-score minimal answer trees.</p><p>An AT may potentially span over the whole graph, (also) because it can traverse G edges in any direction; this makes the problem challenging.</p></div>
<div><head n="8.2.">Search space and complexity</head><p>The problem that we study is related to the (Group) Steiner Tree Problem, which we recall below.</p><p>Given a graph G with weights (costs) on edges, and a set of m nodes n 1 , . . . , n m , the Steiner Tree Problem (STP) <ref type="bibr" target="#b18">[19]</ref> consists of finding the smallestcost tree in G that connects all the nodes together. We could answer our queries by solving one STP problem for each combination of nodes matching the keywords w 1 , . . . , w m . However, there are several obstacles left: ( ) STP is a known NP-hard problem in the size of G, denoted |G|; ( ) as we consider that each edge can be taken in the direct or reverse direction, this amounts to "doubling" every edge in G. Thus, our search space is 2 |G| larger than the one of the STP, or that considered in similar works, discussed in Section 12. This is daunting even for small graphs of a few hundred edges; ( ) we need the k smallest-cost trees, not just one; (•) each keyword may match several nodes, not just one.</p><p>The closely related Group STP (GSTP, in short) <ref type="bibr" target="#b18">[19]</ref> is: given m sets of nodes from G, find the minimum-cost subtree connecting one node from each of these sets. GSTP does not raise the problem (•), but still has all the others.</p><p>In conclusion, the complexity of the problem we consider is extremely high. Therefore, fully solving it is unfeasible for large and/or high-connectivity graphs. Instead, our approach is: (i) Attempt to find all answers from the smallest (fewest edges) to the largest. Enumerating small trees first is both a practical decision (we use them to build larger ones) and fits the intuition that we should not miss small answers that a human could have found manually. However, as we will explain, we still "opportunistically" build some trees before exhausting the enumeration of smaller ones, whenever this is likely to lead faster to answers. The strategy for choosing to move towards bigger instead of smaller tress leaves room for optimizations on the search order. (ii) Stop at a given time-out or when m answers have been found, for some m ≥ k; (iii) Return the k top-scoring answers found.</p></div>
<div><head n="9.">Scoring answer trees</head><p>We now discuss how to evaluate the quality of an answer. Section 9.1 introduces the general notion of score on which we base our approach. Sec-tion 9.2 describes the metric that we attach to edges to instantiate this score, and Section 9.3 details the actual score function we used.</p></div>
<div><head n="9.1.">Generic score function</head><p>We have configured our problem setting to allow any scoring function, which enables the use of different scoring schemes fitting different users' requirements. As a consequence, this approach allows us to study the interaction of the scoring function with different properties of the graph.</p><p>Given an answer tree t to a query Q, we consider a score function consisting of (at least) the following two components. First, the matching score ms(t), which reflects the quality of the answer tree, that is, how well its leaves match the query terms. Second, the connection score cs(t), which reflects the quality of the tree connecting the edges. Any formula can be used here, considering the number of edges, the confidence or any other property attached to edges, or a query-independent property of the nodes, such as their PageRank or betweenness centrality score, etc.</p><p>The score of t for Q, denoted s(t), is computed as a combination of the two independent components ms(t) and cs(t). Popular combination functions (a weighted sum, or product, etc.) are monotonous in both components, however, our framework does not require monotonicity. Finally, both ms(t) and cs(t) can be tuned based on a given user's preferences, to personalize the score or make them evolve in time through user feedback etc.</p></div>
<div><head n="9.2.">Edge specificity</head><p>We now describe a metric on edges, which we used (through the connection score cs(t)) to favor edges that are "rare" for both nodes they connect. This metric was inspired by our experiments with real-world data sources and it helped return interesting answer trees in our experience.</p><p>For a given node n and label l, let N l →n be the number of l-labeled edges entering n, and N l n→ the number of l-labeled edges exiting n. The specificity of an edge e = n 1 l -→ n 2 is defined as:</p><formula xml:id="formula_3">s(e) = 2/(N l n 1 → + N l →n 2 ).</formula><p>Specificity is 1.0 for edges that are "unique" for both their source and their target and decreases when the edge does not "stand out" among the edges of these two nodes. For instance, the city council of Levallois-Perret comprises only one mayor (and one individual cannot be mayor of two cities in France). Thus, the edge from the city council to P. Balkany has a specificity of 2/(1.0 + 1.0) = 1.0. In contrast, there are 54 countries in Africa (we show only two), and each country is in exactly one continent; thus, the specificity of the dbo:partOf edges in the DBPedia fragment, going from the node named Morocco (or the one named Central African Republic) to the node named Africa is 2/(1 + 54) .036.</p></div>
<div><head n="9.3.">Concrete score function</head><p>We have implemented the following prototype scoring function in our system. For an answer t to the query Q, we compute the matching score ms(t) as the average, over all query keywords w i , of the similarity between the t node matching w i and the keyword w i itself; we used the edit distance.</p><p>We compute the connection score cs(t) based on edge confidence, on the one hand, and edge specificity on the other. We multiply the confidence values, since we consider that uncertainty (confidence &lt; 1) multiplies; and we also multiply the specificities of all edges in t, to discourage many lowspecificity edges. Specifically, our score is computed as:</p><formula xml:id="formula_4">score(t, Q) = α • ms(t, Q) + β • e∈E c(e) + (1 -α -β) • e∈E s(e)</formula><p>where α, β are parameters of the system such that 0 ≤ α, β &lt; 1 and α+β ≤ 1.</p></div>
<div><head n="10.">Answering keyword queries</head><p>We now present our approach for computing query answers on the graph, which integrates the heterogeneous datasets.</p></div>
<div><head n="10.1.">Grow and Merge</head><p>Our algorithm relies on concepts from prior literature <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b24">25]</ref> while exploring many more trees. Specifically, it starts from the sets of nodes N 1 , . . . , N m where the nodes in N i all match the query keyword w i ; each node n i,j ∈ N i forms a one-node partial tree. For instance, in Figure <ref type="figure" target="#fig_2">3</ref>, one-node trees are built from the nodes with boldface text, labeled "Africa", "Real Estate" and "I. Balkany". We identify two transformations that can be applied to form increasingly larger trees, working toward query answers: Grow(t, e), where t is a tree, e is an edge adjacent to the root of t, and e does not close a loop with a node in t, creates a new tree t having all the edges of t plus e; the root of the new tree is the other end of the edge e. For instance, starting from the node labeled "Africa", a Grow can add the edge labeled dbo:name. Merge(t 1 , t 2 ), where t 1 , t 2 are trees with the same root, whose other nodes are disjoint, and matching disjoint sets of keywords, creates a tree t with the same root and with all edges from t 1 and t 2 . Intuitively, Grow moves away from the keywords, to explore the graph; Merge fuses two trees into one that matches more keywords than both t 1 and t 2 .</p><p>In a single-dataset context, Grow and Merge have the following properties. (gm 1 ) Grow alone is complete (guaranteed to find all answers) for k = 1, 2 only; for higher k, Grow and Merge together are complete. (gm 2 ) Using Merge steps helps to find answers faster than using just Grow <ref type="bibr" target="#b24">[25]</ref>: partial trees, each starting from a leaf that matches a keyword, are merged into an answer as soon as they have reached the same root. (gm 3 ) An answer can be found through multiple combinations of Grow and Merge. For instance, consider a linear graph n 1 → n 2 → . . . n p and the two-keyword query {a 1 , a p } where a i matches the label of n i . The answer is obviously the full graph. It can be found: starting from n 1 and applying p -1 Grow steps; starting from n p and applying p -1 Grow steps; and in p -2 ways of the form Merge(Grow(Grow. . . ), Grow(Grow. . . )), each merging in an intermediary node n 2 , . . . , n p-1 . These are all the same according to our definition of an answer (Section 8.1), which does not distinguish a root in an answer tree; this follows users' need to know how things are connected, and for which the tree root is irrelevant.</p></div>
<div><head n="10.2.">Adapting to multi-datasets graphs</head><p>The changes we brought for our harder problem (bidirectional edges and multiple interconnected datasets) are as follows. 1. Bidirectional growth. We allow Grow to traverse an edge both going from the source to the target, and going from the target to the source. 2. Many-dataset answers. As defined in a single-dataset scenario, Grow and Merge do not allow to connect multiple datasets. To make that possible, we need to enable one, another, or both to also traverse similarity and equivalence edges (shown in solid or dotted red lines in Figure <ref type="figure" target="#fig_2">3</ref>). We decide to simply extend Grow to allow it to traverse not just data edges, but also similarity edges between nodes of the same or different datasets. We handle equivalence edges as follows: Grow-to-representative Let t be a partial tree developed during the search, rooted in a node n, such that the representative of n is a node n rep = n. Grow2Rep creates a new tree by adding to t the edge n ≡ -→ n rep ; this new tree is rooted in n rep . If n is part of a group of p equivalent nodes, only one Grow2Rep step is possible from t, to the unique representative of n; Grow2Rep does not apply again on Grow2Rep(t), because the root of this tree is n rep , which is its own representative.</p><p>Together, Grow, Grow2Rep and Merge enable finding answers that span multiple data sources, as follows: Grow allows exploring data edges within a dataset and similarity edges within or across datasets. Grow2Rep goes from a node to its representative when they differ; the representative may be in a different dataset. Merge merges trees with the same root: when that root represents p equivalent nodes, this allows connecting partial trees, including Grow2Rep results, containing nodes from different datasets. Thus, Merge can build trees spanning multiple datasets.</p><p>One potential performance problem remains. Consider again p equivalent nodes n 1 , . . . , n p ; assume without loss of generality that their representative is n 1 . Assume that during the search, a tree t i is created rooted in each of these p nodes. Grow2Rep applies to all but the first of these trees, creating the trees t 2 , t 3 , . . . , t p , all rooted in n 1 . Now, Merge can merge any pair of them, and can then repeatedly apply to merge three, then four such trees, etc., as they all have the same root n 1 . The exponential explosion of Grow trees, avoided by introducing Grow2Rep, is still present due to Merge.</p><p>We solve this problem as follows. Observe that in an answer, a path of two or more equivalence edges of the form n 1 ≡ -→ n 2 ≡ -→ n 3 such that a node internal to the path, e.g. n 2 , has no other adjacent edge, even if allowed by our definition, is redundant. Intuitively, such a node brings nothing to the answer since its neighbors, e.g., n 1 and n 3 , could have been connected directly by a single equivalence edge, thanks to the transitivity of equivalence. We call non-redundant an answer that does not feature any such path, and decide to search for non-redundant answers only.</p><p>The following properties hold on non-redundant answers:</p><p>Property 1. There exists a graph G and a k-keyword query Q such that a non-redundant answer contains k -1 adjacent equivalence edges (edges that, together, form a single connected subtree).</p><p>We prove this by exhibiting such an instance. Let G be a graph of 2k nodes shown in Figure <ref type="figure">4 (a)</ref>, such that all the x i are equivalent, and consider the k-keyword query Q = {a 1 , . . . , a k } (each keyword matches exactly the respective a i node). An answer needs to traverse all the k edges from a i to x i , and then connect the nodes x i , . . . , x k ; we need k -1 equivalence edges for this. We prove this by induction over k. For k = 1, each answer has 1 node and 0 edge (trivial case). Now, consider this true for k and let us prove it for k + 1. Assume by contradiction that a non-redundant answer t Q to a query Q of k + 1 keywords comprises k + 1 adjacent equivalence edges. Let Q be the query having only the first k keywords of Q, and t be a of t that is a non-redundant answer to Q :</p><p>• t exists, because t connects all Q keywords, thus also the Q keywords;</p><p>• t is non-redundant, because all its edges are in the (non-redundant) t.</p><p>By the induction hypothesis, t has at most k -1 adjacent equivalence edges. This means that there are two adjacent equivalent edges in t \ t .</p><p>1. If these edges, together, lead to two distinct leaves of t, then t has two leaves not in t . This is not possible, because by definition of an answer, t has k + 1 leaves (each matching a keyword) and similarly t has k leaves.</p><p>2. It follows, then, that the two edges lead to a single leaf of t, therefore the edges form a redundant path. This contradicts the non-redundancy of t, and concludes our proof.</p><p>Property 2 gives us an important way to control the exponential development of trees due to p equivalent nodes. Grow, Grow2Rep and Merge, together, can generate trees with up to k (instead of k -1) adjacent equivalence edges. This happens because Grow2Rep may "force" the search to visit the representative of a set of k equivalent nodes (see Figure <ref type="figure">4</ref>(b), assuming x 1 is the representative of all the equivalent x i s, and the query {a 2 , . . . , a k }). The resulting answer may be redundant if the representative has no other adjacent edges in the answer other than equivalence edges. In such cases, in a post-processing step, we remove from the answer the representative and its equivalence edges, then reconnect the respective equivalent nodes using k -1 equivalence edges. This guarantees to obtain a non-redundant tree, such as the one in Figure <ref type="figure">4</ref>(c).</p></div>
<div><head n="10.3.">The GAM algorithm</head><p>We now have the basic exploration steps we need: Grow, Grow2Rep and Merge. In this section, we explain how we use them in our integrated keyword search algorithm.</p><p>We decide to apply in sequence: one Grow or Grow2Rep (see below), leading to a new tree t, immediately followed by all the Merge operations possible on t. Thus, we call our algorithm Grow and Aggressive Merge (GAM, in short). We merge aggressively to detect as quickly as possible when some of our trees, merged at the root, form an answer.</p><p>Given that every node of a currently explored answer tree can be connected with several edges, we need to decide which Grow (or Grow2Rep) to apply at a certain point. For that, we use a priority queue U in which we add (tree, edge) entries: for Grow, with the notation above, we add the (t, e) pair, while for Grow2Rep, we add t together with the equivalence edge leading to the representative of t's root. In both cases, when a (t, e) pair is extracted from U , we just extend t with the edge e (adjacent to its root), leading to a new tree t G , whose root is the other end of the edge e. Then we aggressively merge t G with all compatible trees explored so far. Finally, we read from the graph the (data, similarity, or equivalence) edges adjacent to t G 's root and add to U more (tree, edge) pairs to be considered further during the search. The algorithm then picks the highest-priority pair in U and reiterates; it stops when U is empty, at a timeout, or when a maximum number of answers are found (whichever comes first).</p><p>The last parameter impacting the exploration order is the priority used in U : at any point, U gives the highest-priority (t, e) pair, which determines the operations performed next. i. Let N T be a set of new trees obtained from the Merge (initially ∅). ii. Let p 1 be the keyword set of t iii. For each keyword subset p 2 that is a key within K, and such that p 1 ∩ p 2 = ∅ A. For each tree t i that corresponds to p 2 , try to merge t with t i . Process any possible result; if it is new (not in E previously), add it to N T . (d) Re-plenish U (add more entries in it) as in step 3, based on the trees {t } ∪ N T . 2. At the same number of matched keywords, smaller trees are preferable in order not to miss small answers;</p><p>3. Finally, among (t 1 , e 1 ), (t 2 , e 2 ) with the same number of nodes and matched keywords, we prefer the pair with the higher specificity edge.</p><p>Algorithm details Beyond the priority queue U described above, the algorithm also uses a memory of all the trees explored, called E. It also organizes all the (non-answer) trees into a map K in which they can be accessed by the subset of query keywords that they match. The algorithm is shown in pseudocode in Figure <ref type="figure" target="#fig_4">5</ref>, following the notations introduced in the above discussion.</p><p>While not shown in Figure <ref type="figure" target="#fig_4">5</ref> to avoid clutter, the algorithm only develops minimal trees (thus, it only finds minimal answers). This is guaranteed:</p><p>• When creating Grow and Grow2Rep opportunities (steps 3 and 4d):</p><p>we check not only that the newly added does not close a cycle, but also that the matches present in the new tree satisfy our minimality condition (Section 8.1).</p><p>• Similarly, when selecting potential Merge candidates (step 4(c)iiiA).</p></div>
<div><head n="11.">Experimental evaluation</head><p>The algorithms described above are implemented in the Connection-Lens prototype, available online. Below, we report the results of an experimental evaluation we carried out to study the performance of its algorithms, as well as quality aspects of the constructed graphs. Section 11.1 describes the software and hardware setup, and our datasets. Section 11.2 focuses on graph construction, while Section 11.3 targets keyword query answering.</p></div>
<div><head n="11.1.">Software, hardware, and datasets</head><p>ConnectionLens is a Java application (44.700 lines) that relies on a relational database to store the constructed graphs and as a back-end used by the query algorithm. It features controllable-size caches to keep in memory as many nodes and edges as possible; this allows adapting to different memory sizes. It also comprises Python <software ContextAttributes="used">code</software> (6.300 lines) which implements entity extraction (Section 4) and content extraction from PDF documents to JSON (see <ref type="bibr" target="#b7">[8]</ref>), tasks for which the most suitable libraries are in Python.</p><p>The Flair extractor (Section 4) and the disambiguator (Section 5) are Web services which ConnectionLens calls. The former is deployed on the machine where ConnectionLens runs. We deployed the latter on a dedicated Inria server, adapting the original <software>Ambiverse</software> code to our new pipeline introduced in Section 5; the disambiguator consists of 842 Java classes.</p><p>For our experiments, we used a regular server from 2016, equipped with 2x10-core Intel Xeon E5-2640 (Broadwell) CPUs clocked at 2.40GHz, and 128GB DRAM, which uses <software ContextAttributes="used">PostgreSQL</software> 12.4 to store the graph content in a set of tables. This is a medium-capacity machine without special capabilities; recall our requirement R3 that our algorithms be feasible on off-the-shelf hardware. We also used a GPU server from 2020, with a 2x16-core Intel Xeon Gold 5218 (Skylake) CPUs clocked at 2.30GHz, an NVIDIA Tesla V100 GPU and 128GB DRAM. To show our software's applicability to standard hardware configurations, we focus on the results that we obtained with our regular server. However, we also include some results on the more advanced server to show that our platform adapts seamlessly to modern as well as heterogeneous hardware, which includes both CPUs and GPUs. When needed to separate them, we will refer to each server with its CPU generation name. Data sources Most of our evaluation is on real-world datasets, described below from the smallest to the largest (measuring their size on disk before being input to ConnectionLens). 1. We crawled the French online newspaper <software ContextAttributes="used">Mediapart</software> and obtained 256 articles for the search keywords "corona, chloroquine, covid" (256 documents), and 886 for "economie, chomage, crise, budget" (1142 documents and 18.4 MB overall). 2. An XML document <ref type="foot" target="#foot_9">9</ref>comprising business interest statements of French public figures, provided by HATVP (Haute Autorité pour la Transparence de la Vie Publique); the file occupies 35 MB. 3. A subset of the YAGO 4 <ref type="bibr" target="#b45">[46]</ref> RDF knowledge base, comprising entities present in the French Wikipedia and their properties; this takes 2.49 GB on disk (17.36 M triples). For a fine-granularity, controlled study of our query algorithm, we also devised a set of small synthetic graphs, described in Section 11.3.</p></div>
<div><head n="11.2.">Construction evaluation</head><p>We present the results of our experiments measuring the performance and quality of the graph construction modules. We study graph construction performance in Section 11.2.1, the quality of our information extraction in Section 11.2.2, and that of disambiguation in Section 11.2.3.</p></div>
<div><head n="11.2.1.">Graph construction</head><p>We start by studying the impact of node factorization (Section 3.2) on the number of graph nodes and the graph storage time. For that, we rely on the XML dataset, and disable entity extraction, entity disambiguation, and node matching. Its (unchanged) number of edges |E| is 1.588.839. For each type of loading, we report the number of nodes |N |, the time spent storing nodes and edges to disk T DB , and the total running time T in Table <ref type="table">6</ref>.  Moving from per-instance to per-path node creation reduces the number of nodes by a third. However, this introduces some errors, as the dataset features many null codes (Section 3.2); for instance, with per-instance value creation, there are 1.113 nodes labeled "néant" (meaning "nonexistent"), 32.607 nodes labeled "Données non publiées" (unavailable information) etc. Using per-path, the latter are reduced to just 1.154, which means that in the dataset, "Données non publiées" appears 32.607 times on 1.154 different paths. However, this factorization, which introduces connections between the XML nodes which are parents of this "null" value, may be seen as wrong. When the null codes were input to ConnectionLens, such nodes are no longer unified; the number of nodes increases, and so does the storage time. In this graph, consisting of one data source, per-dataset and per-graph produce the same number of nodes, overall the smallest; it also increases when null codes are not factorized. We conclude that per-graph value creation combined with null code detection is a practical alternative.</p></div>
<div><head>Value node creation policy</head><formula xml:id="formula_5">|N | T DB (s) T (s) Per-instance<label>1</label></formula><p>Next, we study the impact of named entity extraction (Section 4) and disambiguation (Section 5) on the graph construction time.</p><p>For this, we load 100.000 triples from our Yago subset, with per-graph factorization, natural for the RDF data model where each literal or URI denotes one node in the RDF graph. In Figure <ref type="figure" target="#fig_6">7</ref>, we load the triples using several configurations: without any entity extraction (NONE); with <software ContextAttributes="used">SNER</software> entity extraction, without and then with disambiguation; with FLAIR entity extraction, without and then with disambiguation. While Flair is slower, its results are qualitatively much better than those obtained with <software ContextAttributes="used">SNER</software> (see Section 11.2.2 below). To make it faster, we also implement a batch extraction mechanism whereas l B labels are input a time to each extraction service, to take advantage of the parallel processing capacity available in current CPUs. In Figure <ref type="figure" target="#fig_6">7</ref>, in the "FLAIR, BATCH" column, we used l B = 128 which maximized performance in our experiments. A second optimization leverages the fact that so-called sequence to sequence (seq2seq) models such as that used in our Flair extractor, when given a batch of inputs, pad the shortest ones to align them to the longest input, and some computation effort is lost on useless padding tokens. Instead, if several batches, say n B , are received by the extraction service, it can re-group the inputs so that one call to the seq2seq model is made over inputs of very similar length, thus no effort is wasted. In our experiments, we used n B = 10.  Storing the graph <software ContextAttributes="used">PostgreSQL</software> dominates the loading time in the absence of extraction, or when using <software ContextAttributes="used">SNER</software>. In contrast, Flair extraction takes more than one order of magnitude longer; batching reduces it by a factor of two. Finally, disambiguation, relying on computationally complex operations, takes longest; it also incurs a modest invocation overhead as it resides on a different server (with the regular server hardware described in Section 11.1), in the same fast network. Next, we study node matching (Section 6). For this, we loaded the XML dataset, which comes from individual declaration of interest, filled in by hundreds of users, with numerous spelling variations, small errors and typos. Loaded per-graph mode, with batched Flair extraction, the resulting graph has 1.102.209 nodes and 1.615.291 edges. We test two configurations: comparing all leaf node pairs to find possible similarity edges, respectively, comparing only entity pairs. On the regular server, in both cases, data stor-age took 3 minutes, and extraction 13 minutes. When all comparisons are made, they take 39 minutes, dominating the total time of 56 minutes. A total of 28.875 similar pairs are found to be above our similarity thresholds, and lead to the same number of cl:sameAs edges stored in the graph, together with the respective similarity values. Only 748 among these are entity pairs; the others are pairs of similar strings. This confirms the interest of node matching; we hope to reduce its cost further by using techniques such as Locality-Sensitive Hashing (LSH). To study the scalability of our loading process, we loaded our YAGO subset by slices of 1M triples and measured the running time for these increasing data sizes, using the best extractor (Flair) with the same batch size(s) as above. Figure <ref type="figure" target="#fig_8">8</ref> shows the loading time as the data grows, in three different hardware settings: on our regular server, our more powerful GPU server disabling GPU use, and the same exploiting the GPU. Figure <ref type="figure" target="#fig_8">8</ref> shows that loading time scales linearly in the data volume on all configurations, and batching helps make the most out of our regular server. Complete graph Finally, we loaded all the data sources described in Section 11.1 in a single graph, using per-graph node creation, batched Flair extraction, and disambiguation for HTML and XML (not for the RDF Yago subset, whose literals are already associated to URIs). The graph has |N | = 8.019.651 nodes (including 677.459 person entities, 275.316 location entities, and 61.452 organization entities), and |E| = 20.642.207 edges. Many entities occur across data sources, e.g., 330 person entities occur, each, in at least 10 sources; the French president E. Macron occurs in 183 distinct sources. All these lead to interconnections across sources. On our fastest (GPU) hardware configuration, loading the RDF data took 128 minutes; the HTML articles another 260, and the XML document 96 minutes more. The last two times reflect the relatively high cost of disambiguation (recall Figure <ref type="figure" target="#fig_6">7</ref>). Connec-tionLens users can turn it off when it is not needed (e.g., if users feel they know the real-world entities behind entity labels encountered in the graph), or trigger it selectively, e.g., on organizations but not on people nor locations etc.</p></div>
<div><head n="11.2.2.">Named-Entity Recognition quality</head><p>Due to the unavailability of an off-the-shelf, good-quality entity extractor for French text, we decided to train a new model. To decide the best NLP framework to use, we experimented with the Flair <ref type="bibr" target="#b0">[1]</ref> and SpaCy (https: //spacy.io/) frameworks. Flair allows combining several embeddings, which can lead to significant quality gains. Following <ref type="bibr" target="#b0">[1]</ref>, after testing different word embedding configurations, we trained a Flair model using stacked forward and backward French Flair embeddings with French fastText embeddings on the WikiNER dataset. We will refer to this model as Flair-SFTF.</p><p>Below, we describe a qualitative comparison of Flair-SFTF with the French Flair and SpaCy pre-trained models. The French pre-trained Flair model is trained with the WikiNER dataset, and uses French character embeddings trained on Wikipedia, and French fastText embeddings. As for SpaCy, two pre-trained models are available for French: a medium (SpaCymd ) and a small one (SpaCy-sm). They are both trained with the WikiNER dataset and the same parameterization. The difference is that SpaCy-sm does not include word vectors, thus, in general, SpaCy-md is expected to perform better, since word vectors will most likely impact the model performance positively. Our evaluation also includes the model previously present in ConnectionLens <ref type="bibr" target="#b9">[10]</ref>, trained using Stanford NER <ref type="bibr" target="#b16">[17]</ref>, with the Quaero Old Press Extended Named Entity corpus <ref type="bibr" target="#b17">[18]</ref>.</p><p>We measured the precision, recall, and F 1-score of each model using the <software ContextAttributes="used">conlleval</software> evaluation script, previously used for such tasks <ref type="foot" target="#foot_10">10</ref> . <software ContextAttributes="used">conlleval</software> evaluates exact matches, i.e., both the text segment of the proposed entity and its type, need to match "gold standard" annotation, to be considered correct. Precision, recall, and F 1-score (harmonic mean of precision and recall) are computed for each named-entity type. To get an aggregated, single quality measure, <software ContextAttributes="used">conlleval</software> computes the micro-average precision, recall, and F 1-score over all recognized entity instances, of all named-entity types.</p><p>For evaluation, we used the entire <software ContextAttributes="used">FTBNER</software> dataset <ref type="bibr" target="#b40">[41]</ref>. We pre-processed it to convert its entities from the seven types they used, to the three we consider, namely, persons, locations and organizations. After pre-processing, the dataset contains 12K sentences and 11K named-entities (2K persons, 3K locations and 5K organizations). The evaluation results are shown in Table <ref type="table" target="#tab_2">1</ref>. All models perform better overall than the Stanford NER model previously used in ConnectionLens <ref type="bibr" target="#b9">[10]</ref>, which has a micro F 1-score of about 45%. The SpaCy-sm model has a slightly better overall performance than SpaCy-md, with a small micro F 1score difference of 0.28%. SpaCy-md shows higher F 1-scores for locations and organizations, but is worse on people, driving down its overall quality. All Flair models surpass the micro scores of SpaCy models. In particular, for people and organizations, Flair models show more than 11% higher F 1scores than SpaCy models. Flair models score better on all named-entity types, except for locations when comparing the SpaCy models, specifically, with the Flair-pre-trained. Flair-SFTF has an overall F 1-score of 73.31% and has better scores than the Flair-pre-trained for all metrics and namedentity types, with the exception of the recall of organizations, lower by 1.06%. In conclusion, Flair-SFTF is the best NER model we evaluated.</p></div>
<div><head>Entities</head></div>
<div><head n="11.2.3.">Disambiguation quality</head><p>We now evaluate the quality of the disambiguation module. As mentioned in Section 5, our module works for both English and French.</p><p>The performance for English has been measured on the CoNLL-YAGO dataset <ref type="bibr" target="#b25">[26]</ref>, by the developers of <software ContextAttributes="used">Ambiverse</software>. They report a micro-accuracy of 84.61% and a macro-accuracy of 82.67%. To the best of our knowledge, there is no labeled corpus for entity disambiguation in French. Thus we evaluate the module's performance on the <software ContextAttributes="used">FTBNER</software> dataset previously introduced. <software ContextAttributes="used">FTBNER</software> consists of sentences annotated with named entities. The disambiguation module takes a sentence, the type, and offsets of the entities extracted from it, and returns for each entity either the URI of the matched entity or an empty result if the entity was not found in the KB. In our experiment, 19% of entities have not been disambiguated, more precisely 22% of organizations, 29% of persons, and 2% of locations. For a fine-grained error analysis, we sampled 150 sentences, and we manually verified the disambiguation results (Figure <ref type="figure" target="#fig_9">9</ref>). The module performs very well, with excellent results for locations (F 1 = 98.01%), followed by good results for organizations (F 1 = 82.90%) and for persons (F 1 = 76.62%). In addition to these results, we obtain a micro-accuracy of 90.62% and a macro-accuracy of 90.92%. The performance is comparable with the one reported by the <software ContextAttributes="used">Ambiverse</software> authors for English. We should note that the improvement for French might be due to our smaller test set.  This section presents the results that we obtained by using synthetic and real-world datasets. In each case, we first describe the datasets, and then we present and explain our findings. We bound the query execution time to 120 seconds, after which the algorithm stops searching for matches.</p></div>
<div><head n="11.3.1.">Queries on synthetic datasets</head><p>We first study the performance of our algorithm on different types of synthetic datasets. The first type is the line graph, where every node is connected with two others, having one edge for each node, except two nodes connected with only one. We use the line graph to clearly show the performance of Grow and Merge operations with respect to the graph size. The second type is the chain graph, which is the same as the line, but it has two edges (instead of one) connecting every pair of nodes. We use the chain graph to show the algorithm's performance as we double the number of edges of the line graph, and we give more options to Grow and Merge. The third type is the star graph, where we have several line graphs connected through a strongly connected cluster of nodes with a representative. We use this type to show the performance of Grow2Rep, by placing the query keywords on different line graphs. The fourth type is a random graph based on the Barabasi-Albert (BA) model <ref type="bibr" target="#b3">[4]</ref>, which generates scale-free networks with only a few nodes (hubs) of the graph having a much higher degree than the rest. The graph in this model is created in a two-stage process. During the first stage, a network of some nodes is created. Then, during the second stage, new nodes are inserted in the graph and they are connected to nodes created during the first stage. We set every node created at the second stage to be connected with exactly one node created at the first stage. In the following, The black line shows the time elapsed until the first answer is found, whereas the grey line shows the overall execution time.</p><p>Figure <ref type="figure" target="#fig_10">10</ref> includes the results that we obtained by querying the synthetic datasets. The black line shows the time elapsed until the first answer is found, whereas the grey line shows the overall execution time.</p><p>Figure <ref type="figure" target="#fig_10">10a</ref> shows the execution time of our algorithm when executing a query with two keywords on a line graph, as we vary the number of nodes of the graph. We place the keywords on the two "ends" of the graph to show the distance's impact on the execution time. Our algorithm's performance is naturally affected by the size of the graph, as it generates 2 • N answer trees, where N is the number of nodes. Given that this is a line graph, there is only one answer, which is the whole graph, and, therefore, the time to find the first answer is also the overall execution time.</p><p>Figure <ref type="figure" target="#fig_10">10b</ref> shows the performance of our algorithm on a chain graph. The execution times for the first answer are almost the same, as the graph size increases slowly. Instead, the overall execution times increase at a much higher (exponential) rate; note the logarithmic scale of the y axis. The reason is that every pair of nodes is connected with two edges, which increases the number of answers exponentially with the number of nodes in the graph.</p><p>In Figure <ref type="figure" target="#fig_10">10c</ref>, we report the execution time for the star graph. We place keywords in two different lines connected through the center of the graph, forcing the algorithm to use Grow2Rep, whereas in the previous cases it only had to use Grow and Merge. The number of branches, depicted on the x axis, corresponds to the number of line graphs connected in the star. Each line graph has 10 nodes, and we place the query keywords at the extremities of two different line graphs. The number of merges is exponential to the number of branches, that is O(2 K ) where K is the number of branches since the algorithm will check all possible answers. This behavior is clearly shown in both lines of Figure <ref type="figure" target="#fig_10">10c</ref>, where on the y axis (in logarithmic scale) we show the times to find the first, and, respectively, all answers. Above 12 branches, the timeout of 120 seconds that we have set is hit and, thus, the search is terminated, as shown when we search for all answers.</p><p>Figure <ref type="figure" target="#fig_10">10d</ref> depicts the query performance with the Barabasi-Albert model. We fix the graph size to 2000 nodes and we vary the position of two keywords, by choosing nodes that have a distance, as given in the x axis; note the logarithmic y axis. As the graph is randomly generated within the BA model, we note some irregularity in the time to the first solution, which, however, grows at a moderate pace as the distance between the keyword node grows. The overall relationship between the time to the first solution and the total time confirms that the search space is very large but that most of the exploration is not needed, since the first solution is found quite fast.</p></div>
<div><head n="11.3.2.">Queries on the complete, real-world graph</head><p>Next, we describe results that we obtained querying a graph obtained by loading all the real-world data sources described in Section 11.1. We report our findings in Table <ref type="table" target="#tab_4">2</ref>. The queries feature terms that appear in recent French news; they are related to the economy, the Covid crisis, world events, and/or French politics. In most queries, keywords are exact entity names, with their most common spelling. In other cases, we allowed node labels to approximately match a keyword (based on <software ContextAttributes="used">PostgreSQL</software>' stemming and string pattern matching); such keywords are shown in italic in the table. Again, we gave a timeout of 2 minutes, and on this large graph, all the GAM searches stopped at a time-out. The table shows that queries return a varied number of answers, but many ATs are developed in all cases, and the first is found quite before the timeout. Finally, an inspection of the results showed that most are obtained from different datasets, confirming the interest in linking datasets in ConnectionLens. These results show the feasibility and interest of GAMSearch on large heterogeneous graphs.</p></div>
<div><head n="12.">Related work and perspectives</head><p>Our work belongs to the area of data integration <ref type="bibr" target="#b13">[14]</ref>. Data integration can be achieved either in a warehouse fashion (consolidating all the data sources into a single repository), or in a mediator fashion (preserving the data in their original repository, and querying through a mediator module which distributes the work to each data source, and combines the results). In this work, in contrast with our previous platforms built for data journalism <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b5">6]</ref>, we (i) pursue a warehouse approach; (ii) base our architecture on a relational DBMS; (iii) simplify the query paradigm to keyword querying.</p><p>ConnectionLens integrates a wide variety of data sources: JSON, relational, RDF and text since <ref type="bibr" target="#b9">[10]</ref>, to which we have added XML, multidimensional tables, and PDF documents. We integrate such heterogeneous content in a graph, therefore, our work recalls the production of Linked Data. A significant difference is that we do not impose that our graph is RDF, and we do not assume, require, or use a domain ontology.</p><p>Graphs are also produced when constructing knowledge bases, e.g., Yago <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b45">46]</ref>. Our setting is more limited in that we are only allowed to integrate a given set of datasets that journalists trust to not "pollute" the database. Therefore, we use a KB only for disambiguation and accept (as stated in Section 1) that the KB does not cover some entities found in our input datasets. Our simple techniques for matching (thus, connecting) nodes are reminiscent of data cleaning, entity resolution <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b38">39]</ref>, and key finding in knowledge bases, e.g. <ref type="bibr" target="#b44">[45]</ref>. Much more elaborate techniques exist, notably, when the data is regular, its structure is known and fixed, an ontology is available, etc.; none of these holds in our setting.</p><p>Keyword search (KS) is widely used for searching in unstructured (typically text) data, and it is also the best search method for novice users, as witnessed by the enormous success of keyword-based search engines. As databases grow in size and complexity, KS has been proposed as a method for searching also in structured data <ref type="bibr" target="#b49">[50]</ref>, when users are not perfectly familiar with the data, or to get answers enabled by different tuple connections. For relational data, in <ref type="bibr" target="#b26">[27]</ref> and subsequent works, tuples are represented as nodes, and two tuples are interconnected through primary key-foreign key pairs. The resulting graphs are thus quite uniform, e.g., they consist of "Company nodes", "Employee nodes" etc. The same model was considered in <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b50">51]</ref>; <ref type="bibr" target="#b42">[43]</ref> also establishes links based on similarity of constants appearing in different relational attributes. As explained in Section 8.2, our problem is (much) harder since our trees can traverse edges in both directions, and paths can be (much) longer than those based on PK-FK alone. <ref type="bibr" target="#b48">[49]</ref> proposes to incorporate user feedback through active learning to improve the quality of answers in a relational data integration setting. We are working to devise such a learning-to-rank approach for our graphs, also.</p><p>KS has also been studied in XML documents <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b32">33]</ref>, where an answer is defined as a subtree of the original document, whose leaves match the query keywords. This problem is much easier than ours, since: (i) an XML document is a tree, guaranteeing just one connection between any two nodes; in contrast, there can be any number of such connections in our graphs;</p><p>(ii) the maximum size of an answer to a k-keywords query is k • h where h, the height of an XML tree, is almost always quite small, e.g., 20 is considered "quite high"; in contrast, with our bi-directional search, the bound is k • D where D is the diameter of our graph -which can be much larger.</p><p>Our Grow and Merge steps are borrowed from <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b24">25]</ref>, which address KS for graphs, assuming optimal-substructure, which does not hold for us, and single-direction edge traversal. For RDF graphs <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b29">30]</ref> traverse edges in their direction only; moreover, <ref type="bibr" target="#b29">[30]</ref> also make strong assumptions on the graph, e.g., that all non-leaf nodes have types, and that there are very few types (regular graph). In <ref type="bibr" target="#b10">[11]</ref>, the authors investigate a different kind of answers, called r-clique graphs, which they find using specific indexes.</p><p>Keyword search across heterogeneous datasets has been previously studied in <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b30">31]</ref>. However, in these works, each answer comes from a single dataset, that is, they never consider answers spanning over and combining multiple datasets, such as the one shown in Figure <ref type="figure" target="#fig_2">3</ref>. In turn, multiple datasets lead to equivalence and similarity edges; we have shown how to compactly encode the latter using representatives, and efficiently enumerate solutions which may span data, equivalence, and similarity edges.</p><p>A recent work <ref type="bibr" target="#b33">[34]</ref> leverages text data sources to find more answers to queries over knowledge graphs. An important difference is that we integrate all datasets prior to querying whereas in <ref type="bibr" target="#b33">[34]</ref>, text is accessed when required to complement the loaded data graph. The effort we invest in building the graphs pays off by making queries faster. Further, our work is more general in the data formats we support. Finally, <ref type="bibr" target="#b33">[34]</ref> applies a set of heuristics, tied to the nature of the graphs they used, to find the most relevant answers; our work does not make such assumptions.</p><p>(G)STP has been addressed under simplifications that do not hold in our context. For instance: the quality of a solution decreases exponentially with the tree size, thus search can stop when all trees are under a certain threshold <ref type="bibr" target="#b6">[7]</ref>; edges are considered in a single direction <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b29">30]</ref>; the cost function has the suboptimal-structure property <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b31">32]</ref> etc. These assumptions reduce the computational cost; in contrast, to leave our options open as to the best score function, we build a feasible solution for the general problem we study. Some works have focused on finding bounded (G)STP approximations, i.e., (G)STP trees solutions whose cost is at most f times higher than the optimal cost, e.g., <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b22">23]</ref>. Beyond the differences between our problem and (G)STP, due notably to the fact that our score is much more general (Section 9), non-expert users find it hard to set f .</p></div><figure xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Motivating example: collection D of four datasets.</figDesc><graphic coords="4,128.33,125.80,349.68,161.71" type="bitmap" /></figure>
<figure xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Outline of the ConnectionLens system architecture..</figDesc><graphic coords="8,130.28,125.80,349.68,131.13" type="bitmap" /></figure>
<figure xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Integrated graph corresponding to the datasets of Figure 1. An answer to the keyword query {"I. Balkany", Africa, Estate} is highlighted in light green; the labels three keyword matches in this answer are in bold font.</figDesc><graphic coords="9,128.33,125.80,349.68,192.80" type="bitmap" /></figure>
<figure xml:id="fig_3"><head>Figure 4 :Property 2 .</head><label>42</label><figDesc>Figure 4: Sample graph and answer trees.</figDesc><graphic coords="24,112.79,173.66,225.36,50.43" type="bitmap" /></figure>
<figure xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Outline of GAM algorithm</figDesc></figure>
<figure xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Impact of node factorization.</figDesc></figure>
<figure xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Graph construction time (seconds).</figDesc><graphic coords="30,110.85,361.18,233.13,122.35" type="bitmap" /></figure>
<figure xml:id="fig_7"><head>Figure 7</head><label>7</label><figDesc>Figure7shows the time taken by storage, extraction and disambiguation (when applied) and the total graph creation time; note the logarithmic y axis.Storing the graph PostgreSQL dominates the loading time in the absence of extraction, or when using SNER. In contrast, Flair extraction takes more than one order of magnitude longer; batching reduces it by a factor of two. Finally, disambiguation, relying on computationally complex operations, takes longest; it also incurs a modest invocation overhead as it resides on a different server (with the regular server hardware described in Section 11.1), in the same fast network. Next, we study node matching (Section 6). For this, we loaded the XML dataset, which comes from individual declaration of interest, filled in by hundreds of users, with numerous spelling variations, small errors and typos. Loaded per-graph mode, with batched Flair extraction, the resulting graph has 1.102.209 nodes and 1.615.291 edges. We test two configurations: comparing all leaf node pairs to find possible similarity edges, respectively, comparing only entity pairs. On the regular server, in both cases, data stor-</figDesc></figure>
<figure xml:id="fig_8"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: YAGO loading time (minutes) using Flair.</figDesc><graphic coords="31,110.85,248.72,233.13,106.94" type="bitmap" /></figure>
<figure xml:id="fig_9"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Quality of disambiguation for French.</figDesc></figure>
<figure xml:id="fig_10"><head>Figure 10 :</head><label>10</label><figDesc>Query execution time on different synthetic graph types.</figDesc></figure>
<figure type="table" xml:id="tab_0"><head /><label /><figDesc>if t has matches for all the query keywords then post-process t if needed; output the result as an answerelse insert t into K Algorithm GAMSearch(query Q = {w 1 , w 2 , . . . , w k }) 1. For each w i , 1 ≤ i ≤ k• For each node n j i matching w i , let t j i be the 1-node tree consisting of n j i ; process(t j i ) 2. Initial merge * : try to merge every pair of trees from E, and process any resulting answer tree. 3. Initialize U (empty so far): If t was not already in E, agressively Merge:</figDesc><table><row><cell>Procedure process(tree t)</cell></row><row><cell>• if t is not already in E</cell></row><row><cell>• then</cell></row><row><cell>-add t to E</cell></row><row><cell>4. While (U is not empty)</cell></row><row><cell>(a) Pop out of U the highest-priority pair (t, e).</cell></row><row><cell>(b) Apply the corresponding Grow or Grow2Rep, resulting in a</cell></row><row><cell>new tree t ; process(t ).</cell></row><row><cell>(c)</cell></row><row><cell>1. Trees matching many query keywords are preferable, to go toward com-</cell></row><row><cell>plete query answers;</cell></row></table><note><p>(a) Create Grow opportunities: Insert into U the pair (t, e), for each t ∈ E and e a data or similarity edge adjacent to t's root. (b) Create Grow2Rep opportunities: Insert into U the pair (t, n → n rep ) for each t ∈ E whose root is n, such that the representative of n is n rep = n.</p></note></figure>
<figure type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Quality of NER from French text.</figDesc><table><row><cell /><cell>Flair-</cell><cell>Flair-pre-</cell><cell cols="3">SpaCy-md SpaCy-sm Stanford</cell></row><row><cell /><cell>SFTF</cell><cell>trained</cell><cell /><cell /><cell>NER</cell></row><row><cell>LOC-P</cell><cell>59.52%</cell><cell>53.26%</cell><cell>55.77%</cell><cell>54.92%</cell><cell>62.17%</cell></row><row><cell>LOC-R</cell><cell>79.36%</cell><cell>77.71%</cell><cell>78.00%</cell><cell>79.41%</cell><cell>69.05%</cell></row><row><cell>LOC-F 1</cell><cell>68.02%</cell><cell>63.20%</cell><cell>65.04%</cell><cell>64.93%</cell><cell>65.43%</cell></row><row><cell>ORG-P</cell><cell>76.56%</cell><cell>74.57%</cell><cell>72.72%</cell><cell>71.92%</cell><cell>15.82%</cell></row><row><cell>ORG-R</cell><cell>74.55%</cell><cell>75.61%</cell><cell>54.85%</cell><cell>53.23%</cell><cell>5.39%</cell></row><row><cell>ORG-F 1</cell><cell>75.54%</cell><cell>75.09%</cell><cell>62.53%</cell><cell>61.18%</cell><cell>8.04%</cell></row><row><cell>PER-P</cell><cell>72.29%</cell><cell>71.76%</cell><cell>53.09%</cell><cell>57.32%</cell><cell>55.31%</cell></row><row><cell>PER-R</cell><cell>84.94%</cell><cell>84.89%</cell><cell>74.98%</cell><cell>79.19%</cell><cell>88.26%</cell></row><row><cell>PER-F 1</cell><cell>78.10%</cell><cell>77.78%</cell><cell>62.16%</cell><cell>66.50%</cell><cell>68.00%</cell></row><row><cell>Micro-P</cell><cell>69.20%</cell><cell>65.55%</cell><cell>61.06%</cell><cell>61.25%</cell><cell>50.12%</cell></row><row><cell>Micro-R</cell><cell>77.94%</cell><cell>77.92%</cell><cell>65.93%</cell><cell>66.32%</cell><cell>40.69%</cell></row><row><cell>Micro-F 1</cell><cell>73.31%</cell><cell>71.20%</cell><cell>63.40%</cell><cell>63.68%</cell><cell>44.91%</cell></row></table></figure>
<figure type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Query results on the complete graph.</figDesc><table><row><cell>Query keyword(s)</cell><cell cols="3">Answers Answer trees Time to 1st (ms)</cell></row><row><cell>aéronautique, Macron</cell><cell>2779</cell><cell>1152577</cell><cell>7225</cell></row><row><cell>Brigitte Macron, Clara Gaymard</cell><cell>4584</cell><cell>545020</cell><cell>1412</cell></row><row><cell>Chine, covid, France</cell><cell>17</cell><cell>25974</cell><cell>8380</cell></row><row><cell>chômage, covid</cell><cell>108</cell><cell>205584</cell><cell>4476</cell></row><row><cell>covid, El Khomri</cell><cell>16</cell><cell>215952</cell><cell>52486</cell></row><row><cell>confinement, Christophe Castaner</cell><cell>4</cell><cell>120367</cell><cell>3820</cell></row><row><cell>Chine, France, Didier Raoult</cell><cell>36</cell><cell>146261</cell><cell>14666</cell></row><row><cell>Ebola, Raoult</cell><cell>1</cell><cell>37751</cell><cell>75759</cell></row><row><cell>entreprise, Raffarin</cell><cell>6336</cell><cell>1174822</cell><cell>6589</cell></row><row><cell>Julien Denormandie, Macron</cell><cell>464</cell><cell>20181</cell><cell>2661</cell></row><row><cell>Khalid al-Falih, Kristalina Georgieva</cell><cell>1</cell><cell>1775</cell><cell>765</cell></row><row><cell>Kristalina Georgieva, Walter Butler</cell><cell>1</cell><cell>3224</cell><cell>353</cell></row><row><cell>Louis Beam, Ku Klux Klan, Trump</cell><cell>1</cell><cell>24172</cell><cell>15207</cell></row><row><cell>Macron, Royal</cell><cell>102</cell><cell>6413</cell><cell>4107</cell></row><row><cell>Marisol Touraine, Jean-François Delfraissy</cell><cell>3</cell><cell>1497</cell><cell>1183</cell></row><row><cell>masque, France</cell><cell>35</cell><cell>15082</cell><cell>7126</cell></row><row><cell>Michael Ryan, Anthony Fauci</cell><cell>2</cell><cell>6653</cell><cell>12215</cell></row><row><cell>Pascale Gruny, Jean-François Delfraissy</cell><cell>2</cell><cell>1332</cell><cell>91591</cell></row><row><cell>vaccination, Trump</cell><cell>12</cell><cell>1188</cell><cell>6518</cell></row><row><cell>Yazdan Yazdanpanah, Jean-François Delfraissy</cell><cell>29</cell><cell>5446</cell><cell>1687</cell></row></table></figure>
			<note place="foot" n="1" xml:id="foot_0"><p>https://en.wikipedia.org/wiki/Data_journalism Preprint submitted to Elsevier September 8,</p></note>
			<note place="foot" n="2021" xml:id="foot_1"><p /></note>
			<note place="foot" n="2" xml:id="foot_2"><p>https://github.com/flairNLP/flair</p></note>
			<note place="foot" n="3" xml:id="foot_3"><p>https://www.clips.uantwerpen.be/conll2003/ner/</p></note>
			<note place="foot" n="4" xml:id="foot_4"><p>https://www.mpi-inf.mpg.de/departments/databases-and-information-systems/ research/ambiverse-nlu/</p></note>
			<note place="foot" n="5" xml:id="foot_5"><p>https://github.com/ambiverse-nlu/ambiverse-nlu# maintainers-and-contributors</p></note>
			<note place="foot" n="6" xml:id="foot_6"><p>https://uima.apache.org/doc-uima-why.html</p></note>
			<note place="foot" n="7" xml:id="foot_7"><p>We had also experimented with Neo4J<ref type="bibr" target="#b41">[42]</ref> as a DBMS, specifically with shortest-path search, but found no performance advantage for the operations ConnectionLens requires.</p></note>
			<note place="foot" n="8" xml:id="foot_8"><p>This may be the case using a more advanced indexing system that includes some natural language understanding, term dictionaries, etc.</p></note>
			<note place="foot" n="9" xml:id="foot_9"><p>https://www.hatvp.fr/livraison/merge/declarations.xml</p></note>
			<note place="foot" n="10" xml:id="foot_10"><p>The <software>script</software> https://www.clips.uantwerpen.be/conll2002/ner/ has been made available in conjunction with the CoNLL (Conference on Natural Language Learning).</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements. We thank <rs type="person">Julien Leblay</rs> for his contribution to earlier versions of this work [10]. This work has been partially funded by the <rs type="grantName">AI Chair</rs> project <rs type="grantName">SourcesSay Grant</rs> no <rs type="grantNumber">ANR-20-CHIA-0015-01</rs> and by <rs type="funder">Portuguese national</rs> funds through <rs type="funder">FCT</rs> with reference <rs type="grantNumber">UIDB/50021/2020</rs> (<rs type="projectName">INESC</rs><rs type="grantNumber">-ID</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_8nGCncS">
					<orgName type="grant-name">AI Chair</orgName>
				</org>
				<org type="funding" xml:id="_nQHx3Yv">
					<idno type="grant-number">ANR-20-CHIA-0015-01</idno>
					<orgName type="grant-name">SourcesSay Grant</orgName>
				</org>
				<org type="funded-project" xml:id="_ySfDteS">
					<idno type="grant-number">UIDB/50021/2020</idno>
					<orgName type="project" subtype="full">INESC</orgName>
				</org>
				<org type="funding" xml:id="_wtrmbpP">
					<idno type="grant-number">-ID</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Flair: An easy-to-use framework for state-of-the-art NLP</title>
		<author>
			<persName><forename type="first">A</forename><surname>Akbik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Bergmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Blythe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Rasul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Schweter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vollgraf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Contextual string embeddings for sequence labeling</title>
		<author>
			<persName><forename type="first">A</forename><surname>Akbik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Blythe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vollgraf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Towards Scalable Hybrid Stores: Constraint-Based Rewriting to the Rescue</title>
		<author>
			<persName><forename type="first">R</forename><surname>Alotaibi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bursztyn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Deutsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Manolescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zampetakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Emergence of scaling in random networks</title>
		<author>
			<persName><forename type="first">A.-L</forename><surname>Barabási</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Albert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">286</biblScope>
			<biblScope unit="issue">5439</biblScope>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Enriching word vectors with subword information</title>
		<author>
			<persName><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TACL</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Mixed-instance querying: a lightweight integration architecture for data journalism</title>
		<author>
			<persName><forename type="first">R</forename><surname>Bonaque</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">D</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Cautis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Goasdoué</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Letelier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Manolescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Mendoza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ribeiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tannier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Thomazo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PVLDB</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">13</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Social, structured and semantic search</title>
		<author>
			<persName><forename type="first">R</forename><surname>Bonaque</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Cautis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Goasdoué</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Manolescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EDBT</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Graph integration of structured, semistructured and unstructured data for data journalism</title>
		<author>
			<persName><forename type="first">O</forename><surname>Bǎlǎlǎu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Conceiçao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Galhardas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Manolescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Merabti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Youssef</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BDA</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>informal publication only</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Extracting linked data from statistic spreadsheets</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">D</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Manolescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tannier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Semantic Big Data Workshop</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">ConnectionLens: Finding connections across heterogeneous data sources</title>
		<author>
			<persName><forename type="first">C</forename><surname>Chanial</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Dziri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Galhardas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leblay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H L</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Manolescu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>VLDB</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Keyword query over error-tolerant knowledge bases</title>
		<author>
			<persName><forename type="first">Y.-R</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G.-R</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computer Science and Technology</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Ranking candidate networks of relations to improve keyword search over relational databases</title>
		<author>
			<persName><forename type="first">P</forename><surname>Oliveira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Da Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">S</forename><surname>De Moura</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Finding top-k min-cost connected trees in databases</title>
		<author>
			<persName><forename type="first">B</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDE</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Principles of Data Integration</title>
		<author>
			<persName><forename type="first">A</forename><surname>Doan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Halevy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">G</forename><surname>Ives</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>Morgan Kaufmann</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Indexing dataspaces</title>
		<author>
			<persName><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Halevy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD. ACM</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Keyword search over RDF graphs</title>
		<author>
			<persName><forename type="first">S</forename><surname>Elbassuoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Blanco</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Incorporating non-local information into information extraction systems by gibbs sampling</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Grenager</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Extended named entity annotation on OCRed documents: From corpus constitution to evaluation campaign</title>
		<author>
			<persName><forename type="first">O</forename><surname>Galibert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rosset</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Grouin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zweigenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Quintard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LREC</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Computers and Intractability: A Guide to the Theory of NP-Completeness</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Garey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Johnson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990">1990</date>
			<publisher>W. H. Freeman &amp; Co</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A polylogarithmic approximation algorithm for the group Steiner tree problem</title>
		<author>
			<persName><forename type="first">N</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Konjevod</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ravi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM</title>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Fact Checking and Analyzing the Web</title>
		<author>
			<persName><forename type="first">F</forename><surname>Goasdoué</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Karanasos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Katsis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leblay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Manolescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zampetakis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>SIGMOD</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">The Data Journalism Handbook: How Journalists can Use Data to Improve the News</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chambers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bounegru</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>O'Reilly</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Fast approximation of Steiner trees in large graphs</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gubichev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Neumann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">XRANK: Ranked keyword search over XML documents</title>
		<author>
			<persName><forename type="first">L</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Botev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shanmugasundaram</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">BLINKS: ranked keyword searches on graphs</title>
		<author>
			<persName><forename type="first">H</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Robust disambiguation of named entities in text</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hoffart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Yosef</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Bordino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fürstenau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pinkal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Spaniol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Taneva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Thater</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Weikum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">DISCOVER: keyword search in relational databases</title>
		<author>
			<persName><forename type="first">V</forename><surname>Hristidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Papakonstantinou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VLDB</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Advances in record linkage methodology as applied to the 1985 census of Tampa, Florida</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Jaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">84</biblScope>
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">CloudMdsQL: querying heterogeneous cloud data stores with a common language</title>
		<author>
			<persName><forename type="first">B</forename><surname>Kolev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Valduriez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bondiombouy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jiménez-Peris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Distributed and parallel databases</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Scalable keyword search on large RDF data</title>
		<author>
			<persName><forename type="first">W</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kementsietsidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Duan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Knowl. Data Eng</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">11</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">EASE: An effective 3-in-1 keyword search method for unstructured, semi-structured and structured data</title>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">C</forename><surname>Ooi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Efficient and progressive group Steiner tree search</title>
		<author>
			<persName><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Identifying meaningful return information for XML keyword search</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGMOD</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="329" to="340" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Answering complex questions by joining multi-document evidence with quasi knowledge graphs</title>
		<author>
			<persName><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pramanik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Abujabal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Weikum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGIR</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="105" to="114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">YAGO3: A knowledge base from multilingual wikipedias</title>
		<author>
			<persName><forename type="first">F</forename><surname>Mahdisoltani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Biega</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">M</forename><surname>Suchanek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIDR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A survey of named entity recognition and classification</title>
		<author>
			<persName><forename type="first">D</forename><surname>Nadeau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sekine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lingvisticae Investigationes</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning multilingual named entity recognition from wikipedia</title>
		<author>
			<persName><forename type="first">J</forename><surname>Nothman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ringland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Curran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">194</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Entity resolution: Past, present and yet-to-come</title>
		<author>
			<persName><forename type="first">G</forename><surname>Papadakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ioannou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Palpanas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EDBT. OpenProceedings.org</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Referential named entity annotation of the Paris 7 French TreeBank)</title>
		<author>
			<persName><forename type="first">B</forename><surname>Sagot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Stern</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>TALN</publisher>
		</imprint>
	</monogr>
	<note>in French</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Sahovic</surname></persName>
		</author>
		<ptr target="https://team.inria.fr/cedar/files/2021/04/Haris-SAHOVIC-2019-report.pdf" />
		<title level="m">Graph querying in ConnectionLens (internship report)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Efficient keyword search across heterogeneous relational databases</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sayyadian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lekhac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Doan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Gravano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDE</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">PARIS: probabilistic alignment of relations, instances, and schema</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">M</forename><surname>Suchanek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Abiteboul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Senellart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. VLDB Endow</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">BECKEY: understanding, comparing and discovering keys of different semantics in knowledge bases</title>
		<author>
			<persName><forename type="first">D</forename><surname>Symeonidou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Armant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Pernelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowl. Based Syst</title>
		<imprint>
			<biblScope unit="volume">195</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">YAGO 4: A reason-able knowledge base</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">P</forename><surname>Tanon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Weikum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">M</forename><surname>Suchanek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ESWC</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Wikidata: A free collaborative knowledgebase</title>
		<author>
			<persName><forename type="first">D</forename><surname>Vrandečić</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Krötzsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="78" to="85" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">A graph method for keyword-based selection of the top-k databases</title>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">H</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">C</forename><surname>Ooi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Papadias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K H</forename><surname>Tung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Active learning in keyword search-based data integration</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">G</forename><surname>Ives</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">P</forename><surname>Talukdar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">VLDB J</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Keyword Search in Databases</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Synthesis Lectures on Data Management</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Keyword search in relational databases: A survey</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Data Eng. Bull</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</teiCorpus></text>
</TEI>