<?xml version='1.0' encoding='utf-8'?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" version="1.1" xsi:schemaLocation="http://www.tei-c.org/ns/1.0 http://api.archives-ouvertes.fr/documents/aofr-sword.xsd">
  <teiHeader>
    <fileDesc>
      <titleStmt>
        <title>HAL TEI export of hal-03522878</title>
      </titleStmt>
      <publicationStmt>
        <distributor>CCSD</distributor>
        <availability status="restricted">
          <licence target="http://creativecommons.org/licenses/by/4.0/">Distributed under a Creative Commons Attribution 4.0 International License</licence>
        </availability>
        <date when="2024-04-25T23:33:01+02:00" />
      </publicationStmt>
      <sourceDesc>
        <p part="N">HAL API platform</p>
      </sourceDesc>
    </fileDesc>
  </teiHeader>
  <text>
    <body>
      <listBibl>
        <biblFull>
          <titleStmt>
            <title xml:lang="en">Explainable Anomaly Detection on High-Dimensional Time Series Data</title>
            <author role="aut">
              <persName>
                <forename type="first">Bijan</forename>
                <surname>Rad</surname>
              </persName>
              <email type="md5">76f4fa739afea5b10a58b8518ff3edfc</email>
              <email type="domain">polytechnique.edu</email>
              <idno type="idhal" notation="numeric">1122857</idno>
              <idno type="halauthorid" notation="string">2311919-1122857</idno>
              <affiliation ref="#struct-451441" />
            </author>
            <author role="aut">
              <persName>
                <forename type="first">Fei</forename>
                <surname>Song</surname>
              </persName>
              <email type="md5">e74eb1dd307139408649c9f522066ccb</email>
              <email type="domain">polytechnique.edu</email>
              <idno type="idhal" notation="numeric">1122858</idno>
              <idno type="halauthorid" notation="string">1492132-1122858</idno>
              <affiliation ref="#struct-451441" />
            </author>
            <author role="aut">
              <persName>
                <forename type="first">Vincent</forename>
                <surname>Jacob</surname>
              </persName>
              <email type="md5">a3c25903698d699f5a483e2e518636d9</email>
              <email type="domain">polytechnique.edu</email>
              <idno type="idhal" notation="numeric">1122859</idno>
              <idno type="halauthorid" notation="string">129644-1122859</idno>
              <idno type="ORCID">https://orcid.org/0000-0002-7055-213X</idno>
              <idno type="IDREF">https://www.idref.fr/119707748</idno>
              <affiliation ref="#struct-451441" />
            </author>
            <author role="aut">
              <persName>
                <forename type="first">Yanlei</forename>
                <surname>Diao</surname>
              </persName>
              <email type="md5">8e5957ff0be464645ecda731b674af7c</email>
              <email type="domain">polytechnique.edu</email>
              <idno type="idhal" notation="numeric">1052430</idno>
              <idno type="halauthorid" notation="string">1213328-1052430</idno>
              <affiliation ref="#struct-451441" />
            </author>
            <editor role="depositor">
              <persName>
                <forename>Fei</forename>
                <surname>Song</surname>
              </persName>
              <email type="md5">298a6bba6c41ec7bf4c145036014efb7</email>
              <email type="domain">inria.fr</email>
            </editor>
          </titleStmt>
          <editionStmt>
            <edition n="v1" type="current">
              <date type="whenSubmitted">2022-01-12 12:17:57</date>
              <date type="whenModified">2024-01-29 14:51:25</date>
              <date type="whenReleased">2022-01-12 13:02:07</date>
              <date type="whenProduced">2021-06-28</date>
              <date type="whenEndEmbargoed">2022-01-12</date>
              <ref type="file" target="https://inria.hal.science/hal-03522878/document">
                <date notBefore="2022-01-12" />
              </ref>
              <ref type="file" subtype="author" n="1" target="https://inria.hal.science/hal-03522878/file/DEBS2021_HIDR.pdf">
                <date notBefore="2022-01-12" />
              </ref>
            </edition>
            <respStmt>
              <resp>contributor</resp>
              <name key="615130">
                <persName>
                  <forename>Fei</forename>
                  <surname>Song</surname>
                </persName>
                <email type="md5">298a6bba6c41ec7bf4c145036014efb7</email>
                <email type="domain">inria.fr</email>
              </name>
            </respStmt>
          </editionStmt>
          <publicationStmt>
            <distributor>CCSD</distributor>
            <idno type="halId">hal-03522878</idno>
            <idno type="halUri">https://inria.hal.science/hal-03522878</idno>
            <idno type="halBibtex">rad:hal-03522878</idno>
            <idno type="halRefHtml">&lt;i&gt;The 15th ACM International Conference on Distributed and Event-based Systems (DEBS ’21)&lt;/i&gt;, Jun 2021, virtual event, Italy. &lt;a target="_blank" href="https://dx.doi.org/10.1145/3465480.3468292"&gt;&amp;#x27E8;10.1145/3465480.3468292&amp;#x27E9;&lt;/a&gt;</idno>
            <idno type="halRef">The 15th ACM International Conference on Distributed and Event-based Systems (DEBS ’21), Jun 2021, virtual event, Italy. &amp;#x27E8;10.1145/3465480.3468292&amp;#x27E9;</idno>
          </publicationStmt>
          <seriesStmt>
            <idno type="stamp" n="X">Ecole Polytechnique</idno>
            <idno type="stamp" n="CNRS">CNRS - Centre national de la recherche scientifique</idno>
            <idno type="stamp" n="INRIA">INRIA - Institut National de Recherche en Informatique et en Automatique</idno>
            <idno type="stamp" n="LIX">Laboratoire d'informatique de l'école polytechnique</idno>
            <idno type="stamp" n="INRIA-SACLAY" corresp="INRIA">INRIA Saclay - Ile de France</idno>
            <idno type="stamp" n="X-LIX" corresp="X">Laboratoire d'informatique de l'X (LIX)</idno>
            <idno type="stamp" n="X-DEP" corresp="X">Polytechnique</idno>
            <idno type="stamp" n="X-DEP-INFO" corresp="X-DEP">Département d'informatique</idno>
            <idno type="stamp" n="INRIA_TEST">INRIA - Institut National de Recherche en Informatique et en Automatique</idno>
            <idno type="stamp" n="TESTALAIN1">TESTALAIN1</idno>
            <idno type="stamp" n="INRIA2">INRIA 2</idno>
            <idno type="stamp" n="IP_PARIS">Institut Polytechnique de Paris</idno>
            <idno type="stamp" n="GS-COMPUTER-SCIENCE">Graduate School Computer Science</idno>
          </seriesStmt>
          <notesStmt>
            <note type="audience" n="2">International</note>
            <note type="invited" n="0">No</note>
            <note type="popular" n="0">No</note>
            <note type="peer" n="1">Yes</note>
            <note type="proceedings" n="1">Yes</note>
          </notesStmt>
          <sourceDesc>
            <biblStruct>
              <analytic>
                <title xml:lang="en">Explainable Anomaly Detection on High-Dimensional Time Series Data</title>
                <author role="aut">
                  <persName>
                    <forename type="first">Bijan</forename>
                    <surname>Rad</surname>
                  </persName>
                  <email type="md5">76f4fa739afea5b10a58b8518ff3edfc</email>
                  <email type="domain">polytechnique.edu</email>
                  <idno type="idhal" notation="numeric">1122857</idno>
                  <idno type="halauthorid" notation="string">2311919-1122857</idno>
                  <affiliation ref="#struct-451441" />
                </author>
                <author role="aut">
                  <persName>
                    <forename type="first">Fei</forename>
                    <surname>Song</surname>
                  </persName>
                  <email type="md5">e74eb1dd307139408649c9f522066ccb</email>
                  <email type="domain">polytechnique.edu</email>
                  <idno type="idhal" notation="numeric">1122858</idno>
                  <idno type="halauthorid" notation="string">1492132-1122858</idno>
                  <affiliation ref="#struct-451441" />
                </author>
                <author role="aut">
                  <persName>
                    <forename type="first">Vincent</forename>
                    <surname>Jacob</surname>
                  </persName>
                  <email type="md5">a3c25903698d699f5a483e2e518636d9</email>
                  <email type="domain">polytechnique.edu</email>
                  <idno type="idhal" notation="numeric">1122859</idno>
                  <idno type="halauthorid" notation="string">129644-1122859</idno>
                  <idno type="ORCID">https://orcid.org/0000-0002-7055-213X</idno>
                  <idno type="IDREF">https://www.idref.fr/119707748</idno>
                  <affiliation ref="#struct-451441" />
                </author>
                <author role="aut">
                  <persName>
                    <forename type="first">Yanlei</forename>
                    <surname>Diao</surname>
                  </persName>
                  <email type="md5">8e5957ff0be464645ecda731b674af7c</email>
                  <email type="domain">polytechnique.edu</email>
                  <idno type="idhal" notation="numeric">1052430</idno>
                  <idno type="halauthorid" notation="string">1213328-1052430</idno>
                  <affiliation ref="#struct-451441" />
                </author>
              </analytic>
              <monogr>
                <meeting>
                  <title>The 15th ACM International Conference on Distributed and Event-based Systems (DEBS ’21)</title>
                  <date type="start">2021-06-28</date>
                  <settlement>virtual event</settlement>
                  <country key="IT">Italy</country>
                </meeting>
                <imprint>
                  <date type="datePub">2021-06-28</date>
                </imprint>
              </monogr>
              <idno type="doi">10.1145/3465480.3468292</idno>
            </biblStruct>
          </sourceDesc>
          <profileDesc>
            <langUsage>
              <language ident="en">English</language>
            </langUsage>
            <textClass>
              <keywords scheme="author">
                <term xml:lang="en">• Computing methodologies → Anomaly detection</term>
                <term xml:lang="en">Feature selection</term>
                <term xml:lang="en">explanation discovery</term>
                <term xml:lang="en">time series data analysis</term>
                <term xml:lang="en">dimensionality reduction</term>
              </keywords>
              <classCode scheme="halDomain" n="info">Computer Science [cs]</classCode>
              <classCode scheme="halTypology" n="COMM">Conference papers</classCode>
              <classCode scheme="halOldTypology" n="COMM">Conference papers</classCode>
              <classCode scheme="halTreeTypology" n="COMM">Conference papers</classCode>
            </textClass>
            <abstract xml:lang="en">
              <p>As enterprise information systems are collecting event streams from various sources, the ability of a system to automatically detect anomalous events and further provide human-readable explanations is of paramount importance. In this paper, we present an approach to integrated anomaly detection (AD) and explanation discovery (ED), which is able to leverage state-of-the-art Deep Learning (DL) techniques for anomaly detection, while being able to recover human-readable explanations for detected anomalies. At the core of the framework is a new human-interpretable dimensionality reduction (HIDR) method that not only reduces the dimensionality of the data, but also maintains a meaningful mapping from the original features to the transformed low-dimensional features. Such transformed features can be fed into any DL technique designed for anomaly detection, and the feature mapping will be used to recover human-readable explanations through a suite of new feature selection and explanation discovery methods. Evaluation using a recent explainable anomaly detection benchmark demonstrates the efficiency and effectiveness of HIDR for AD, and the result that while all three recent ED techniques failed to generate quality explanations on high-dimensional data, our HIDR-based ED framework can enable them to generate explanations with dramatic improvements in the quality of explanations and computational efficiency.</p>
            </abstract>
          </profileDesc>
        </biblFull>
      </listBibl>
    </body>
    <back>
      <listOrg type="structures">
        <org type="researchteam" xml:id="struct-451441" status="VALID">
          <idno type="RNSR">201622056J</idno>
          <orgName>Rich Data Analytics at Cloud Scale</orgName>
          <orgName type="acronym">CEDAR</orgName>
          <date type="start">2016-01-01</date>
          <desc>
            <address>
              <addrLine>1 rue Honoré d'Estienne d'OrvesBâtiment Alan TuringCampus de l'École Polytechnique91120 Palaiseau</addrLine>
              <country key="FR" />
            </address>
          </desc>
          <listRelation>
            <relation active="#struct-2071" type="direct" />
            <relation active="#struct-300340" type="indirect" />
            <relation name="UMR7161" active="#struct-441569" type="indirect" />
            <relation active="#struct-118511" type="direct" />
            <relation active="#struct-300009" type="indirect" />
          </listRelation>
        </org>
        <org type="laboratory" xml:id="struct-2071" status="VALID">
          <idno type="IdRef">196509955</idno>
          <idno type="RNSR">200519331V</idno>
          <orgName>Laboratoire d'informatique de l'École polytechnique [Palaiseau]</orgName>
          <orgName type="acronym">LIX</orgName>
          <desc>
            <address>
              <addrLine>Route de Saclay 91128 PALAISEAU CEDEX</addrLine>
              <country key="FR" />
            </address>
            <ref type="url">https://www.lix.polytechnique.fr/</ref>
          </desc>
          <listRelation>
            <relation active="#struct-300340" type="direct" />
            <relation name="UMR7161" active="#struct-441569" type="direct" />
          </listRelation>
        </org>
        <org type="institution" xml:id="struct-300340" status="VALID">
          <idno type="IdRef">027309320</idno>
          <idno type="ROR">https://ror.org/05hy3tk52</idno>
          <orgName>École polytechnique</orgName>
          <orgName type="acronym">X</orgName>
          <date type="start">1794-03-11</date>
          <desc>
            <address>
              <addrLine>École polytechnique, 91128 Palaiseau Cedex</addrLine>
              <country key="FR" />
            </address>
            <ref type="url">https://www.polytechnique.edu/</ref>
          </desc>
        </org>
        <org type="regroupinstitution" xml:id="struct-441569" status="VALID">
          <idno type="IdRef">02636817X</idno>
          <idno type="ISNI">0000000122597504</idno>
          <idno type="ROR">https://ror.org/02feahw73</idno>
          <orgName>Centre National de la Recherche Scientifique</orgName>
          <orgName type="acronym">CNRS</orgName>
          <date type="start">1939-10-19</date>
          <desc>
            <address>
              <country key="FR" />
            </address>
            <ref type="url">https://www.cnrs.fr/</ref>
          </desc>
        </org>
        <org type="laboratory" xml:id="struct-118511" status="VALID">
          <idno type="RNSR">200818248E</idno>
          <idno type="ROR">https://ror.org/0315e5x55</idno>
          <orgName>Inria Saclay - Ile de France</orgName>
          <desc>
            <address>
              <addrLine>1 rue Honoré d'Estienne d'OrvesBâtiment Alan TuringCampus de l'École Polytechnique91120 Palaiseau</addrLine>
              <country key="FR" />
            </address>
            <ref type="url">http://www.inria.fr/centre/saclay</ref>
          </desc>
          <listRelation>
            <relation active="#struct-300009" type="direct" />
          </listRelation>
        </org>
        <org type="institution" xml:id="struct-300009" status="VALID">
          <idno type="ROR">https://ror.org/02kvxyf05</idno>
          <orgName>Institut National de Recherche en Informatique et en Automatique</orgName>
          <orgName type="acronym">Inria</orgName>
          <desc>
            <address>
              <addrLine>Domaine de VoluceauRocquencourt - BP 10578153 Le Chesnay Cedex</addrLine>
              <country key="FR" />
            </address>
            <ref type="url">http://www.inria.fr/en/</ref>
          </desc>
        </org>
      </listOrg>
    </back>
  <teiCorpus>
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Explainable Anomaly Detection on High-Dimensional Time Series Data</title>
				<funder ref="#_wep52rx">
					<orgName type="full">European Research Council</orgName>
					<orgName type="abbreviated">ERC</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher />
				<availability status="unknown"><licence /></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Bijan</forename><surname>Rad</surname></persName>
							<email>bijan.rad@polytechnique.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">École Polytechnique</orgName>
								<address>
									<settlement>Palaiseau</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Fei</forename><surname>Song</surname></persName>
							<email>fei.song@polytechnique.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">École Polytechnique</orgName>
								<address>
									<settlement>Palaiseau</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Vincent</forename><surname>Jacob</surname></persName>
							<email>vincent.jacob@polytechnique.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">École Polytechnique</orgName>
								<address>
									<settlement>Palaiseau</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yanlei</forename><surname>Diao</surname></persName>
							<email>yanlei.diao@polytechnique.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">École Polytechnique</orgName>
								<address>
									<settlement>Palaiseau</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Explainable Anomaly Detection on High-Dimensional Time Series Data</title>
					</analytic>
					<monogr>
						<imprint>
							<date />
						</imprint>
					</monogr>
					<idno type="MD5">31084873B0939214BB6959A20AA710A9</idno>
					<idno type="DOI">10.1145/3465480.3468292</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-04-12T14:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid" />
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Computing methodologies → Anomaly detection</term>
					<term>Feature selection</term>
					<term>Neural networks</term>
					<term>• Information systems → Data stream mining anomaly detection, explanation discovery, time series data analysis, dimensionality reduction, neural networks</term>
				</keywords>
			</textClass>
			<abstract>
<div><p>As enterprise information systems are collecting event streams from various sources, the ability of a system to automatically detect anomalous events and further provide human-readable explanations is of paramount importance. In this paper, we present an approach to integrated anomaly detection (AD) and explanation discovery (ED), which is able to leverage state-of-the-art Deep Learning (DL) techniques for anomaly detection, while being able to recover humanreadable explanations for detected anomalies. At the core of the framework is a new human-interpretable dimensionality reduction (HIDR) method that not only reduces the dimensionality of the data, but also maintains a meaningful mapping from the original features to the transformed low-dimensional features. Such transformed features can be fed into any DL technique designed for anomaly detection, and the feature mapping will be used to recover humanreadable explanations through a suite of new feature selection and explanation discovery methods. Evaluation using a recent explainable anomaly detection benchmark demonstrates the efficiency and effectiveness of HIDR for AD, and the result that while all three recent ED techniques failed to generate quality explanations on highdimensional data, our HIDR-based ED framework can enable them to generate explanations with dramatic improvements in the quality of explanations and computational efficiency.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div><head n="1">INTRODUCTION</head><p>Enterprise information systems are collecting large amounts of event log data from various sources such as databases, transaction logs, audit trails, system monitors, and application monitors. Timely analysis of such event log data has become critical to business operations such as system health monitoring, application performance monitoring, and user behavior analysis.</p><p>As a concrete example, some of the largest E-commerce platforms are running <software ContextAttributes="used">Spark</software> jobs on petabytes of data each day to analyze customer purchase patterns, target offers, and enhance customer experiences <ref type="bibr" target="#b31">[32]</ref>. Since results of these jobs affect the immediate business operations for inventory management, sales strategies, etc., they are often specified with deadlines. Anomalies that occur in job execution would prevent analytical jobs from meeting their deadlines and hence cause disruption to critical operations on those E-commerce platforms. Therefore, there is a growing demand to collect event logs from the <software ContextAttributes="used">Spark</software> applications as well as the underlying OS during job execution, and have a software tool to analyze these event logs in a timely manner in order to detect performance issues and enable corrective actions, such as allocating more resources, to meet application deadlines.</p><p>Motivated by the above use case, as well as similar use cases in other application domains, our work aims to design a data analytics system that can automatically detect anomalies from high-volume event log data and further provide human-readable explanations for such events, hence enabling preventive or corrective actions.</p><p>In this work, we consider anomalies as the patterns in data that deviate from expected behaviors <ref type="bibr" target="#b8">[9]</ref>. An anomaly can be detected by an automatic procedure, for which existing techniques include statistical, SVM, and clustering based methods <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b13">14]</ref>, as well as recent Deep Learning based methods <ref type="bibr" target="#b7">[8]</ref>. However, there is one key component missing in all these techniques: finding the best explanation for the anomalies detected, or more precisely, a humanreadable formula offering useful information about what has led to the anomaly. Without a good explanation, anomaly detection is only of limited use: the end user knows that something anomalous has just happened, but has little understanding of how it has arisen, how to react to the situation, and how to avoid it in the future.</p><p>To illustrate the importance of generating explanations for detected anomalies, Figure <ref type="figure" target="#fig_1">1</ref> shows three example recordings of <software ContextAttributes="used">Spark</software> streaming application execution provided by the <software ContextAttributes="used">Exathlon</software> anomaly detection benchmark <ref type="bibr" target="#b16">[17]</ref> (described in more detail in the experimental section). Figure <ref type="figure" target="#fig_3">1(a)</ref> shows the time series of three metrics that engineers often monitor in the <software ContextAttributes="used">Spark</software> UI to check if their <software ContextAttributes="used">Spark</software> streaming applications are making progress: (a) scheduling delay captures the delay between the scheduling of a task and the start of its processing; (b) processing time is the time taken to process  a batch of data from the streaming input; (c) the number of processed records in current execution. There are no anomalies in this job. However, the times series data already exhibits a great deal of variability: both scheduling delay and processing time can rise high beyond a normal range of values, which are likely to be caused by other system operations such as checkpointing in <software ContextAttributes="used">Spark</software> or CPU usage by a DataNode in HDFS.</p><p>On the other day, the three metrics look somewhat different during job execution, as shown in Figure <ref type="figure" target="#fig_3">1(b)</ref>. Even if an anomaly detection tool could flag an alert for each interval in which the batch processing time and scheduling delay spike up, it is difficult for the user to figure out what is going on: "What is happening with my submitted job?" "Is the phenomenon caused by the bugs in the code or some system anomalies?" "Should I wait for the job to complete or re-submit it?" "What should I do to bring the job progress back to normal?" It turns out that the spikes are due to bursty input of streaming datathere are periods when the input rate rises significantly higher than its normal values, e.g., due to customer response to promotions or increased requests for a service. If the user could not understand the reason for the anomaly, it is hard to take actions to remedy the situation. Figure <ref type="figure" target="#fig_3">1(c</ref>) further shows that if no actions are taken to allocate more resources to this application, an extended period of bursty input can push the application to crash, as shown by the number of processed records reset to zero.</p><p>Challenges. Designing an integrated analytics system for anomaly detection and explanation discovery raises a host of challenges.</p><p>First, these two topics have been addressed largely in isolation in the literature: while anomaly detection has been studied intensively in the data mining community <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b13">14]</ref>, explanation discovery with the goal of human-readable formulas has recently received attention in the database community <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b40">41]</ref>. On the latter topic, recent works <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b36">37]</ref> explain outliers for group-by aggregate queries and find a logical formula to describe a subset of tuples that contribute the most to the excessively high or low aggregate value of a specific group. It is hard to extend such work to explain anomalies detected by an arbitrary data mining algorithm. <software ContextAttributes="used">EXstream</software> <ref type="bibr" target="#b40">[41]</ref> assumes that the normal and abnormal time periods are already given by the user (treated as ground truth) and finds explanations to best distinguish the abnormal periods from the normal ones. It does not support automated anomaly detection if such ground truth is not available.</p><p>Second, existing integrated approaches cannot address complex anomalies that are of contextual or collective types <ref type="bibr" target="#b8">[9]</ref> and need to be detected on high-dimensional time series data. The best known integrated system is MacroBase <ref type="bibr" target="#b2">[3]</ref>, a data analytics engine that helps the user prioritize attention over data streams, and offers modules for both outlier detection and explanation discovery. However, it performs outlier detection by a density-based method called MAD. While being simple and robust, MAD is suitable only for detecting point outliers, not contextual or collective outliers <ref type="bibr" target="#b8">[9]</ref> which are often related to time series and sequence data.</p><p>Third, a recent position paper <ref type="bibr" target="#b30">[31]</ref> argues for a new system that can harness the power of Deep Learning for detecting complex anomaly types, while at the same time can recover human-readable explanations for detected anomalies. In particular, Deep Neural Networks (DNNs) have demonstrated capabilities for handling highdimensional time series data and detecting both contextual and collective anomalies <ref type="bibr" target="#b7">[8]</ref>. However, DNNs are known to be hard to interpret, whose fully connected layers in a deep architecture essentially make the model a blackbox. How to obtain human-readable explanations while leveraging the power of DNNs for anomaly detection remains an unsolved issue.</p><p>Contributions. In this paper, we present an approach to integrated anomaly detection and explanation discovery that allows us to leverage state-of-the-art Deep Learning techniques for anomaly detection, while being able to recover human-readable explanations for detected anomalies. At the core of the framework is a new humaninterpretable dimensionality reduction method that not only reduces the dimensionality of the data, but also maintains a meaningful mapping from the original features to the transformed low-dimensional features. Such transformed features can be fed into any DNN architecture designed for anomaly detection. Once an anomaly is flagged, we can leverage the feature mapping structure to recover humanreadable explanations. More specifically, our contributions include:</p><p>1. Human-Interpretable Dimensionality Reduction (HIDR). We develop a human-interpretable dimensionality reduction (HIDR) technique with two distinct features: First, it reduces dimensionality by capturing statistical correlation among the features in the input dataset. Given the fact that many features in real-world event log data are correlated, HIDR clusters features based on their statistical correlation, and for each cluster of features, finds a new transformed feature through training an autoencoder that is able to reproduce all the correlated features in the cluster. Such transformed features can be fed into any DNN-based anomaly detection methods, such as Autoencoder-based <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b38">39]</ref> and LSTM-based <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b21">22]</ref> methods. Second, HIDR maintains an explicit mapping between original features and their corresponding transformed features, which will help us recover human-readable explanations using the original features.</p><p>2. HIDR Feature Selection. Since the transformed features are not interpretable, we further develop a HIDR-based feature selection (FS) method to build a filtered interpretable feature space by leveraging the encoder gradient information from the cluster autoencoders trained in HIDR and selecting a subset from the original feature space for the ED task. Our work also customizes the cluster autoencoders to provide a theoretical guarantee that such feature selection enables consistent explanations to be discovered later.</p><p>3. Human-Interpretable Low-dimensional Explanation Discovery (<software>HILED</software>). Once an anomaly is flagged by the detection algorithm, our work returns an interpretable explanation using one of two approaches. First, we can run the feature selection to return a filtered interpretable feature space, as described above, and then any existing ED method on this space to return an explanation. Alternatively, we also provide a two-step approach that repeatedly calls existing ED methods to discover low and high-level explanations, optionally combined with our FS method, to further improve the quality of returned explanations.</p><p>4. Evaluation. Evaluation using a recent explainable anomaly detection benchmark <ref type="bibr" target="#b16">[17]</ref> show the following results: (1) HIDR outperforms other dimensionality reduction methods in the size of reduced feature space, as well as in the accuracy of anomaly detection (AD) results. (2) None of the three state-of-the-art ED methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b40">41]</ref> can handle high-dimensional time series data. Using our HIDR Feature Selection method, these methods can generate explanations with dramatic improvements in conciseness and consistency of the explanations, as well as in computational efficiency. (3) Further, our <software ContextAttributes="used">HILED</software> approach provides a flexible framework for trying different ways to combine an ED method with our gradient-based feature selection method, offering an opportunity for the user to fine-tune the performance to achieve better explanations.</p></div>
<div><head n="2">RELATED WORK</head><p>Explaining outliers in SQL query results. <software ContextAttributes="used">Scorpion</software> <ref type="bibr" target="#b36">[37]</ref> explains outliers in group-by aggregate queries. Users annotate outliers on the results of group-by queries, and <software ContextAttributes="used">Scorpion</software> then searches for predicates that remove these outliers while minimally affecting the legitimate answers. This method does not suit our problem because it works only for group-by aggregation queries and searches through various subsets of the tuples that were used to compute the query answers. Recent work <ref type="bibr" target="#b24">[25]</ref> extends it to support richer and insightful explanations through some pre-computations enabling interactive explanation discovery. This work assumes a set of explanation templates given by the user and requires pre-computations in a given database. Neither of these assumptions fit our problem setting of explainable anomaly detection in incoming event log data, and it is hard to extend such a work to explain anomalies detected by an arbitrary machine learning algorithm. Given a multi-dimensional dataset, recent work <ref type="bibr" target="#b11">[12]</ref> constructs an explanation table and finds patterns that affect a binary value of each tuple. This problem is different from ours because it tries to summarize existing data in a database with statistical information, without considering time series data, nor detecting and explaining anomalies in newly arriving time series data. Some recent industrial efforts were made towards time series anomaly explanation and root cause analysis in DB systems <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b20">21]</ref>. These approaches however require a variety of inputs from the user, e.g., causal hypotheses <ref type="bibr" target="#b17">[18]</ref>, or root cause labels <ref type="bibr" target="#b20">[21]</ref>, while our work focuses on automatically learning for explainable anomaly detection.</p><p>Explaining outliers in stream processing. <software ContextAttributes="used">EXsteam</software> <ref type="bibr" target="#b40">[41]</ref> assumes that the normal and abnormal time periods are already given by the user (treated as ground truth) and finds explanations to best distinguish the abnormal periods from the normal ones. Some of its key techniques require user-labeled data, which is not available in our problem setting. The MacroBase <ref type="bibr" target="#b2">[3]</ref> analytics engine is designed to help the user prioritize attention over data streams, with modules for both outlier detection and explanation discovery. However, it performs outlier detection by a simple density-based method called MAD. While being simple and robust, MAD is suitable only for detecting point outliers, not contextual or collective outliers <ref type="bibr" target="#b8">[9]</ref> which are often related to time series and sequence data. For explanation discovery, MacroBase also encounters a scalability issue regarding the number of features in time series data. We will evaluate both of these techniques in our experimental study.</p><p>Explaining outputs in iterative analytics. Recent work <ref type="bibr" target="#b9">[10]</ref> focuses on tracking, maintaining, and querying lineage and "how" provenance in the context of arbitrary iterative data flows. It aims to create a set of recursively defined rules that determine which records in a data-parallel computation inputs, intermediate records, and outputs require explanation. It allows one to identify when (i.e., the points in the computation) and how a data collection changes, and provides explanations for only these few changes.</p><p>Interpretable machine learning. In the ML community, there has been relevant work on interpretable models. Several projects have explored methods for obtaining interpretable classifiers based on perturbing the classifier inputs and observing the response <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b32">33]</ref>. Among them, the most relevant work <ref type="bibr" target="#b22">[23]</ref> proposed a model-agnostic approach, called LIME, that uses sparse linear models as explanations of any classifier. In particular, LIME explains a specific prediction of any classifier by approximating it locally with a visually interpretable linear model, and explains the overall model by selecting a set of representative instances with explanations. We evaluate LIME in our experimental study and demonstrate that it has a scalability issue for large datasets. Anchors <ref type="bibr" target="#b23">[24]</ref> improved upon LIME by replacing a linear model with a logical rule that explains a single data instance and offers better coverage of data points in the local neighborhood, but it does not support time series data like in our work. SHAP scores <ref type="bibr" target="#b19">[20]</ref> and RESP scores <ref type="bibr" target="#b3">[4]</ref> are also instancelevel explanations that assign a numerical score to each feature, representing their importance in the outcome. But these methods are computationally too expensive to suit data stream processing. Other recent works develop decision trees <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b37">38]</ref> and decision sets <ref type="bibr" target="#b18">[19]</ref> as explainable models. While decision trees are visually interpretable, their construction algorithm often fails for two main reasons <ref type="bibr" target="#b29">[30]</ref>: <ref type="bibr" target="#b0">(1)</ref> in the presence of class imbalance, such as in anomaly detection where anomalies are rare, the decision trees can simply classify all instances to the majority class; (2) the decision trees returned by the algorithm might be very large, making them hard to read and understand by a human.</p><p>Anomaly detection. Anomaly detection or outlier detection <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b34">35]</ref> has been studied intensively in the data mining community. Researchers are still contributing actively to this area to achieve more efficient detection algorithms <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b15">16]</ref>. Common anomaly detection techniques (e.g., <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b34">35]</ref>) take one of two main approaches. In the first approach, a model is trained to perform a prediction task on data assumed mostly normal. An outlier is then defined as an incoming data point for which the model makes a significant error. The other approach relies on distance functions <ref type="bibr" target="#b34">[35]</ref>, where outliers are defined as points that lie far from most of the others. A recent effort to handle high-dimensional data and complex types of anomalies was made through exploring new Deep Learning based techniques <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b38">39]</ref>. However, all of the above approaches only report outliers, without the reasons (explanations) why they occurred. Our work aims to embrace DL-based anomaly detection methods while recovering human-readable explanations.</p></div>
<div><head n="3">SYSTEM OVERVIEW</head><p>In this section, we present an overview of our EXplainable Anomaly Detection system, referred to as <software>EXAD</software>. <software ContextAttributes="used">EXAD</software> analyzes incoming data streams, from which it flags some data windows as anomalous and generates an explanation for each detected anomaly.</p><p>Our system design is based on a two-pass approach to supporting anomaly detection (AD) and explanation discovery (ED) in the same stream analytics system. The approach is motivated by the observation that though closely related, AD and ED often differ in the nature of computation (e.g., the optimization objective), and it is hard to achieve both in a single procedure. Prior work <ref type="bibr" target="#b40">[41]</ref> showed evidence when logistic regression is used to perform both AD and ED. While the regression model is a good predictive model for some anomalous events, it does not serve as a good explanation. For instance, the learned model assigns non-zero weights to 30 out of 345 input features, and it is hard for the human to understand an explanation with 30 features. Through manual exploration, the domain expert finally selected two features to construct an explanation; however, these two features are ranked low (23 and 24 out of 30) in the learned model. This indicates that the ED method needs to include a penalty term in the optimization objective to favor small explanations, while such a penalty term can often decrease accuracy of an AD model.</p><p>Therefore, our two-pass approach is designed to handle AD and ED in two different passes of the data. In the forward pass, the live data streams are used to drive anomaly detection and also archived for further analysis. The detected anomalies will be delivered immediately to the explanation discovery module. Then in the backward pass, explanation discovery runs on both the archived raw streams and feature sets created in the forward pass. Once the explanation is found, it is delivered to the user with only a slight delay.</p><p>Such decoupling of AD and ED functionalities allows <software ContextAttributes="used">EXAD</software> to have an open architecture to embrace new AD and ED methods as they are proposed in the literature. There are many AD methods including recent Deep Neural Networks (DNNs) based ones <ref type="bibr" target="#b7">[8]</ref> for detecting complex anomaly types, as well as multiple ED methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b40">41]</ref>. However, there are still technical challenges we face in the design of <software ContextAttributes="used">EXAD</software>: (1) AD/ED integration: We still need to link these two models to be able to explain the results of the complex (e.g., DNN-based) anomaly detection model in a humanreadable form, while DNN models are known to be hard to interpret.</p><p>(2) High-dimensionality: The problem is further compounded by the fact that event streams are often in the form of high-dimensional time series data. While DNN-based AD methods have the ability to transform high-dimensional data into low-dimensional embeddings to enable anomaly detection, such embeddings are not interpretable. Hence, the ED methods cannot build explanations on these lowdimensional embeddings, but instead have to deal with the original high-dimensional data. As our evaluation will show, most ED methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b40">41]</ref> fail given 1000's of features in input data.</p><p>To address the above issues, <software ContextAttributes="used">EXAD</software> employs a set of novel techniques, as shown in Figure <ref type="figure" target="#fig_2">2</ref>. At the core of our system is a humaninterpretable dimensionality reduction (HIDR) method to decrease the high dimensionality of time series data and to draw connections between the AD and ED models.</p><p>In offline processing, HIDR (a) clusters input features based on their statistical correlation, and (b) for each cluster of features, finds a single transformed feature through training an autoencoder (AE) that is able to reproduce all the correlated features in the cluster. The output of such offline processing includes a set of AEs, one for each feature cluster, and an explicit mapping between original features and their corresponding transformed features, which will be used to recover human-readable explanations using the original features.</p><p>In online processing, arriving data streams are transformed from the original feature space to the low-dimensional transformed feature space, by running the data through the encoder of each cluster AE. For the AD task, the AD model will run on the HIDR reduced space, which can greatly decrease its training time and online inference time. In order to get good detection results, however, this gain in efficiency must come with the transformed features from HIDR carrying sufficient information for anomaly detection, which our experimental study will verify.</p><p>Once an anomaly is flagged by the AD method, our work provides a new ED framework for generating explanations using the original features -the original feature space is interpretable to the human user whereas the transformed feature space is not. This is achieved via a suite of new techniques: (a) Our ED framework provides a HIDRbased feature selection (FS) method to build a filtered interpretable feature space by leveraging the encoder gradient information from the cluster AEs and selecting a subset from the original feature space. Then this space can be used by any existing ED task to return an explanation. (b) We further provide a two-step approach that repeatedly calls existing ED methods to discover low-level and highlevel explanations, optionally combined with our FS method, to further improve the quality of returned explanations.</p><p>Notation in this paper. Let x (t ) ∈ R m denote each original data item in a time series, and z (t ) ∈ R d denote the transformed item. We have d &lt;&lt; m. Moreover, let F i denote the id of the i-th feature in each original data item x (t ) , and V j denote the id of the j-th feature in z (t ) . Then we maintain explicitly the mapping Φ : R m → R d such that the features {F i } in the same cluster are mapped to the same embedding V j . In addition, we use x (t ) i and z (t ) j to represent the value of the i-th feature in x (t ) and the j-th feature in z (t ) , respectively.</p></div>
<div><head n="3.1">Requirements and Survey of Related Work</head><p>Since real-world time series data is often of high dimensionality (e.g., including 100's to 1000's of features), the first requirement for effective explainable anomaly detection is dimensionality reduction. However, we have to generate explanations using the original features for the human user, which requires recovering the original features from the transformed space. This motivates the concept of a human-interpretable dimensionality reduction (HIDR) method.</p><p>More specifically, we have the following requirements on a dimensionality reduction method that can enable both anomaly detection (AD) and explanation discovery (ED) on high-dimensional time series data: (1) AD Efficiency: whether AD is performed in a lowdimensional embedding space such that the offline training time and online inference time on data streams are minimized; (2) AD Accuracy: whether AD is provided with rich information in such low-dimensional embeddings such that the anomaly detection accuracy is maximized; (3) ED Efficiency: whether ED can be performed efficiently, that is, as soon as an anomaly is flagged on incoming data, an explanation is returned shortly to enable human inspection and potentially corrective actions. ( <ref type="formula">4</ref>) ED Interpretability: whether the explanation returned by the ED method is built over compact, interpretable features that the human can easily understand.</p><p>We next survey the most relevant work on explainable anomaly detection on high-dimensional time series data and examine their performance with respect to the above requirements. Table <ref type="table" target="#tab_0">1</ref> summarizes the analyses.</p><p>Dimensionality Reduction Methods: The first three rows of the table list common dimensionality reduction methods, including PCA <ref type="bibr" target="#b26">[27]</ref>, kernel PCA <ref type="bibr" target="#b26">[27]</ref> and Factor Analysis (FA) <ref type="bibr" target="#b27">[28]</ref>. They are known to be able to reduce high-dimensional data into low-dimensional representations (through linear or non-linear transformations), which can be further fed into any AD method (marked as AD*) for anomaly detection. The issue, however, is that these lowdimensional representations are not interpretable, and hence running an ED method on them will not return human-readable explanations.</p><p>Explanation Discovery Methods: The next three rows list three state-of-the-art ED methods, which we shall evaluate thoroughly in our performance study. <software ContextAttributes="used">EXstream</software> <ref type="bibr" target="#b40">[41]</ref> and LIME <ref type="bibr" target="#b22">[23]</ref> were designed for the ED functionality only, and hence the AD functionality does not apply. For ED, <software ContextAttributes="used">EXstream</software> is shown to be quite fast on highdimensional data, but the interpretability is poor due to the lack of stability (i.e., the explanation for a particular instance changes a lot under just small perturbation of the data) or lack of consistency (i.e., the explanations of a set of anomalies that are known to have the same anomaly type do not agree with each other). This is largely due to the fact that high-dimensional time series data contains many features that are incidentally correlated with an anomaly period and the high dimensionality makes it hard for <software ContextAttributes="used">EXstream</software> to discern the truly relevant ones. LIME <ref type="bibr" target="#b22">[23]</ref> suffers in efficiency and cannot even run on the high-dimensional data (1000's of features) in our performance study. Macrobase <ref type="bibr" target="#b2">[3]</ref> has both the AD and ED methods, which are disjoint methods. While the AD method of Macrobase is efficient but works only for point anomalies, its ED method lacks efficiency and also fails to run on our high-dimensional dataset.</p><p>The new methods provided in <software>EXAD</software>, including Human-Interpretable Dimensionality Reduction (HIDR) and Human-Interpretable Low-dimensional Explanation Discovery (<software ContextAttributes="used">HILED</software>), finally enable both efficiency and accuracy in AD, as well as efficiency and interpretability in ED, as we shall show in our evaluation.</p></div>
<div><head n="4">HIDR DIMENSIONALITY REDUCTION</head><p>To enable dimensionality reduction (DR) of input data into lowdimensional space while being able to explain the detection results, for each feature in the transformed low-dimensional space, we would like to understand which subset of input features and which method were used for its generation. This a key factor in the explanation generation phase. The current state of the art does not provide us with such a method. This motivated us to design a new method that is custom-made for this context. HIDR Principle. A good method for performing dimensionality reduction on a data stream is to train an autoencoder <ref type="bibr" target="#b14">[15]</ref>, which performs a non-linear transformation of its input data into lowdimensional vectors via its encoder and then reconstructs the input via its decoder, with the goal to match the input and decoder output. In our context, we can apply its encoder to the data stream to reduce dimensionality. However, since the explanation discovery (ED) methods tend to analyze features in the low-dimensional space separately, there should be no information overlap between them. To solve this issue, our solution is to first perform correlation clustering on the original features and then train a separate autoencoder for each cluster having a one-dimensional representation.</p><p>More formally, our rationale for using an autoencoder is that each feature in our data is considered as a random variable. We can exploit the high correlations between the variables by regrouping them into correlation clusters and considering them as groups for our models. </p><formula xml:id="formula_0">1: M ← correlation_matrix({x (t ) }), M is the pairwise correlation matrix 2: C = {C 1 , . . . , C d } ← form_cluster(M, t ). ∀i, j C i ∩ C j = ∅,<label>and</label></formula><formula xml:id="formula_1">∪ d i =1 C i = F 3: for each C i ∈ C do 4: E i ← train_autoencoder({x (t ) C i }), the output layer of encoder E i is one-dimensional 5: end for 6: return C = {C 1 , . . . , C d }, E = {E 1 , . . . , E d }</formula></div>
<div><head>Algorithm 2 HIDR: online transformation</head><p>Require: Dataset to be transformed: {x (t ) } Require: Clustering groups:</p><formula xml:id="formula_2">C = {C 1 , . . . , C d } Require: Encoders: E = {E 1 , . . . , E d } 1: for each C i ∈ C do 2: {z (t ) i } ← E i ({x (t ) C i }) 3: end for 4: return {z (t ) } = {[z (t ) 1 , . . . , z (t ) d ]}</formula><p>For each cluster, we can find a single underlying variable that can generate all the features of that cluster, due to their high pairwise correlation. This could be achieved using an autoencoder trained to minimize its reconstruction error, with its encoder being the mapping from each cluster of correlated features to the underlying variable. In contrast, a standard clustering method such as computing the centroid on its own is not general enough since it considers all features to be equally important, which is generally not the case.</p><p>Algorithm. We divide the HIDR method into offline training as in Algorithm 1, and online transformation as in Algorithm 2.</p><p>In the offline training phase (Algorithm 1), the algorithm takes a training dataset as input. It first computes the pairwise Pearson correlation coefficients between the features, thereby producing the correlation matrix. To reduce redundancy in this dataset, the algorithm seeks to regroup these features by clustering them. After the correlation clustering, the algorithm moves onto dimensionality reduction using autoencoders. For each cluster, the algorithm trains a feedforward autoencoder <ref type="bibr" target="#b14">[15]</ref> taking the features of that cluster as input and learning their one-dimensional representation through minimizing its reconstruction error. The final output of this phase includes the feature clusters, {C 1 , . . . , C d }, and the encoder for each cluster, {E 1 , . . . , E d }.</p><p>In the online transformation phase (Algorithm 2), the algorithm takes an incoming dataset as input. Using the encoders trained from the offline phase, {E 1 , . . . , E d }, it can transform each data point, x (t ) ∈ R m , from the original feature space to the transformed feature space R d , where d &lt;&lt; m, thereby generating a low-dimensional vector,</p><formula xml:id="formula_3">z (t ) = [z (t ) 1 , . . . , z (t )</formula><p>d ] in the transformed space. Implementation Details. Throughout <software>EXAD</software>, we use one large reference normal trace for the correlation clustering, and all the normal traces for training the autoencoders. In the clustering step, we do not specify the number of clusters to the method; otherwise, it would potentially force some features into clusters where they do </p><formula xml:id="formula_4">G = {G 1 , . . . , G |C i | } ← cal_gradient(E i , {x (t ) C i }) 4:</formula><p>for each j ∈ {1, . . . ,</p><formula xml:id="formula_5">|C i | } do 5: if G j &gt; t then 6:</formula><p>add index C i (j) to FS 7:</p><p>end if</p><formula xml:id="formula_6">8:</formula><p>end for 9: end for 10: return FS not belong. The method that is best fit to this context is hierarchical clustering <ref type="bibr" target="#b0">[1]</ref>. By applying this method to our high-dimensional dataset with 1,055 features (to be detailed in our evaluation section), we obtained 13 clusters which in turn will reduce the dimension of our data from 1,055 to 13.</p><p>Advantages over other DR methods. One of the advantages of the HIDR method is that we know which features were combined in order to create each specific variable in the transformed space. This is not the case for methods such as PCA or kernel PCA. This is very important for the explanation phase since we must present explanations using the original features to human users.</p><p>The most similar work is the Factor Analysis method. In Factor Analysiswe assume that the observations on the original feature space are a linear transformation of independent latent features (factors) in a space of a lower dimension plus some added Gaussian noise. Without loss of generality, the noise as well as each factor are assumed to follow a Gaussian distribution with zero mean and unit covariance. As such, it tries to reduce the dimensionality and pairwise correlation of the transformed space. However, one cannot separate the latent factors during explanation discovery and more specifically, cannot find the relationship between a latent factor and the original features. Therefore it can not be used for the ED task.</p><p>Another method for interpretable dimensionality reduction is Isomap <ref type="bibr" target="#b33">[34]</ref>. It is a non-linear dimensionality reduction algorithm which considers the intrinsic geometry of the data cloud. Although it is geometrically interpretable, it fails to provide the human users with a simple relevant explanation.</p></div>
<div><head n="5">HIDR FEATURE SELECTION</head><p>HIDR relies on the autoencoders to reduce the dimensionality. This results in an irreversible feature transformation. However, in some tasks such as explanation discovery (ED), it is desirable that the features to be used carry the semantic meanings. The feature set generated by HIDR does not satisfy this requirement. Motivated by this observation, and leveraging the Encoder Gradient information, we further develop a feature selection method based on HIDR, and call it HIDR Feature Selection. This method selects a subset of the original feature space, achieving the dimensionality reduction and keeping the semantic meaning of each feature at the same time. We summarize this method in Algorithm 3 and discuss it more below.</p><p>Encoder Gradients. Inspired by LIME <ref type="bibr" target="#b22">[23]</ref> and Saliency maps <ref type="bibr" target="#b28">[29]</ref>, our solution to feature selection is based on computing the gradient of each cluster encoder with respect to its input features.</p><p>Mathematically, every encoder E is a mapping e : R n → R where n is the number of features of its corresponding cluster. It can be seen as the function mapping from an input vector x (t ) to its representation feature z (t ) . In addition, for each input vector x (t ) , the gradient ∇e(x (t ) ) gives the importance of each feature F i in x (t ) given the mapping e. This indicates that if we use the reconstruction error to train the autoencoders, then we can extract the most important features from a correlation cluster by choosing the ones with higher corresponding absolute values in the gradient.</p><p>In theory, for this extraction, we could use any activation function for the autoencoder. However, we here want to choose an activation function that guarantees the consistency property when deriving the feature importance. The consistency of an explanation is an important property we must achieve. Intuitively, it means that the explanations of similar anomalous datasets should be similar. To ensure this property, we have to impose a constraint on the autoencoders. Mathematically speaking, to ensure explanation consistency, the encoder mapping e must be a uniformly continuous function having a bounded uniformly continuous gradient.</p><p>Next, we first define consistency within the encoder context. Then, we introduce the proposition stating the conditions under which this consistency is guaranteed. After the proof of this proposition, we discuss the choice we made for our activation function.</p><p>Consistency Property: Let E be an encoder, and e : R n → R the corresponding mapping. Consistency in this context means that similar input vectors x (t ) and y (t ) must have similar representations e(x (t ) ) and e(y (t ) ), as well as similar explanations (in our case, ∇e(x (t ) ) and ∇e(y (t ) )). Mathematically:</p><p>(1) ∥x (t ) -y (t ) ∥ ≤ η =⇒ |e(x (t ) ) -e(y (t ) )| ≤ ϵ (2) ∥x (t ) -y (t ) ∥ ≤ η =⇒ ∥∇e(x (t ) ) -∇e(y (t ) )∥ ≤ γ where η, ϵ, γ are positive constants. Proposition 5.1. If all of its activation functions are uniformly continuous (hereafter referred to as UC) and have bounded uniformly continuous derivatives (hereafter referred to BUC) , then a uniformly continuous function e with a uniformly continuous gradient can be learned by the neural network. This proposition states that if all the activation functions of a neural network are both UC and BUC, then the Consistency Property is guaranteed. This proposition can be proved by induction on the number of layers of the network:</p><p>• Base Case: The first layer h 1 has the necessary properties.</p><p>• Inductive Hypothesis: Assume the layer h n shares these properties. • Inductive step: Show that every neuron in the layer h n+1 has a bounded uniform continuous (BUC) derivative with respect to any variable x (t ) F i in any given vector x (t ) . Now we show the proof for the inductive step. For the ease of notation, we drop the superscript t, and write x (t ) F i simply as x k .</p><p>Proof 5.1. We prove that any neuron in layer h n+1 has a BUC derivative with respect to any x k . If we denote the output of a given neuron as o, then this amounts to proving that ∂o ∂x k is BUC.</p><p>Inductive step: Let д be the activation function of a neuron in layer h n+1 . The activation value of this neuron then reads:</p><formula xml:id="formula_7">o = д l i=1 w i • h (i) n</formula><p>Leading to the following partial derivative:</p><formula xml:id="formula_8">∂o ∂x k = l i=1 ∂o ∂h (i) n ∂h (i) n ∂x k</formula><p>Which, by applying the chain rule, is equal to:</p><formula xml:id="formula_9">∂o ∂x k = l i=1 ∂h (i ) n ∂x k • д ′ l i=1 w i • h (i) n</formula><p>Where the righthand side is BUC as a linear combination, composition and product of BUC functions (from the Inductive Hypothesis, h n is BUC), proving the result of the inductive step. By induction, we conclude that the function e learned by a neural network having the specified properties is UC with a BUC gradient, which means that it gives consistent representations with consistent explanations, thus satisfying the Consistency Property. Now the only issue left is the choice of the activation function for our autoencoders. To ensure the consistency property for the derived explanations, we saw that our activation function must be UC with a BUC gradient. For this reason, in this work, we chose to use the ELU (Exponential Linear Unit) activation function for our autoencoders instead of the more commonly used ReLU (Rectified Linear Unit) function. More specifically, ELU and ReLU take both the form of an identity function for non-negative inputs. For negative inputs, however, ELU changes smoothly and slowly as its output approaches a given negative parameter, while ReLU turns directly to zero, making a sharp turn. To conclude, having this architecture with ELU activation functions, we can expect the encoders gradients to give us consistent explanations in the required format.</p></div>
<div><head n="6">LOW-DIMENSIONAL EXPLANATION DISCOVERY</head><p>As discussed above, the output of the HIDR Feature Selection method can be used directly as the input to any existing ED method. However, we noticed that this method does not fully leverage the clustering information. In HIDR dimensionality reduction, each cluster has been transformed to one variable, where the clustering information has been used. In contrast, in HIDR Feature Selection, the clustering/correlation information is used only in the training of the encoder, and then wasted. Motivated by this, we developed a two-step approach which is called <software>HILED</software> (Human-Interpretable Low-dimensional Explanation Discovery). It tries to leverage the clustering information as much as possible, and by introducing the use of an ED algorithm in both steps, it has the potential to generate high-quality explanations.</p><p>The key idea is as follows: First, we call HIDR to transform the dataset to a low-dimensional space. Then we call any existing ED method to generate an explanation. This is the first step of the <software>HILED</software> where the result is a subset of features in the reduced space, V ′ ⊆ {V 1 , . . . , V d }. We call this subset the low-level explanation. Note that these features are not interpretable yet, but correspond to only a subset of clusters (hence simpler to deal with). Then we use only the feature subset {F i } from each of the corresponding clusters, run an ED method again and generate the explanation using the original features, which is called a high-level explanation.</p></div>
<div><head n="6.1">Leveraging Existing ED Methods</head><p>The first step of <software>HILED</software> is to call an ED method to select a subset of features in the transformed space to be the most important ones. Any existing ED algorithm that takes as input the dataset and ground truth (label) and outputs the important feature subset can be used in <software ContextAttributes="used">HILED</software>. In this work, we experiment with three existing algorithms.</p><p><software ContextAttributes="used">EXstream</software> <ref type="bibr" target="#b40">[41]</ref> is an explanation model specifically designed for monitoring high-volume event streams from enterprise information systems <ref type="bibr" target="#b40">[41]</ref>. This model compares a reference normal period with an anomalous period (which is detected by the AD method of choice) by analyzing the segmentation of the values of the two periods. This way, it looks for an anomalous interval of the values of each feature; for a given feature, the more we can separate the normal values from the anomalous ones, the more important it becomes in explaining the detected anomaly.</p><p>The method starts by computing a single-feature reward for every feature and only keeps the ones with the highest scores. It then performs a correlation clustering to detect the pairs of correlated features and discards the feature with the lower rewards. This results in choosing a subset V * = {V * 1 , . . . , V * l } from the original features V which have the most important contribution to the anomalous behavior. It generates explanations in the form of</p><formula xml:id="formula_10">(V * 1 ∈ S 1 ) ∧ • • • ∧ (V * l ∈ S l )</formula><p>, where S i ⊂ IR is the range of anomalous values corresponding to V * i for all i ∈ {1, . . . , l }. LIME <ref type="bibr" target="#b22">[23]</ref> was originally designed to derive explanations for a classifier's predictions, but also provides an interface for regression models and temporal data. Internally, LIME first leverages Lasso <ref type="bibr" target="#b10">[11]</ref> to identify k important features, and then uses a customized loss function to learn a new linear model to approximate the original model's output locally. In our implementation, we used the <software ContextAttributes="used">RecurrentTabularExplainer</software> interface provided by LIME, with k = 5.</p><p>MacroBase <ref type="bibr" target="#b2">[3]</ref> proposes an explanation method called the MDP explanation operator. The algorithm and logic of this method were designed for categorical features (referred to as attributes). This operator takes as input a set of outliers O and inliers I , as well as two parameters called the minimum risk ratio, r , and minimum support, s. It first finds attributes with sufficient support in O and sufficiently high risk ratio in O and I , to remove the attributes that do not contribute to anomalous behaviors. It then runs frequent itemset mining (building an FP-Growth Tree) over O using only the previously found attributes to form the best attribute combination as the final explanation. This method was designed exclusively for explaining datasets consisting of categorical features, but not numerical features. In our work, we applied discretization to extract categorical features from our dataset.</p></div>
<div><head n="6.2">The HILED Approach</head><p>We next explain how our <software>HILED</software> technique uses an existing ED method in a two-step approach, as shown in Algorithm 4.</p><p>Low-level Explanation. The input to the first step is the dataset {x (t ) } that was marked as anomalous by an AD method. Depending</p></div>
<div><head>Algorithm 4 HILED</head><p>Require: Dataset to be explained: {x (t ) } Require: Point-wise label information from the AD method: {l }, l ∈ {0, 1} Require: Clustering groups: C = {C 1 , . . . , C d } Require: Encoders: E = {E 1 , . . . , E d } Require: Additional parameters p for the ED method 1: {z (t ) } ← HIDR_online_trans({x (t ) }, C, E) 2: V ′ = {V i 1 , . . . , V im } ← ED({z (t ) }, {l }, p). This is the end of the first step, low-level explanation 3: <ref type="figure">E</ref>). This is the optional step to further reduce the dimensionality 5: {F i 1 , . . . , F in } ← ED({x</p><formula xml:id="formula_11">F ′ = ∪ m j =1 C V i j 4: [optional] F ′ ← HIDR_feature_selection(F ′ , C,</formula><formula xml:id="formula_12">(t )</formula><p>F ′ }, {l }, p). This is the second step, highlevel explanation 6: return {F i 1 , . . . , F in } on the ED method used, a normal dataset might be required to serve as a reference. Then, our system extends {x (t ) } with a time window of the data directly preceding it, assumed to be normal as not flagged anomalous by the AD method. As Algorithm 4 shows, the low-level explanation discovery step applies HIDR online transformation (i.e., running {x (t ) } through the cluster encoders) to generate the features {z (t ) } in the reduced space. The output of the ED method is a selected set of important features, V ′ = {V i1 , . . . , V im }, in the reduced space.</p><p>By performing this step, we gain two advantages compared to running the ED method on the raw input data directly. First, we can fully leverage the clustering information. Second, it will reduce the complexity dramatically compared to using the raw feature space. For example, if we denote the number of features as m, and only consider the complexity terms regarding m, Macrobase is exponential in the worst case, O(2 m ); <software>EXstream</software> is O(m 3 ), and LIME is also O(m 3 ). Hence, reducing the number of features from m = 1055 to d = 13 will certainly bring enormous performance benefits.</p><p>Optional Step for Further Dimensionality Reduction. For the ED task, we need to use the original features, not the transformed features, to return interpretable explanations. A straightforward solution is as step 3 in Algorithm 4, which constructs a filtered interpretable feature space by merging the original features only from the clusters selected in the low-level explanation. However, this filtered feature space may still contain too many features for an ED algorithm to handle. For example, Macrobase cannot run on this filtered feature space (the number of features per cluster is 1055  13 and the worst case complexity of Macrobase is exponential). In such a scenario, we need an optional step to further prune the interpretable feature space. This optional step is step 4 in Algorithm 4. For each transformed feature V i ∈ V ′ , we compute the gradient of the corresponding encoder ∇E i and keep only the original features corresponding to the top values of the gradient. This way, we can find the most informative features within a cluster which can also have the highest contribution to the anomaly. More precisely, the low-level explanation gives us a subset of the reduced features that are causing the anomaly (V ′ ). Also, for each V i ∈ V ′ we know the cluster of features C i and the corresponding encoder E i . Therefore, in order to find the original features that had the greatest effect on the anomalous behavior, we can analyze the effect of every feature in C i on the behavior of E i . Since the autoencoders are trained using the reconstruction error, finding the most important input variables of the encoder implies finding the most important features in the correlation cluster. This step is equivalent to running HIDR Feature Selection on the clusters selected in the low-level explanation discovery.</p><p>High-Level Explanation. Given the filtered interpretable feature space (computed by step 3 and optionally, step 4), the next step is to discover the high-level explanation (step 5). This step will run an existing ED method on this filtered feature space and generate the final explanation. The interpretability comes with the usage of the original features, and the efficiency comes from using only a subset of these original features (step 1 and optionally, step 4).</p></div>
<div><head n="7">EXPERIMENTAL RESULTS</head><p>In this section, we evaluate our HIDR and <software ContextAttributes="used">HILED</software> techniques using a recent benchmark for explainable anomaly detection <ref type="bibr" target="#b16">[17]</ref>.</p></div>
<div><head n="7.1">Experimental Setup</head><p>We implemented <software ContextAttributes="used">EXAD</software> with three main modules: 1. Dimensionality reduction (DR) using (i) the HIDR method, (ii) PCA <ref type="bibr" target="#b26">[27]</ref>, a standard DR method, and (iii) Factor Analysis <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b35">36]</ref>, a more elaborate data reduction technique that works by modeling the observed data from theoretical latent factors.</p><p>2. Anomaly detection using an autoencoder (AE) network <ref type="bibr" target="#b14">[15]</ref> along with an unsupervised threshold selection method. First, the AE is trained to accurately reconstruct time windows of normal data only. The outlier score of a test window is then defined as proportional to the reconstruction error by the AE, and the outlier score of a test record as the average outlier score of the windows it belongs to. The final threshold on the outlier scores, above which we flag a record as anomalous, is determined through the statistical modeling of a subset of the normal data, using the same methodology as in <ref type="bibr" target="#b16">[17]</ref>.</p><p>3. The <software>HILED</software> approach for explanation discovery, including (1) HIDR feature selection followed by an existing ED method;</p><p>(2) two-level explanation discovery, denoted as ED + ED for lowlevel and high-level explanations, respectively, optionally with feature selection before the high-level explanation method is run. We have integrated a number of existing ED methods in <software ContextAttributes="used">EXAD</software> for evaluation: (i) the gradient-based method described in the previous sections; (ii) <software ContextAttributes="used">EXstream</software> <ref type="bibr" target="#b40">[41]</ref>, (iii) MacroBase <ref type="bibr" target="#b2">[3]</ref>, (iv) LIME <ref type="bibr" target="#b22">[23]</ref>.</p><p>Anomaly Detection Benchmark. We used the <software ContextAttributes="used">Exathlon</software> anomaly detection benchmark <ref type="bibr" target="#b16">[17]</ref>. This benchmark was designed for detecting anomalies in the execution of <software ContextAttributes="used">Spark</software> streaming applications based on metrics collected from both the <software ContextAttributes="used">Spark</software> UI (Monitoring and Instrumentation) interface and the underlying OS. The <software ContextAttributes="used">Exathlon</software> dataset includes 93 traces and a ground truth table. The 93 traces, corresponding to 93 recurrent runs of <software ContextAttributes="used">Spark</software> streaming applications, contain a total of 2,335,781 records, collected at the frequency of each second over 27 days. In their raw format, the records contain 2,283 metrics (features), including (i) 243 <software ContextAttributes="used">Spark</software> driver metrics; (ii) 140 <software ContextAttributes="used">Spark</software> executor metrics for each of 5 executor spots that can be used in a <software ContextAttributes="used">Spark</software> application; (iii) 335 OS metrics for each of the 4 nodes of the cluster the <software ContextAttributes="used">Spark</software> applications were run on. Hence, the benchmark presents high-dimensional time series data for analysis.</p><p>Among the 93 traces, 59 were left undisturbed, while 34 were disturbed by injecting external events of 6 different types: bursty input, bursty input until crash, stalled input, CPU contention, driver failure and executor failure (more details regarding the anomaly  types can be found in <ref type="bibr" target="#b16">[17]</ref>). Each disturbed trace includes periods of normal state as well as abnormal state corresponding to a given type of anomaly. The ground truth table records a total of 97 anomaly instances with their root cause events, in rows of the form (app_id, trace_id, anomaly_type, root_cause_start, root_cause_end, extended_effect_start, extended_effect_end). Throughout this study, the anomalous intervals are considered as spanning from the start of the root cause to the end of the extended effect. Feature Preprocessing. Prior to using any of the techniques described, we applied some basic preprocessing to the raw features of the <software ContextAttributes="used">Exathlon</software> dataset. This preprocessing involved dropping some unused metrics and performing some aggregation, leading us to having 1,055 features as input, instead of the original 2,283 features.</p><p>System. All of our experiments were carried out on a server with 2 Intel Xeon Gold 6130 processors with 16 cores each, 768GB of memory, and 64 TB disks.</p></div>
<div><head n="7.2">HI Dimensionality Reduction</head><p>In this section, we compare HIDR to other popular dimensionality reduction methods from the state of the art. We chose PCA <ref type="bibr" target="#b26">[27]</ref> and Factor Analysis <ref type="bibr" target="#b27">[28]</ref> as representatives. PCA is the most commonly used method for tabular data, while Factor Analysis is the closest method to our work: it aims to find underlying variables in the data with very little pairwise statistical correlation, which is one of the essential ideas behind HIDR. The purpose of this experiment is to examine how HIDR compares to PCA or Factor Analysis for dimensionality reduction as a means for anomaly detection.</p><p>Size Comparison. Given the very high dimensionality of our dataset, the first measure we consider is the number of features resulting from each method. We present these results in Table <ref type="table" target="#tab_3">2</ref>. From this table, we observe that applying HIDR results in a significantly smaller number of features than PCA keeping 95% of variance, with PCA leading to more than forty times as many features. For a fairer comparison to HIDR with respect to the input dimensionality, we also consider both PCA and Factor Analysis with their number of output features explicitly set to 13 in the following.</p><p>Accuracy of Anomaly Detection. We next compare the performance of our anomaly detection model on different feature spaces to evaluate the amount of information carried by each feature space.</p><p>Table <ref type="table" target="#tab_4">3</ref> presents the AD results for this experiment, averaged across five different runs. CUSTOM refers to using the 19 manually constructed features of FS custom in <ref type="bibr" target="#b16">[17]</ref>, that we treat as an upper-bound performance in our experiment. NO DR stands for "no dimensionality reduction" and FA for "Factor Analysis". For each run, the Autoencoder-based anomaly detection (AD) model was trained on the undisturbed traces of the <software ContextAttributes="used">Exathlon</software> dataset and tested on its disturbed traces (LS4 in <ref type="bibr" target="#b16">[17]</ref>). The same AD model hyperparameters were used for each dimensionality reduction method. The AD performance is here reported as the "global point-based"  F1-score, Precision and Recall, that is, considering all anomaly types the same and using the classical, point-based, definitions of Precision and Recall. For each run of the experiment, we evaluated the SD, MAD and IQR thresholding methods <ref type="bibr" target="#b39">[40]</ref> fit on a subset of undisturbed traces that was not used for the AD model training/validation, each with the same 8 parameter combinations as in <ref type="bibr" target="#b16">[17]</ref>, and retained only the threshold resulting in the best F1-score.</p><p>For the HIDR method, the correlation matrix was computed on the largest undisturbed trace only, and the autoencoders were trained for 20 epochs per trace, to remove any biases arising from different traces having different lengths.</p><p>The main observations from Table <ref type="table" target="#tab_4">3</ref> are: (1) The AD performance using the features produced by HIDR is comparable to the one obtained without dimensionality reduction and using PCA with 95% of kept variance, while also being close to the one obtained with the FS custom feature set. On the other hand, HIDR achieves such accuracy using only 13 features, as opposed to 1,055 raw features, or 577 features based on PCA (95%). ( <ref type="formula">2</ref>) When PCA and Factor Analysis are asked to use only 13 features, as in HIDR, there is an obvious drop in accuracy. HIDR significantly outperforms these methods that also used 13 output features in accuracy, while also being more interpretable. These results show that the best AD performance can be maintained using HIDR while working with a much more efficient and interpretable feature space.</p><p>Training Efficiency. Using HIDR, the average training epoch time for the AD model was reduced by a factor of around 6 compared to PCA (95%), and by a factor of 14 compared to not applying dimensionality reduction.</p><p>Online Inference. Table <ref type="table" target="#tab_5">4</ref> reports the details of inference speed as live data arrives for the five considered methods, for both the dimensionality reduction (DR) and AD model computation (i.e., computing the outlier scores, labeled as SCORING) steps. These results show that, despite the HIDR inference being slower than the other regular dimensionality reduction methods, it is still able to process data records much faster than they arrive, with a total inference rate of around 1,500 records/sec, compared to the data arrival rate of one record/sec for each running <software ContextAttributes="used">Spark</software> application of this benchmark dataset. This means that our inference speed is fast enough to handle around 1,500 <software ContextAttributes="used">Spark</software> applications on a single CPU (assuming that there is enough network bandwidth to do so).</p></div>
<div><head n="7.3">ED Evaluation Results and Discussion</head><p>In this section, we report the results of running existing ED algorithms within the <software ContextAttributes="created">HILED</software> framework. We selected <software ContextAttributes="created">EXstream</software> <ref type="bibr" target="#b40">[41]</ref>, MacroBase <ref type="bibr" target="#b2">[3]</ref>, and LIME <ref type="bibr" target="#b22">[23]</ref> (tabular explanations and recurrent tabular explanations) as the explanation methods due to their popularity in the databases and machine learning communities.</p><p>Regarding evaluation metrics, we adopt the ones of the <software ContextAttributes="created">Exathlon</software> benchmark <ref type="bibr" target="#b16">[17]</ref>, used in the context of the ED1 and ED2 setups of its Explanation Discovery Functionality. For the sake of completeness, we briefly describe these concepts here.</p><p>In the ED1 setup (Local Explanation), we evaluate the explanation of each anomaly instance individually. The explanation returned for an anomaly instance is a set of important features. (1) Conciseness is the length of each explanation (size of the feature set). ( <ref type="formula">2</ref>) Consistency captures how stable the explanation is. It is computed by running the ED method multiple times where each run only uses a random selection of 80% of the data from the detected anomaly interval to generate the explanation. Multiple runs of the ED method return a set of explanations. We put all of these explanations into a combined feature set, and then compare how they agree with each other by calculating the entropy of the combined set (ideally, the fewer features in the set, the better). This entropy-based measurement is called consistency. (3) Normalized Consistency: However, the consistency metric depends also on the size of the explanation (i.e., conciseness). Hence we use one more measurement, called the normalized consistency, to eliminate the impact of the conciseness. The formula for it is 2 cons is t e ncy conciseness , and its best value is 1. In the ED2 setup (Global Explanation), we consider multiple anomaly instances that belong to the same anomaly type (based on the ground truth given for evaluation). The ED2 measurements capture how the explanations of the same anomaly type agree with each other. (1) Conciseness is defined as the average explanation size of these multiple anomalies. (2) Consistency: We put the explanations of multiple instances of the same anomaly type into a combined feature set, and then calculate the entropy of the combined set (again, the fewer features in the set, the better). (3) Normalized Consistency is defined as in ED1.</p><p>EXPT1: Generating Explanations From Raw Features. In the first experiment, we generate explanations from the original feature space using the three ED methods. For each method, we report in Table <ref type="table" target="#tab_7">5</ref> the conciseness, consistency, and normalized consistency for ED1 and ED2, as well as the running time in second. Each row corresponds to a specific anomaly type (listed as T1-T6), and the final row is the average result across all anomaly types.</p><p>As we can see in Table <ref type="table" target="#tab_7">5</ref>, with 1,055 features in the raw input data, Macrobase and LIME cannot return any result within 24 hours. <software ContextAttributes="created">EXstream</software> manages to generate explanations. However, the consistency performance is poor: the normalized consistency for ED2 is 10.38, far from the ideal value of 1. This is because high-dimensional time series data contains many features that are incidentally correlated with an anomaly period, and the high dimensionality makes it hard for EXstream to discern the truly relevant ones. The results  here indicate that none of the three algorithms are able to generate quality explanations while handling the high dimensionality.</p></div>
<div><head>EXstream</head><p>EXPT2: Generating Explanations From Low-Level Features: Using HIDR for Dimensionality Reduction. In this experiment, we generate explanations from the feature space of HIDR. For each cluster of HIDR, we use its encoder as a feature transformation tool, to generate one new feature per cluster. This way, for each data point in the original feature space, we transform it to a data point in a low-dimensional feature space. The dimensionality is significantly reduced; in this dataset, we reduce the number of features from 1,055 to 13. In the <software>HILED</software> terminology, the explanation built on this reduced feature space is called a low-level explanation.</p><p>In Table <ref type="table" target="#tab_8">6</ref>, we list all the results of the three ED methods for building low-level explanations. Here we can draw two main conclusions. First, LIME and Macrobase can now generate results, while previously they could not. Second, the performance of <software ContextAttributes="created">EXstream</software> has been significantly improved.</p><p>We should emphasize that despite such an improvement, there is a significant drawback limiting the usage of low-level explanations: the low-level features do not carry any semantic meaning for a human user. However, we listed the results here for two purposes. First, we show the potential improvements assuming these low-level features can be understood by a human user -hence these improvements represent an "upper bound" on what an interpretable ED method can actually achieve. Second, and more importantly, we are going to use the low-level explanation as the intermediate step in the two-step <software>HILED</software> approach. Given our current evaluation results, we decide to use Macrobase as the ED method for the first step of <software ContextAttributes="created">HILED</software> in the following experiments.</p><p>Also note that, from this point onward, we only use Macrobase and <software>EXstream</software> as the ED algorithms. We do not evaluate LIME, because it requires a pre-trained AE model on a fixed feature set. In the following experiments, it is very often that we need to vary the feature set according to the different input datasets, and was impractical to train AE models on different feature sets dynamically.</p><p>EXPT3: Generating Explanations using Gradient Information: Using HI as Feature Selection. As discussed before, using HIDR for dimensionality reduction results in the low-level features, that do not carry any semantic meaning. We have two different ways to resolve this issue. The first way is to leverage the encoder gradient method as a feature selection tool, as discussed in Section 5. This method selects a subset of the original feature space, where for each cluster of features, only the one that contributes the most to the cluster representation is selected. Hence, this method achieves the dimensionality reduction while keeping the semantic meanings of the original features at the same time. Then we can run either Macrobase or <software ContextAttributes="created">EXstream</software> as an ED method on the selected feature set. We list all the results in Table <ref type="table" target="#tab_9">7</ref>. As we can see, compared to the ED results on the raw input data, Macrobase and <software ContextAttributes="created">EXstream</software> achieve a significant improvement. The quality of their results is indeed comparable to the one using low-level features in Table <ref type="table" target="#tab_8">6</ref>.</p><p>EXPT4: Generating Explanations with the Two-Step Approach. As mentioned before, using the HIDR feature selection tool is the first way to run an existing ED method to return interpretable explanations. The second way is to use the two-step <software>HILED</software> approach discussed in Section 6.2. More precisely, we use the aforementioned reduced space to run Macrobase or <software ContextAttributes="created">EXstream</software> to select a set of clusters. Then, on the subset of original features corresponding to these clusters, we run the high-level experiment to generate the explanations for the human user. Running the low-level explanation first provides two main advantages. First, it can reduce the size of the feature space, and take care of the correlation issue among features. Second, it leverages the gradient information which can be used independently in the second step for generating the high-level explanation, or as an optional step between the two steps to further reduce the number of features.</p><p>In Table <ref type="table" target="#tab_10">8</ref>, we list the results of the two-step approach with different variants of using the ED method and HIDR feature selection. Since Macrobase achieved a better result in the low-level explanation experiment, in this experiment we always run Macrobase as the first step. The results listed here are for three different combinations. It is worth mentioning that we could not generate the results for Macrobase+Macrobase, since the high-level experiment with Macrobase was still too slow to run. Therefore, we added Gradient as an additional feature selection over the original features. Here, our main observation is that this two-step approach offers high flexibility for the user to try different combinations of an ED method and the Gradient method for feature selection. Also, as we can see in some cases, if properly tuned, the ED methods can lead to results of high quality. For example, compared to Table <ref type="table" target="#tab_9">7</ref>, Marcobase+Gradient and Macrobase+Gradient+Macrobase both lead to reduced values for most of the conciseness and consistency metrics.</p><p>Summary. Our main results are as follows: (1) Our HIDR technique outperforms other dimensionality reduction methods in the size of the reduced feature space, as well as in the accuracy of anomaly detection results based on the reduced feature space; (2) None of the three state-of-the-art ED methods can handle high-dimensional feature spaces. Using our HIDR Feature Selection method, these methods can generate explanations with dramatic improvements in conciseness and consistency of the explanations, as well as in efficiency, compared to running ED directly on the raw feature space.</p><p>(3) Further, our HILED approach provides a flexible framework for trying different ways to combine an ED method with our gradientbased feature selection method, offering an opportunity for the user to fine-tune the performance to achieve better explanations.</p></div>
<div><head n="8">CONCLUSIONS</head><p>In this paper, we presented <software ContextAttributes="created">EXAD</software> for integrated anomaly detection and explanation discovery that allows us to leverage state-of-theart Deep Learning techniques for anomaly detection, while being able to recover human-readable explanations for detected anomalies. Our key techniques include HIDR dimensionality reduction and feature selection, as well as the <software ContextAttributes="created">HILED</software> approach to explanation discovery. Our main results from the <software ContextAttributes="created">Exathlon</software> <ref type="bibr" target="#b16">[17]</ref> benchmark are: (1) HIDR outperforms other dimensionality reduction methods in the size of reduced feature space, as well as in the accuracy of anomaly detection (AD) results based on the reduced feature space.</p><p>(2) None of the three state-of-the-art ED methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b40">41]</ref> can handle high-dimensional time series data. Using our HIDR feature selection method, these methods can generate explanations with dramatic improvements in conciseness and consistency of the explanations, as well as in computational efficiency. (3) Further, our <software ContextAttributes="created">HILED</software> approach provides a flexible framework for combining existing ED methods with our gradient-based feature selection method, offering an opportunity for the user to fine-tune the performance to achieve better explanations.</p></div><figure xml:id="fig_0"><head /><label /><figDesc>(a) Normal execution of a job (b) Job execution under bursty input (c) Job execution under bursty input until crash</figDesc></figure>
<figure xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Illustration of anomalies in Spark job execution. A pair of red vertical bars represent the interval of an anomaly</figDesc><graphic coords="3,48.86,82.46,166.46,99.21" type="bitmap" /></figure>
<figure xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: An integrated system for anomaly detection and explanation discovery</figDesc><graphic coords="6,79.02,85.60,453.96,125.44" type="bitmap" /></figure>
<figure xml:id="fig_3"><head>Algorithm 1</head><label>1</label><figDesc>HIDR: offline training Require: Training dataset: {x (t ) } Require: Fearure set: F = {F 1 , . . . , F m } Require: Hierarchical clustering threshold: t</figDesc></figure>
<figure type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Requirements on explainable anomaly detection on highdimensional time series data and a survey of related work</figDesc><table><row><cell /><cell>AD</cell><cell /><cell>ED</cell><cell /></row><row><cell /><cell cols="4">Efficiency Accuracy Efficiency Interpret.</cell></row><row><cell>1) PCA [27] + AD* + ED*</cell><cell>Yes</cell><cell>Decent</cell><cell>Maybe</cell><cell>No</cell></row><row><cell>2) kPCA [27] + AD* + ED*</cell><cell>Yes</cell><cell>Variable</cell><cell>Maybe</cell><cell>No</cell></row><row><cell>3) FA [28] + AD* + ED*</cell><cell>Yes</cell><cell>Variable</cell><cell>Maybe</cell><cell>No</cell></row><row><cell>4) ED: EXstream [41]</cell><cell>-</cell><cell>-</cell><cell>Maybe</cell><cell>Poor</cell></row><row><cell>5) ED: LIME [23]</cell><cell>-</cell><cell>-</cell><cell>No</cell><cell>Poor</cell></row><row><cell>6) AD+ED: Macrobase [3]</cell><cell>Yes</cell><cell>Poor</cell><cell>No</cell><cell>Poor</cell></row><row><cell>7) HIDR + AD* + HILED</cell><cell>Yes</cell><cell>Good</cell><cell>Yes</cell><cell>Good</cell></row></table></figure>
<figure type="table" xml:id="tab_1"><head /><label /><figDesc>Algorithm 3 HIDR Feature Selection Require: Dataset for the gradient calculation: {x (t ) } Require: Clustering groups: C = {C 1 , . . . , C d } Require: Encoders: E = {E 1 , . . . , E d } Require: Threshold for gradient selection: t 1: FS ← {}, FS is the set of important features to be selected 2: for each C i ∈ C do</figDesc><table><row><cell>3:</cell></row></table></figure>
<figure type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>The size of different feature spaces. Note that Factor Analysis is not included in the table because it must take the number of factors as input, but cannot automatically identify an appropriate number itself</figDesc><table /></figure>
<figure type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Accuracy of Autoencoder-based anomaly detection on differ-</figDesc><table><row><cell>Method</cell><cell cols="3">F1-score Precision Recall</cell></row><row><cell>CUSTOM</cell><cell>0.59</cell><cell>0.50</cell><cell>0.72</cell></row><row><cell>NO DR</cell><cell>0.57</cell><cell>0.46</cell><cell>0.75</cell></row><row><cell>HIDR</cell><cell>0.58</cell><cell>0.48</cell><cell>0.74</cell></row><row><cell>PCA (95%)</cell><cell>0.57</cell><cell>0.49</cell><cell>0.70</cell></row><row><cell>PCA (13)</cell><cell>0.52</cell><cell>0.46</cell><cell>0.59</cell></row><row><cell>FA (13)</cell><cell>0.50</cell><cell>0.42</cell><cell>0.62</cell></row><row><cell>ent feature spaces</cell><cell /><cell /><cell /></row><row><cell>Method</cell><cell>DR</cell><cell cols="2">SCORING TOTAL</cell></row><row><cell>NO DR</cell><cell>N/A</cell><cell>20.5</cell><cell>20.5</cell></row><row><cell>HIDR</cell><cell>671.6</cell><cell>10.9</cell><cell>682.5</cell></row><row><cell>PCA (95%)</cell><cell>6.8</cell><cell>16.7</cell><cell>23.5</cell></row><row><cell>PCA (13)</cell><cell>4.6</cell><cell>11.3</cell><cell>15.8</cell></row><row><cell>FA (13)</cell><cell>5.1</cell><cell>11.0</cell><cell>16.1</cell></row></table></figure>
<figure type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Inference speed of Autoencoder-based anomaly detection on different feature spaces in µs/record</figDesc><table /></figure>
<figure type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Results of explanations returned by EXstream, MacroBase (MB), and LIME when running on raw input data (1,055 features)</figDesc><table /></figure>
<figure type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Results of low-level explanations returned by EXstream, MacroBase, and LIME when running on the HIDR reduced space of 13 features</figDesc><table><row><cell /><cell /><cell /><cell cols="4">EXstream (low-level)</cell><cell /><cell /><cell /><cell /><cell cols="4">MacroBase (low-level)</cell><cell /><cell /><cell /><cell /><cell cols="3">LIME (low-level)</cell></row><row><cell /><cell cols="2">Concise</cell><cell cols="2">Consistency</cell><cell cols="2">Norm.Consis</cell><cell cols="2">Time (sec)</cell><cell cols="2">Concise</cell><cell cols="2">Consistency</cell><cell cols="2">Norm.Consis</cell><cell cols="2">Time (sec)</cell><cell cols="2">Concise</cell><cell cols="2">Consistency</cell><cell cols="2">Norm.Consis</cell><cell>Time (sec)</cell></row><row><cell /><cell>ED1</cell><cell>ED2</cell><cell>ED1</cell><cell>ED2</cell><cell>ED1</cell><cell>ED2</cell><cell cols="2">ED1/2</cell><cell>ED1</cell><cell>ED2</cell><cell>ED1</cell><cell>ED2</cell><cell>ED1</cell><cell>ED2</cell><cell cols="2">ED1/2</cell><cell>ED1</cell><cell>ED2</cell><cell>ED1</cell><cell>ED2</cell><cell>ED1</cell><cell>ED2</cell><cell>ED1/2</cell></row><row><cell>T1</cell><cell>4.00</cell><cell>4.03</cell><cell>1.99</cell><cell>3.50</cell><cell>1.31</cell><cell>2.81</cell><cell cols="2">0.0168</cell><cell>1.50</cell><cell>1.38</cell><cell>0.62</cell><cell>2.60</cell><cell>1.17</cell><cell>4.40</cell><cell cols="2">0.43</cell><cell>2.60</cell><cell>4.10</cell><cell>2.14</cell><cell>3.27</cell><cell>1.87</cell><cell>2.36</cell><cell>194</cell></row><row><cell>T2</cell><cell>1.80</cell><cell>1.86</cell><cell>0.67</cell><cell>2.72</cell><cell>1.11</cell><cell>3.55</cell><cell cols="2">0.0065</cell><cell>1.71</cell><cell>1.43</cell><cell>1.04</cell><cell>1.90</cell><cell>1.30</cell><cell>2.60</cell><cell cols="2">0.83</cell><cell>3.37</cell><cell>6.57</cell><cell>2.80</cell><cell>3.39</cell><cell>2.13</cell><cell>1.59</cell><cell>179</cell></row><row><cell>T3</cell><cell>7.38</cell><cell>8.08</cell><cell>3.03</cell><cell>3.54</cell><cell>1.19</cell><cell>1.43</cell><cell cols="2">0.0185</cell><cell>1.70</cell><cell>1.67</cell><cell>0.90</cell><cell>2.32</cell><cell>1.19</cell><cell>3.00</cell><cell cols="2">0.31</cell><cell>3.67</cell><cell>6.17</cell><cell>3.01</cell><cell>3.42</cell><cell>2.25</cell><cell>1.74</cell><cell>202</cell></row><row><cell>T4</cell><cell>3.73</cell><cell>4.60</cell><cell>1.47</cell><cell>3.63</cell><cell>1.15</cell><cell>2.69</cell><cell cols="2">0.0088</cell><cell>1.54</cell><cell>1.55</cell><cell>0.59</cell><cell>3.04</cell><cell>1.06</cell><cell>5.30</cell><cell cols="2">0.13</cell><cell>3.44</cell><cell>4.45</cell><cell>2.80</cell><cell>3.22</cell><cell>2.08</cell><cell>2.09</cell><cell>185</cell></row><row><cell>T5</cell><cell>2.80</cell><cell>2.43</cell><cell>1.90</cell><cell>3.10</cell><cell>1.59</cell><cell>3.54</cell><cell cols="2">0.0056</cell><cell>2.74</cell><cell>2.14</cell><cell>2.17</cell><cell>2.46</cell><cell>1.75</cell><cell>2.57</cell><cell cols="2">0.16</cell><cell>3.60</cell><cell>2.00</cell><cell>2.65</cell><cell>2.29</cell><cell>1.77</cell><cell>2.45</cell><cell>167</cell></row><row><cell>T6</cell><cell>3.50</cell><cell>3.38</cell><cell>1.73</cell><cell>3.61</cell><cell>1.46</cell><cell>3.61</cell><cell cols="2">0.0075</cell><cell>2.17</cell><cell>2.00</cell><cell>1.34</cell><cell>3.16</cell><cell>1.30</cell><cell>4.46</cell><cell cols="2">0.11</cell><cell>3.34</cell><cell>3.29</cell><cell>2.60</cell><cell>2.99</cell><cell>2.06</cell><cell>2.43</cell><cell>167</cell></row><row><cell>Ave</cell><cell>3.87</cell><cell>4.06</cell><cell>1.80</cell><cell>3.35</cell><cell>1.30</cell><cell>2.94</cell><cell cols="2">0.0106</cell><cell>1.90</cell><cell>1.69</cell><cell>1.11</cell><cell>2.58</cell><cell>1.30</cell><cell>3.72</cell><cell cols="2">0.33</cell><cell>3.34</cell><cell>4.43</cell><cell>2.67</cell><cell>3.10</cell><cell>2.03</cell><cell>2.11</cell><cell>183</cell></row><row><cell /><cell /><cell /><cell /><cell /><cell /><cell /><cell cols="4">Feature Selection + EXstream</cell><cell /><cell /><cell /><cell /><cell cols="4">Feature Selection + MacroBase</cell><cell /><cell /><cell /></row><row><cell /><cell /><cell /><cell /><cell /><cell cols="2">Concise</cell><cell cols="2">Consistency</cell><cell cols="2">Norm.Consis</cell><cell cols="2">Time (sec)</cell><cell cols="2">Concise</cell><cell cols="2">Consistency</cell><cell cols="2">Norm.Consis</cell><cell cols="2">Time (sec)</cell><cell /></row><row><cell /><cell /><cell /><cell /><cell /><cell>ED1</cell><cell>ED2</cell><cell>ED1</cell><cell>ED2</cell><cell>ED1</cell><cell>ED2</cell><cell cols="2">ED1/2</cell><cell>ED1</cell><cell>ED2</cell><cell>ED1</cell><cell>ED2</cell><cell>ED1</cell><cell>ED2</cell><cell cols="2">ED1/2</cell><cell /></row><row><cell /><cell /><cell /><cell /><cell>T1</cell><cell>2.24</cell><cell>2.55</cell><cell>1.02</cell><cell>3.20</cell><cell>1.34</cell><cell>3.61</cell><cell cols="2">0.33</cell><cell>1.90</cell><cell>1.86</cell><cell>0.97</cell><cell>2.43</cell><cell>1.16</cell><cell>2.89</cell><cell cols="2">0.86</cell><cell /></row><row><cell /><cell /><cell /><cell /><cell>T2</cell><cell>1.57</cell><cell>1.57</cell><cell>0.74</cell><cell>2.30</cell><cell>1.21</cell><cell>3.13</cell><cell cols="2">0.50</cell><cell>1.51</cell><cell>1.43</cell><cell>0.55</cell><cell>1.49</cell><cell>1.05</cell><cell>1.96</cell><cell cols="2">0.97</cell><cell /></row><row><cell /><cell /><cell /><cell /><cell>T3</cell><cell>3.85</cell><cell>4.75</cell><cell>2.19</cell><cell>3.39</cell><cell>1.56</cell><cell>2.21</cell><cell cols="2">0.24</cell><cell>2.93</cell><cell>2.67</cell><cell>1.71</cell><cell>2.03</cell><cell>1.18</cell><cell>1.54</cell><cell>1.0</cell><cell /><cell /></row><row><cell /><cell /><cell /><cell /><cell>T4</cell><cell>4.02</cell><cell>3.85</cell><cell>1.48</cell><cell>3.58</cell><cell>1.12</cell><cell>3.10</cell><cell cols="2">0.22</cell><cell>1.86</cell><cell>1.85</cell><cell>0.86</cell><cell>2.81</cell><cell>1.13</cell><cell>3.80</cell><cell cols="2">0.35</cell><cell /></row><row><cell /><cell /><cell /><cell /><cell>T5</cell><cell>1.77</cell><cell>1.43</cell><cell>1.50</cell><cell>2.65</cell><cell>1.89</cell><cell>4.38</cell><cell cols="2">0.03</cell><cell>4.09</cell><cell>3.71</cell><cell>2.55</cell><cell>3.23</cell><cell>1.52</cell><cell>2.52</cell><cell cols="2">0.13</cell><cell /></row><row><cell /><cell /><cell /><cell /><cell>T6</cell><cell>3.15</cell><cell>2.75</cell><cell>1.33</cell><cell>3.45</cell><cell>1.07</cell><cell>3.97</cell><cell cols="2">0.11</cell><cell>2.50</cell><cell>2.38</cell><cell>1.54</cell><cell>2.75</cell><cell>1.20</cell><cell>2.84</cell><cell cols="2">0.44</cell><cell /></row><row><cell /><cell /><cell /><cell /><cell>Ave</cell><cell>2.77</cell><cell>2.82</cell><cell>1.38</cell><cell>3.09</cell><cell>1.37</cell><cell>3.40</cell><cell cols="2">0.24</cell><cell>2.46</cell><cell>2.32</cell><cell>1.36</cell><cell>2.46</cell><cell>1.21</cell><cell>2.59</cell><cell cols="2">0.63</cell><cell /></row></table></figure>
<figure type="table" xml:id="tab_9"><head>Table 7 :</head><label>7</label><figDesc>Results of feature selection followed by the ED method, EXstream or MacroBase</figDesc><table><row><cell /><cell /><cell /><cell cols="4">MacroBase+EXstream</cell><cell /><cell /><cell /><cell cols="3">MacroBase+Gradient</cell><cell /><cell /><cell /><cell cols="5">MacroBase+Gradient+MacroBase</cell><cell /></row><row><cell /><cell cols="2">Concise</cell><cell cols="2">Consistency</cell><cell cols="2">Norm.Consis</cell><cell>Time (sec)</cell><cell cols="2">Concise</cell><cell cols="2">Consistency</cell><cell cols="2">Norm.Consis</cell><cell>Time (sec)</cell><cell cols="2">Concise</cell><cell cols="2">Consistency</cell><cell cols="2">Norm.Consis</cell><cell>Time (sec)</cell></row><row><cell /><cell>ED1</cell><cell>ED2</cell><cell>ED1</cell><cell>ED2</cell><cell>ED1</cell><cell>ED2</cell><cell>ED1/2</cell><cell>ED1</cell><cell>ED2</cell><cell>ED1</cell><cell>ED2</cell><cell>ED1</cell><cell>ED2</cell><cell>ED1/2</cell><cell>ED1</cell><cell>ED2</cell><cell>ED1</cell><cell>ED2</cell><cell>ED1</cell><cell>ED2</cell><cell>ED1/2</cell></row><row><cell>T1</cell><cell>2.05</cell><cell>2.17</cell><cell>1.27</cell><cell>4.99</cell><cell>1.48</cell><cell>14.60</cell><cell>0.69</cell><cell>1.38</cell><cell>1.38</cell><cell>0.33</cell><cell>2.60</cell><cell>1.00</cell><cell>4.40</cell><cell>0.59</cell><cell>1.14</cell><cell>1.14</cell><cell>0.19</cell><cell>2.35</cell><cell>1.07</cell><cell>4.48</cell><cell>0.85</cell></row><row><cell>T2</cell><cell>2.26</cell><cell>2.29</cell><cell>1.32</cell><cell>3.58</cell><cell>1.25</cell><cell>5.22</cell><cell>1.12</cell><cell>1.43</cell><cell>1.43</cell><cell>0.43</cell><cell>1.90</cell><cell>1.00</cell><cell>2.60</cell><cell>1.06</cell><cell>1.00</cell><cell>1.00</cell><cell>0.00</cell><cell>1.45</cell><cell>1.00</cell><cell>2.73</cell><cell>1.52</cell></row><row><cell>T3</cell><cell>2.38</cell><cell>2.25</cell><cell>1.57</cell><cell>3.93</cell><cell>1.47</cell><cell>6.78</cell><cell>0.50</cell><cell>1.67</cell><cell>1.67</cell><cell>0.63</cell><cell>2.32</cell><cell>1.00</cell><cell>3.00</cell><cell>0.46</cell><cell>1.22</cell><cell>1.08</cell><cell>0.39</cell><cell>1.88</cell><cell>1.21</cell><cell>3.40</cell><cell>0.68</cell></row><row><cell>T4</cell><cell>1.82</cell><cell>2.05</cell><cell>1.48</cell><cell>5.02</cell><cell>1.93</cell><cell>15.78</cell><cell>0.28</cell><cell>1.55</cell><cell>1.55</cell><cell>0.51</cell><cell>3.04</cell><cell>1.00</cell><cell>5.30</cell><cell>0.22</cell><cell>1.21</cell><cell>1.15</cell><cell>0.25</cell><cell>2.79</cell><cell>1.04</cell><cell>6.00</cell><cell>0.36</cell></row><row><cell>T5</cell><cell>2.77</cell><cell>2.57</cell><cell>1.81</cell><cell>4.17</cell><cell>1.69</cell><cell>7.00</cell><cell>0.27</cell><cell>2.14</cell><cell>2.14</cell><cell>1.02</cell><cell>2.46</cell><cell>1.00</cell><cell>2.57</cell><cell>0.24</cell><cell>1.57</cell><cell>1.71</cell><cell>0.97</cell><cell>2.29</cell><cell>1.65</cell><cell>2.86</cell><cell>0.33</cell></row><row><cell>T6</cell><cell>2.88</cell><cell>3.00</cell><cell>2.11</cell><cell>4.42</cell><cell>1.60</cell><cell>7.13</cell><cell>0.19</cell><cell>2.00</cell><cell>2.00</cell><cell>0.84</cell><cell>3.16</cell><cell>1.00</cell><cell>4.46</cell><cell>0.15</cell><cell>1.38</cell><cell>1.38</cell><cell>0.32</cell><cell>2.66</cell><cell>1.00</cell><cell>4.61</cell><cell>0.24</cell></row><row><cell>Ave</cell><cell>2.36</cell><cell>2.39</cell><cell>1.59</cell><cell>4.35</cell><cell>1.57</cell><cell>9.42</cell><cell>0.51</cell><cell>1.69</cell><cell>1.69</cell><cell>0.63</cell><cell>2.58</cell><cell>1.00</cell><cell>3.72</cell><cell>0.45</cell><cell>1.25</cell><cell>1.24</cell><cell>0.35</cell><cell>2.24</cell><cell>1.16</cell><cell>4.01</cell><cell>0.66</cell></row></table></figure>
<figure type="table" xml:id="tab_10"><head>Table 8 :</head><label>8</label><figDesc>Results of the two-step approach with different variants of using MacroBase or EXstream as the ED method and HIDR feature selection</figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgments. This work was supported by the <rs type="funder">European Research Council (ERC)</rs> <rs type="programName">Horizon 2020 research and innovation programme</rs> (grant <rs type="grantNumber">n725561</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_wep52rx">
					<idno type="grant-number">n725561</idno>
					<orgName type="program" subtype="full">Horizon 2020 research and innovation programme</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<ptr target="https://docs.scipy.org/doc/scipy/reference/cluster.hierarchy.html" />
		<title level="m">Scipy hierarchical clustering</title>
		<imprint />
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Variational autoencoder based anomaly detection using reconstruction probability</title>
		<author>
			<persName><forename type="first">J</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cho</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<pubPlace>Seoul, Korea</pubPlace>
		</imprint>
		<respStmt>
			<orgName>TR ; SNU Data Mining Center</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">MacroBase: Prioritizing Attention in Fast Data</title>
		<author>
			<persName><forename type="first">P</forename><surname>Bailis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Gan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="541" to="556" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Causality-based explanation of classification outcomes</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">E</forename><surname>Bertossi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">DEEM</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1" to="6" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Network anomaly detection: Methods, systems and tools</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Bhuyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Communications Surveys and Tutorials</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="303" to="336" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Collective anomaly detection based on long short-term memory recurrent neural networks</title>
		<author>
			<persName><forename type="first">L</forename><surname>Bontemps</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">L</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">FDSE</title>
		<imprint>
			<biblScope unit="volume">10018</biblScope>
			<biblScope unit="page" from="141" to="152" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Sharing-aware outlier analytics over high-volume data streams</title>
		<author>
			<persName><forename type="first">L</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="527" to="540" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Deep Learning for Anomaly Detection: A Survey</title>
		<author>
			<persName><forename type="first">R</forename><surname>Chalapathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chawla</surname></persName>
		</author>
		<idno>CoRR, abs/1901.03407</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Anomaly Detection: A Survey</title>
		<author>
			<persName><forename type="first">V</forename><surname>Chandola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Banerjee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Explaining outputs in modern data analytics</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Chothia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liagouris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PVLDB</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Least angle regression</title>
		<author>
			<persName><forename type="first">B</forename><surname>Efron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Interpretable and informative explanations of outcomes</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">El</forename><surname>Gebaly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Agrawal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PVLDB</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="72" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Distilling a neural network into a soft decision tree</title>
		<author>
			<persName><forename type="first">N</forename><surname>Frosst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int'l Workshop on Comprehensibility and Explanation in AI and ML</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Outlier detection for temporal data: A survey</title>
		<author>
			<persName><forename type="first">M</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TKDE</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2250" to="2267" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Reducing the dimensionality of data with neural networks</title>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">313</biblScope>
			<biblScope unit="issue">5786</biblScope>
			<biblScope unit="page" from="504" to="507" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Streaming anomaly detection using randomized matrix sketching</title>
		<author>
			<persName><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Kasiviswanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PVLDB</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="192" to="203" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Exathlon: A benchmark for explainable anomaly detection over time series</title>
		<author>
			<persName><forename type="first">V</forename><surname>Jacob</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Song</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2010.05073" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">ExplainIt! -A Declarative Root-cause Analysis Engine for Time Series Data</title>
		<author>
			<persName><forename type="first">V</forename><surname>Jeyakumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Madani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="333" to="348" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Interpretable decision sets: A joint framework for description and prediction</title>
		<author>
			<persName><forename type="first">H</forename><surname>Lakkaraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1675" to="1684" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A unified approach to interpreting model predictions</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Lundberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4765" to="4774" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Diagnosing Root Causes of Intermittent Slow Queries in Large-Scale Cloud Databases</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PVLDB</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1176" to="1189" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Long short term memory networks for anomaly detection in time series</title>
		<author>
			<persName><forename type="first">P</forename><surname>Malhotra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Vig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ESANN</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="89" to="94" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Explaining the predictions of any classifier</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Ribeiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1135" to="1144" />
		</imprint>
	</monogr>
	<note>why should I trust you?</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Anchors: High-precision model-agnostic explanations</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Ribeiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Explaining query answers with explanation-ready databases</title>
		<author>
			<persName><forename type="first">S</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Orr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Suciu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PVLDB</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="348" to="359" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Unsupervised anomaly detection with generative adversarial networks to guide marker discovery</title>
		<author>
			<persName><forename type="first">T</forename><surname>Schlegl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Seeböck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IPMI</title>
		<imprint>
			<biblScope unit="page" from="146" to="157" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<ptr target="https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html" />
		<title level="m">scikit-learn documentation: Factor analysis</title>
		<imprint />
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<ptr target="http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.FactorAnalysis.html" />
		<title level="m">scikit-learn documentation: Principal component analysis (pca)</title>
		<imprint />
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Deep inside convolutional networks: Visualising image classification models and saliency maps</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6034</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">EXAD: A System for Explainable Anomaly Detection on Big Data Traces</title>
		<author>
			<persName><forename type="first">F</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Diao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDM Workshops</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1435" to="1440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Anomaly Detection and Explanation Discovery on Event Streams</title>
		<author>
			<persName><forename type="first">F</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BIRTE</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1-5</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<ptr target="https://medium.com/@tao_66792/how-are-big-companies-using-apache-spark-413743dbbbae" />
		<title level="m">How are big companies using apache spark</title>
		<imprint />
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Axiomatic attribution for deep networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sundararajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Taly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3319" to="3328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A global geometric framework for nonlinear dimensionality reduction</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">De</forename><surname>Silva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">science</title>
		<imprint>
			<biblScope unit="volume">290</biblScope>
			<biblScope unit="issue">5500</biblScope>
			<biblScope unit="page" from="2319" to="2323" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Distance based outlier detection for data streams</title>
		<author>
			<persName><forename type="first">L</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shahabi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PVLDB</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1089" to="1100" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Automatic database management system tuning through large-scale machine learning</title>
		<author>
			<persName><forename type="first">D</forename><surname>Van Aken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pavlo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1009" to="1024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Scorpion: Explaining away outliers in aggregate queries</title>
		<author>
			<persName><forename type="first">E</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Madden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PVLDB</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="553" to="564" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Beyond sparsity: Tree regularization of deep models for interpretability</title>
		<author>
			<persName><forename type="first">M</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Hughes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AAAI</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Unsupervised anomaly detection via variational autoencoder for seasonal kpis in web applications</title>
		<author>
			<persName><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="187" to="196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Outlier detection: How to threshold outlier scores?</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rahardja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AIIPCC</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="1" to="37" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Exstream: Explaining anomalies in event stream monitoring</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Diao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EDBT</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="156" to="167" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</teiCorpus></text>
</TEI>