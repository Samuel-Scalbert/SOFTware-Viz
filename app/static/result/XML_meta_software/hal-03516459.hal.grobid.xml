<?xml version='1.0' encoding='utf-8'?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" version="1.1" xsi:schemaLocation="http://www.tei-c.org/ns/1.0 http://api.archives-ouvertes.fr/documents/aofr-sword.xsd">
  <teiHeader>
    <fileDesc>
      <titleStmt>
        <title>HAL TEI export of hal-03516459</title>
      </titleStmt>
      <publicationStmt>
        <distributor>CCSD</distributor>
        <availability status="restricted">
          <licence target="http://creativecommons.org/licenses/by/4.0/">Distributed under a Creative Commons Attribution 4.0 International License</licence>
        </availability>
        <date when="2024-04-29T11:45:15+02:00" />
      </publicationStmt>
      <sourceDesc>
        <p part="N">HAL API platform</p>
      </sourceDesc>
    </fileDesc>
  </teiHeader>
  <text>
    <body>
      <listBibl>
        <biblFull>
          <titleStmt>
            <title xml:lang="en">De-icing federated SPARQL pipelines: a method for assessing the "freshness" of result sets</title>
            <author role="aut">
              <persName>
                <forename type="first">Damien</forename>
                <surname>Graux</surname>
              </persName>
              <email type="md5">fe51d3728e9cad95f1e7132ec5e604ad</email>
              <email type="domain">tcd.ie</email>
              <idno type="idhal" notation="numeric">1122264</idno>
              <idno type="halauthorid" notation="string">989510-1122264</idno>
              <affiliation ref="#struct-178918" />
              <affiliation ref="#struct-22205" />
            </author>
            <author role="aut">
              <persName>
                <forename type="first">Fabrizio</forename>
                <surname>Orlandi</surname>
              </persName>
              <email type="md5">1e837234b3cd082d7c111289b2643032</email>
              <email type="domain">tcd.ie</email>
              <idno type="idhal" notation="numeric">1140213</idno>
              <idno type="halauthorid" notation="string">1068709-1140213</idno>
              <affiliation ref="#struct-22205" />
            </author>
            <author role="aut">
              <persName>
                <forename type="first">Declan</forename>
                <surname>O'Sullivan</surname>
              </persName>
              <email type="md5">efab179731dbe32671b41470dc6bf2bc</email>
              <email type="domain">tcd.ie</email>
              <idno type="idhal" notation="numeric">1140214</idno>
              <idno type="halauthorid" notation="string">173290-1140214</idno>
              <affiliation ref="#struct-22205" />
            </author>
            <editor role="depositor">
              <persName>
                <forename>Damien</forename>
                <surname>Graux</surname>
              </persName>
              <email type="md5">4e1c8ae4a472d3170317709d7db7e14d</email>
              <email type="domain">gmail.com</email>
            </editor>
          </titleStmt>
          <editionStmt>
            <edition n="v1" type="current">
              <date type="whenSubmitted">2022-01-07 11:07:42</date>
              <date type="whenModified">2024-02-26 11:22:08</date>
              <date type="whenReleased">2022-01-07 13:49:35</date>
              <date type="whenProduced">2021-10-25</date>
              <date type="whenEndEmbargoed">2022-01-07</date>
              <ref type="file" target="https://inria.hal.science/hal-03516459/document">
                <date notBefore="2022-01-07" />
              </ref>
              <ref type="file" subtype="author" n="1" target="https://inria.hal.science/hal-03516459/file/De-icing_Federated_SPARQL_MEPDaW2021.pdf">
                <date notBefore="2022-01-07" />
              </ref>
            </edition>
            <respStmt>
              <resp>contributor</resp>
              <name key="1067469">
                <persName>
                  <forename>Damien</forename>
                  <surname>Graux</surname>
                </persName>
                <email type="md5">4e1c8ae4a472d3170317709d7db7e14d</email>
                <email type="domain">gmail.com</email>
              </name>
            </respStmt>
          </editionStmt>
          <publicationStmt>
            <distributor>CCSD</distributor>
            <idno type="halId">hal-03516459</idno>
            <idno type="halUri">https://inria.hal.science/hal-03516459</idno>
            <idno type="halBibtex">graux:hal-03516459</idno>
            <idno type="halRefHtml">&lt;i&gt;MEPDaW 2021 - 7th Workshop on Managing the Evolution and Preservation of the Data Web&lt;/i&gt;, Oct 2021, Virtual Event, United States</idno>
            <idno type="halRef">MEPDaW 2021 - 7th Workshop on Managing the Evolution and Preservation of the Data Web, Oct 2021, Virtual Event, United States</idno>
          </publicationStmt>
          <seriesStmt>
            <idno type="stamp" n="UNICE">Université Nice Sophia Antipolis</idno>
            <idno type="stamp" n="CNRS">CNRS - Centre national de la recherche scientifique</idno>
            <idno type="stamp" n="INRIA">INRIA - Institut National de Recherche en Informatique et en Automatique</idno>
            <idno type="stamp" n="INRIA-SOPHIA">INRIA Sophia Antipolis - Méditerranée</idno>
            <idno type="stamp" n="I3S">Laboratoire d'Informatique, Signaux et Systèmes de Sophia-Antipolis</idno>
            <idno type="stamp" n="INRIASO">INRIA-SOPHIA</idno>
            <idno type="stamp" n="INRIA_TEST">INRIA - Institut National de Recherche en Informatique et en Automatique</idno>
            <idno type="stamp" n="TESTALAIN1">TESTALAIN1</idno>
            <idno type="stamp" n="WIMMICS">WIMMICS: Web-Instrumented Man-Machine Interactions, Communities, and Semantics</idno>
            <idno type="stamp" n="INRIA2">INRIA 2</idno>
            <idno type="stamp" n="UNIV-COTEDAZUR">Université Côte d'Azur</idno>
            <idno type="stamp" n="INRIA_WEB">Inria &amp; web</idno>
          </seriesStmt>
          <notesStmt>
            <note type="audience" n="2">International</note>
            <note type="invited" n="0">No</note>
            <note type="popular" n="0">No</note>
            <note type="peer" n="1">Yes</note>
            <note type="proceedings" n="1">Yes</note>
          </notesStmt>
          <sourceDesc>
            <biblStruct>
              <analytic>
                <title xml:lang="en">De-icing federated SPARQL pipelines: a method for assessing the "freshness" of result sets</title>
                <author role="aut">
                  <persName>
                    <forename type="first">Damien</forename>
                    <surname>Graux</surname>
                  </persName>
                  <email type="md5">fe51d3728e9cad95f1e7132ec5e604ad</email>
                  <email type="domain">tcd.ie</email>
                  <idno type="idhal" notation="numeric">1122264</idno>
                  <idno type="halauthorid" notation="string">989510-1122264</idno>
                  <affiliation ref="#struct-178918" />
                  <affiliation ref="#struct-22205" />
                </author>
                <author role="aut">
                  <persName>
                    <forename type="first">Fabrizio</forename>
                    <surname>Orlandi</surname>
                  </persName>
                  <email type="md5">1e837234b3cd082d7c111289b2643032</email>
                  <email type="domain">tcd.ie</email>
                  <idno type="idhal" notation="numeric">1140213</idno>
                  <idno type="halauthorid" notation="string">1068709-1140213</idno>
                  <affiliation ref="#struct-22205" />
                </author>
                <author role="aut">
                  <persName>
                    <forename type="first">Declan</forename>
                    <surname>O'Sullivan</surname>
                  </persName>
                  <email type="md5">efab179731dbe32671b41470dc6bf2bc</email>
                  <email type="domain">tcd.ie</email>
                  <idno type="idhal" notation="numeric">1140214</idno>
                  <idno type="halauthorid" notation="string">173290-1140214</idno>
                  <affiliation ref="#struct-22205" />
                </author>
              </analytic>
              <monogr>
                <meeting>
                  <title>MEPDaW 2021 - 7th Workshop on Managing the Evolution and Preservation of the Data Web</title>
                  <date type="start">2021-10-25</date>
                  <settlement>Virtual Event</settlement>
                  <country key="US">United States</country>
                </meeting>
                <imprint />
              </monogr>
            </biblStruct>
          </sourceDesc>
          <profileDesc>
            <langUsage>
              <language ident="en">English</language>
            </langUsage>
            <textClass>
              <classCode scheme="halDomain" n="info">Computer Science [cs]</classCode>
              <classCode scheme="halTypology" n="COMM">Conference papers</classCode>
              <classCode scheme="halOldTypology" n="COMM">Conference papers</classCode>
              <classCode scheme="halTreeTypology" n="COMM">Conference papers</classCode>
            </textClass>
            <abstract xml:lang="en">
              <p>In recent years, the ever-increasing number of available linkeddata endpoints has allowed the creation of complex data pipelines leveraging these massive amounts of information. One crucial challenge for federated pipeline designers is to know when to query the various sources they use in order to obtain fresher final results. In other words, they want to know when a data update on a specific source impacts their own final results. Unfortunately, the SPARQL standard does not provide them with a method to be aware of such updates; and therefore pipelines are regularly relaunched from scratch, often uselessly. To help them decide when to get fresher results, we propose a constructive method. Practically, it relies on digitally signing result sets from federated endpoints in order to create a specific query able to warn when, and explain why, the pipeline result set is outdated. In addition, as our solution is exclusively based on SPARQL 1.1 built-in functions, it is fully-compliant with all the endpoints.</p>
            </abstract>
          </profileDesc>
        </biblFull>
      </listBibl>
    </body>
    <back>
      <listOrg type="structures">
        <org type="researchteam" xml:id="struct-178918" status="VALID">
          <idno type="RNSR">201221031M</idno>
          <orgName>Web-Instrumented Man-Machine Interactions, Communities and Semantics</orgName>
          <orgName type="acronym">WIMMICS</orgName>
          <desc>
            <address>
              <country key="FR" />
            </address>
            <ref type="url">http://wimmics.inria.fr/</ref>
          </desc>
          <listRelation>
            <relation active="#struct-34586" type="direct" />
            <relation active="#struct-300009" type="indirect" />
            <relation active="#struct-452156" type="direct" />
            <relation active="#struct-13009" type="indirect" />
            <relation active="#struct-117617" type="indirect" />
            <relation name="UMR7271" active="#struct-441569" type="indirect" />
            <relation active="#struct-1039632" type="indirect" />
          </listRelation>
        </org>
        <org type="institution" xml:id="struct-22205" status="VALID">
          <idno type="ROR">https://ror.org/02tyrky19</idno>
          <orgName>Trinity College Dublin</orgName>
          <desc>
            <address>
              <addrLine>College Green, Dublin 2, Ireland</addrLine>
              <country key="IE" />
            </address>
            <ref type="url">http://www.tcd.ie/</ref>
          </desc>
        </org>
        <org type="laboratory" xml:id="struct-34586" status="VALID">
          <idno type="RNSR">198318250R</idno>
          <idno type="ROR">https://ror.org/01nzkaw91</idno>
          <orgName>Inria Sophia Antipolis - Méditerranée</orgName>
          <orgName type="acronym">CRISAM</orgName>
          <desc>
            <address>
              <addrLine>2004 route des Lucioles BP 93 06902 Sophia Antipolis</addrLine>
              <country key="FR" />
            </address>
            <ref type="url">http://www.inria.fr/centre/sophia/</ref>
          </desc>
          <listRelation>
            <relation active="#struct-300009" type="direct" />
          </listRelation>
        </org>
        <org type="institution" xml:id="struct-300009" status="VALID">
          <idno type="ROR">https://ror.org/02kvxyf05</idno>
          <orgName>Institut National de Recherche en Informatique et en Automatique</orgName>
          <orgName type="acronym">Inria</orgName>
          <desc>
            <address>
              <addrLine>Domaine de VoluceauRocquencourt - BP 10578153 Le Chesnay Cedex</addrLine>
              <country key="FR" />
            </address>
            <ref type="url">http://www.inria.fr/en/</ref>
          </desc>
        </org>
        <org type="department" xml:id="struct-452156" status="VALID">
          <orgName>Scalable and Pervasive softwARe and Knowledge Systems</orgName>
          <orgName type="acronym">Laboratoire I3S - SPARKS</orgName>
          <date type="start">2016-03-03</date>
          <desc>
            <address>
              <addrLine>Laboratoire I3SCS 4012106903 Sophia Antipolis Cedex</addrLine>
              <country key="FR" />
            </address>
            <ref type="url">http://www.i3s.unice.fr/sparks</ref>
          </desc>
          <listRelation>
            <relation active="#struct-13009" type="direct" />
            <relation active="#struct-117617" type="indirect" />
            <relation name="UMR7271" active="#struct-441569" type="indirect" />
            <relation active="#struct-1039632" type="indirect" />
          </listRelation>
        </org>
        <org type="laboratory" xml:id="struct-13009" status="VALID">
          <orgName>Laboratoire d'Informatique, Signaux, et Systèmes de Sophia Antipolis</orgName>
          <orgName type="acronym">I3S</orgName>
          <desc>
            <address>
              <addrLine>2000, route des Lucioles - Les Algorithmes - bât. Euclide B 06900 Sophia Antipolis</addrLine>
              <country key="FR" />
            </address>
            <ref type="url">http://www.i3s.unice.fr/</ref>
          </desc>
          <listRelation>
            <relation active="#struct-117617" type="direct" />
            <relation name="UMR7271" active="#struct-441569" type="direct" />
            <relation active="#struct-1039632" type="direct" />
          </listRelation>
        </org>
        <org type="institution" xml:id="struct-117617" status="VALID">
          <idno type="IdRef">026403498</idno>
          <idno type="ISNI">0000000123372892</idno>
          <idno type="ROR">https://ror.org/02k9vew78</idno>
          <orgName>Université Nice Sophia Antipolis (1965 - 2019)</orgName>
          <orgName type="acronym">UNS</orgName>
          <date type="start">1965-10-23</date>
          <date type="end">2019-12-31</date>
          <desc>
            <address>
              <addrLine>Parc Valrose, 06100 Nice</addrLine>
              <country key="FR" />
            </address>
            <ref type="url">http://unice.fr/</ref>
          </desc>
        </org>
        <org type="regroupinstitution" xml:id="struct-441569" status="VALID">
          <idno type="IdRef">02636817X</idno>
          <idno type="ISNI">0000000122597504</idno>
          <idno type="ROR">https://ror.org/02feahw73</idno>
          <orgName>Centre National de la Recherche Scientifique</orgName>
          <orgName type="acronym">CNRS</orgName>
          <date type="start">1939-10-19</date>
          <desc>
            <address>
              <country key="FR" />
            </address>
            <ref type="url">https://www.cnrs.fr/</ref>
          </desc>
        </org>
        <org type="regroupinstitution" xml:id="struct-1039632" status="VALID">
          <idno type="IdRef">241035694</idno>
          <idno type="ROR">https://ror.org/019tgvf94</idno>
          <orgName>Université Côte d'Azur</orgName>
          <orgName type="acronym">UniCA</orgName>
          <date type="start">2020-01-01</date>
          <desc>
            <address>
              <addrLine>Parc Valrose, 28, avenue Valrose 06108 Nice Cedex 2</addrLine>
              <country key="FR" />
            </address>
            <ref type="url">https://univ-cotedazur.fr</ref>
          </desc>
        </org>
      </listOrg>
    </back>
  <teiCorpus>
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">De-icing federated SPARQL pipelines: a method for assessing the "freshness" of result sets</title>
			</titleStmt>
			<publicationStmt>
				<publisher />
				<availability status="unknown"><licence /></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Damien</forename><surname>Graux</surname></persName>
							<email>grauxd@tcd.ie</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Inria</orgName>
								<orgName type="institution" key="instit2">Université Côte d'Azur</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<address>
									<region>I3S</region>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">ADAPT SFI Centre</orgName>
								<orgName type="institution">Trinity College Dublin</orgName>
								<address>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Fabrizio</forename><surname>Orlandi</surname></persName>
							<email>orlandif@tcd.ie</email>
							<idno type="ORCID">0000-0003-3392-3162</idno>
							<affiliation key="aff1">
								<orgName type="department">ADAPT SFI Centre</orgName>
								<orgName type="institution">Trinity College Dublin</orgName>
								<address>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Declan</forename><surname>O'sullivan</surname></persName>
							<email>declan.osullivan@tcd.ie</email>
							<idno type="ORCID">0000-0003-1090-3548</idno>
							<affiliation key="aff1">
								<orgName type="department">ADAPT SFI Centre</orgName>
								<orgName type="institution">Trinity College Dublin</orgName>
								<address>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">De-icing federated SPARQL pipelines: a method for assessing the "freshness" of result sets</title>
					</analytic>
					<monogr>
						<imprint>
							<date />
						</imprint>
					</monogr>
					<idno type="MD5">7509DDA571EE29CDBA464C39EE57CEE8</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-04-12T14:51+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid" />
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div><p>In recent years, the ever-increasing number of available linkeddata endpoints has allowed the creation of complex data pipelines leveraging these massive amounts of information. One crucial challenge for federated pipeline designers is to know when to query the various sources they use in order to obtain fresher final results. In other words, they want to know when a data update on a specific source impacts their own final results. Unfortunately, the <software>SPARQL</software> standard does not provide them with a method to be aware of such updates; and therefore pipelines are regularly relaunched from scratch, often uselessly. To help them decide when to get fresher results, we propose a constructive method. Practically, it relies on digitally signing result sets from federated endpoints in order to create a specific query able to warn when, and explain why, the pipeline result set is outdated. In addition, as our solution is exclusively based on <software ContextAttributes="created">SPARQL</software> 1.1 built-in functions, it is fully-compliant with all the endpoints.</p><p>2 https://www.w3.org/TR/sparql11-query/#func-hash 3 select * where{ values ?x {"ab" "ab"^^xsd:string} bind (md5(?x) as ?H)}</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div><head n="1">Introduction</head><p>During the past decades, the number of linked open datasets has rapidly increased <ref type="foot" target="#foot_0">1</ref> . These datasets are structured following the W3C standard Resource Description Framework (RDF) <ref type="bibr" target="#b5">[6]</ref> and share knowledge on various domains, from the generalist ones such as DBpedia <ref type="bibr" target="#b0">[1]</ref> or WikiData <ref type="bibr" target="#b6">[7]</ref> to the most specialised ones, e.g. SemanGit <ref type="bibr" target="#b4">[5]</ref>. This abundance of open datasets and <software ContextAttributes="created">SPARQL</software> <ref type="bibr" target="#b1">[2]</ref> endpoints led not only researchers but also businesses to integrate RDF graphs into their complex data pipelines. In particular, businesses are increasingly leveraging Semantic Web technologies to structure their own data and create value, sometimes integrating external Linked Data to enrich their analyses <ref type="bibr" target="#b3">[4]</ref>.</p><p>Benefiting from these two decades of developments made by the community, it is now possible to deploy Semantic Web pipelines and to adapt them to a wide range of needs and use-cases. Recent developments have been, for example, focused on distributed systems or on connecting Semantic Web data management systems together with non-RDF centric systems, paving the road to querying heterogeneous data. As a consequence of this increasing complexity of the usecases, the pipelines themselves are getting more complicated, and often rely on several distinct data sources using <software>SPARQL</software> federation techniques, in order to compute their final results.</p><p>Hence, as data available may change, these pipelines (or parts of them) are frequently re-run in order to get fresher results. However, lots of times they are re-run unnecessarily as datasets have not been updated in the meantime in ways that impact the result sets of the pipeline. All these operations are leading to a waste of computation power and loads on the network.</p><p>In this article, mainly dedicated to <software ContextAttributes="created">SPARQL</software> practitioners and data pipeline designers, we present some possibilities provided by <software ContextAttributes="created">SPARQL</software> 1.1 <ref type="bibr" target="#b1">[2]</ref> to hash query result sets with a focus on the case of <software ContextAttributes="created">SPARQL</software> federation of sources with the <software ContextAttributes="created">SERVICE</software> keyword. In particular, we will discuss how these methods can be used to optimise data pipelines avoiding expensive re-computation of results when data triples have not been updated.</p></div>
<div><head n="2">Hashing features in SPARQL</head><p>In this Section, we review several use-cases where the use of <software>SPARQL</software> hashing features becomes relevant. In particular, we focus on various methods to obtain a "signature" of a <software ContextAttributes="used">SPARQL</software> result set.</p></div>
<div><head n="2.1">SPARQL 1.1 capabilities</head><p>The <software>SPARQL</software> standard provides a large set of built-in functions, from ones dedicated to strings to specific ones about dates. These can be used by query designers to refine their result set. In particular, the standard offers a set of five hash functions 2 : MD5, SHA1, SHA256, SHA384 &amp; SHA512.</p><p>General signature of the hash functions: simple literal hash_function (simple literal arg) simple literal hash_function (xsd:string arg)</p></div>
<div><head>Example using MD5:</head><p>H = md5("ab") = md5("ab"^^xsd:string) H = "187ef4436122d1cc2f40dc2b92f0eba0" These functions accept either RDF literals or strings as argument and return the hash as a literal. In addition, a xsd:string or its corresponding literal should return the same result. In the 'MD5' example above, the hash value represents the result of a simple <software>SPARQL</software> query 3 .</p></div>
<div><head>2.2</head><p>From hashing an RDF graph . . . Technically, the standard hash functions provide the query designer with useful tools to, for instance, check/sign a specific value within a filter field. Nevertheless, they could also be used for more complex tasks that simply checking a value. For example, in a more extreme case, they could be used over a complete RDF graph through a <software>SPARQL</software> endpoint: one can extract all the triples available with select * where {?s ?p ?o}, and then hash all of them, aggregated with a group concat function. This could look like so:</p><p>SELECT (SHA1(GROUP_CONCAT(?tripleStr ; separator=' \n'))) AS ?nTriples WHERE { ?s ?p ?o BIND(CONCAT(STR(?s), " ", STR(?p), " ", STR(?o)) AS ?tripleStr) }</p><p>In the previous query, the triples ?s ?p ?o are cast by element to a string (STR), and then concatenated to form a "triple". The recomposed list of triples is then grouped into one single string (GROUP CCONCAT) and finally hashed. Obviously, this "naive" approach has drawbacks. First, the result depends on the order of the triples returned by the triplestore: a workaround can be achieved adding e.g. ORDER BY ?s ?p ?o datatype(?o) lcase(lang(?o)). Second, this method has a scalability issue, as all the graph is loaded in-memory before the hash call. We therefore recommend this approach to sign small RDF graphs, e.g. ontologies or small result sets. Finally, this method does not address the complex case of blank node identification as e.g. { :a p o} and { :b p o} do not have the same hashes (see e.g. <ref type="bibr" target="#b2">[3]</ref> for algorithmic solutions).</p></div>
<div><head n="2.3">. . . To hashing SPARQL result sets</head><p>More pragmatically however, these hash functions can be used in order to sign <software>SPARQL</software> query result sets, typically, they allow users to compare different query results for the same <software ContextAttributes="used">SPARQL</software> endpoint. As we know, on the same endpoint, the same query (without calls to functions like RAND or NOW) is supposed to return the same result set for the same dataset. A hash of the results could be computed by the endpoint and be compared with a previously obtained one. In case of a mismatch, the query (and the rest of the pipeline) could be run again. Assuming Q is the considered <software ContextAttributes="used">SPARQL</software> select query, we propose the following steps to generate the query which computes the hash of the results of Q:</p><p>1. Extract and sort the list of distinguished variables V (if a * is given, the considered variables are the ones involved in the where); 2. Wrap Q in a select * query ordered by V; 3. Embed the obtained query in a select query computing the hash of the grouped concatenation of the cast (to string) distinguished variables.</p><p>To give an example, if we consider the query <ref type="foot" target="#foot_2">4</ref>  Using this method, a <software ContextAttributes="used">SPARQL</software> pipeline designer can easily obtain a hash corresponding to a result set, and store it for computing a comparison later. She would therefore know if RDF data updates have been impacting, in the meanwhile, her results. On advantage of such a method lays in the place where the computations happens: the <software ContextAttributes="used">SPARQL</software> endpoint which could thereby be in charge of having otpimisation techniques (such as result caching or sub-view updating) to retrieve the result sets faster.</p></div>
<div><head n="2.4">Gaining in precision with the SERVICE keyword</head><p><software ContextAttributes="used">SPARQL</software> pipelines often rely on several data sources that are queried on-the-fly and whose sub-results are aggregated to form the final result set. Practically, the <software ContextAttributes="used">SPARQL</software> standard provides the designers with a built-in function to call an external endpoint with specific conditions: the <software ContextAttributes="used">SERVICE</software> keyword. This keyword is supposed to take as arguments a <software ContextAttributes="used">SPARQL</software> endpoint URI<ref type="foot" target="#foot_3">5</ref> and a set of conditions similarly to the where clause.</p><p>Consequently, by the mean of <software ContextAttributes="used">SPARQL</software> sub-queries it is possible to refine the previous method when the designer sets up federated queries. Indeed, each <software ContextAttributes="used">SERVICE</software> call can be extracted and encapsulated following the steps presented previously. For example, considering SELECT ?s WHERE { ?s ?p ?o . <software ContextAttributes="used">SERVICE</software> &lt;URI&gt; { ?s ?t ?u . }}, we can obtain a finer precision computing the following two hashing <software ContextAttributes="used">SPARQL</software> queries: The first query (left-hand side) focuses on computing a hash of the results set obtained from the external source; and the second one hashes the complete result set obtained once the local and the external results are joined. More generally, one needs to generate one dedicated hashing query per <software ContextAttributes="used">SERVICE</software> call plus a general hashing query for the main/top-level <software ContextAttributes="used">SPARQL</software> query. By using this strategy, the query designer is then able to create her merkle tree<ref type="foot" target="#foot_4">6</ref> of her different sources' sub-results. Moreover, this method is fully compliant with the <software ContextAttributes="used">SPARQL</software> standard as it uses exclusively built-in functions. Finally, this "incremental" approach allows the users to know which source has been updated in a way that might impact the general query.</p></div>
<div><head n="3">Automation for complex federated SPARQL queries</head><p>The aforedescribed steps to generate the queries that obtain the hashes, splitting the query into pieces based on <software>SERVICE</software>, are easy to automate and allow users to know when to relaunch their pipelines. In order to help pipeline designers in practice, we serve our method at: https://dgraux.github.io/<software ContextAttributes="used">SPARQL</software>-hash/ where our query converters can be used directly by developers to generate queries computing the hash of their result sets both for "regular" <software ContextAttributes="used">SPARQL</software> queries and federated ones.</p><p>In the case of source federation, as the generation of several hashing queries (as presented in the previous section) is tedious and prone to committing errors, we also provide a generator dedicated to federated queries. Practically, our generator takes the <software ContextAttributes="used">SPARQL</software> query Q and returns a new one Q hash which take care of computing all the necessary hashing sub-queries once run on the designers' <software ContextAttributes="used">SPARQL</software> endpoint <ref type="foot" target="#foot_5">7</ref> . Specifically, Q hash's result is another <software ContextAttributes="used">SPARQL</software> query Q check which is in charge of running<ref type="foot" target="#foot_6">8</ref> the various hashing sub-queries in order to compare the hashes with the previous ones obtained by Q hash. By construction, Q hash is supposed to be run only once and Q check should be the one -once obtained-used regularly to check if Q needs to be relaunched in order to update the designer's final result set. Furthermore, Q check is built in a way which allows the query designers to know why she should update her final result set; it actually explains which specific source is updated.</p><p>As Q check and more Q hash tend to be long, we refer the reader to the extensive set of examples on our website for details. Notwithstanding, for the sake of clarity, let's consider the following simplified query (Q): select * where { ... service uri {...}}. Let's now name Q serv and Q gene the sub-queries generating "123" and "abc": the hashes (following the methods proposed in Sections 2.3 &amp; 2.4) for the service part and for Q respectively. Therefore, Q check would have the following sketch: select ?res where { { Q_serv } # To compute a fresh hash for the service { Q_gene } # To compute a fresh hash for Q if (Q_serv="123"){ if (Q_gene="abc") {?res="No update"} else {?res="Local data has changed"} } else { ?res="External source has changed" } }</p><p>As presented above, Q check inspects the possible updates of hash results using a set of nested conditions. In particular, this set of conditions follows the exploration of a merkle tree starting from the bottom layer. Therefore, by structuring the query like so, an optimised <software>SPARQL</software> endpoint might be able to properly order its query plan in order to evaluate sequentially the various sub-queries and stop as soon as possible, saving computational resources.</p></div>
<div><head n="4">Conclusions</head><p>This article describes how to improve existing Semantic Web data pipelines with a <software>SPARQL</software>-based method that helps in identifying when query results have changed. In particular, it sheds lights on a possible strategy when dealing with complex federated queries. It allows to re-run pipelines only when interesting parts of the original datasets have been updated. By using <software ContextAttributes="created">SPARQL</software> to compute the signature of the query results, it avoids large result sets to be sent over the network. Finally, it also helps the designers identify what are the external sources that have been updated. We hope this will inspire developers to use the hash functions provided by the standard.</p></div><figure type="table" xml:id="tab_0"><head /><label /><figDesc>which extracts from DBpedia the current members of English-named Punk rock groups, Q= Its sorted list of distinguished variables would be ?bandName ?members. And to obtain a (MD5-)hash of the results of Q, we should run:</figDesc><table><row><cell cols="2">SELECT MD5(GROUP_CONCAT(CONCAT(STR(?bandName),STR(?members)); separator=' \n'))</cell></row><row><cell>as ?H WHERE {</cell><cell /></row><row><cell>SELECT * WHERE {</cell><cell># Collecting all the ordered results</cell></row><row><cell>SELECT ?members ?bandName WHERE {</cell><cell># The original query</cell></row><row><cell cols="2">?band dbo:genre dbr:Punk_rock . ?band dbp:currentMembers ?members.</cell></row><row><cell cols="2">?band foaf:name ?bandName FILTER(langMatches(lang(?bandName), "en")) }</cell></row><row><cell>} ORDER BY ?bandName ?members</cell><cell># Ordering by distinguished variables</cell></row><row><cell>}</cell><cell /></row><row><cell>SELECT ?members ?bandName WHERE {</cell><cell /></row><row><cell cols="2">?band dbo:genre dbr:Punk_rock . ?band dbp:currentMembers ?members .</cell></row><row><cell cols="2">?band foaf:name ?bandName FILTER(langMatches(lang(?bandName), "en")) }</cell></row></table></figure>
			<note place="foot" n="1" xml:id="foot_0"><p>From</p></note>
			<note place="foot" n="2010" xml:id="foot_1"><p>to 2020, the LOD-cloud has grown from 203 to 1 255 datasets, approximately: https://lod-cloud.net/</p></note>
			<note place="foot" n="4" xml:id="foot_2"><p>The queries can be tested directly on DBpedia: the query Q and its Q hash . As of April 12 th 2021, Q hash returns ?H = "967d2c8c0a82038d8478d476fa41e14f".</p></note>
			<note place="foot" n="5" xml:id="foot_3"><p>According to the <software>SPARQL</software> 1.1 standard, this <software ContextAttributes="used">SPARQL</software> endpoint URI may also be a variable, e.g. "<software ContextAttributes="used">SERVICE</software> ?url { ... }", requiring the endpoint to first retrieve the possible entities for this variable and then call the external endpoints applying the set of conditions. However, this case is currently out of our scope.</p></note>
			<note place="foot" n="6" xml:id="foot_4"><p>Following recursively the <software>SERVICE</software> calls.</p></note>
			<note place="foot" n="7" xml:id="foot_5"><p>i.e. the same endpoint as the one where Q is supposed to be run.</p></note>
			<note place="foot" n="8" xml:id="foot_6"><p>Like Q hash, Q check should be run on the endpoint where Q is supposed to be run.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">DBpedia: A nucleus for a web of open data</title>
		<author>
			<persName><forename type="first">S</forename><surname>Auer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bizer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kobilarov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cyganiak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ives</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The semantic web</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Sparql 1.1 query language</title>
		<author>
			<persName><forename type="first">S</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Seaborne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Prud'hommeaux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">W3C recommendation</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">778</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Canonical forms for isomorphic and equivalent RDF graphs: algorithms for leaning and labelling blank nodes</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hogan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on the Web (TWEB)</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="62" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Knowledge graph-based legal search over german court cases</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Junior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Orlandi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Graux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hossari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>O'sullivan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Dirschl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Semantic Web: ESWC 2020 Satellite Events. LNCS</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12124</biblScope>
			<biblScope unit="page" from="293" to="297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">SemanGit: A linked dataset from git</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">O</forename><surname>Kubitza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Böckmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Graux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Semantic Web Conference</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="215" to="228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">RDF primer</title>
		<author>
			<persName><forename type="first">F</forename><surname>Manola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mcbride</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">W3C recommendation</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1-107</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Wikidata: a free collaborative knowledgebase</title>
		<author>
			<persName><forename type="first">D</forename><surname>Vrandečić</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Krötzsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="78" to="85" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</teiCorpus></text>
</TEI>