<?xml version='1.0' encoding='utf-8'?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" version="1.1" xsi:schemaLocation="http://www.tei-c.org/ns/1.0 http://api.archives-ouvertes.fr/documents/aofr-sword.xsd">
  <teiHeader>
    <fileDesc>
      <titleStmt>
        <title>HAL TEI export of lirmm-03793591</title>
      </titleStmt>
      <publicationStmt>
        <distributor>CCSD</distributor>
        <availability status="restricted">
          <licence target="http://creativecommons.org/licenses/by/">Attribution</licence>
        </availability>
        <date when="2024-04-27T10:55:59+02:00" />
      </publicationStmt>
      <sourceDesc>
        <p part="N">HAL API platform</p>
      </sourceDesc>
    </fileDesc>
  </teiHeader>
  <text>
    <body>
      <listBibl>
        <biblFull>
          <titleStmt>
            <title xml:lang="en">Overview of PlantCLEF 2022: Image-based plant identification at global scale</title>
            <author role="aut">
              <persName>
                <forename type="first">Herv√©</forename>
                <surname>Go√´au</surname>
              </persName>
              <email type="md5">c769b7a209218a79039e6d8afb087073</email>
              <email type="domain">cirad.fr</email>
              <idno type="idhal" notation="string">herve-goeau</idno>
              <idno type="idhal" notation="numeric">741423</idno>
              <idno type="halauthorid" notation="string">31413-741423</idno>
              <idno type="ORCID">https://orcid.org/0000-0003-3296-3795</idno>
              <idno type="IDREF">https://www.idref.fr/138462259</idno>
              <affiliation ref="#struct-1100781" />
              <affiliation ref="#struct-168219" />
            </author>
            <author role="aut">
              <persName>
                <forename type="first">Pierre</forename>
                <surname>Bonnet</surname>
              </persName>
              <email type="md5">2262ba6624516f8abf022b1d75090fbe</email>
              <email type="domain">cirad.fr</email>
              <idno type="idhal" notation="string">pierre-bonnet-amap</idno>
              <idno type="idhal" notation="numeric">19230</idno>
              <idno type="halauthorid" notation="string">3124-19230</idno>
              <idno type="ORCID">https://orcid.org/0000-0002-2828-4389</idno>
              <idno type="IDREF">https://www.idref.fr/131139533</idno>
              <affiliation ref="#struct-1100781" />
              <affiliation ref="#struct-168219" />
            </author>
            <author role="aut">
              <persName>
                <forename type="first">Alexis</forename>
                <surname>Joly</surname>
              </persName>
              <email type="md5">2969b8fd490aeeb72d850079cee7bbe0</email>
              <email type="domain">inria.fr</email>
              <idno type="idhal" notation="string">alexis-joly</idno>
              <idno type="idhal" notation="numeric">12088</idno>
              <idno type="halauthorid" notation="string">22964-12088</idno>
              <idno type="ORCID">https://orcid.org/0000-0002-2161-9940</idno>
              <idno type="IDREF">https://www.idref.fr/107969394</idno>
              <affiliation ref="#struct-1100621" />
            </author>
            <editor role="depositor">
              <persName>
                <forename>Isabelle</forename>
                <surname>Gouat</surname>
              </persName>
              <email type="md5">01a8910ec35817770bca127295d8d38a</email>
              <email type="domain">lirmm.fr</email>
            </editor>
          </titleStmt>
          <editionStmt>
            <edition n="v1" type="current">
              <date type="whenSubmitted">2022-10-01 12:35:37</date>
              <date type="whenModified">2024-01-23 11:06:03</date>
              <date type="whenReleased">2022-10-01 12:36:51</date>
              <date type="whenProduced">2022-09-05</date>
              <date type="whenEndEmbargoed">2022-10-01</date>
              <ref type="file" target="https://hal-lirmm.ccsd.cnrs.fr/lirmm-03793591/document">
                <date notBefore="2022-10-01" />
              </ref>
              <ref type="file" subtype="author" n="1" target="https://hal-lirmm.ccsd.cnrs.fr/lirmm-03793591/file/paper-153.pdf">
                <date notBefore="2022-10-01" />
              </ref>
            </edition>
            <respStmt>
              <resp>contributor</resp>
              <name key="102079">
                <persName>
                  <forename>Isabelle</forename>
                  <surname>Gouat</surname>
                </persName>
                <email type="md5">01a8910ec35817770bca127295d8d38a</email>
                <email type="domain">lirmm.fr</email>
              </name>
            </respStmt>
          </editionStmt>
          <publicationStmt>
            <distributor>CCSD</distributor>
            <idno type="halId">lirmm-03793591</idno>
            <idno type="halUri">https://hal-lirmm.ccsd.cnrs.fr/lirmm-03793591</idno>
            <idno type="halBibtex">goeau:lirmm-03793591</idno>
            <idno type="halRefHtml">&lt;i&gt;CLEF 2022 - Conference and Labs of the Evaluation Forum&lt;/i&gt;, Sep 2022, Bologne, Italy. pp.1916-1928</idno>
            <idno type="halRef">CLEF 2022 - Conference and Labs of the Evaluation Forum, Sep 2022, Bologne, Italy. pp.1916-1928</idno>
            <availability status="restricted">
              <licence target="http://creativecommons.org/licenses/by/">Attribution</licence>
            </availability>
          </publicationStmt>
          <seriesStmt>
            <idno type="stamp" n="IRD">IRD - Institut de recherche pour le d√©veloppement</idno>
            <idno type="stamp" n="CIRAD">CIRAD - Centre de coop√©ration internationale en recherche agronomique pour le d√©veloppement</idno>
            <idno type="stamp" n="SDE">Sciences De l'Environnement</idno>
            <idno type="stamp" n="CNRS">CNRS - Centre national de la recherche scientifique</idno>
            <idno type="stamp" n="INRIA">INRIA - Institut National de Recherche en Informatique et en Automatique</idno>
            <idno type="stamp" n="INRIA-SOPHIA">INRIA Sophia Antipolis - M√©diterran√©e</idno>
            <idno type="stamp" n="AMAP">Botanique et bio-informatique de l'architecture des plantes et des v√©g√©tations</idno>
            <idno type="stamp" n="INRIASO">INRIA-SOPHIA</idno>
            <idno type="stamp" n="INRIA_TEST">INRIA - Institut National de Recherche en Informatique et en Automatique</idno>
            <idno type="stamp" n="GIP-BE">GIP Bretagne Environnement</idno>
            <idno type="stamp" n="TESTALAIN1">TESTALAIN1</idno>
            <idno type="stamp" n="ZENITH" corresp="LIRMM">Scientific Data Management</idno>
            <idno type="stamp" n="LIRMM">Laboratoire d'Informatique de Robotique et de Micro√©lectronique de Montpellier</idno>
            <idno type="stamp" n="INRIA2">INRIA 2</idno>
            <idno type="stamp" n="AGREENIUM">Archive ouverte en agrobiosciences</idno>
            <idno type="stamp" n="UNIV-MONTPELLIER">Universit√© de Montpellier</idno>
            <idno type="stamp" n="INRAE">Institut National de Recherche en Agriculture, Alimentation et Environnement</idno>
            <idno type="stamp" n="INRAEOCCITANIEMONTPELLIER" corresp="INRAE">INRAE Occitanie Montpellier</idno>
            <idno type="stamp" n="UM-2015-2021" corresp="UNIV-MONTPELLIER">Universit√© de Montpellier (2015-2021)</idno>
            <idno type="stamp" n="UM-EPE" corresp="UNIV-MONTPELLIER">Universit√© de Montpellier - EPE</idno>
            <idno type="stamp" n="TEST3-HALCNRS">TEST3-HALCNRS</idno>
            <idno type="stamp" n="TEST4-HALCNRS">collection test</idno>
            <idno type="stamp" n="RESEAU-EAU">R√©seau "Syst√®mes Agricoles et Eau"</idno>
            <idno type="stamp" n="DPT_ECODIV">Ecologie et biodiversit√©</idno>
            <idno type="stamp" n="TEST5-HALCNRS">collection test 5</idno>
            <idno type="stamp" n="INEE-CNRS">Institut √©cologie et environnement du CNRS</idno>
          </seriesStmt>
          <notesStmt>
            <note type="audience" n="2">International</note>
            <note type="invited" n="0">No</note>
            <note type="popular" n="0">No</note>
            <note type="peer" n="1">Yes</note>
            <note type="proceedings" n="1">Yes</note>
          </notesStmt>
          <sourceDesc>
            <biblStruct>
              <analytic>
                <title xml:lang="en">Overview of PlantCLEF 2022: Image-based plant identification at global scale</title>
                <author role="aut">
                  <persName>
                    <forename type="first">Herv√©</forename>
                    <surname>Go√´au</surname>
                  </persName>
                  <email type="md5">c769b7a209218a79039e6d8afb087073</email>
                  <email type="domain">cirad.fr</email>
                  <idno type="idhal" notation="string">herve-goeau</idno>
                  <idno type="idhal" notation="numeric">741423</idno>
                  <idno type="halauthorid" notation="string">31413-741423</idno>
                  <idno type="ORCID">https://orcid.org/0000-0003-3296-3795</idno>
                  <idno type="IDREF">https://www.idref.fr/138462259</idno>
                  <affiliation ref="#struct-1100781" />
                  <affiliation ref="#struct-168219" />
                </author>
                <author role="aut">
                  <persName>
                    <forename type="first">Pierre</forename>
                    <surname>Bonnet</surname>
                  </persName>
                  <email type="md5">2262ba6624516f8abf022b1d75090fbe</email>
                  <email type="domain">cirad.fr</email>
                  <idno type="idhal" notation="string">pierre-bonnet-amap</idno>
                  <idno type="idhal" notation="numeric">19230</idno>
                  <idno type="halauthorid" notation="string">3124-19230</idno>
                  <idno type="ORCID">https://orcid.org/0000-0002-2828-4389</idno>
                  <idno type="IDREF">https://www.idref.fr/131139533</idno>
                  <affiliation ref="#struct-1100781" />
                  <affiliation ref="#struct-168219" />
                </author>
                <author role="aut">
                  <persName>
                    <forename type="first">Alexis</forename>
                    <surname>Joly</surname>
                  </persName>
                  <email type="md5">2969b8fd490aeeb72d850079cee7bbe0</email>
                  <email type="domain">inria.fr</email>
                  <idno type="idhal" notation="string">alexis-joly</idno>
                  <idno type="idhal" notation="numeric">12088</idno>
                  <idno type="halauthorid" notation="string">22964-12088</idno>
                  <idno type="ORCID">https://orcid.org/0000-0002-2161-9940</idno>
                  <idno type="IDREF">https://www.idref.fr/107969394</idno>
                  <affiliation ref="#struct-1100621" />
                </author>
              </analytic>
              <monogr>
                <idno type="localRef">BIAS</idno>
                <meeting>
                  <title>CLEF 2022 - Conference and Labs of the Evaluation Forum</title>
                  <date type="start">2022-09-05</date>
                  <date type="end">2022-09-08</date>
                  <settlement>Bologne</settlement>
                  <country key="IT">Italy</country>
                </meeting>
                <imprint>
                  <biblScope unit="serie">CEUR Workshop Proceedings</biblScope>
                  <biblScope unit="volume">3180</biblScope>
                  <biblScope unit="issue">153</biblScope>
                  <biblScope unit="pp">1916-1928</biblScope>
                  <date type="datePub">2022</date>
                </imprint>
              </monogr>
              <ref type="publisher">http://ceur-ws.org/Vol-3180/paper-153.pdf</ref>
            </biblStruct>
          </sourceDesc>
          <profileDesc>
            <langUsage>
              <language ident="en">English</language>
            </langUsage>
            <textClass>
              <keywords scheme="author">
                <term xml:lang="en">LifeCLEF</term>
                <term xml:lang="en">fine-grained classification</term>
                <term xml:lang="en">species identification</term>
                <term xml:lang="en">biodiversity informatics</term>
                <term xml:lang="en">evaluation</term>
                <term xml:lang="en">benchmark</term>
              </keywords>
              <classCode scheme="halDomain" n="sdv.bid.spt">Life Sciences [q-bio]/Biodiversity/Systematics, Phylogenetics and taxonomy</classCode>
              <classCode scheme="halDomain" n="sdv.ee.eco">Life Sciences [q-bio]/Ecology, environment/Ecosystems</classCode>
              <classCode scheme="halDomain" n="sdv.bv.bot">Life Sciences [q-bio]/Vegetal Biology/Botanics</classCode>
              <classCode scheme="halDomain" n="sde.be">Environmental Sciences/Biodiversity and Ecology</classCode>
              <classCode scheme="halTypology" n="COMM">Conference papers</classCode>
              <classCode scheme="halOldTypology" n="COMM">Conference papers</classCode>
              <classCode scheme="halTreeTypology" n="COMM">Conference papers</classCode>
            </textClass>
            <abstract xml:lang="en">
              <p>It is estimated that there are more than 300,000 species of vascular plants in the world. Increasing our knowledge of these species is of paramount importance for the development of human civilization (agriculture, construction, pharmacopoeia, etc.), especially in the context of the biodiversity crisis. However, the burden of systematic plant identification by human experts strongly penalizes the aggregation of new data and knowledge. Since then, automatic identification has made considerable progress in recent years as highlighted during all previous editions of PlantCLEF. Deep learning techniques now seem mature enough to address the ultimate but realistic problem of global identification of plant biodiversity in spite of many problems that the data may present (a huge number of classes, very strongly unbalanced classes, partially erroneous identifications, duplications, variable visual quality, diversity of visual contents such as photos or herbarium sheets, etc). The PlantCLEF2022 challenge edition proposes to take a step in this direction by tackling a multi-image (and metadata) classification problem with a very large number of classes (80k plant species). This paper presents the resources and evaluations of the challenge, summarizes the approaches and systems employed by the participating research groups, and provides an analysis of key findings.</p>
            </abstract>
          </profileDesc>
        </biblFull>
      </listBibl>
    </body>
    <back>
      <listOrg type="structures">
        <org type="laboratory" xml:id="struct-1100781" status="VALID">
          <idno type="IdRef">034922970</idno>
          <idno type="ISNI">000000012160870X</idno>
          <idno type="RNSR">200317641S</idno>
          <idno type="ROR">https://ror.org/020nks034</idno>
          <orgName>Botanique et Mod√©lisation de l'Architecture des Plantes et des V√©g√©tations</orgName>
          <orgName type="acronym">UMR AMAP</orgName>
          <date type="start">2022-01-01</date>
          <desc>
            <address>
              <addrLine>Bd de la Lironde TA A-51/ PS 2 34398 Montpellier cedex 5</addrLine>
              <country key="FR" />
            </address>
            <ref type="url">http://amap.cirad.fr/</ref>
          </desc>
          <listRelation>
            <relation name="UMR 51-2003" active="#struct-11574" type="direct" />
            <relation name="UMR 5120" active="#struct-441569" type="direct" />
            <relation name="UMR 123" active="#struct-451860" type="direct" />
            <relation name="UMR 931" active="#struct-577435" type="direct" />
            <relation active="#struct-1100589" type="direct" />
          </listRelation>
        </org>
        <org type="regrouplaboratory" xml:id="struct-168219" status="VALID">
          <orgName>D√©partement Syst√®mes Biologiques</orgName>
          <orgName type="acronym">Cirad-BIOS</orgName>
          <date type="start">2007-01-01</date>
          <desc>
            <address>
              <addrLine>Avenue Agropolis TA A-DIR / 04 34398 Montpellier Cedex 5 France</addrLine>
              <country key="FR" />
            </address>
            <ref type="url">https://www.cirad.fr/qui-sommes-nous/organigramme/departements-scientifiques/systemes-biologiques-bios/presentation</ref>
          </desc>
          <listRelation>
            <relation active="#struct-11574" type="direct" />
          </listRelation>
        </org>
        <org type="researchteam" xml:id="struct-1100621" status="VALID">
          <idno type="RNSR">201121208J</idno>
          <orgName>Scientific Data Management</orgName>
          <orgName type="acronym">ZENITH</orgName>
          <date type="start">2022-01-01</date>
          <desc>
            <address>
              <addrLine>LIRMM, 161 rue Ada, 34000 Montpellier</addrLine>
              <country key="FR" />
            </address>
            <ref type="url">https://team.inria.fr/zenith/</ref>
          </desc>
          <listRelation>
            <relation active="#struct-34586" type="direct" />
            <relation active="#struct-300009" type="indirect" />
            <relation active="#struct-1100620" type="direct" />
            <relation name="UMR5506" active="#struct-441569" type="indirect" />
            <relation name="UMR5506" active="#struct-1100589" type="indirect" />
          </listRelation>
        </org>
        <org type="institution" xml:id="struct-11574" status="VALID">
          <idno type="ISNI">0000000121539871</idno>
          <idno type="ROR">https://ror.org/05kpkpg04</idno>
          <orgName>Centre de Coop√©ration Internationale en Recherche Agronomique pour le D√©veloppement</orgName>
          <orgName type="acronym">Cirad</orgName>
          <date type="start">1984-06-01</date>
          <desc>
            <address>
              <addrLine>Si√®ge 42, rue Scheffer 75116 Paris</addrLine>
              <country key="FR" />
            </address>
            <ref type="url">http://www.cirad.fr</ref>
          </desc>
        </org>
        <org type="regroupinstitution" xml:id="struct-441569" status="VALID">
          <idno type="IdRef">02636817X</idno>
          <idno type="ISNI">0000000122597504</idno>
          <idno type="ROR">https://ror.org/02feahw73</idno>
          <orgName>Centre National de la Recherche Scientifique</orgName>
          <orgName type="acronym">CNRS</orgName>
          <date type="start">1939-10-19</date>
          <desc>
            <address>
              <country key="FR" />
            </address>
            <ref type="url">https://www.cnrs.fr/</ref>
          </desc>
        </org>
        <org type="institution" xml:id="struct-451860" status="VALID">
          <orgName>Institut de Recherche pour le D√©veloppement</orgName>
          <orgName type="acronym">IRD [France-Sud]</orgName>
          <desc>
            <address>
              <addrLine>911 avenue Agropolis,BP 6450134394 Montpellier cedex 5</addrLine>
              <country key="FR" />
            </address>
            <ref type="url">http://www.france-sud.ird.fr/</ref>
          </desc>
        </org>
        <org type="institution" xml:id="struct-577435" status="VALID">
          <idno type="ROR">https://ror.org/003vg9w96</idno>
          <orgName>Institut National de Recherche pour l‚ÄôAgriculture, l‚ÄôAlimentation et l‚ÄôEnvironnement</orgName>
          <orgName type="acronym">INRAE</orgName>
          <date type="start">2020-01-01</date>
          <desc>
            <address>
              <country key="FR" />
            </address>
          </desc>
        </org>
        <org type="regroupinstitution" xml:id="struct-1100589" status="VALID">
          <idno type="ROR">https://ror.org/051escj72</idno>
          <orgName>Universit√© de Montpellier</orgName>
          <orgName type="acronym">UM</orgName>
          <date type="start">2022-01-01</date>
          <desc>
            <address>
              <addrLine>163 rue Auguste Broussonnet - 34090 Montpellier</addrLine>
              <country key="FR" />
            </address>
            <ref type="url">http://www.umontpellier.fr/</ref>
          </desc>
        </org>
        <org type="laboratory" xml:id="struct-34586" status="VALID">
          <idno type="RNSR">198318250R</idno>
          <idno type="ROR">https://ror.org/01nzkaw91</idno>
          <orgName>Inria Sophia Antipolis - M√©diterran√©e</orgName>
          <orgName type="acronym">CRISAM</orgName>
          <desc>
            <address>
              <addrLine>2004 route des Lucioles BP 93 06902 Sophia Antipolis</addrLine>
              <country key="FR" />
            </address>
            <ref type="url">http://www.inria.fr/centre/sophia/</ref>
          </desc>
          <listRelation>
            <relation active="#struct-300009" type="direct" />
          </listRelation>
        </org>
        <org type="institution" xml:id="struct-300009" status="VALID">
          <idno type="ROR">https://ror.org/02kvxyf05</idno>
          <orgName>Institut National de Recherche en Informatique et en Automatique</orgName>
          <orgName type="acronym">Inria</orgName>
          <desc>
            <address>
              <addrLine>Domaine de VoluceauRocquencourt - BP 10578153 Le Chesnay Cedex</addrLine>
              <country key="FR" />
            </address>
            <ref type="url">http://www.inria.fr/en/</ref>
          </desc>
        </org>
        <org type="laboratory" xml:id="struct-1100620" status="VALID">
          <idno type="IdRef">139590827</idno>
          <idno type="ISNI">0000000405990488</idno>
          <idno type="RNSR">199111950H</idno>
          <idno type="ROR">https://ror.org/013yean28</idno>
          <orgName>Laboratoire d'Informatique de Robotique et de Micro√©lectronique de Montpellier</orgName>
          <orgName type="acronym">LIRMM</orgName>
          <date type="start">2022-01-01</date>
          <desc>
            <address>
              <addrLine>161 rue Ada - 34095 Montpellier</addrLine>
              <country key="FR" />
            </address>
            <ref type="url">https://www.lirmm.fr</ref>
          </desc>
          <listRelation>
            <relation name="UMR5506" active="#struct-441569" type="direct" />
            <relation name="UMR5506" active="#struct-1100589" type="direct" />
          </listRelation>
        </org>
      </listOrg>
    </back>
  <teiCorpus>
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Overview of PlantCLEF 2022: Image-based plant identification at global scale</title>
			</titleStmt>
			<publicationStmt>
				<publisher />
				<availability status="unknown"><licence /></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Herv√©</forename><surname>Go√´au</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">CIRAD</orgName>
								<orgName type="institution">UMR AMAP</orgName>
								<address>
									<settlement>Montpellier, Occitanie</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Pierre</forename><surname>Bonnet</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">CIRAD</orgName>
								<orgName type="institution">UMR AMAP</orgName>
								<address>
									<settlement>Montpellier, Occitanie</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Alexis</forename><surname>Joly</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory" key="lab1">Inria</orgName>
								<orgName type="laboratory" key="lab2">LIRMM</orgName>
								<orgName type="institution" key="instit1">Univ Montpellier</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<address>
									<settlement>Montpellier</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Overview of PlantCLEF 2022: Image-based plant identification at global scale</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">CFCCAC2F02003E00925031D7FDA15F65</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-04-12T14:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid" />
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>LifeCLEF</term>
					<term>fine-grained classification</term>
					<term>species identification</term>
					<term>biodiversity informatics</term>
					<term>evaluation</term>
					<term>benchmark</term>
				</keywords>
			</textClass>
			<abstract>
<div><p>It is estimated that there are more than 300,000 species of vascular plants in the world. Increasing our knowledge of these species is of paramount importance for the development of human civilization (agriculture, construction, pharmacopoeia, etc.), especially in the context of the biodiversity crisis. However, the burden of systematic plant identification by human experts strongly penalizes the aggregation of new data and knowledge. Since then, automatic identification has made considerable progress in recent years as highlighted during all previous editions of <software>PlantCLEF</software>. Deep learning techniques now seem mature enough to address the ultimate but realistic problem of global identification of plant biodiversity in spite of many problems that the data may present (a huge number of classes, very strongly unbalanced classes, partially erroneous identifications, duplications, variable visual quality, diversity of visual contents such as photos or herbarium sheets, etc). The <software ContextAttributes="used">PlantCLEF</software>2022 challenge edition proposes to take a step in this direction by tackling a multi-image (and metadata) classification problem with a very large number of classes (80k plant species). This paper presents the resources and evaluations of the challenge, summarizes the approaches and systems employed by the participating research groups, and provides an analysis of key findings.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div><head n="1.">Introduction</head><p>It is estimated that there are more than 300,000 species of vascular plants in the world and new plant species are still discovered and described each year <ref type="bibr" target="#b0">[1]</ref>. This plant diversity has been one of the major elements in the development of human civilization (food, medicine, building materials, recreation, genes, etc.) and it is known to play a crucial role in the functioning and stability of ecosystems <ref type="bibr" target="#b1">[2]</ref>. However, our knowledge of plants at the species level is still in its infancy. For the vast majority of species, we have no idea of their specific ecosystemic role or their potential use by humans. Even our knowledge of the geographic distribution and abundance of populations remains very limited for most species <ref type="bibr" target="#b2">[3]</ref>. The biodiversity informatics community has made significant efforts over the past two decades to develop global initiatives, digital platforms and tools to help biologists organize, share, visualize and analyze biodiversity data <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref>. However, the burden of systematic plant identification severely penalizes the aggregation CLEF 2022: Conference and Labs of the Evaluation Forum, September 5-8, 2022, Bologna, Italy herve.goeau@cirad.fr (H. Go√´au); pierre.bonnet@cirad.fr (P. Bonnet); alexis.joly@inria.fr (A. Joly) 0000-0003-3296-3795 (H. Go√´au); 0000-0002-2828-4389 (P. Bonnet); 0000-0002-2161-9940 (A. <ref type="bibr">Joly)</ref> of new data and knowledge at the species level. Botanists, taxonomists and other plant experts spend a lot of time and energy identifying species when their expertise could be more useful in analyzing the data collected. As already discussed in <ref type="bibr" target="#b5">[6]</ref>, the routine identification of specimens of previously described species has many of the characteristics of other humankind activities that have been automated successfully in the past. Since then, automated identification has made considerable progress, particularly in recent years, thanks to the development of deep learning and Convolutional Neural Networks (CNN) in particular <ref type="bibr" target="#b6">[7]</ref>. The long-term evaluation of automated plant identification organized as part of the <software ContextAttributes="used">LifeCLEF</software> initiative <ref type="bibr" target="#b7">[8]</ref>, illustrated how the arrival of CNNs has impacted performance in a few years. In 2011, the accuracy of the best system evaluated was barely 57% on a very simple classification task involving 71 species photographed under very homogeneous conditions (scans or photos of leaves on a white background). In 2017, the accuracy of the best CNN was 88.5% on a much more complex task related to 10K plant species illustrated by highly imbalanced, heterogeneous and noisy visual data <ref type="bibr" target="#b8">[9]</ref>. In 2018, the best system was able to provide more accurate results than 5 of the 9 specialists who were asked to re-identify a subset of the test images <ref type="bibr" target="#b9">[10]</ref>.</p><p>Thanks to their fast growing audience, existing plant identification applications are an opportunity for high-throughput biodiversity monitoring and for the aggregation of new specific knowledge <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13]</ref>. However, they face the problem of being either too restricted to the flora of particular regions or of being limited to the most common species. As there are more and more species with a transcontinental range (such as naturalized alien species <ref type="bibr" target="#b13">[14]</ref> or cultivated plants), fragmenting the identification in regional floras is less and less a reliable approach. On the other hand, focusing only on the most common species on Earth is clearly not a better idea in terms of biodiversity.</p><p>The PlantCLEF2022 challenge edition proposes to take a step in this direction by tackling a multi-image (and metadata) classification problem with a very large number of classes (80k plant species). CNNs and the recent Vision Transformers techniques are without doubt the most promising solution today for addressing such a very large scale image classification task. However, no previous work had reported image classification results of this order of magnitude, whether or not they are biological entities. This paper presents the resources and evaluations of the challenge, summarizes the approaches and systems employed by the participating research groups, and provides an analysis of key findings.</p></div>
<div><head n="2.">Dataset</head><p>To evaluate the above mentioned scenario at a large scale and in realistic conditions, we built and shared two training datasets: "trusted" and "web" (i.e. with or without species labels provided and checked by human experts), totaling 4M images on 80k classes coming from different sources.</p><p>Training set "trusted": this training dataset is based on a selection of more than 2.9M images covering 80k plant species shared and collected mainly by GBIF (and EOL to a lesser extent). These images come mainly from academic sources (museums, universities, national institutions) and collaborative platforms such as inaturalist or Pl@ntNet, implying a fairly high certainty of determination quality. Nowadays, many more photographs are available on these platforms for a few thousand species, but the number of images has been globally limited to around 100 images per species, favouring types of views adapted to the identification of plants (close-ups of flowers, fruits, leaves, trunks, ...), in order to not unbalance the classes and to not explode the size of the training dataset.</p><p>Training set "web": in contrast, the second data set is based on a collection of web images provided by search engines <software>Google</software> and <software ContextAttributes="used">Bing</software>. This initial collection of several million images suffers however from a significant rate of species identification errors and a massive presence of duplicates and images less adapted for visual identification of plants (herbariums, landscapes, microscopic views...), or even off-topic (portrait photos of botanists, maps, graphs, other kingdoms of the living, manufactured objects, ...). The initial collection has been then semi-automatically revised to drastically reduce the number of these irrelevant pictures and to maximise, as for the trusted dataset, close-ups of flowers, fruits, leaves, trunks, etc. The "web" dataset finally contains about 1.1 million images covering around 57k species.</p><p>Test set: is built from multi-image plant observations collected on the Pl@ntNet platform during the year 2021 (observations not yet shared through GBIF, and thus not present in the training set). Only observations that received a very high confidence score in the Pl@ntNet collaborative review process were selected for the challenge to ensure the highest possible quality of determination. This process involves people with a wide range of skills (from beginners to world-leading experts), but these have different weights in the decision algorithms. Finally, the test set contains about 27k plant observations related to about 55k images (a plant can be associated with several images) covering about 7.3k species.</p><p>Table1 shows various statistics about the three datasets. We can note a significant difference between the number of species present in the training sets and the test set mainly due to the fact that it was difficult to collect so much expert data by botanists at such a scale. However, having fewer species in the test set remains consistent with a realistic scenario faced by automatic identification systems such as Pl@ntNet, <software>Inaturalist</software>: these systems must be able to recognize as many species as possible without knowing in advance which species will be the most frequently requested and which will never be requested. </p></div>
<div><head n="3.">Task Description</head><p>The challenge was hosted in the AICrowd plateform <ref type="foot" target="#foot_0">1</ref> . The task was evaluated as a plant species retrieval task based on multi-image plant observations from the test set. The goal was to retrieve the correct plant species among the top results of a ranked list of species returned by the evaluated system. The participants had access to the training set in mid-February 2022, the test set was published 6 weeks later in early April, and the round of submissions was then open during 5 weeks.</p><p>The metric used for the evaluation of the task is the Macro Average (by species) Mean Reciprocal Rank (MA-MRR). The Mean Reciprocal Rank (MRR) is a statistic measure for evaluating any process that produces a list of possible responses to a sample of queries ordered by probability of correctness. The reciprocal rank of a query response is the multiplicative inverse of the rank of the first correct answer. The MRR is the average of the reciprocal ranks for the whole test set:</p><formula xml:id="formula_0">ùëÄ ùëÖùëÖ = 1 ùëÇ ùëÇ ‚àëÔ∏Å ùëñ=1 1 rank ùëñ (<label>1</label></formula><formula xml:id="formula_1">)</formula><p>where ùëÇ is the total number of plant observations (query occurrences) in the test set and rank ùëñ is the rank of the correct species of the plant observation ùëñ.</p><p>However, the Macro-Average version of the MRR (average MRR per species in the test set) was used because of the long tail of the data distribution to rebalance the results between underand over-represented species in the test set:</p><formula xml:id="formula_2">ùëÄ ùê¥ -ùëÄ ùëÖùëÖ = 1 ùëÜ ùëÜ ‚àëÔ∏Å ùëó=1 1 ùëÇ ùëó ùëÇ ùëó ‚àëÔ∏Å ùëñ=1 1 rank ùëñ (<label>2</label></formula><formula xml:id="formula_3">)</formula><p>where ùëÜ is the total number of species in the test set, ùëÇ ùëó is the number of plant observations related to a species ùëó.</p></div>
<div><head n="4.">Participants and methods</head><p>90 research groups registered to the <software ContextAttributes="used">LifeCLEF</software> plant challenge 2022. Among this large raw audience, 8 research groups finally succeeded in submitting run files. Details of the used methods and evaluated systems are synthesized below and further developed in the working notes of the participants (Mingle Xu <ref type="bibr" target="#b14">[15]</ref>, Neuon AI <ref type="bibr" target="#b15">[16]</ref>, Chans Temple <ref type="bibr" target="#b16">[17]</ref>, <software ContextAttributes="used">BioMachina</software> <ref type="bibr" target="#b17">[18]</ref>, KL-SSN-CE <ref type="bibr" target="#b18">[19]</ref> and SVJ-SSN-CE <ref type="bibr" target="#b19">[20]</ref>). Table <ref type="table" target="#tab_1">2</ref> reports the results obtained by each run as well as a brief synthesis of the methods used in each of them. Complementary, the following paragraphs give a few more details about the methods and the overall strategy employed by each participant (the paragraphs are sorted in descending order of the best score obtained by each team; the number of runs does not reflect the total number of submitted runs but the ones described in the participants' working notes).</p><p>Mingle Xu, South Korea, 7 runs, <ref type="bibr" target="#b14">[15]</ref>: this team based their work on a vision transformer pre-trained with a Self Supervised Learning (SSL) method, a recent and increasingly popular approach in the field of computer vision. This type of approach is quite disruptive since it does not use labels compared to the usual Supervised Transfer Learning (STL) method where typically a network is pre-trained first to perform a classification task on a generic dataset such as ImageNet, and then finetuned on a specific dataset. It is expected that a network pre-trained with an SSL method extract better features, with more generalization power, which can then be then efficiently finetuned in a supervised manner on various downstream tasks such as image classification or object detection. SSL methods generally work with two models (ViT or CNN), for instance with a "student" model that tries to extract similar features learned by a "teacher" model despite several alterations of the image (data augmentation) such as for DINO <ref type="bibr" target="#b20">[21]</ref>. The Masked Auto-Encoder (MAE) <ref type="bibr" target="#b21">[22]</ref> used by the team is an other way to perform a self-supervised learning inspired by the successful idea of masked language modeling in Natural Language Processing, especially since BERT <ref type="bibr" target="#b22">[23]</ref>. The masking process was difficult to apply to CNN-based architectures whereas it becomes quite straightforward with vision transformers since they work internally in the form of visual patches or "tokens" with positional embedding. MAE is similar to BEIT <ref type="bibr" target="#b23">[24]</ref> where the self supervised task consists in training a backbone vision transformer to predict missing tokens from partially masked images. Beyond the originality of the pre-training method, the successive runs of this team consist in following snapshots over several days of training.</p><p>Neuon AI, Malaysia, 4 runs, <ref type="bibr" target="#b15">[16]</ref>: this participant used various ensembles of models finetuned on the "trusted", and from some of them on also the "web", training dataset and based on Inception-v4 and Inception-ResNet-v2 architectures <ref type="bibr" target="#b24">[25]</ref>. Most of the models are directly finetuned CNNs but as a multi-task classification network related to the five taxonomy levels: Species (the main task), Genus, Family, Order and Class (in the botanical sense). A second type of model is triplet network based also on a Inception-v4 or Inception-ResNet-v2 CNN models where the last fully connected layer is used for embedding representation limited to 500 visual words instead of the heavy 80k outputs typically necessary for the species classification task. However, the triplet network seems to be longer to saturate the training and worked only on the species levels making difficult to compare the real contribution of this type of network in the ensemble results submitted by this team.</p><p>Chans Temple, Malaysia, 3 runs, <ref type="bibr" target="#b16">[17]</ref>: inspired by the Hierarchical Deep Convolutional Neural Networks (HD-CNN) in <ref type="bibr" target="#b25">[26]</ref>, this participant explored the taxonomy information in different ways with a more recent architecture and deep learning framework. A first strategy is to first finetune a ResNet34 model on a family classification task and then finetune it to the species level. A second similar strategy begins with a multi-task classification by finetuning a ResNet34 model on all five levels of taxonomy at once (species, genus, family, order, botanical class) before continuing with finetuning on the species level only. The best submission was an ensemble of these two models combined with two regular models without taxonomy (a ResNet50, and a ResNet50-Wide).</p><p><software ContextAttributes="used">BioMachina</software>, Costa Rica, 5 runs, <ref type="bibr" target="#b17">[18]</ref>: the main contribution and very interesting idea explored by this team was to let a model learn its own hierarchy instead exploiting directly the taxonomy provided in the dataset. They proposed a 2-level hierarchical softmax which has the interesting property to reduce drastically the weights of the usual fully connected layer of the classification head while maintaining the same performances. This property is illustrated by comparing a EfficientNetB4 with its learnt hierarchical version. Considering that the output size of a EfficientNetB4 backbone is 2048, the proposed hierarchical design allows to reduce from 160 millions of parameters in a fully connected layer (2048x80k weights + 80k biases) to 28.2 millions of parameters, enabling potentially a much more faster training since it allows to increase the batch size. Aside this hierarchical approach, they also highlighted on a standard ResNet50 that a good strategy of training was first to finetune a model on the web training dataset before finetuning it on the trusted dataset. The results obtained with a heavier ResNet101 model on the other hand did not reveal to have a great impact. Following the modern best practices of deep learning, various techniques to reduce learning times and ensure better convergence of models during learning have also been explored (automatic mixed precision, batch accumulation, gradient clipping).</p></div>
<div><head>KL-SSN-CE, India, 1 runs, [19]</head><p>: this team tried to train in a recent deep learning framework the famous historical model AlexNet which marked the revival of neural networks 10 years ago. They compared many optimizers and loss functions, and selected for their main submission AdaGrad and KL Divergence.</p></div>
<div><head>SVJ-SSN-CE, India, 2 runs, [20]</head><p>: this team focused their contribution on the classification head on the top of a ResNet50 pre-trained model. After disappointing results with a standard classifier using only one Fully Connected (FC) layer, they expected more relevant features and better classification performances by adding a second intermediate FC layer and using a sparse categorical cross-entropy loss. In a second multi-level classification ResNet50 model, they implemented a probabilistic tree approach to use the taxonomy information.</p></div>
<div><head n="5.">Results</head><p>We report in Figure <ref type="figure" target="#fig_0">1</ref> the performance achieved by the collected runs. Table <ref type="table" target="#tab_1">2</ref> provides the results achieved by each run as well as a brief synthesis of the methods used in each of them.</p><p>The main outcomes we can derive from that results are the following:</p><p>A new supremacy of the vision transformer: the best results were obtained by the only team which used vision transformers <ref type="bibr" target="#b14">[15]</ref> contrary to the others which used convolutional neural networks, i.e. the traditional approach of the state-of-the-art for image-based plant identification. If we compare the performance of the best vision transformer (Mingle XU Run 8, MA-MRR=0.626) to the one of the best CNN trained on the same data (Neuon AI Run 2, MA-MRR=0.553), we can observe that the gain is very high.</p><p>The race for GPUs: however, the gain in identification performance obtained by the vision transformers is paid for by a significant increase of the training time. The winning team reported that they had to stop the training of the model in order to submit their run to the challenge. Thus, better results could have surely been obtained with a few more days of training (as demonstrated through post-challenge evaluations reported in the their working note <ref type="bibr" target="#b14">[15]</ref>.</p></div>
<div><head>Rationalization of the use of memory:</head><p>One of the main difficulties of the challenge was the very large number of classes (80K). For most of the models used, the majority of the weights to be trained are those of the last fully connected layer of the classifier. This was an important consideration for all participants in their model selection strategy. Some teams have tried to limit this cost through specific approaches. The <software ContextAttributes="used">BioMachina</software> team <ref type="bibr" target="#b17">[18]</ref>, in particular, . Chans Temple highlighted in his preliminary results on a validation set that pretraining a model with the Family level, and in a lesser extent all the taxonomy levels, improve the performance of a single species classification task. But all these approach are at the expense of additional layers or/and computing time, contrary to the original <software ContextAttributes="used">BioMachina</software>'s approach which aims at reducing the memory footprint by letting a model to learn its own hierarchy.</p><p>The noisy web training dataset may help: as yet noticed in <software ContextAttributes="used">PlantCLEF</software> 2017 <ref type="bibr" target="#b8">[9]</ref> Neuon AI showed again that the noisy data from the web training dataset does improve the generalisation of their CNN models, as well as <software ContextAttributes="used">BioMachina</software> who successfully pre-training models on the web training dataset before finetuning them on the trusted dataset.</p></div>
<div><head n="6.">Additional analyses</head><p>During the previous years of <software ContextAttributes="created">PlantCLEF</software>, it was shown that it was much more difficult to identify species from the Amazon rainforest than species from Europe and North America <ref type="bibr" target="#b26">[27]</ref>. By extension, we can assume that most identification systems would encounter the same difficulties in other tropical rainforests in Equatorial Africa or Indonesia, but without really measuring this. This year's global challenge can give us some information and a first overview of the performance of automatic systems in different large regional areas. Table <ref type="table" target="#tab_2">3</ref> and Figure <ref type="figure" target="#fig_1">2</ref> show the MA-MRR averaged over all runs submitted and detailed for different regional species checklists. The regional division follows the level 3 of the standard WGSRPD (World Geographical Scheme for Recording Plant Distributions <ref type="bibr" target="#b27">[28]</ref> managed by the Biodiversity Information Standards (TDWG).</p><p>In this table, we can see that globally the areas corresponding to the western countries (Europe, North America, Australia and New Zealand) obtain performances among the highest, while in the lower part of the table we can note many areas corresponding to tropical regions from South and Central America, India and Africa. This result is to be expected since the average number of images per species tends to be correlated with the average run performances. However, it is harder to explain some good performances like those obtained over Papuasia, a typically tropical region with less data. This type of analysis would deserve to be more developed and detailed in order to draw useful conclusions that could be used in the future as recommendations for new data collection efforts around the world. </p></div>
<div><head n="7.">Conclusion</head><p>This paper presented the overview and the results of the <software>LifeCLEF</software> 2022 plant identification challenge following the 11 previous ones conducted within CLEF evaluation forum. This year the task was performed on the biggest plant images dataset ever published in the literature. This dataset was composed of two distinct sources: a trusted set built from the GBIF and a noisy web dataset totaling both 4M images and covering 80k species. The main conclusion of our evaluation is that vision transformers performed better than CNNs as demonstrated by the Mingle Xu team knowing that the training of their model was not yet completed at the time of the challenge closure. This shows the potential of these techniques on huge datasets such as the one of <software ContextAttributes="created">PlantCLEF</software>. However, training those models requires more computational resources that only participants with access to large computational clusters can afford. Beyond duality between free vision transformers and CNNs, the <software ContextAttributes="created">BioMachina</software> team has demonstrated that it is possible to drastically reduce the number of parameters of the classification head at the output of a backbone while maintaining the performance of a classical approach with a fully connected layer. This result is of great importance because it allows to consider more serenely in the future classification models of species that would address the 300,000 species of plants on a global scale. This contribution is also interesting because it allows us to redefine an optimized hierarchy for visual feature extraction mechanisms by moving away from the expert hierarchical framework of taxonomy, which may be a bit too rigid because it does not systematically reflect morphological similarities between species but also functional trait proximities and phylogenetic relationships. Beyond the technical aspects, the complementary analysis of the performances detailed by sub-continental the world would deserve to be more developed and detailed in order to draw useful conclusions of high importance in botany and biodiversity informatics in general, such as future recommendations for new data collection efforts around the world.</p></div><figure xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Scores achieved by all systems evaluated within the plant identification task of LifeCLEF 2022</figDesc><graphic coords="9,99.71,84.19,395.86,260.71" type="bitmap" /></figure>
<figure xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Map of MA-MRR averaged over all runs by regional species checklists (World Geographical Scheme for Recording Plant Distributions -WGSRPD -TDWG level 3)</figDesc><graphic coords="10,89.29,84.19,416.69,255.07" type="bitmap" /></figure>
<figure type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc>Statistics of the LifeCLEF 2022 Plant Identification Task: "n/s" means not specified</figDesc><table><row><cell>Dataset</cell><cell cols="6">Images Observations Classes (species) Genera Families Orders</cell></row><row><cell cols="2">Train "trusted" 2,886,761</cell><cell>n/s</cell><cell>80,000</cell><cell>9,603</cell><cell>483</cell><cell>84</cell></row><row><cell cols="2">Train "web" 1,071,627</cell><cell>n/s</cell><cell>57,314</cell><cell>8,649</cell><cell>479</cell><cell>84</cell></row><row><cell>Test</cell><cell>55,307</cell><cell>26,869</cell><cell>7,339</cell><cell>2,527</cell><cell /></row></table></figure>
<figure type="table" xml:id="tab_1"><head>Table 2</head><label>2</label><figDesc>Results of the LifeCLEF 2022 Plant Identification Task (limited to the runs described in the participants' working notes).</figDesc><table><row><cell cols="6">Architecture: AN: AlexNet, EN4: EfficientNetB4, HEN4: Hierarchical EfficientNetB4,</cell></row><row><cell cols="6">IRv2: Inception-ResNet-v2, Iv4: Inception-v4, RN: ResNet, Tr: triplet network, ViT-L: Vision Transformer</cell></row><row><cell cols="6">Large. Datasets: IN1k: ImageNet1k, PlantCLEF2022: T (Trusted), W (Web), TW (Trusted &amp; Web).</cell></row><row><cell cols="6">Pre-training methods: SSL: Self Supervised Learning, STL: Supervised Transfer Learning. Taxonomy:</cell></row><row><cell cols="5">-(no), A (All: species, genus, family, order, class), F: family, LH: Learned Hierarchy)</cell><cell /></row><row><cell>Team run name</cell><cell>Architecture</cell><cell>Pre-training</cell><cell cols="3">Training Taxonomy MA-MRR</cell></row><row><cell>Mingle Xu Run 8</cell><cell>ViT-L</cell><cell>SSL MAE IN1k</cell><cell>T</cell><cell>-</cell><cell>0.62692</cell></row><row><cell>Mingle Xu Run 7</cell><cell>ViT-L</cell><cell>SSL MAE IN1k</cell><cell>T</cell><cell>-</cell><cell>0.62497</cell></row><row><cell>Mingle Xu Run 6</cell><cell>ViT-L</cell><cell>SSL MAE IN1k</cell><cell>T</cell><cell>-</cell><cell>0.61632</cell></row><row><cell>Neuon AI Run 7</cell><cell>Iv4, IRv2</cell><cell>STL IN1k</cell><cell>TW</cell><cell>-</cell><cell>0.60781</cell></row><row><cell>Neuon AI Run 3</cell><cell>Iv4, IRv2</cell><cell>STL IN1k</cell><cell>TW</cell><cell>A</cell><cell>0.60583</cell></row><row><cell>Neuon AI Run 4</cell><cell>Iv4, IRv2</cell><cell>STL IN1k</cell><cell>TW</cell><cell>A</cell><cell>0.60381</cell></row><row><cell>Neuon AI Run 9</cell><cell>Iv4, IRv2</cell><cell>STL IN1k</cell><cell>TW</cell><cell>A</cell><cell>0.60301</cell></row><row><cell>Mingle Xu Run 5</cell><cell>ViT</cell><cell>SSL MAE IN1k</cell><cell>T</cell><cell>-</cell><cell>0.60219</cell></row><row><cell>Neuon AI Run 8</cell><cell>Iv4, IRv2</cell><cell>STL IN1k</cell><cell>TW</cell><cell>A</cell><cell>0.60113</cell></row><row><cell>Neuon AI Run 5</cell><cell>Iv4, IRv2, Tr</cell><cell>STL IN1k</cell><cell>TW</cell><cell>A</cell><cell>0.59892</cell></row><row><cell>Neuon AI Run 6</cell><cell>IRv2</cell><cell>STL IN1k</cell><cell>TW</cell><cell>A</cell><cell>0.58874</cell></row><row><cell>Mingle Xu Run 4</cell><cell>ViT-L</cell><cell>SSL MAE IN1k</cell><cell>T</cell><cell>-</cell><cell>0.58110</cell></row><row><cell>Mingle Xu Run 3</cell><cell>ViT-L</cell><cell>SSL MAE IN1k</cell><cell>T</cell><cell>-</cell><cell>0.56772</cell></row><row><cell>Mingle Xu Run 2</cell><cell>ViT-L</cell><cell>SSL MAE IN1k</cell><cell>T</cell><cell>-</cell><cell>0.55865</cell></row><row><cell>Neuon AI Run 2</cell><cell>Iv4, IRv2</cell><cell>STL IN1k</cell><cell>T</cell><cell>A</cell><cell>0.55358</cell></row><row><cell>Neuon AI Run 1</cell><cell>IRv2</cell><cell>STL IN1k</cell><cell>T</cell><cell>A</cell><cell>0.54613</cell></row><row><cell cols="2">Chans Temple Run 10 RN34, RN50</cell><cell>STL IN1k</cell><cell>T</cell><cell>-,A,F</cell><cell>0.51043</cell></row><row><cell>Chans Temple Run 9</cell><cell>RN34</cell><cell>STL IN1k</cell><cell>T</cell><cell>F</cell><cell>0.49994</cell></row><row><cell>Chans Temple Run 8</cell><cell>RN34</cell><cell>STL IN1k</cell><cell>T</cell><cell>A</cell><cell>0.47447</cell></row><row><cell>BioMachina Run 5</cell><cell>RN50</cell><cell>STL IN1k‚ÜíW</cell><cell>T</cell><cell>-</cell><cell>0.46010</cell></row><row><cell>BioMachina Run 6</cell><cell>RN101</cell><cell>STL IN1k‚ÜíW</cell><cell>T</cell><cell>-</cell><cell>0.45011</cell></row><row><cell>BioMachina Run 3</cell><cell>RN50</cell><cell>STL IN1k</cell><cell>T</cell><cell>-</cell><cell>0.43820</cell></row><row><cell>BioMachina Run 1</cell><cell>HEN4</cell><cell>STL IN1k</cell><cell>T</cell><cell>LH</cell><cell>0.41950</cell></row><row><cell>BioMachina Run 8</cell><cell>EN4</cell><cell>STL IN1k</cell><cell>T</cell><cell>-</cell><cell>0.41240</cell></row><row><cell>KL-SSN-CE Run 1</cell><cell>AN</cell><cell>STL IN1k</cell><cell>T</cell><cell>-</cell><cell>0.00029</cell></row><row><cell>SVJ-SSN-CSE Run 3</cell><cell>RN50</cell><cell>STL IN1k</cell><cell>T</cell><cell>A</cell><cell>0.00015</cell></row><row><cell>SVJ-SSN-CSE Run 1</cell><cell>RN50</cell><cell>STL IN1k</cell><cell>T</cell><cell>A</cell><cell>0.00005</cell></row></table></figure>
<figure type="table" xml:id="tab_2"><head>Table 3</head><label>3</label><figDesc>MA-MRR averaged over all runs by regional species checklists (World Geographical Scheme for Recording Plant Distributions -WGSRPD -TDWG level 3). Img/sp gives the average number of trusted images per species on the current checklist.</figDesc><table><row><cell>Checklist</cell><cell>Species</cell><cell>Img/sp</cell><cell>Mean MA-MRR</cell></row><row><cell>Middle Atlantic Ocean</cell><cell>299</cell><cell>96</cell><cell>0.5706</cell></row><row><cell>New Zealand</cell><cell>809</cell><cell>97</cell><cell>0.5411</cell></row><row><cell>South-Central Pacific</cell><cell>479</cell><cell>94</cell><cell>0.5351</cell></row><row><cell>Papuasia</cell><cell>439</cell><cell>89</cell><cell>0.5204</cell></row><row><cell>Northwestern Pacific</cell><cell>210</cell><cell>93</cell><cell>0.5150</cell></row><row><cell>North-Central Pacific</cell><cell>448</cell><cell>95</cell><cell>0.5094</cell></row><row><cell>Western Canada</cell><cell>739</cell><cell>99</cell><cell>0.5038</cell></row><row><cell>Subantarctic Islands</cell><cell>268</cell><cell>98</cell><cell>0.5029</cell></row><row><cell>Eastern Europe</cell><cell>1916</cell><cell>97</cell><cell>0.5029</cell></row><row><cell>Middle Europe</cell><cell>2537</cell><cell>94</cell><cell>0.4988</cell></row><row><cell>Antarctic Continent</cell><cell>2</cell><cell>68</cell><cell>0.4974</cell></row><row><cell>Eastern Canada</cell><cell>934</cell><cell>98</cell><cell>0.4951</cell></row><row><cell>Caucasus</cell><cell>1265</cell><cell>96</cell><cell>0.4949</cell></row><row><cell>Northern Europe</cell><cell>1876</cell><cell>97</cell><cell>0.4946</cell></row><row><cell>Southeastern Europe</cell><cell>3055</cell><cell>91</cell><cell>0.4942</cell></row><row><cell>South-Central U.S.A.</cell><cell>1032</cell><cell>98</cell><cell>0.4893</cell></row><row><cell>Australia</cell><cell>1098</cell><cell>93</cell><cell>0.4878</cell></row><row><cell>Northwestern U.S.A.</cell><cell>1041</cell><cell>98</cell><cell>0.4864</cell></row><row><cell>Southwestern Pacific</cell><cell>619</cell><cell>92</cell><cell>0.4861</cell></row><row><cell>Southeastern U.S.A.</cell><cell>1766</cell><cell>96</cell><cell>0.4835</cell></row><row><cell>Malesia</cell><cell>880</cell><cell>86</cell><cell>0.4831</cell></row><row><cell>Southwestern U.S.A.</cell><cell>1176</cell><cell>97</cell><cell>0.4812</cell></row><row><cell>North-Central U.S.A.</cell><cell>1290</cell><cell>98</cell><cell>0.4807</cell></row><row><cell>Arabian Peninsula</cell><cell>701</cell><cell>88</cell><cell>0.4807</cell></row><row><cell>Siberia</cell><cell>881</cell><cell>98</cell><cell>0.4803</cell></row><row><cell>Russian Far East</cell><cell>746</cell><cell>96</cell><cell>0.4765</cell></row><row><cell>Northeastern U.S.A.</cell><cell>1507</cell><cell>98</cell><cell>0.4760</cell></row><row><cell>Indo-China</cell><cell>1142</cell><cell>86</cell><cell>0.4750</cell></row><row><cell>Macaronesia</cell><cell>1344</cell><cell>94</cell><cell>0.4737</cell></row><row><cell>Southwestern Europe</cell><cell>3201</cell><cell>90</cell><cell>0.4736</cell></row><row><cell>South Tropical Africa</cell><cell>875</cell><cell>84</cell><cell>0.4728</cell></row><row><cell>Subarctic America</cell><cell>440</cell><cell>99</cell><cell>0.4722</cell></row><row><cell>Western Asia</cell><cell>2093</cell><cell>93</cell><cell>0.4718</cell></row><row><cell>Northern Africa</cell><cell>1846</cell><cell>91</cell><cell>0.4665</cell></row><row><cell>Middle Asia</cell><cell>1202</cell><cell>96</cell><cell>0.4659</cell></row><row><cell>Mongolia</cell><cell>352</cell><cell>97</cell><cell>0.4605</cell></row><row><cell>Southern South America</cell><cell>1575</cell><cell>82</cell><cell>0.4538</cell></row><row><cell>West-Central Tropical Africa</cell><cell>908</cell><cell>82</cell><cell>0.4531</cell></row><row><cell>Brazil</cell><cell>1142</cell><cell>79</cell><cell>0.4523</cell></row><row><cell>Northeast Tropical Africa</cell><cell>1016</cell><cell>81</cell><cell>0.4513</cell></row><row><cell>Eastern Asia</cell><cell>1376</cell><cell>91</cell><cell>0.4503</cell></row><row><cell>Western Indian Ocean</cell><cell>929</cell><cell>84</cell><cell>0.4461</cell></row><row><cell>Caribbean</cell><cell>1418</cell><cell>88</cell><cell>0.4404</cell></row><row><cell>Northern South America</cell><cell>921</cell><cell>84</cell><cell>0.4379</cell></row><row><cell>Indian Subcontinent</cell><cell>1661</cell><cell>89</cell><cell>0.4363</cell></row><row><cell>West Tropical Africa</cell><cell>774</cell><cell>82</cell><cell>0.4332</cell></row><row><cell>Central America</cell><cell>1215</cell><cell>87</cell><cell>0.4329</cell></row><row><cell>China</cell><cell>1475</cell><cell>86</cell><cell>0.4325</cell></row><row><cell>Southern Africa</cell><cell>1226</cell><cell>86</cell><cell>0.4311</cell></row><row><cell>East Tropical Africa</cell><cell>898</cell><cell>78</cell><cell>0.4291</cell></row><row><cell>Mexico</cell><cell>1477</cell><cell>88</cell><cell>0.4071</cell></row><row><cell>Western South America</cell><cell>1655</cell><cell>80</cell><cell>0.4021</cell></row></table></figure>
			<note place="foot" n="1" xml:id="foot_0"><p>https://www.aicrowd.com/challenges/lifeclef-2022-plant</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The number of known plants species in the world and its annual increase</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Christenhusz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Byng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phytotaxa</title>
		<imprint>
			<biblScope unit="volume">261</biblScope>
			<biblScope unit="page" from="201" to="217" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Naeem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Bunker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hector</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Loreau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Perrings</surname></persName>
		</author>
		<title level="m">Biodiversity, ecosystem functioning, and human wellbeing: an ecological and economic perspective</title>
		<imprint>
			<publisher>OUP Oxford</publisher>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Plant extinctions take time</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Cronk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">353</biblScope>
			<biblScope unit="page" from="446" to="447" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The encyclopedia of life v2: providing global access to knowledge about life on earth</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Parr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">N</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Leary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">S</forename><surname>Schulz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Lans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Walley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Hammock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Goddard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Rice</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Studer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biodiversity data journal</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">What if gbif?</title>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">D</forename><surname>Wheeler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BioScience</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="717" to="717" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Automated species identification: why not?</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Gaston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>O'neill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Philosophical Transactions of the Royal Society of London. Series B: Biological Sciences</title>
		<imprint>
			<biblScope unit="volume">359</biblScope>
			<biblScope unit="page" from="655" to="667" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="page" from="436" to="444" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Biodiversity information retrieval through large scale content-based identification: a long-term evaluation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Go√´au</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Spampinato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-P</forename><surname>Vellinga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-C</forename><surname>Lombardo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Planqu√©</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Palazzo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>M√ºller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Information Retrieval Evaluation in a Changing World</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="389" to="413" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Plant identification based on noisy web data: the amazing performance of deep learning (lifeclef 2017)</title>
		<author>
			<persName><forename type="first">H</forename><surname>Go√´au</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CLEF: Conference and Labs of the Evaluation Forum</title>
		<meeting><address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-09">Sep. 2017. 2017</date>
		</imprint>
	</monogr>
	<note>CLEF task overview 2017</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Overview of expertlifeclef 2018: how far automated identification systems are from the best experts? lifeclef experts vs. machine plant identification task</title>
		<author>
			<persName><forename type="first">H</forename><surname>Go√´au</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CLEF: Conference and Labs of the Evaluation Forum</title>
		<meeting><address><addrLine>Avignon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-09">2018. Sep. 2018. 2018</date>
		</imprint>
	</monogr>
	<note>CLEF task overview 2018</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">inaturalist</title>
		<author>
			<persName><forename type="first">J</forename><surname>Nugent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science Scope</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="12" to="13" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Automated plant species identification-trends and future directions</title>
		<author>
			<persName><forename type="first">J</forename><surname>W√§ldchen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rzanny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Seeland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>M√§der</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS computational biology</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">1005993</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Pl@ ntnet app in the era of deep learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Affouard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Go√´au</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-C</forename><surname>Lombardo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR: International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The global naturalized alien flora (glonaf) database</title>
		<author>
			<persName><forename type="first">M</forename><surname>Van Kleunen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Py≈°ek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dawson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kreft</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pergl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Weigelt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dullinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>K√∂nig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lenzner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ecology</title>
		<imprint>
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Vision transformer-based unsupervised transfer learning for large scale plant identification</title>
		<author>
			<persName><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Working Notes of CLEF 2022 -Conference and Labs of the Evaluation Forum</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A global-scale plant identification using deep learning: Neuon submission to plantclef</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chulif</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">L</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Working Notes of CLEF 2022 -Conference and Labs of the Evaluation Forum</title>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Image-based plant identification with taxonomy aware architecture</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Min</forename><surname>Ong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">W</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Working Notes of CLEF 2022 -Conference and Labs of the Evaluation Forum</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Extreme automatic plant identification under constrained resources</title>
		<author>
			<persName><forename type="first">J</forename><surname>Carranza-Rojas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gonzalez-Villanueva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Jimenez-Morales</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Quesada-Montero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Esquivel-Barboza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Carvajal-Barboza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Working Notes of CLEF 2022 -Conference and Labs of the Evaluation Forum</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Classification of plant species using alexnet architecture</title>
		<author>
			<persName><forename type="first">K</forename><surname>Pravinkrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sivakumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Balasundaram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kalinathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Working Notes of CLEF 2022 -Conference and Labs of the Evaluation Forum</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Plant species identification using probability tree approach of deep learning models</title>
		<author>
			<persName><forename type="first">A</forename><surname>Karun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Divyasri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Balasundaram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Sella</surname></persName>
		</author>
		<author>
			<persName><surname>Veluswami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Working Notes of CLEF 2022 -Conference and Labs of the Evaluation Forum</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Emerging properties in self-supervised vision transformers</title>
		<author>
			<persName><forename type="first">M</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>J√©gou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="9650" to="9660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Masked autoencoders are scalable vision learners</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Doll√°r</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="16000" to="16009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bert</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.08254</idno>
		<title level="m">Beit: Bert pre-training of image transformers</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Alemi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.07261</idno>
		<title level="m">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Hd-cnn: hierarchical deep convolutional neural networks for large scale visual recognition</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Piramuthu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Jagadeesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Decoste</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Di</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2740" to="2748" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Overview of lifeclef plant identification task 2019: diving into data deficient tropical countries</title>
		<author>
			<persName><forename type="first">H</forename><surname>Go√´au</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CLEF: Conference and Labs of the Evaluation Forum</title>
		<meeting><address><addrLine>Lugano, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-09">Sep. 2019. 2019</date>
		</imprint>
	</monogr>
	<note>CLEF task overview 2019</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">World geographical scheme for recording plant distributions</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">K</forename><surname>Brummitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Pando</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hollis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Brummitt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International working group on taxonomic databases for plant sciences</title>
		<imprint>
			<publisher>TDWG</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="volume">951</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</teiCorpus></text>
</TEI>