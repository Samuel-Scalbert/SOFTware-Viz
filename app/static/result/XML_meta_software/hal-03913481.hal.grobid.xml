<?xml version='1.0' encoding='utf-8'?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" version="1.1" xsi:schemaLocation="http://www.tei-c.org/ns/1.0 http://api.archives-ouvertes.fr/documents/aofr-sword.xsd">
  <teiHeader>
    <fileDesc>
      <titleStmt>
        <title>HAL TEI export of hal-03913481</title>
      </titleStmt>
      <publicationStmt>
        <distributor>CCSD</distributor>
        <availability status="restricted">
          <licence target="http://creativecommons.org/licenses/by/4.0/">Distributed under a Creative Commons Attribution 4.0 International License</licence>
        </availability>
        <date when="2024-04-29T11:45:27+02:00" />
      </publicationStmt>
      <sourceDesc>
        <p part="N">HAL API platform</p>
      </sourceDesc>
    </fileDesc>
  </teiHeader>
  <text>
    <body>
      <listBibl>
        <biblFull>
          <titleStmt>
            <title xml:lang="en">Learning to Classify Logical Formulas Based on Their Semantic Similarity</title>
            <author role="aut">
              <persName>
                <forename type="first">Ali</forename>
                <surname>Ballout</surname>
              </persName>
              <email type="md5">4c7a70a0ac901065dc584da56f9774ca</email>
              <email type="domain">inria.fr</email>
              <idno type="idhal" notation="numeric">1208196</idno>
              <idno type="halauthorid" notation="string">2677272-1208196</idno>
              <affiliation ref="#struct-178918" />
            </author>
            <author role="aut">
              <persName>
                <forename type="first">Célia</forename>
                <surname>da Costa Pereira</surname>
              </persName>
              <email type="md5">80528e59c442110652c2c3609e201931</email>
              <email type="domain">unice.fr</email>
              <idno type="idhal" notation="string">celia-da-costa-pereira</idno>
              <idno type="idhal" notation="numeric">8874</idno>
              <idno type="halauthorid" notation="string">27310-8874</idno>
              <idno type="IDREF">https://www.idref.fr/204083907</idno>
              <affiliation ref="#struct-452156" />
            </author>
            <author role="aut">
              <persName>
                <forename type="first">Andrea G. B.</forename>
                <surname>Tettamanzi</surname>
              </persName>
              <email type="md5">5339dabafc9d4c5c3b9a4760701f908c</email>
              <email type="domain">unice.fr</email>
              <ptr type="url" target="http://www.i3s.unice.fr/~tettaman/" />
              <idno type="idhal" notation="string">andrea-g-b-tettamanzi</idno>
              <idno type="idhal" notation="numeric">3981</idno>
              <idno type="halauthorid" notation="string">19733-3981</idno>
              <idno type="ORCID">https://orcid.org/0000-0002-8877-4654</idno>
              <idno type="GOOGLE SCHOLAR">https://scholar.google.fr/citations?user=_UEWjDoAAAAJ&amp;hl=fr&amp;oi=ao</idno>
              <idno type="IDREF">https://www.idref.fr/072248807</idno>
              <affiliation ref="#struct-178918" />
            </author>
            <editor role="depositor">
              <persName>
                <forename>Andrea G. B.</forename>
                <surname>Tettamanzi</surname>
              </persName>
              <email type="md5">5339dabafc9d4c5c3b9a4760701f908c</email>
              <email type="domain">unice.fr</email>
            </editor>
            <funder ref="#projanr-50589" />
            <funder ref="#projanr-56235" />
          </titleStmt>
          <editionStmt>
            <edition n="v1" type="current">
              <date type="whenSubmitted">2022-12-27 07:33:38</date>
              <date type="whenModified">2024-02-26 11:22:08</date>
              <date type="whenReleased">2022-12-27 10:00:07</date>
              <date type="whenProduced">2022-11-16</date>
              <date type="whenEndEmbargoed">2022-12-27</date>
              <ref type="file" target="https://inria.hal.science/hal-03913481/document">
                <date notBefore="2022-12-27" />
              </ref>
              <ref type="file" subtype="author" n="1" target="https://inria.hal.science/hal-03913481/file/PRIMA_2022-cr.pdf">
                <date notBefore="2022-12-27" />
              </ref>
            </edition>
            <respStmt>
              <resp>contributor</resp>
              <name key="188560">
                <persName>
                  <forename>Andrea G. B.</forename>
                  <surname>Tettamanzi</surname>
                </persName>
                <email type="md5">5339dabafc9d4c5c3b9a4760701f908c</email>
                <email type="domain">unice.fr</email>
              </name>
            </respStmt>
          </editionStmt>
          <publicationStmt>
            <distributor>CCSD</distributor>
            <idno type="halId">hal-03913481</idno>
            <idno type="halUri">https://inria.hal.science/hal-03913481</idno>
            <idno type="halBibtex">ballout:hal-03913481</idno>
            <idno type="halRefHtml">&lt;i&gt;PRIMA 2022 - Principles and Practice of Multi-Agent Systems - 24th International Conference&lt;/i&gt;, Nov 2022, Valencia, Spain. pp.364-380, &lt;a target="_blank" href="https://dx.doi.org/10.1007/978-3-031-21203-1_22"&gt;&amp;#x27E8;10.1007/978-3-031-21203-1_22&amp;#x27E9;&lt;/a&gt;</idno>
            <idno type="halRef">PRIMA 2022 - Principles and Practice of Multi-Agent Systems - 24th International Conference, Nov 2022, Valencia, Spain. pp.364-380, &amp;#x27E8;10.1007/978-3-031-21203-1_22&amp;#x27E9;</idno>
          </publicationStmt>
          <seriesStmt>
            <idno type="stamp" n="UNICE">Université Nice Sophia Antipolis</idno>
            <idno type="stamp" n="CNRS">CNRS - Centre national de la recherche scientifique</idno>
            <idno type="stamp" n="INRIA">INRIA - Institut National de Recherche en Informatique et en Automatique</idno>
            <idno type="stamp" n="INRIA-SOPHIA">INRIA Sophia Antipolis - Méditerranée</idno>
            <idno type="stamp" n="I3S">Laboratoire d'Informatique, Signaux et Systèmes de Sophia-Antipolis</idno>
            <idno type="stamp" n="INRIASO">INRIA-SOPHIA</idno>
            <idno type="stamp" n="INRIA_TEST">INRIA - Institut National de Recherche en Informatique et en Automatique</idno>
            <idno type="stamp" n="TESTALAIN1">TESTALAIN1</idno>
            <idno type="stamp" n="WIMMICS">WIMMICS: Web-Instrumented Man-Machine Interactions, Communities, and Semantics</idno>
            <idno type="stamp" n="INRIA2">INRIA 2</idno>
            <idno type="stamp" n="UNIV-COTEDAZUR">Université Côte d'Azur</idno>
            <idno type="stamp" n="PNRIA">Programme National de Recherche en IA</idno>
            <idno type="stamp" n="3IA-COTEDAZUR">3IA Côte d’Azur – Interdisciplinary Institute for Artificial Intelligence</idno>
            <idno type="stamp" n="ANR">ANR</idno>
          </seriesStmt>
          <notesStmt>
            <note type="audience" n="2">International</note>
            <note type="invited" n="0">No</note>
            <note type="popular" n="0">No</note>
            <note type="peer" n="1">Yes</note>
            <note type="proceedings" n="1">Yes</note>
          </notesStmt>
          <sourceDesc>
            <biblStruct>
              <analytic>
                <title xml:lang="en">Learning to Classify Logical Formulas Based on Their Semantic Similarity</title>
                <author role="aut">
                  <persName>
                    <forename type="first">Ali</forename>
                    <surname>Ballout</surname>
                  </persName>
                  <email type="md5">4c7a70a0ac901065dc584da56f9774ca</email>
                  <email type="domain">inria.fr</email>
                  <idno type="idhal" notation="numeric">1208196</idno>
                  <idno type="halauthorid" notation="string">2677272-1208196</idno>
                  <affiliation ref="#struct-178918" />
                </author>
                <author role="aut">
                  <persName>
                    <forename type="first">Célia</forename>
                    <surname>da Costa Pereira</surname>
                  </persName>
                  <email type="md5">80528e59c442110652c2c3609e201931</email>
                  <email type="domain">unice.fr</email>
                  <idno type="idhal" notation="string">celia-da-costa-pereira</idno>
                  <idno type="idhal" notation="numeric">8874</idno>
                  <idno type="halauthorid" notation="string">27310-8874</idno>
                  <idno type="IDREF">https://www.idref.fr/204083907</idno>
                  <affiliation ref="#struct-452156" />
                </author>
                <author role="aut">
                  <persName>
                    <forename type="first">Andrea G. B.</forename>
                    <surname>Tettamanzi</surname>
                  </persName>
                  <email type="md5">5339dabafc9d4c5c3b9a4760701f908c</email>
                  <email type="domain">unice.fr</email>
                  <ptr type="url" target="http://www.i3s.unice.fr/~tettaman/" />
                  <idno type="idhal" notation="string">andrea-g-b-tettamanzi</idno>
                  <idno type="idhal" notation="numeric">3981</idno>
                  <idno type="halauthorid" notation="string">19733-3981</idno>
                  <idno type="ORCID">https://orcid.org/0000-0002-8877-4654</idno>
                  <idno type="GOOGLE SCHOLAR">https://scholar.google.fr/citations?user=_UEWjDoAAAAJ&amp;hl=fr&amp;oi=ao</idno>
                  <idno type="IDREF">https://www.idref.fr/072248807</idno>
                  <affiliation ref="#struct-178918" />
                </author>
              </analytic>
              <monogr>
                <idno type="isbn">978-3-031-21203-1</idno>
                <title level="m">Lecture Notes in Computer Science.  Lecture Notes in Artificial Intelligence</title>
                <meeting>
                  <title>PRIMA 2022 -  Principles and Practice of Multi-Agent Systems - 24th International Conference</title>
                  <date type="start">2022-11-16</date>
                  <date type="end">2022-11-18</date>
                  <settlement>Valencia</settlement>
                  <country key="ES">Spain</country>
                </meeting>
                <imprint>
                  <publisher>Springer International Publishing</publisher>
                  <biblScope unit="serie">PRIMA 2022: Principles and Practice of Multi-Agent Systems</biblScope>
                  <biblScope unit="volume">LNCS. LNAI-13753</biblScope>
                  <biblScope unit="pp">364-380</biblScope>
                  <date type="datePub">2023-11-12</date>
                </imprint>
              </monogr>
              <idno type="doi">10.1007/978-3-031-21203-1_22</idno>
              <ref type="publisher">https://link.springer.com/book/10.1007/978-3-031-21203-1</ref>
            </biblStruct>
          </sourceDesc>
          <profileDesc>
            <langUsage>
              <language ident="en">English</language>
            </langUsage>
            <textClass>
              <classCode scheme="halDomain" n="info.info-ai">Computer Science [cs]/Artificial Intelligence [cs.AI]</classCode>
              <classCode scheme="halTypology" n="COMM">Conference papers</classCode>
              <classCode scheme="halOldTypology" n="COMM">Conference papers</classCode>
              <classCode scheme="halTreeTypology" n="COMM">Conference papers</classCode>
            </textClass>
            <abstract xml:lang="en">
              <p>An important task in logic, given a formula and a knowledge base which represents what an agent knows of the current state of the world, is to be able to guess the truth value of the formula. Logic reasoners are designed to perform inferences, that is, to decide whether a formula is a logical consequence of the knowledge base, which is stronger than that and can be intractable in some cases. In addition, under the open-world assumption, it may turn out impossible to infer a formula or its negation. In many practical situations, however, when an agent has to make a decision, it is acceptable to resort to heuristic methods to determine the probable veracity or falsehood of a formula, even in the absence of a guarantee of correctness, to avoid blocking the decisionmaking process and move forward. This is why we propose a method to train a classification model based on available knowledge in order to be able of accurately guessing whether an arbitrary, unseen formula is true or false. Our method exploits a kernel representation of logical formulas based on a model-theoretic measure of semantic similarity. The results of experiments show that the proposed method is highly effective and accurate.</p>
            </abstract>
          </profileDesc>
        </biblFull>
      </listBibl>
    </body>
    <back>
      <listOrg type="structures">
        <org type="researchteam" xml:id="struct-178918" status="VALID">
          <idno type="RNSR">201221031M</idno>
          <orgName>Web-Instrumented Man-Machine Interactions, Communities and Semantics</orgName>
          <orgName type="acronym">WIMMICS</orgName>
          <desc>
            <address>
              <country key="FR" />
            </address>
            <ref type="url">http://wimmics.inria.fr/</ref>
          </desc>
          <listRelation>
            <relation active="#struct-34586" type="direct" />
            <relation active="#struct-300009" type="indirect" />
            <relation active="#struct-452156" type="direct" />
            <relation active="#struct-13009" type="indirect" />
            <relation active="#struct-117617" type="indirect" />
            <relation name="UMR7271" active="#struct-441569" type="indirect" />
            <relation active="#struct-1039632" type="indirect" />
          </listRelation>
        </org>
        <org type="department" xml:id="struct-452156" status="VALID">
          <orgName>Scalable and Pervasive softwARe and Knowledge Systems</orgName>
          <orgName type="acronym">Laboratoire I3S - SPARKS</orgName>
          <date type="start">2016-03-03</date>
          <desc>
            <address>
              <addrLine>Laboratoire I3SCS 4012106903 Sophia Antipolis Cedex</addrLine>
              <country key="FR" />
            </address>
            <ref type="url">http://www.i3s.unice.fr/sparks</ref>
          </desc>
          <listRelation>
            <relation active="#struct-13009" type="direct" />
            <relation active="#struct-117617" type="indirect" />
            <relation name="UMR7271" active="#struct-441569" type="indirect" />
            <relation active="#struct-1039632" type="indirect" />
          </listRelation>
        </org>
        <org type="laboratory" xml:id="struct-34586" status="VALID">
          <idno type="RNSR">198318250R</idno>
          <idno type="ROR">https://ror.org/01nzkaw91</idno>
          <orgName>Inria Sophia Antipolis - Méditerranée</orgName>
          <orgName type="acronym">CRISAM</orgName>
          <desc>
            <address>
              <addrLine>2004 route des Lucioles BP 93 06902 Sophia Antipolis</addrLine>
              <country key="FR" />
            </address>
            <ref type="url">http://www.inria.fr/centre/sophia/</ref>
          </desc>
          <listRelation>
            <relation active="#struct-300009" type="direct" />
          </listRelation>
        </org>
        <org type="institution" xml:id="struct-300009" status="VALID">
          <idno type="ROR">https://ror.org/02kvxyf05</idno>
          <orgName>Institut National de Recherche en Informatique et en Automatique</orgName>
          <orgName type="acronym">Inria</orgName>
          <desc>
            <address>
              <addrLine>Domaine de VoluceauRocquencourt - BP 10578153 Le Chesnay Cedex</addrLine>
              <country key="FR" />
            </address>
            <ref type="url">http://www.inria.fr/en/</ref>
          </desc>
        </org>
        <org type="laboratory" xml:id="struct-13009" status="VALID">
          <orgName>Laboratoire d'Informatique, Signaux, et Systèmes de Sophia Antipolis</orgName>
          <orgName type="acronym">I3S</orgName>
          <desc>
            <address>
              <addrLine>2000, route des Lucioles - Les Algorithmes - bât. Euclide B 06900 Sophia Antipolis</addrLine>
              <country key="FR" />
            </address>
            <ref type="url">http://www.i3s.unice.fr/</ref>
          </desc>
          <listRelation>
            <relation active="#struct-117617" type="direct" />
            <relation name="UMR7271" active="#struct-441569" type="direct" />
            <relation active="#struct-1039632" type="direct" />
          </listRelation>
        </org>
        <org type="institution" xml:id="struct-117617" status="VALID">
          <idno type="IdRef">026403498</idno>
          <idno type="ISNI">0000000123372892</idno>
          <idno type="ROR">https://ror.org/02k9vew78</idno>
          <orgName>Université Nice Sophia Antipolis (1965 - 2019)</orgName>
          <orgName type="acronym">UNS</orgName>
          <date type="start">1965-10-23</date>
          <date type="end">2019-12-31</date>
          <desc>
            <address>
              <addrLine>Parc Valrose, 06100 Nice</addrLine>
              <country key="FR" />
            </address>
            <ref type="url">http://unice.fr/</ref>
          </desc>
        </org>
        <org type="regroupinstitution" xml:id="struct-441569" status="VALID">
          <idno type="IdRef">02636817X</idno>
          <idno type="ISNI">0000000122597504</idno>
          <idno type="ROR">https://ror.org/02feahw73</idno>
          <orgName>Centre National de la Recherche Scientifique</orgName>
          <orgName type="acronym">CNRS</orgName>
          <date type="start">1939-10-19</date>
          <desc>
            <address>
              <country key="FR" />
            </address>
            <ref type="url">https://www.cnrs.fr/</ref>
          </desc>
        </org>
        <org type="regroupinstitution" xml:id="struct-1039632" status="VALID">
          <idno type="IdRef">241035694</idno>
          <idno type="ROR">https://ror.org/019tgvf94</idno>
          <orgName>Université Côte d'Azur</orgName>
          <orgName type="acronym">UniCA</orgName>
          <date type="start">2020-01-01</date>
          <desc>
            <address>
              <addrLine>Parc Valrose, 28, avenue Valrose 06108 Nice Cedex 2</addrLine>
              <country key="FR" />
            </address>
            <ref type="url">https://univ-cotedazur.fr</ref>
          </desc>
        </org>
      </listOrg>
      <listOrg type="projects">
        <org type="anrProject" xml:id="projanr-50589" status="VALID">
          <idno type="anr">ANR-19-P3IA-0002</idno>
          <orgName>3IA@cote d'azur</orgName>
          <desc>3IA Côte d'Azur</desc>
          <date type="start">2019</date>
        </org>
        <org type="anrProject" xml:id="projanr-56235" status="VALID">
          <idno type="anr">ANR-21-CE23-0004</idno>
          <orgName>CROQUIS</orgName>
          <desc>Collecte, représentation, complétion, fusion et interrogation de données de réseaux d'eau urbains hétérogènes et incertaines</desc>
          <date type="start">2021</date>
        </org>
      </listOrg>
    </back>
  <teiCorpus>
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning to Classify Logical Formulas based on their Semantic Similarity</title>
				<funder ref="#_nk5GYh5">
					<orgName type="full">French government</orgName>
				</funder>
				<funder>
					<orgName type="full">French National Research Agency (ANR)</orgName>
				</funder>
				<funder ref="#_nUh3hTn #_Czq5ThJ">
					<orgName type="full">Agence Nationale de la Recherche</orgName>
					<orgName type="abbreviated">ANR</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher />
				<availability status="unknown"><licence /></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Ali</forename><surname>Ballout</surname></persName>
							<email>ali.ballout@inria.fr</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Université Côte d'Azur</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<orgName type="institution" key="instit3">Inria</orgName>
								<address>
									<region>I3S</region>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Célia</forename><surname>Da Costa Pereira</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Université Côte d'Azur</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<address>
									<region>I3S</region>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Andrea</forename><forename type="middle">G B</forename><surname>Tettamanzi</surname></persName>
							<email>andrea.tettamanzi@univ-cotedazur.fr</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Université Côte d'Azur</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<orgName type="institution" key="instit3">Inria</orgName>
								<address>
									<region>I3S</region>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning to Classify Logical Formulas based on their Semantic Similarity</title>
					</analytic>
					<monogr>
						<imprint>
							<date />
						</imprint>
					</monogr>
					<idno type="MD5">E70036FA0F5A1E8D48E923B17916F14E</idno>
					<idno type="DOI">10.1007/978-3-031-21203-</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-04-12T14:52+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid" />
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div><p>An important task in logic, given a formula and a knowledge base which represents what an agent knows of the current state of the world, is to be able to guess the truth value of the formula. Logic reasoners are designed to perform inferences, that is, to decide whether a formula is a logical consequence of the knowledge base, which is stronger than that and can be intractable in some cases. In addition, under the open-world assumption, it may turn out impossible to infer a formula or its negation. In many practical situations, however, when an agent has to make a decision, it is acceptable to resort to heuristic methods to determine the probable veracity or falsehood of a formula, even in the absence of a guarantee of correctness, to avoid blocking the decisionmaking process and move forward. This is why we propose a method to train a classification model based on available knowledge in order to be able of accurately guessing whether an arbitrary, unseen formula is true or false. Our method exploits a kernel representation of logical formulas based on a model-theoretic measure of semantic similarity. The results of experiments show that the proposed method is highly effective and accurate.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div><head n="1">Introduction and Related Work</head><p>A defining feature for intelligent agents is their ability to reason, that is to draw logical conclusions from the available premises, which constitute their knowledge <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5]</ref>. While this capability is very important, an equally important and useful, but weaker, capability for an agent would be to be able to recognize if a given formula is likely to be true or false, given the current knowledge, in the current state of the world, even though not necessarily in general.</p><p>As a matter of fact, we often experience situations where our incomplete knowledge would not allow us to make exact inferences, yet this does not prevent us to make decisions, because even when we don't know a fact that we need in order to move forward, we are able to make an educated guess (i.e., a prediction based on what we already know) about the veracity of that fact and proceed with our decision making.</p><p>It is this weaker task that we are interested in studying here. We propose a simple but effective idea, which is to train a classifier against the knowledge base of the agent, which may be viewed as a set of formulas labeled with their truth value. The formulas are represented as vectors of similarities to the labeled formulas; to this end, we propose a model-theoretic semantic similarity measure which can be computed efficiently. This kernel-representation and its associated similarity measure are the key ingredients of our proposal.</p><p>Recently, a rise of interest in developing connectionist methods for reasoning can be observed, with proposals such the so-called Logic Tensor Networks <ref type="bibr" target="#b3">[4]</ref>, or the Logic-Integrated Neural Network <ref type="bibr" target="#b13">[14]</ref> to integrate the power of deep learning and logic reasoning, or approaches that employ state-of-the-art methods for training deep neural networks to learn to perform some basic ontology reasoning tasks <ref type="bibr" target="#b10">[11]</ref>. As a further witness of the attention this research field is attracting, some conferences are beginning to feature tutorials on it, like KDD'21 <ref type="bibr" target="#b14">[15]</ref> and there is even an upcoming Dagstuhl seminar on "Machine Learning and Logical Reasoning: The New Frontier". <ref type="foot" target="#foot_0">3</ref>Unlike these approaches, what we propose does not require sophisticated neural architectures or resource-intensive deep learning; in addition, we do not attack the more ambitious challenge of logical deduction, but just that of heuristically guessing (as human beings do), the truth value of a formula, independently of its being logically entailed by the available knowledge.</p><p>Actually, what people do in case of incomplete knowledge is to somehow measure the similarity between known/familiar situations and unknown/unfamiliar situations <ref type="bibr" target="#b15">[16]</ref>. Several cognitive tasks, such as learning and interpolation require the concept of similarity to be performed <ref type="bibr" target="#b8">[9]</ref>. There exists a vast literature on similarity measures, with many proposal arising in the field of machine learning <ref type="bibr" target="#b6">[7]</ref>. However, it appears that the problem of measuring the similarity of logical formulas has been less investigated and, when it has, that's often in relation with specific contexts.</p><p>While not directly addressing the problem of defining similarity among logical formulas, Bowles <ref type="bibr" target="#b5">[6]</ref> studies the nature of relevance and irrelevance of a proposition with respect to another. His work shares with the definition of semantic similarity that we propose here, a basic intuition, which is that the probability that one proposition is true given that another one is true should play a central role. The definition of relevance proposed by Makinson in <ref type="bibr" target="#b12">[13]</ref>, instead, is not in line with what we are proposing here because it is defined in terms of lettersharing. A way of measuring the similarity of a Boolean vector to a given set of Boolean vectors, motivated in part by certain data mining or machine learning problems, was proposed by Anthony and Hammer <ref type="bibr" target="#b1">[2]</ref>. A similarity measure for Boolean function was proposed by Fišer et al. <ref type="bibr" target="#b9">[10]</ref> in a quite different context, that of circuit synthesis, which explains the differences with our proposal. One measure of similarity between functions is the existence of a Lipschitz mapping (with small constant) between them <ref type="bibr" target="#b11">[12]</ref>. A problem somehow related to the one we are dealing with is the problem of measuring the similarity between logical arguments, which has been studied by Amgoud and David <ref type="bibr" target="#b0">[1]</ref>.</p><p>Through an empirical validation we show that the framework we propose allows a number of quite standard and unsophisticated classification techniques, like support-vector machines, to learn very accurate models that are capable of "guessing" whether a given, unseen formula is true or false in the current state of affairs, without the need to perform any logical deduction.</p><p>The rest of the paper is structured as follows: Section 2 states the problem of formula classification; Section 3 defines a semantic similarity measure for formulas that is the cornerstone of the proposed approach. Section 4 provides an empirical validation of the approach and Section 5 draws some conclusions and suggestions for future work.</p></div>
<div><head n="2">Problem Statement</head><p>Let Φ be a set of formulas in a logical language L and let I be an interpretation, which represents a particular state of affairs or the current state of the world. Under interpretation I, the formulas in Φ may be labeled as being true or false. One could thus construct a table</p><formula xml:id="formula_0">   ϕ 1 , ϕ I 1 ϕ 2 , ϕ I 2 . . . . . .    ,</formula><p>where ϕ i ∈ Φ ⊂ L, for i = 1, 2, . . . , and ϕ I i is the truth value of ϕ i according to interpretation I. This table can be viewed as a representation of a knowledge base K consisting of all the formulas ϕ i ∈ Φ such that ϕ I i = T and all the formulas ¬ϕ i ∈ Φ such that ϕ I i = F . K represents what an agent knows (or believes) about the current state of affairs but, of course, I, the actual state of affairs, is not known in full, which is like saying that the open world hypothesis holds.</p><p>Consider now the problem of guessing or predicting whether a new formula ψ / ∈ Φ is true or false in I, given K. To be sure, one could use a reasoner to check whether K ⊢ ψ or K ⊢ ¬ψ. If the reasoner is sound and complete, this can even allow one to decide whether K |= ψ or K |= ¬ψ. However, even in cases where K ̸ |= ψ and K ̸ |= ¬ψ, which are entirely possible in an open world, it would be useful for an agent to be able to make educated guesses at the truth value of ψ. By an educated guess we mean a prediction, based on the truth values of the formulas the agent already knows. If a model to make that type of predictions existed and were fast and accurate enough, the agent might even used it instead of the reasoner, for time-critical tasks where having a quick answer is more important than having an answer that is guaranteed to be always correct.</p><p>What we have just described is a classification problem, where given a set of labeled examples (here, formulas with their truth value), a model is sought for that is able to accurately predict the label of an unseen case (i.e., a new formula).</p><p>To solve this problem, we propose to use a kernel representation, i.e., to represent formulas as vectors of similarities to a restricted set of formulas whose label is already known (Φ) and to train a classification model on these labeled examples, later to be used to classify new, unseen formulas. To this aim, we will stick to very standard and unsophisticated classification methods.</p></div>
<div><head n="3">Semantic Similarity</head><p>We need to define similarity among logical formulas. It is quite obvious that such a similarity should not be based on syntax, due to the fact that formulas with widely different syntactical forms may be equivalent. Now, the semantics of logical formulas is defined in model-theoretic terms. What we are looking for is, therefore, a model-theoretic notion of formula similarity.</p><p>To keep technical complications at a minimum and without loss of generality, let us consider propositional logic. As a matter of fact, more expressive logical languages can be mapped to the propositional case (e.g., description logics and first-order logic under the Herbrand semantics).</p><p>Definition 1 (Language) Let A be a finite set of atomic propositions and let L be the propositional language such that A ∪ {⊤, ⊥} ⊆ L, and, ∀ϕ,</p><formula xml:id="formula_1">ψ ∈ L, ¬ϕ ∈ L, ϕ ∧ ψ ∈ L, ϕ ∨ ψ ∈ L.</formula><p>Additional connectives can be defined as useful shorthands for combination of connectives of L, e.g., ϕ ⊃ ψ ≡ ¬ϕ ∨ ψ.</p><p>We will denote by Ω = {0, 1} A the set of all interpretations on A, which we may also call the "universe". An interpretation I ∈ Ω is a function We might begin by defining the semantic distance between two formulas ϕ and ψ as the Hamming distance between the two binary string that represent their respective sets of models:</p><formula xml:id="formula_2">I : A → {0,</formula><formula xml:id="formula_3">d(ϕ, ψ) = I∈Ω [ϕ I ̸ = ψ I ],<label>(1)</label></formula><p>where [expr] denotes the indicator function, which equals 1 if expr is true and 0 otherwise.</p><p>According to this definition, d(ϕ, ¬ϕ) = ∥Ω∥ and d(ϕ, ϕ) = 0, which is in good agreement with our intuition. Also, two formulas that are totally unrelated, 4 like, say, p and q, where p, q ∈ A, will have a distance which is half-way in between these two extreme cases, d(p, q) = 1 2 ∥Ω∥. One problem with this notion of distance is that the distance between two given formulas depends on the number of propositional constants in the language, which is a little counter-intuitive. For instance, d(p, q) = 2 if A = {p, q}, but d(p, q) = 4 if A = {p, q, r}, and so on. In addition, to compute it, we have to consider all interpretations in Ω, even though many of them might be indifferent when it comes to two given formulas: for example, pqr and pqr are indifferent when comparing p to q.</p><p>The former problem disappears if, instead of a distance, we define a similarity, ranging between 0 and 1 based on the same idea, as follows:</p><formula xml:id="formula_4">sim(ϕ, ψ) = 1 ∥Ω∥ I∈Ω [ϕ I = ψ I ].</formula><p>(</p><formula xml:id="formula_5">)<label>2</label></formula><p>The latter problem is also solved by defining A ϕ ⊆ A as the set of atoms that occur in formula ϕ and by letting Ω ϕ,ψ = 2 A ϕ ∪A ψ ; Equation 2 can now be rewritten as</p><formula xml:id="formula_6">sim(ϕ, ψ) = 1 ∥Ω∥ I∈Ω [ϕ I = ψ I ] = 1 ∥Ω ϕ,ψ ∥ I∈Ω ϕ,ψ [ϕ I = ψ I ].<label>(3)</label></formula><p>According to this definition, for all formula ϕ ∈ L, sim(ϕ, ϕ) = 1, sim(ϕ, ¬ϕ) = 0 and, no matter how many atoms are involved, if</p><formula xml:id="formula_7">A ϕ ∩ A ψ = ∅, sim(ϕ, ψ) = 1 2 .</formula><p>Another interesting property of this semantic similarity is the following, which ensures that the proposed similarity is consistent with logical negation.</p><p>Theorem 1. Let ϕ ψ be any two formulas of L. Then sim(ϕ, ψ) = 1 -sim(¬ϕ, ψ).</p><p>Proof. For all interpretation I, ϕ I = ψ I ⇔ ¬ϕ I ̸ = ψ I and ϕ I ̸ = ψ I ⇔ ¬ϕ I = ψ I . Therefore, {I :</p><formula xml:id="formula_8">ϕ I = ψ I } = {I : ¬ϕ I ̸ = ψ I } = Ω \ {I : ¬ϕ I = ψ I } and we can thus write sim(ϕ, ψ) = 1 ∥Ω∥ I∈Ω [ϕ I = ψ I ] = 1 ∥Ω∥ ∥{I : ϕ I = ψ I }∥ = 1 ∥Ω∥ ∥Ω \ {I : ¬ϕ I = ψ I }∥ = ∥Ω∥ ∥Ω∥ - 1 ∥Ω∥ ∥{I : ¬ϕ I = ψ I }∥ = 1 - 1 ∥Ω∥ I∈Ω [¬ϕ I = ψ I ] = 1 -sim(¬ϕ, ψ).</formula></div>
<div><head>⊓ ⊔</head><p>Another interesting property of the semantic similarity, as we have defined it, is that, if Ω ϕ,ψ is too large, we are not obliged to perform an exact computation of sim(ϕ, ψ), but we can approximate it with acceptable accuracy by randomly sampling n interpretations from Ω ϕ,ψ and counting for how many of them ϕ I = ψ I . Indeed, sim(ϕ, ψ) may be construed as a probability, namely the probability that, in a random interpretation, ϕ and ψ are both true or both false. What we get is an unbiased estimator of sim(ϕ, ψ), which behaves like a binomial parameter ŝϕ,ψ , whose confidence interval is given by the Wald confidence interval, based on the asymptotic normality of ŝϕ,ψ and estimating the standard error. This (1 -α) confidence interval for sim(ϕ, ψ) would be ŝϕ,ψ</p><formula xml:id="formula_9">± z α/2 ŝϕ,ψ (1 -ŝϕ,ψ )/n,<label>(4)</label></formula><p>where z c denotes the 1 -c quantile of the standard normal distribution.</p><p>For example, if we set n = 30, with a 99% confidence, the actual similarity will be within a deviation of 2.576 1/120 = 0.2351 from ŝϕ,ψ , in the worst case, which corresponds to ŝϕ,ψ = 0.5; for n = 100, the approximation error will be less than 0.1288 and for n = 1000 it will be less than 0.0407. As a matter of fact, a precise computation of the similarity between formulas is not really required for the proposed approach to work.</p><p>This also suggests a way to deal with non-finite interpretations, which might arise in expressive languages involving variables and functions.</p></div>
<div><head n="4">Experiments and Results</head></div>
<div><head n="4.1">An Example from the Block World</head><p>As a first test and example of our proposal, we define a language with four individual constants, A, B, C, Table, one unary predicate, covered(•), and one binary predicate on(•, •). The Herbrand base of this language is finite and consists of twenty ground atoms, but we can only consider a subset of it, after dropping atoms like on(A, A), covered(Table ), and on(Table, A), which would always be false in every state of the block world: </p><formula xml:id="formula_10">A 12 = { covered(A), on(A, B), on(A, C), on(A,</formula><formula xml:id="formula_11">A 20 = { covered(A), on(A, B), on(A, C), on(A, D), on(A, Table), covered(B), on(B, A), on(B, C), on(B, D), on(B, Table), covered(C), on(C, A), on(C, B), on(C, D), on(C, Table), covered(D), on(D, A), on(D, B), on(D, C), on(D, Table) },</formula><p>of size 20, with ∥Ω 20 ∥ = 2 20 = 1, 048, 576, and, similarly, by adding a further block E, a set A 30 of 30 atoms, with ∥Ω 30 ∥ = 2 30 = 1, 073, 741, 824.</p><p>The language may then be completed by a minimal set of logical operators, ¬, ∧, ∨. Then we select a reference interpretation, for example</p><formula xml:id="formula_12">I * 12 = {on(A, Table), on(C, Table), on(B, A), covered(A)},</formula><p>corresponding to a given state of a very simple block world containing one table and three blocks, arranged as in Figure <ref type="figure" target="#fig_1">1a</ref>. We then generate a set Φ of random logical formulas and we assign them a truth label based on I * and train various models on it.</p></div>
<div><head n="4.2">Experimental Protocol</head><p>The experiment was divided into two parts. In the first part, we created 3 datasets, each of which is based on a different universe generated using the language explained in Section 4.1. The universes are depicted in Figure <ref type="figure" target="#fig_1">1</ref>. We used these sets to test the performance of models learned using our proposed similarity measure. We considered different universe complexities and used very small training sets to simulate a realistic scenario. For this part, no sampling was done, all interpretations in Ω ϕ,ψ were considered as per Equation 3. This can be very time-consuming when the two formulas involve many atoms. For the second part, we created 3 additional sets for each of the universes used in the first part. These additional sets contain the exact same formulas as those in the first part, the only difference being the way the similarity was calculated. To investigate what we mentioned in Section 3 regarding the ability to approximate the similarity with acceptable accuracy by randomly sampling n interpretations, we approximated the similarities for each of the 3 additional sets using n = 30, n = 100, and n = 1000 respectively. We then compared the performance of each of these sets with the base one created in the first part. 5   Part One: Baseline Experiment. To see how the proposed method performs, we created the 3 universes depicted in Figure <ref type="figure" target="#fig_1">1</ref>, and denoted by Ω 12 , Ω 20 , and Ω 30 . The universes consist of 12, 20, and 30 atoms respectively (sets A 12 , A 20 , and A 30 as defined above). The following are the reference interpretations: 5 All the code and data used for the experiments described in this paper can be found in the following repository: https://github.com/ali-ballout/ Learning-to-Classify-Logical-Formulas-based-on-their-Semantic-Similarity.</p></div>
<div><head>7</head><p>- <ref type="figure" target="#fig_1">1c</ref>;</p><formula xml:id="formula_13">I * 12 = {on(A, Table), on(C, Table), on(B, A), covered(A)}, cf. Figure 1a; -I * 20 = {on(A, Table), on(C, Table), on(B, A), on(D, C), covered(A), covered(C)}, cf. Figure 1b; -I * 30 = {on(A, Table), on(C, Table), on(E, Table), on(B, A), on(D, C), covered(A), covered(C)}, cf. Figure</formula><p>Following that, we generated 500 random formulas for each of the universes using Algorithm 1. Algorithm 1 generates a given number N of random formulas, taking as input a list of ground atoms A. It is recursive and chooses its next step and which symbols to add at random. It uses a variable that reduces the probability of adding a nested subformula the more complex a formula becomes.</p><p>We then labeled each of the formulas with its truth, based on the reference interpretation of its universe. The next step was to create the similarity matrix for each set of formulas. For this part, no sampling was done, in other words all interpretations were taken into account and no noise was added. The similarity between each formula and all other formulas in the set is calculated using Equation 3. To simplify, we compare formula ϕ to all other formulas in the set of 500 formulas. At each comparison we check all the unique atoms included in the compared formulas ϕ and ψ, for an example of what atoms are refer to A 12 in Section 4.1. We then generate all interpretations for this set of unique atoms extracted from both formulas. Then we record the truth for each of the formulas based on each of the generated interpretations. After that, we count the instances where the truth of formulas ϕ and ψ are the same. We divide that number by the total number of generated interpretations and the result obtained is the similarity between ϕ and ψ. We do this, once, for all pairs of formulas to obtain a symmetric similarity matrix of the shape 500×500. Figure <ref type="figure">2</ref>  Fig. <ref type="figure">2</ref>: Formula similarity matrix with truth labels.</p><p>similarity matrix between formulas of a set of formulas of size m with S being the similarity between each pair. The diagonal is all 1 since it is the similarity between a formula ϕ and itself. The truth labels of all the formulas are attached as column Truth to the similarity matrix to obtain the final product of the process, which is the input to be used for training and testing a machine learning model. Now that we have created our labeled datasets, we need to choose a machine learning method that is suitable for the task. Through a process of model selec-tion we decided to use a support vector classifier, it performed the best with the small training sets that we provided. After performing a grid search we determined the best hyper parameters, we set the regularization parameter C to 0.1 and the kernel type to polynomial, of degree 3. We ran, for each of the 3 datasets, a 20-fold cross validation processes to establish the baseline performance of our proposed method. All results presented in Table <ref type="table" target="#tab_2">1</ref> are averages of the scores obtained from all the runs for each dataset. Similarly, confusion matrices displayed in Figure <ref type="figure" target="#fig_4">3</ref> are the result of summing up all confusion matrices of the 20 runs and then normalizing them. We set the number of formulas included in the training sets of each universe to less than or equal to ∥A∥ of said universe, the training set sizes were as follows: 10 for Ω 12 , 20 for Ω 20 , and 30 for Ω 30 . Table <ref type="table">2</ref> presents the labeled formulas used in the training set of one of the runs for Ω 12 .</p><p>No agent, human or artificial, has in its knowledge the exhaustive list of all possible formulas. The rationale for using small training sets in our experiments is that the knowledge base of an agent is unlikely to contain many formulas; some of them might be handcrafted and be part of the "background knowledge" of the agent, and the others acquired through sensors or messages received from other agents. In any case, it is a principle of economy that the knowledge of an agent be encoded using as few and as simple formulas as required. We want to test our proposal against a realistic scenario in which the available knowledge (the formulas whose truth value is known) is very small compared to the number of semantically distinct formulas that can be stated in the language. This also has the advantage of demonstrating the generalization capability of our models.</p><p>Notice that, for a language whose set of interpretations is Ω, there are 2 ∥Ω∥ semantically distinct formulas, which is a really huge number, although most of them would be very complicated formulas that one would not expect to find in a real knowledge base. However, even factoring out very complicated formulas, the number of possible formulas would be exceedingly large.</p><p>To be sure, there exists a choice of formulas that would make the problem we are studying absolutely trivial. That is when the training set contains at least ∥A∥ formulas, each consisting of a single literal (i.e., a positive or negated atom), such that the atoms of these formulas are all distinct. It is easy to see that those formulas, with their truth labeling, would directly give the reference interpretation, from which the truth value of any other formula can be mechanically computed in linear time with respect to the length of the formula, without performing any logical deduction or reasoning. This is the reason why the formulas of each dataset are extracted from a distribution that is skewed in favor of simpler (i.e., realistic), but not too simple formulas. Indeed, we ensure that the training set does not contain literals for all the atoms, by counting the number of literals that are randomly generated and rejecting additional literals once the maximum number of literals has been reached. That maximum is set to ∥A∥/2, well below ∥A∥.</p><p>Since the dataset consists of randomly generated formulas, it is unbalanced,<ref type="foot" target="#foot_2">6</ref> so in addition to the accuracy score, which we report because it gives an intuitive idea of the probability that the prediction is correct, we will provide the Matthews correlation coefficient (MCC) which is a statistical rate between -1 and 1 that produces a high score only if the prediction obtained good results in all of the four confusion matrix categories (true positives, false negatives, true negatives, and false positives), proportionally both to the size of positive elements and the size of negative elements in the dataset. So its a very good metric when we don't have a perfectly balanced dataset <ref type="bibr" target="#b7">[8]</ref>. Results of this part of the experiment are presented in Table <ref type="table" target="#tab_2">1</ref>.</p><p>the instances where formulas ϕ and ψ have the same truth out of these sampled interpretations. After that, we divide the obtained number by n, the size of the sample, since we are now dealing with n interpretations and not all of them. The result from that division is the approximation ŝϕ,ψ of the similarity sim(ϕ, ψ) between ϕ and ψ. We do this for the sets of formulas we randomly generated in Section 4.2 for each of our universes 3 times, once for each number n of samples we mentioned.</p><p>With these 9 new matrices we are able to study how the sample size n might affect the performance of the method when dealing with different universe complexities. It will also show us how well an approximation of the similarity performs. We used the same model to test the performance and the same scoring metrics as the baseline. We used the same training set sizes for each universe as in the baseline. The results of this part of the experiment are available in Table <ref type="table" target="#tab_2">1</ref>. </p></div>
<div><head>Universe</head></div>
<div><head n="4.3">Results and Analysis</head><p>Baseline Results We start our analysis with the first part of our experiment, detailed in Section 4.2. The results can be found in Table <ref type="table" target="#tab_2">1</ref> and the corresponding confusion matrices to offer support in Figure <ref type="figure" target="#fig_4">3</ref>. A small sample of formulas from the test set for the smallest universe, with the labels predicted by the model, is provided in Table <ref type="table" target="#tab_3">3</ref>. From this experiment we can determine:</p><p>1. The overall performance of our proposed method without sampling while dealing with universes of different complexities, using very small training sets.</p><p>2. The effect the training set size has on performance with respect to the complexity of the universe addressed.</p><p>Regarding the first, Table <ref type="table" target="#tab_2">1</ref> shows that the overall performance of our proposed method is good. The highest accuracy achieved was 83% for a training set size of 30 formulas and a universe of complexity 30, MCC being 0.66 which is a very good result when training using an unbalanced set. As a worst case, the method achieved 77% accuracy with a minimal training set size of 10 formulas and universe complexity of 12, MCC of 0.56 is acceptable considering the small training set relative to the complexity of the universe. Indeed, 30 formulas for a language with 30 propositional symbols is a really sparse training set, when one thinks that this language has ∼ 10 9 interpretations and there exist ∼ 10 300,000,000 semantically distinct formulas one can construct! After demonstrating that our proposed method is capable of achieving good results with very small training sets, we move on to the second point. We can see that the proposed method can achieve an average accuracy of 80% throughout the runs that use a very small training set of 10 formulas for Ω 12 , 20 formulas for Ω 20 , and 30 formulas for Ω 30 . It would be natural to think that as the universe complexity increases, a model would require a larger training set to maintain performance, which is what the results shown in Table <ref type="table" target="#tab_2">1</ref> and Figure <ref type="figure" target="#fig_4">3</ref> confirm. From the results see in Table <ref type="table" target="#tab_2">1</ref> where no sampling was considered, we can see that increasing the number of formulas included in the training set had a very significant effect on performance. This effect was not limited to maintaining performance, but it resulted in an improvement of up to 8% in accuracy and 0.14 in terms of MCC.</p><p>To put things into perspective, for Ω 12 we used for training 10 formulas out of a possible ∼ 10 1233 compared to 30 out of a possible ∼ 10 300,000,000 for  Ω 30 . In other words, the transition from Ω 12 with 4096 interpretations to Ω 30 with ∼ 10 9 , resulted in a gain of 8% accuracy by just adding 20 formulas to the training set. The increase in the size of the training set is modest relative to the size of Ω 30 or the number of semantically distinct formulas that can be constructed, while the gain of accuracy and balance in predictions in terms of MCC is significant. We observed that the truth value of "simple" formulas turns out to be harder to predict for the trained models than that of "complicated" formulas (see, e.g., Table <ref type="table" target="#tab_3">3</ref>). While this must have to do with the geometry of the space of the kernel representation of formulas induced by the semantic similarity, this phenomenon will have to be the object of further investigation.</p></div>
<div><head>Sampling Results</head><p>We now shift our attention to part two of the experiment detailed in Section 4.2. The results of this experiment are also shown in Table <ref type="table" target="#tab_2">1</ref> and the corresponding confusion matrices to offer support are found in Figure <ref type="figure" target="#fig_4">3</ref>.</p><p>At first glance at Table <ref type="table" target="#tab_2">1</ref>, we can tell that the overall performance of the model does not degrade much when the similarity is approximated using the lowest number of samples n = 30. Indeed, we have a loss of accuracy of almost 4% for 2 of our universes. But this is an acceptable result when considering that in this case we would no longer have to calculate the exact similarity especially when we are limited by computational power. In fact, in this case, we would be looking at 30 random interpretations instead of ∼ 10 9 for a universe the size of Ω 30 .</p><p>The degradation in accuracy and MCC decreases as we increase the number of samples from 30 to 100 and then to 1000, it even approaches baseline performance. This proves what we mentioned in Section 3, we are able to approximate the similarity with very high accuracy even with a low number of sampled interpretations when compared to the number of all interpretations.</p><p>On the other hand, another increase in the number of samples from 100 to 1000 has no significant effect on performance, which is interesting considering that this introduces noise (since we allow for repetitions) yet it does not degrade performance. It also means that for a universe of complexity ∥A∥ there exists an optimal number of samples n that achieves baseline-similar performance. </p></div>
<div><head n="5">Conclusion</head><p>We have proposed a framework that allows an agent to train, based on a set of formulas whose truth values are known, a classification model that predicts the truth-value of a new, arbitrary formula. This framework uses a semantic similarity between formulas, which is a key ingredient of our proposal, to perform a kernel encoding of the formulas, which is then exploited by the classification model. We have tested an implementation of this framework using SVM, showing that the classification model is highly accurate (with accuracy around 80%) even when the similarity is approximated by severely undersampling the interpretations. The practical implications of these results are that the proposed approach is tractable even for languages with a large (or infinite but enumerable) number of atoms; indeed, computing a good approximation of the similarity of two formulas can be done in linear time, as it depends only on the size of the (random) interpretations sampled.</p><p>There is no guarantee that all the predictions made by a model be altogether consistent. There is no built-in mechanism to ensure that and the mutual consistency of all the prediction is not part of the measure of the quality of a classifier: every prediction is made by the model and assessed independently of the others. Of course, if the predictions were all correct, they would also be consistent and that's what we observe empirically, that the predictions tend to be mostly consistent.</p><p>Since the knowledge of an agent may not be complete, some formulas, which are not entailed by it and whose negation is not entailed either, both predictions would be acceptable, and one might be tempted to count them as correct. However, this is not what we did: for the purpose of testing our method, what we did was to arbitrarily fix one interpretation and say it corresponded to the actual state of affairs; use it to label the training set and evaluate the predictions of the classifiers against the label that would be thus assigned, even for those formulas whose truth value is not constrained by the available knowledge. In a sense, we were as strict as one can be when judging a classifier.</p><p>Future work might involve testing other more sophisticated classification methods and applying the proposed framework to real-world scenarios.</p></div><figure xml:id="fig_0"><head /><label /><figDesc>1} assigning a truth value p I to every atomic proposition p ∈ A and, by extension, a truth value ϕ I to all formulas ϕ ∈ L; I |= ϕ means that ϕ I = 1 (I is a model of ϕ); if S ⊆ L is a set of formulas, I |= S means I |= ϕ for all ϕ ∈ S; S |= ϕ means that ∀I |= S, I |= ϕ. The notation [ϕ] denotes the set of all models of formula ϕ ∈ L: [ϕ] = {I ∈ Ω : I |= ϕ}. The semantics of a formula ϕ ∈ L is the set of its models, [ϕ].</figDesc></figure>
<figure xml:id="fig_1"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: The three block worlds corresponding to the reference interpretations, respectively, (a) to I * 12 , (b) to I * 20 , and (c) to I * 30 .</figDesc></figure>
<figure xml:id="fig_2"><head /><label /><figDesc>C, B) ∧ on(B, C)) ∨ (¬¬¬¬on(A, B) ∨ on(B, A)) True ¬(¬(on(A, B) ∧ ((on(C, T bl) ∨ ¬¬on(C, B)) ∧ covered(A))) ∨ (on(C, A) ∨ on(B, C))) False ¬(((covered(B) ∨ ¬on(B, A)) ∨ (on(B, T bl) ∧ (on(B, A) ∧ on(C, A)))) ∧ covered(B)) True ¬¬¬on(A, C) True (on(B, C) ∨ covered(A)) ∨ covered(A) True ¬(¬on(A, C) ∧ ¬covered(C)) False (on(A, C) ∨ on(C, A)) ∧ ¬(covered(C) ∧ on(A, T bl)) False on(C, T bl) ∧ on(C, B) False on(C, B) ∨ on(B, C) False ¬on(C, B) ∧ (¬¬¬on(C, B) ∧ covered(B)) FalseTable 2: A sample training set made of 10 formulas from universe Ω 12 .</figDesc></figure>
<figure xml:id="fig_3"><head /><label /><figDesc>(¬(on(B, A) ∧ covered(B)) ∨ ((covered(C) ∨ on(A, )B) ∧ on(C, T bl))) ∨ ¬((on(B, A) ∧ on(A, T bl)) ∧ ¬covered(A))) False False (on(A, T bl) ∧ on(B, A)) ∨ ¬¬((on(A, C) ∧ ((on(A, T bl) ∨ covered(A)) ∧ covered(A))) ∨ on(C, B)) True True covered(B) ∧ on(A, T bl) False True ((covered(B) ∨ (¬on(A, B) ∧ on(C, T bl))) ∧ (((on(B, A) ∧ (on(A, C)∧on(C, T bl)))∨(on(C, A)∨on(A, T bl)))∧on(A, T bl)))∨ on(C, A) True True ((covered(A) ∧ covered(B)) ∨ (covered(A) ∨ ¬(((covered(A) ∨ on(B, C))∨(on(B, C)∧on(B, T bl)))∨on(C, A))))∨((on(B, A)∨ on(A, C)) ∧ (((on(C, T bl) ∧ on(B, C)) ∧ on(A, C)) ∧ covered(A))) True True covered(B) ∧ (((on(C, B) ∨ ((on(C, B) ∧ on(B, T bl)) ∧ ((¬covered(B) ∨ (on(A, B) ∧ ¬on(A, C))) ∨ (¬¬on(B, C) ∧ (covered(C) ∨ on(A, B)))))) ∨ covered(C)) ∧ (¬(on(A, C) ∨ on(B, C))∧((covered(A)∨on(C, B))∨(on(A, T bl)∧(on(A, T bl)∨ on(C, B))))))False False</figDesc></figure>
<figure xml:id="fig_4"><head>Fig. 3 :</head><label>3</label><figDesc>Fig. 3: Confusion matrices of the similarity approximation using sampling experiment for each universe. Each row represents 4 cases for each universe, starting with the baseline (no sampling) and then n = 30, n = 100, and n = 1000 respectively. Each sub-figure is captioned by the universe notation Ω12,20,30 and in superscript the sample size n used to approximate similarity.</figDesc><graphic coords="16,142.66,437.13,63.19,59.55" type="bitmap" /></figure>
<figure type="table" xml:id="tab_0"><head>Table )</head><label>)</label><figDesc /><table /><note><p><p><p><p><p>, covered(B), on(B, A), on(B, C), on(B, Table</p>)</p>,</p>covered(C), on(C, A), on(C, B), on(C, Table) }.</p>Notice that, given this A 12 , ∥Ω 12 ∥ = 2 12 = 4, 096. By adding another block D to this world we can obtain a larger set of atoms</p></note></figure>
<figure type="table" xml:id="tab_1"><head /><label /><figDesc>depicts the</figDesc><table><row><cell cols="2">T ruth F ormula</cell><cell>ϕ0</cell><cell cols="2">ϕ1 . . .</cell><cell>ϕm</cell></row><row><cell>T ruth0</cell><cell>ϕ0</cell><cell>1</cell><cell cols="3">S0,1 . . . S0,m</cell></row><row><cell>T ruth1</cell><cell>ϕ1</cell><cell>S1,0</cell><cell>1</cell><cell cols="2">. . . S1,m</cell></row><row><cell>. . .</cell><cell>. . .</cell><cell>. . .</cell><cell>. . .</cell><cell>. . .</cell><cell>. . .</cell></row><row><cell cols="2">T ruthm-1 ϕm-1</cell><cell cols="4">Sm-1,0 Sm-1,1 . . . Sm-1,m</cell></row><row><cell>T ruthm</cell><cell>ϕm</cell><cell cols="3">Sm,0 Sm,1 . . .</cell><cell>1</cell></row></table></figure>
<figure type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Accuracy and MCC for experiments done on each universe.</figDesc><table><row><cell /><cell>Training set Size</cell><cell>sample size</cell><cell>Accuracy score</cell><cell>MCC</cell></row><row><cell /><cell /><cell>no sampling</cell><cell>0.77</cell><cell>0.56</cell></row><row><cell>Ω12</cell><cell>10</cell><cell>30 100</cell><cell>0.76 0.77</cell><cell>0.54 0.56</cell></row><row><cell /><cell /><cell>1000</cell><cell>0.77</cell><cell>0.55</cell></row><row><cell /><cell /><cell>no sampling</cell><cell>0.81</cell><cell>0.63</cell></row><row><cell>Ω20</cell><cell>20</cell><cell>30 100</cell><cell>0.81 0.82</cell><cell>0.62 0.63</cell></row><row><cell /><cell /><cell>1000</cell><cell>0.82</cell><cell>0.65</cell></row><row><cell /><cell /><cell>no sampling</cell><cell>0.83</cell><cell>0.66</cell></row><row><cell>Ω30</cell><cell>30</cell><cell>30 100</cell><cell>0.79 0.82</cell><cell>0.56 0.62</cell></row><row><cell /><cell /><cell>1000</cell><cell>0.83</cell><cell>0.64</cell></row></table></figure>
<figure type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>A small sample of the test set with formulas varying in complexity from universe Ω 12 .</figDesc><table /></figure>
			<note place="foot" n="3" xml:id="foot_0"><p>Dagstuhl Seminar 22291, July 17-22, 2022.</p></note>
			<note place="foot" n="4" xml:id="foot_1"><p>Two formulas may be said to be totally unrelated if knowing the truth value of one does not give any information about the truth value of the other.</p></note>
			<note place="foot" n="6" xml:id="foot_2"><p>The dataset for universe Ω12 has 272 false formulas and 227 true ones (1 missing because it was a duplicate), for universe Ω20 278 false and 222 true, and for universe Ω30 300 false and 200 true; of course, these figures vary (between training and test set) for every fold obtained from these datasets.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This work has been partially supported by the <rs type="funder">French government</rs>, through the <rs type="programName">3IA Côte d'Azur "Investments in the Future</rs>" project managed by the <rs type="funder">National Research Agency (ANR)</rs> with the reference number <rs type="grantNumber">ANR-19-P3IA-0002</rs>, as well as through the <rs type="projectName">ANR CROQUIS (Collecte, représentation, complétion, fusion et interrogation de données de réseaux d'eau urbains hétérogènes et incertaines)</rs> project, grant <rs type="grantNumber">ANR-21-CE23-0004</rs> of the <rs type="funder">French National Research Agency (ANR)</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_nk5GYh5">
					<orgName type="program" subtype="full">3IA Côte d'Azur "Investments in the Future</orgName>
				</org>
				<org type="funded-project" xml:id="_nUh3hTn">
					<idno type="grant-number">ANR-19-P3IA-0002</idno>
					<orgName type="project" subtype="full">ANR CROQUIS (Collecte, représentation, complétion, fusion et interrogation de données de réseaux d'eau urbains hétérogènes et incertaines)</orgName>
				</org>
				<org type="funding" xml:id="_Czq5ThJ">
					<idno type="grant-number">ANR-21-CE23-0004</idno>
				</org>
			</listOrg>
			<div type="annex">
<div><head>Algorithm 1 Generating random formulas</head><p>Require: A set of atoms A Ensure: f ormulas, a list of generated formulas N ← N umber of random f ormulas to generate f ← one randomly generated f ormula literals ← 0 ▷ A counter used to limit the number of literals as sentences  Part Two: Sampling Experiment. In this part of our experiment we investigate a property of our proposed similarity, which is the ability to approximate sim(ϕ, ψ) with acceptable accuracy by randomly sampling n interpretations from Ω ϕ,ψ . We will name this approximation ŝϕ,ψ .</p><p>To this end, we created 3 new matrices for each set of formulas used in Section 4.2. We ended up with 9 new datasets, 3 for each of the universes depicted in Figure <ref type="figure">1</ref> and describe in Section 4.2. The similarity in the 9 new matrices was calculated differently than in Section 4.2. For this part, we approximated the similarity between formulas by randomly sampling a set number n of all interpretations, instead of taking all of them into account as we did in Section 4.2. The number of random samples n considered for creating the matrices is n = 30, n = 100, and n = 1000. This allows us to simulate the scenario of having a machine with low computational capacity trying to process a set of interpretations that is too large, and utilizing sampling as a solution. It also allows us to see how the method performs when noise is introduced.</p><p>The way the similarity is approximated using sampling is not much different from how it is calculated: we still count the instances where formulas ϕ and ψ have the same truth, but for n randomly sampled interpretations instead of all interpretations. Algorithm 2 is used to approximate the similarity between two formulas. In simple terms, when Algorithm 2 compares two formulas ϕ and ψ, instead of generating all interpretations corresponding to the set of unique atoms A composing those formulas, it generates a number n of these interpretations randomly. This sampling is done with replacement, which means that it is possible that a given interpretation gets sampled multiple times, especially in case n is larger than the number of all interpretations. We then proceed to count</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Measuring similarity between logical arguments</title>
		<author>
			<persName><forename type="first">L</forename><surname>Amgoud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>David</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Principles of Knowledge Representation and Reasoning: Proceedings of the Sixteenth International Conference</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Thielscher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Toni</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Wolter</surname></persName>
		</editor>
		<meeting><address><addrLine>KR; Tempe, Arizona</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2018-10-30">2018. 30 October -2 November 2018. 2018</date>
			<biblScope unit="page" from="98" to="107" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A boolean measure of similarity</title>
		<author>
			<persName><forename type="first">M</forename><surname>Anthony</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">L</forename><surname>Hammer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Discrete Applied Mathematics</title>
		<imprint>
			<biblScope unit="volume">154</biblScope>
			<biblScope unit="issue">16</biblScope>
			<biblScope unit="page" from="2242" to="2246" />
			<date type="published" when="2006-11">November 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">What is default reasoning good for? applications revisited</title>
		<author>
			<persName><forename type="first">G</forename><surname>Antoniou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ghose</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">32nd Annual Hawaii International Conference on System Sciences (HICSS-32)</title>
		<meeting><address><addrLine>Maui, Hawaii, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="1999">January 5-8, 1999. 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Logic tensor networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Badreddine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Garcez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Serafini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Spranger</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.artint.2021.103649</idno>
		<ptr target="https://doi.org/10.1016/j.artint.2021.103649" />
	</analytic>
	<monogr>
		<title level="j">Artif. Intell</title>
		<imprint>
			<biblScope unit="volume">303</biblScope>
			<biblScope unit="page">103649</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Reasoning with levels of modalities in BDI logic</title>
		<author>
			<persName><forename type="first">J</forename><surname>Blee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Billington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sattar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Agent Computing and Multi-Agent Systems, 10th Pacific Rim International Conference on Multi-Agents, PRIMA 2007</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Ghose</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Governatori</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Sadananda</surname></persName>
		</editor>
		<meeting><address><addrLine>Bangkok, Thailand</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007">November 21-23, 2007. 2007</date>
			<biblScope unit="volume">5044</biblScope>
			<biblScope unit="page" from="410" to="415" />
		</imprint>
	</monogr>
	<note>Revised Papers</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Propositional relevance</title>
		<author>
			<persName><forename type="first">G</forename><surname>Bowles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Informal Logic</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="65" to="77" />
			<date type="published" when="1990">Spring 1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">A survey of similarity measures for time stamped temporal datasets</title>
		<author>
			<persName><forename type="first">A</forename><surname>Cheruvu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Radhakrishna</surname></persName>
		</author>
		<editor>DATA</editor>
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>ACM</publisher>
			<biblScope unit="page" from="193" to="197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The advantages of the matthews correlation coefficient (mcc) over f1 score and accuracy in binary classification evaluation</title>
		<author>
			<persName><forename type="first">D</forename><surname>Chicco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Jurman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Genomics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">On Ruspini's models of similarity-based approximate reasoning</title>
		<author>
			<persName><forename type="first">F</forename><surname>Esteva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Godo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">O</forename><surname>Rodríguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Vetterlein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IPMU (1)</title>
		<title level="s">Communications in Computer and Information Science</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">1237</biblScope>
			<biblScope unit="page" from="3" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Output grouping method based on a similarity of boolean functions</title>
		<author>
			<persName><forename type="first">P</forename><surname>Fišer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kubalík</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kubátová</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of 7th Int. Workshop on Boolean Problems (IWSBP)</title>
		<meeting>of 7th Int. Workshop on Boolean Problems (IWSBP)<address><addrLine>Freiberg (Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">September 21-22. 2006</date>
			<biblScope unit="page" from="107" to="113" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Ontology reasoning with deep neural networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Hohenecker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lukasiewicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Artif. Intell. Res</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="page" from="503" to="540" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Lipschitz bijections between boolean functions. Combinatorics</title>
		<author>
			<persName><forename type="first">T</forename><surname>Johnston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Scott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Probability and Computing</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="513" to="525" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Propositional relevance through letter-sharing</title>
		<author>
			<persName><forename type="first">D</forename><surname>Makinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Appl. Log</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="377" to="387" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Neural logic reasoning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1145/3340531.3411949</idno>
		<ptr target="https://doi.org/10.1145/3340531.3411949" />
	</analytic>
	<monogr>
		<title level="m">CIKM '20: The 29th ACM International Conference on Information and Knowledge Management</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Aquin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Dietze</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Hauff</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Curry</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Cudré-Mauroux</surname></persName>
		</editor>
		<meeting><address><addrLine>Virtual Event, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2020">October 19-23, 2020. 2020</date>
			<biblScope unit="page" from="1365" to="1374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">From deep learning to deep reasoning</title>
		<author>
			<persName><forename type="first">T</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD '21: Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="4076" to="4077" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Recognizing and predicting agent behavior with case based reasoning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wendler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">RoboCup. Lecture Notes in Computer Science</title>
		<imprint>
			<biblScope unit="volume">3020</biblScope>
			<biblScope unit="page" from="729" to="738" />
			<date type="published" when="2003">2003</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</teiCorpus></text>
</TEI>