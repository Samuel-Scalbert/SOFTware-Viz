<?xml version='1.0' encoding='utf-8'?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" version="1.1" xsi:schemaLocation="http://www.tei-c.org/ns/1.0 http://api.archives-ouvertes.fr/documents/aofr-sword.xsd">
  <teiHeader>
    <fileDesc>
      <titleStmt>
        <title>HAL TEI export of hal-03424974v3</title>
      </titleStmt>
      <publicationStmt>
        <distributor>CCSD</distributor>
        <availability status="restricted">
          <licence target="http://creativecommons.org/licenses/by/4.0/">Distributed under a Creative Commons Attribution 4.0 International License</licence>
        </availability>
        <date when="2024-04-27T03:35:15+02:00" />
      </publicationStmt>
      <sourceDesc>
        <p part="N">HAL API platform</p>
      </sourceDesc>
    </fileDesc>
  </teiHeader>
  <text>
    <body>
      <listBibl>
        <biblFull>
          <titleStmt>
            <title xml:lang="en">Differentially Private Coordinate Descent for Composite Empirical Risk Minimization</title>
            <author role="aut">
              <persName>
                <forename type="first">Paul</forename>
                <surname>Mangold</surname>
              </persName>
              <idno type="halauthorid">1133062-0</idno>
              <affiliation ref="#struct-432650" />
              <affiliation ref="#struct-410272" />
            </author>
            <author role="aut">
              <persName>
                <forename type="first">Aurélien</forename>
                <surname>Bellet</surname>
              </persName>
              <email type="md5">7c92d2fc696e1875415477238a601d34</email>
              <email type="domain">inria.fr</email>
              <ptr type="url" target="http://researchers.lille.inria.fr/abellet/" />
              <idno type="idhal" notation="string">aurelien-bellet</idno>
              <idno type="idhal" notation="numeric">9877</idno>
              <idno type="halauthorid" notation="string">30290-9877</idno>
              <idno type="ORCID">https://orcid.org/0000-0003-3440-1251</idno>
              <idno type="GOOGLE SCHOLAR">https://scholar.google.fr/citations?user=j8svx3IAAAAJ</idno>
              <idno type="IDREF">https://www.idref.fr/17653136X</idno>
              <idno type="ARXIV">https://arxiv.org/a/bellet_a_1</idno>
              <affiliation ref="#struct-432650" />
              <affiliation ref="#struct-410272" />
            </author>
            <author role="aut">
              <persName>
                <forename type="first">Joseph</forename>
                <surname>Salmon</surname>
              </persName>
              <email type="md5">a462940c86f48ec7bcf7afd9b0cdf60d</email>
              <email type="domain">umontpellier.fr</email>
              <ptr type="url" target="http://josephsalmon.eu/" />
              <idno type="idhal" notation="string">joseph-salmon</idno>
              <idno type="idhal" notation="numeric">170495</idno>
              <idno type="halauthorid" notation="string">41881-170495</idno>
              <idno type="ORCID">https://orcid.org/0000-0002-3181-0634</idno>
              <idno type="IDREF">https://www.idref.fr/14987233X</idno>
              <affiliation ref="#struct-1100744" />
              <affiliation ref="#struct-56663" />
              <affiliation ref="#struct-1100621" />
            </author>
            <author role="aut">
              <persName>
                <forename type="first">Marc</forename>
                <surname>Tommasi</surname>
              </persName>
              <email type="md5">1200567ad0ac568c3e246f8dac3f3e48</email>
              <email type="domain">univ-lille3.fr</email>
              <idno type="idhal" notation="string">marc-tommasi</idno>
              <idno type="idhal" notation="numeric">399</idno>
              <idno type="halauthorid" notation="string">4412-399</idno>
              <idno type="IDREF">https://www.idref.fr/121846385</idno>
              <idno type="ORCID">https://orcid.org/0000-0003-2838-4408</idno>
              <orgName ref="#struct-0" />
              <affiliation ref="#struct-432650" />
              <affiliation ref="#struct-410272" />
            </author>
            <editor role="depositor">
              <persName>
                <forename>Paul</forename>
                <surname>Mangold</surname>
              </persName>
              <email type="md5">cf16c683480e52bb615677be6ae68a40</email>
              <email type="domain">polytechnique.edu</email>
            </editor>
            <funder ref="#projanr-52219" />
            <funder ref="#projanr-51651" />
            <funder>This work was supported in part by the Inria Exploratory Action FLAMED</funder>
          </titleStmt>
          <editionStmt>
            <edition n="v1">
              <date type="whenSubmitted">2021-11-10 16:39:34</date>
            </edition>
            <edition n="v2">
              <date type="whenSubmitted">2022-02-02 17:55:11</date>
            </edition>
            <edition n="v3" type="current">
              <date type="whenSubmitted">2022-10-21 17:17:44</date>
              <date type="whenModified">2024-04-27 03:10:56</date>
              <date type="whenReleased">2022-10-24 08:49:16</date>
              <date type="whenProduced">2022-07-17</date>
              <date type="whenEndEmbargoed">2022-10-21</date>
              <ref type="file" target="https://inria.hal.science/hal-03424974v3/document">
                <date notBefore="2022-10-21" />
              </ref>
              <ref type="file" subtype="author" n="1" target="https://inria.hal.science/hal-03424974v3/file/paper.pdf">
                <date notBefore="2022-10-21" />
              </ref>
              <ref type="externalLink" target="http://arxiv.org/pdf/2110.11688" />
            </edition>
            <respStmt>
              <resp>contributor</resp>
              <name key="961645">
                <persName>
                  <forename>Paul</forename>
                  <surname>Mangold</surname>
                </persName>
                <email type="md5">cf16c683480e52bb615677be6ae68a40</email>
                <email type="domain">polytechnique.edu</email>
              </name>
            </respStmt>
          </editionStmt>
          <publicationStmt>
            <distributor>CCSD</distributor>
            <idno type="halId">hal-03424974</idno>
            <idno type="halUri">https://inria.hal.science/hal-03424974</idno>
            <idno type="halBibtex">mangold:hal-03424974</idno>
            <idno type="halRefHtml">&lt;i&gt;ICML 2022 - 39th International Conference on Machine Learning&lt;/i&gt;, Jul 2022, Baltimore, United States. pp.14948-14978</idno>
            <idno type="halRef">ICML 2022 - 39th International Conference on Machine Learning, Jul 2022, Baltimore, United States. pp.14948-14978</idno>
          </publicationStmt>
          <seriesStmt>
            <idno type="stamp" n="CNRS">CNRS - Centre national de la recherche scientifique</idno>
            <idno type="stamp" n="INRIA">INRIA - Institut National de Recherche en Informatique et en Automatique</idno>
            <idno type="stamp" n="INRIA-SOPHIA">INRIA Sophia Antipolis - Méditerranée</idno>
            <idno type="stamp" n="INRIA-LILLE">INRIA Lille - Nord Europe</idno>
            <idno type="stamp" n="I3M_UMR5149">Institut de Mathématiques et de Modélisation de Montpellier</idno>
            <idno type="stamp" n="INSMI">CNRS-INSMI - INstitut des Sciences Mathématiques et de leurs Interactions</idno>
            <idno type="stamp" n="INRIASO">INRIA-SOPHIA</idno>
            <idno type="stamp" n="INRIA_TEST">INRIA - Institut National de Recherche en Informatique et en Automatique</idno>
            <idno type="stamp" n="TESTALAIN1">TESTALAIN1</idno>
            <idno type="stamp" n="ZENITH" corresp="LIRMM">Scientific Data Management</idno>
            <idno type="stamp" n="LIRMM">Laboratoire d'Informatique de Robotique et de Microélectronique de Montpellier</idno>
            <idno type="stamp" n="CRISTAL">Centre de Recherche en Informatique, Signal et Automatique de Lille (CRISTAL)</idno>
            <idno type="stamp" n="IMAG-MONTPELLIER">Institut Montpelliérain Alexander Grothendieck</idno>
            <idno type="stamp" n="INRIA2">INRIA 2</idno>
            <idno type="stamp" n="CRISTAL-MAGNET" corresp="CRISTAL">CRISTAL-MAGNET</idno>
            <idno type="stamp" n="UNIV-MONTPELLIER">Université de Montpellier</idno>
            <idno type="stamp" n="UNIV-LILLE">Université de Lille</idno>
            <idno type="stamp" n="ANR">ANR</idno>
            <idno type="stamp" n="UM-2015-2021" corresp="UNIV-MONTPELLIER">Université de Montpellier (2015-2021)</idno>
            <idno type="stamp" n="UM-EPE" corresp="UNIV-MONTPELLIER">Université de Montpellier - EPE</idno>
          </seriesStmt>
          <notesStmt>
            <note type="audience" n="2">International</note>
            <note type="invited" n="0">No</note>
            <note type="popular" n="0">No</note>
            <note type="peer" n="1">Yes</note>
            <note type="proceedings" n="1">Yes</note>
          </notesStmt>
          <sourceDesc>
            <biblStruct>
              <analytic>
                <title xml:lang="en">Differentially Private Coordinate Descent for Composite Empirical Risk Minimization</title>
                <author role="aut">
                  <persName>
                    <forename type="first">Paul</forename>
                    <surname>Mangold</surname>
                  </persName>
                  <idno type="halauthorid">1133062-0</idno>
                  <affiliation ref="#struct-432650" />
                  <affiliation ref="#struct-410272" />
                </author>
                <author role="aut">
                  <persName>
                    <forename type="first">Aurélien</forename>
                    <surname>Bellet</surname>
                  </persName>
                  <email type="md5">7c92d2fc696e1875415477238a601d34</email>
                  <email type="domain">inria.fr</email>
                  <ptr type="url" target="http://researchers.lille.inria.fr/abellet/" />
                  <idno type="idhal" notation="string">aurelien-bellet</idno>
                  <idno type="idhal" notation="numeric">9877</idno>
                  <idno type="halauthorid" notation="string">30290-9877</idno>
                  <idno type="ORCID">https://orcid.org/0000-0003-3440-1251</idno>
                  <idno type="GOOGLE SCHOLAR">https://scholar.google.fr/citations?user=j8svx3IAAAAJ</idno>
                  <idno type="IDREF">https://www.idref.fr/17653136X</idno>
                  <idno type="ARXIV">https://arxiv.org/a/bellet_a_1</idno>
                  <affiliation ref="#struct-432650" />
                  <affiliation ref="#struct-410272" />
                </author>
                <author role="aut">
                  <persName>
                    <forename type="first">Joseph</forename>
                    <surname>Salmon</surname>
                  </persName>
                  <email type="md5">a462940c86f48ec7bcf7afd9b0cdf60d</email>
                  <email type="domain">umontpellier.fr</email>
                  <ptr type="url" target="http://josephsalmon.eu/" />
                  <idno type="idhal" notation="string">joseph-salmon</idno>
                  <idno type="idhal" notation="numeric">170495</idno>
                  <idno type="halauthorid" notation="string">41881-170495</idno>
                  <idno type="ORCID">https://orcid.org/0000-0002-3181-0634</idno>
                  <idno type="IDREF">https://www.idref.fr/14987233X</idno>
                  <affiliation ref="#struct-1100744" />
                  <affiliation ref="#struct-56663" />
                  <affiliation ref="#struct-1100621" />
                </author>
                <author role="aut">
                  <persName>
                    <forename type="first">Marc</forename>
                    <surname>Tommasi</surname>
                  </persName>
                  <email type="md5">1200567ad0ac568c3e246f8dac3f3e48</email>
                  <email type="domain">univ-lille3.fr</email>
                  <idno type="idhal" notation="string">marc-tommasi</idno>
                  <idno type="idhal" notation="numeric">399</idno>
                  <idno type="halauthorid" notation="string">4412-399</idno>
                  <idno type="IDREF">https://www.idref.fr/121846385</idno>
                  <idno type="ORCID">https://orcid.org/0000-0003-2838-4408</idno>
                  <orgName ref="#struct-0" />
                  <affiliation ref="#struct-432650" />
                  <affiliation ref="#struct-410272" />
                </author>
              </analytic>
              <monogr>
                <meeting>
                  <title>ICML 2022 - 39th International Conference on Machine Learning</title>
                  <date type="start">2022-07-17</date>
                  <date type="end">2022-07-23</date>
                  <settlement>Baltimore</settlement>
                  <country key="US">United States</country>
                </meeting>
                <imprint>
                  <publisher>PMLR</publisher>
                  <biblScope unit="volume">162</biblScope>
                  <biblScope unit="pp">14948-14978</biblScope>
                  <date type="datePub">2022</date>
                </imprint>
              </monogr>
              <idno type="arxiv">2110.11688</idno>
              <ref target="https://proceedings.mlr.press/v162/" type="seeAlso" />
              <ref target="https://proceedings.mlr.press/v162/mangold22a.html" type="seeAlso" />
              <ref type="publisher">https://icml.cc/Conferences/2022</ref>
            </biblStruct>
          </sourceDesc>
          <profileDesc>
            <langUsage>
              <language ident="en">English</language>
            </langUsage>
            <textClass>
              <classCode scheme="halDomain" n="info.info-lg">Computer Science [cs]/Machine Learning [cs.LG]</classCode>
              <classCode scheme="halDomain" n="stat.ml">Statistics [stat]/Machine Learning [stat.ML]</classCode>
              <classCode scheme="halTypology" n="COMM">Conference papers</classCode>
              <classCode scheme="halOldTypology" n="COMM">Conference papers</classCode>
              <classCode scheme="halTreeTypology" n="COMM">Conference papers</classCode>
            </textClass>
            <abstract xml:lang="en">
              <p>Machine learning models can leak information about the data used to train them. To mitigate this issue, Differentially Private (DP) variants of optimization algorithms like Stochastic Gradient Descent (DP-SGD) have been designed to trade-off utility for privacy in Empirical Risk Minimization (ERM) problems. In this paper, we propose Differentially Private proximal Coordinate Descent (DP-CD), a new method to solve composite DP-ERM problems. We derive utility guarantees through a novel theoretical analysis of inexact coordinate descent. Our results show that, thanks to larger step sizes, DP-CD can exploit imbalance in gradient coordinates to outperform DP-SGD. We also prove new lower bounds for composite DP-ERM under coordinate-wise regularity assumptions, that are nearly matched by DP-CD. For practical implementations, we propose to clip gradients using coordinate-wise thresholds that emerge from our theory, avoiding costly hyperparameter tuning. Experiments on real and synthetic data support our results, and show that DP-CD compares favorably with DP-SGD.</p>
            </abstract>
          </profileDesc>
        </biblFull>
      </listBibl>
    </body>
    <back>
      <listOrg type="structures">
        <org type="researchteam" xml:id="struct-432650" status="VALID">
          <idno type="RNSR">201321079K</idno>
          <orgName>Machine Learning in Information Networks</orgName>
          <orgName type="acronym">MAGNET</orgName>
          <date type="start">2015-01-01</date>
          <desc>
            <address>
              <country key="FR" />
            </address>
            <ref type="url">http://www.inria.fr/equipes/magnet</ref>
          </desc>
          <listRelation>
            <relation active="#struct-104752" type="direct" />
            <relation active="#struct-300009" type="indirect" />
            <relation active="#struct-410272" type="direct" />
            <relation name="UMR9189" active="#struct-120930" type="indirect" />
            <relation name="UMR9189" active="#struct-374570" type="indirect" />
            <relation name="UMR9189" active="#struct-441569" type="indirect" />
          </listRelation>
        </org>
        <org type="laboratory" xml:id="struct-410272" status="VALID">
          <idno type="IdRef">18388695X</idno>
          <idno type="RNSR">201521249L</idno>
          <idno type="ROR">https://ror.org/05vrs3189</idno>
          <orgName>Centre de Recherche en Informatique, Signal et Automatique de Lille - UMR 9189</orgName>
          <orgName type="acronym">CRIStAL</orgName>
          <date type="start">2015-01-01</date>
          <desc>
            <address>
              <addrLine>Université de Lille - Campus scientifique - Bâtiment ESPRIT - Avenue Henri Poincaré - 59655 Villeneuve d’Ascq</addrLine>
              <country key="FR" />
            </address>
            <ref type="url">https://www.cristal.univ-lille.fr/</ref>
          </desc>
          <listRelation>
            <relation name="UMR9189" active="#struct-120930" type="direct" />
            <relation name="UMR9189" active="#struct-374570" type="direct" />
            <relation name="UMR9189" active="#struct-441569" type="direct" />
          </listRelation>
        </org>
        <org type="laboratory" xml:id="struct-1100744" status="VALID">
          <idno type="ISNI">0000000404890602</idno>
          <idno type="RNSR">200311827X</idno>
          <idno type="ROR">https://ror.org/044kxby82</idno>
          <orgName>Institut Montpelliérain Alexander Grothendieck</orgName>
          <orgName type="acronym">IMAG</orgName>
          <date type="start">2022-01-01</date>
          <desc>
            <address>
              <addrLine>UMR CNRS 5149 - Université Montpellier 2, Case courrier 051, 34095 Montpellier cedex 5 - France</addrLine>
              <country key="FR" />
            </address>
            <ref type="url">https://imag.umontpellier.fr/</ref>
          </desc>
          <listRelation>
            <relation name="UMR5149" active="#struct-441569" type="direct" />
            <relation active="#struct-1100589" type="direct" />
          </listRelation>
        </org>
        <org type="laboratory" xml:id="struct-56663" status="VALID">
          <idno type="IdRef">03442945X</idno>
          <idno type="ISNI">0000000119314817</idno>
          <idno type="ROR">https://ror.org/055khg266</idno>
          <idno type="Wikidata">Q1665127</idno>
          <orgName>Institut universitaire de France</orgName>
          <orgName type="acronym">IUF</orgName>
          <desc>
            <address>
              <addrLine>Maison des Universités 103 Boulevard Saint-Michel 75005 Paris</addrLine>
              <country key="FR" />
            </address>
            <ref type="url">http://iuf.amue.fr/</ref>
          </desc>
          <listRelation>
            <relation active="#struct-301855" type="direct" />
          </listRelation>
        </org>
        <org type="researchteam" xml:id="struct-1100621" status="VALID">
          <idno type="RNSR">201121208J</idno>
          <orgName>Scientific Data Management</orgName>
          <orgName type="acronym">ZENITH</orgName>
          <date type="start">2022-01-01</date>
          <desc>
            <address>
              <addrLine>LIRMM, 161 rue Ada, 34000 Montpellier</addrLine>
              <country key="FR" />
            </address>
            <ref type="url">https://team.inria.fr/zenith/</ref>
          </desc>
          <listRelation>
            <relation active="#struct-34586" type="direct" />
            <relation active="#struct-300009" type="indirect" />
            <relation active="#struct-1100620" type="direct" />
            <relation name="UMR5506" active="#struct-441569" type="indirect" />
            <relation name="UMR5506" active="#struct-1100589" type="indirect" />
          </listRelation>
        </org>
        <org type="laboratory" xml:id="struct-104752" status="VALID">
          <idno type="RNSR">200818245B</idno>
          <idno type="ROR">https://ror.org/04eej9726</idno>
          <orgName>Inria Lille - Nord Europe</orgName>
          <desc>
            <address>
              <addrLine>Parc Scientifique de la Haute Borne 40, avenue Halley Bât.A, Park Plaza 59650 Villeneuve d'Ascq</addrLine>
              <country key="FR" />
            </address>
            <ref type="url">http://www.inria.fr/lille/</ref>
          </desc>
          <listRelation>
            <relation active="#struct-300009" type="direct" />
          </listRelation>
        </org>
        <org type="institution" xml:id="struct-300009" status="VALID">
          <idno type="ROR">https://ror.org/02kvxyf05</idno>
          <orgName>Institut National de Recherche en Informatique et en Automatique</orgName>
          <orgName type="acronym">Inria</orgName>
          <desc>
            <address>
              <addrLine>Domaine de VoluceauRocquencourt - BP 10578153 Le Chesnay Cedex</addrLine>
              <country key="FR" />
            </address>
            <ref type="url">http://www.inria.fr/en/</ref>
          </desc>
        </org>
        <org type="institution" xml:id="struct-120930" status="VALID">
          <idno type="IdRef">256304629</idno>
          <idno type="ISNI">0000000122034461</idno>
          <idno type="ROR">https://ror.org/01x441g73</idno>
          <orgName>Centrale Lille</orgName>
          <desc>
            <address>
              <addrLine>École Centrale de Lille - Cité Scientifique - CS 20048 59651 Villeneuve d'Ascq Cedex</addrLine>
              <country key="FR" />
            </address>
            <ref type="url">https://centralelille.fr/</ref>
          </desc>
        </org>
        <org type="regroupinstitution" xml:id="struct-374570" status="VALID">
          <idno type="IdRef">223446556</idno>
          <idno type="ISNI">0000 0001 2242 6780</idno>
          <idno type="ROR">https://ror.org/02kzqn938</idno>
          <idno type="Wikidata">Q3551621</idno>
          <orgName>Université de Lille</orgName>
          <desc>
            <address>
              <addrLine>EPE Université de Lille. -- 42 rue Paul Duez, 59000 Lille</addrLine>
              <country key="FR" />
            </address>
            <ref type="url">https://www.univ-lille.fr/</ref>
          </desc>
        </org>
        <org type="regroupinstitution" xml:id="struct-441569" status="VALID">
          <idno type="IdRef">02636817X</idno>
          <idno type="ISNI">0000000122597504</idno>
          <idno type="ROR">https://ror.org/02feahw73</idno>
          <orgName>Centre National de la Recherche Scientifique</orgName>
          <orgName type="acronym">CNRS</orgName>
          <date type="start">1939-10-19</date>
          <desc>
            <address>
              <country key="FR" />
            </address>
            <ref type="url">https://www.cnrs.fr/</ref>
          </desc>
        </org>
        <org type="regroupinstitution" xml:id="struct-1100589" status="VALID">
          <idno type="ROR">https://ror.org/051escj72</idno>
          <orgName>Université de Montpellier</orgName>
          <orgName type="acronym">UM</orgName>
          <date type="start">2022-01-01</date>
          <desc>
            <address>
              <addrLine>163 rue Auguste Broussonnet - 34090 Montpellier</addrLine>
              <country key="FR" />
            </address>
            <ref type="url">http://www.umontpellier.fr/</ref>
          </desc>
        </org>
        <org type="institution" xml:id="struct-301855" status="VALID">
          <orgName>Ministère de l'Education nationale, de l’Enseignement supérieur et de la Recherche</orgName>
          <orgName type="acronym">M.E.N.E.S.R.</orgName>
          <desc>
            <address>
              <addrLine>1 rue Descartes - 75231 Paris cedex 05</addrLine>
              <country key="FR" />
            </address>
          </desc>
        </org>
        <org type="laboratory" xml:id="struct-34586" status="VALID">
          <idno type="RNSR">198318250R</idno>
          <idno type="ROR">https://ror.org/01nzkaw91</idno>
          <orgName>Inria Sophia Antipolis - Méditerranée</orgName>
          <orgName type="acronym">CRISAM</orgName>
          <desc>
            <address>
              <addrLine>2004 route des Lucioles BP 93 06902 Sophia Antipolis</addrLine>
              <country key="FR" />
            </address>
            <ref type="url">http://www.inria.fr/centre/sophia/</ref>
          </desc>
          <listRelation>
            <relation active="#struct-300009" type="direct" />
          </listRelation>
        </org>
        <org type="laboratory" xml:id="struct-1100620" status="VALID">
          <idno type="IdRef">139590827</idno>
          <idno type="ISNI">0000000405990488</idno>
          <idno type="RNSR">199111950H</idno>
          <idno type="ROR">https://ror.org/013yean28</idno>
          <orgName>Laboratoire d'Informatique de Robotique et de Microélectronique de Montpellier</orgName>
          <orgName type="acronym">LIRMM</orgName>
          <date type="start">2022-01-01</date>
          <desc>
            <address>
              <addrLine>161 rue Ada - 34095 Montpellier</addrLine>
              <country key="FR" />
            </address>
            <ref type="url">https://www.lirmm.fr</ref>
          </desc>
          <listRelation>
            <relation name="UMR5506" active="#struct-441569" type="direct" />
            <relation name="UMR5506" active="#struct-1100589" type="direct" />
          </listRelation>
        </org>
      </listOrg>
      <listOrg type="projects">
        <org type="anrProject" xml:id="projanr-52219" status="VALID">
          <idno type="anr">ANR-20-CE23-0015</idno>
          <orgName>PRIDE</orgName>
          <desc>Apprentissage automatique décentralisé et préservant la vie privée</desc>
          <date type="start">2020</date>
        </org>
        <org type="anrProject" xml:id="projanr-51651" status="VALID">
          <idno type="anr">ANR-20-CHIA-0001</idno>
          <orgName>CAMELOT</orgName>
          <desc>Apprentissage automatique et optimisation coopératifs.</desc>
          <date type="start">2020</date>
        </org>
      </listOrg>
    </back>
  <teiCorpus>
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Differentially Private Coordinate Descent for Composite Empirical Risk Minimization</title>
				<funder ref="#_94zSSMc">
					<orgName type="full">Agence Nationale de la Recherche</orgName>
					<orgName type="abbreviated">ANR</orgName>
				</funder>
				<funder ref="#_ZHPUaxp">
					<orgName type="full">unknown</orgName>
				</funder>
				<funder>
					<orgName type="full">Inria Exploratory Action FLAMED</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher />
				<availability status="unknown"><licence /></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Paul</forename><surname>Mangold</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Aurélien</forename><surname>Bellet</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Joseph</forename><surname>Salmon</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Marc</forename><surname>Tommasi</surname></persName>
						</author>
						<title level="a" type="main">Differentially Private Coordinate Descent for Composite Empirical Risk Minimization</title>
					</analytic>
					<monogr>
						<imprint>
							<date />
						</imprint>
					</monogr>
					<idno type="MD5">7F8CF50137491B80D84EABBB89F8815B</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-04-12T14:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid" />
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div><p>Machine learning models can leak information about the data used to train them. To mitigate this issue, Differentially Private (DP) variants of optimization algorithms like Stochastic Gradient Descent (DP-SGD) have been designed to tradeoff utility for privacy in Empirical Risk Minimization (ERM) problems. In this paper, we propose Differentially Private proximal Coordinate Descent (DP-CD), a new method to solve composite DP-ERM problems. We derive utility guarantees through a novel theoretical analysis of inexact coordinate descent. Our results show that, thanks to larger step sizes, DP-CD can exploit imbalance in gradient coordinates to outperform DP-SGD. We also prove new lower bounds for composite DP-ERM under coordinate-wise regularity assumptions, that are nearly matched by DP-CD. For practical implementations, we propose to clip gradients using coordinate-wise thresholds that emerge from our theory, avoiding costly hyperparameter tuning. Experiments on real and synthetic data support our results, and show that DP-CD compares favorably with DP-SGD.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div><head n="1">Introduction</head><p>Machine learning fundamentally relies on the availability of data, which can be sensitive or confidential. It is now well-known that preventing learned models from leaking information about individual training points requires particular attention <ref type="bibr" target="#b51">(Shokri et al., 2017)</ref>. A standard approach for training models while provably controlling the amount of leakage is to solve an empirical risk minimization (ERM) 1 Univ. Lille, Inria, CNRS, Centrale Lille, UMR 9189 -CRIStAL, F-59000 Lille, France 2 IMAG, Univ Montpellier, CNRS, Montpellier, France 3 Institut Universitaire de France (IUF) 4 Univ. Lille, CNRS, Inria, Centrale Lille, UMR 9189 -CRIStAL, F-59000 Lille, France. Correspondence to: Paul Mangold &lt;paul.mangold@inria.fr&gt;.</p><p>Proceedings of the 39 th International Conference on Machine Learning, Baltimore, Maryland, USA, PMLR 162, 2022. Copyright 2022 by the author(s).</p><p>problem under a differential privacy (DP) constraint <ref type="bibr" target="#b15">(Chaudhuri et al., 2011)</ref>. In this work, we aim to design a differentially private algorithm which approximates the solution to a composite ERM problem of the form:</p><formula xml:id="formula_0">w * ∈ arg min w∈R p 1 n n i=1 (w; d i ) + ψ(w) ,<label>(1)</label></formula><p>where D = (d 1 , . . . , d n ) is a dataset of n samples drawn from a universe X , : R p ×X → R is a loss function which is convex and smooth in w, and ψ : R p → R is a convex regularizer which is separable (i.e., ψ(w) = p j=1 ψ j (w j )) and typically nonsmooth (e.g., 1 -norm).</p><p>Differential privacy constraints induce a trade-off between the privacy and the utility (i.e., optimization error) of the solution of (1). This trade-off was made explicit by <ref type="bibr" target="#b3">Bassily et al. (2014)</ref>, who derived lower bounds on the achievable error given a fixed privacy budget. To solve the DP-ERM problem in practice, the most popular approaches are based on Differentially Private variants of Stochastic Gradient Descent (DP-SGD) <ref type="bibr" target="#b3">(Bassily et al., 2014;</ref><ref type="bibr" target="#b0">Abadi et al., 2016;</ref><ref type="bibr">Wang et al., 2017)</ref>, in which random perturbations are added to the (stochastic) gradients. <ref type="bibr" target="#b3">Bassily et al. (2014)</ref> analyzed DP-SGD in the non-smooth DP-ERM setting, and <ref type="bibr">Wang et al. (2017)</ref> then proposed an efficient DP-SVRG algorithm for composite DP-ERM. Both algorithms match known lower bounds. SGD-style algorithms perform well in a wide variety of settings, but also have some flaws: they either require small (or decreasing) step sizes or variance reduction schemes to guarantee convergence, and they can be slow when gradients' coordinates are imbalanced. These flaws propagate to the private counterparts of these algorithms. Despite a few attempts at designing other differentially private solvers for ERM under different setups <ref type="bibr" target="#b52">(Talwar et al., 2015;</ref><ref type="bibr" target="#b16">Damaskinos et al., 2021)</ref>, the differentially private optimization toolbox remains limited, which undoubtedly restricts the resolution of practical problems.</p><p>In this paper, we propose and analyze a Differentially Private proximal Coordinate Descent algorithm (DP-CD), which performs updates based on perturbed coordinate-wise gradients (i.e., partial derivatives). Coordinate Descent (CD) methods have encountered a large success in non-private machine learning due to their simplicity and effectiveness <ref type="bibr">(Liu component-smoothness.</ref> Yet, the actual component-wise constants of a function can be much lower than what can be deduced from their global counterparts. This will be crucial for our analysis and in the performance of DP-CD.</p><p>Remark 2.1. When ψ is the characteristic function of a convex set (with separable components), the regularity assumptions only need to hold on this set. This allows considering problem (1) with a smooth objective under box-constraints.</p><p>Differential privacy (DP). Let D be a set of datasets and F a set of possible outcomes. Two datasets D, D ∈ D are said neighboring (denoted by D ∼ D ) if they differ on at most one element.</p><p>Definition 2.2 (Differential Privacy, <ref type="bibr" target="#b18">Dwork 2006)</ref>. A randomized algorithm A : D → F is ( , δ)-differentially private if, for all neighboring datasets D, D ∈ D and all S ⊆ F in the range of A:</p><formula xml:id="formula_1">Pr [A(D) ∈ S] ≤ exp( )Pr [A(D ) ∈ S] + δ .</formula><p>The value of a function h : D → R p can be privately released using the Gaussian mechanism, which adds centered Gaussian noise to h(D) before releasing it <ref type="bibr" target="#b19">(Dwork &amp; Roth, 2014)</ref>. The scale of the noise is calibrated to the sensitivity ∆(h) = sup D∼D h(D) -h(D ) 2 of h. In our setting, we will perturb coordinate-wise gradients: we denote by ∆(∇ j ) the sensitivity of the j-th coordinate of gradient of the loss function with respect to the data. When (•; d) is Lcomponent-Lipschitz for all d ∈ X , upper bounds on these sensitivities are readily available: we have ∆(∇ j ) ≤ 2L j for any j ∈ [p] (see Appendix A). The following quantity, relating the coordinate-wise sensitivities of gradients to coordinate-wise smoothness is central in our analysis:</p><formula xml:id="formula_2">∆ M -1 (∇ ) = p j=1 1 M j ∆(∇ j ) 2 1 2 ≤ 2 L M -1 . (2)</formula><p>In this paper, we consider the classic central model of DP, where a trusted curator has access to the raw dataset and releases a model trained on this dataset<ref type="foot" target="#foot_0">1</ref> .</p></div>
<div><head n="3">Differentially Private Coordinate Descent</head><p>In this section, we introduce the Differentially Private proximal Coordinate Descent (DP-CD) algorithm to solve problem (1) under ( , δ)-DP constraints. We first describe our algorithm, show how to parameterize it to satisfy the desired privacy constraint, and prove corresponding utility results. Finally, we compare these utility guarantees with DP-SGD.</p></div>
<div><head n="3.1">Private Proximal Coordinate Descent</head><p>Let D = {d 1 , . . . , d n } ∈ X n be a dataset. We denote by f (w) = 1 n n i=1 (w; d i ) the M -component-smooth part of (1), by ψ(w) = p j=1 ψ j (w j ) its separable part, and let F (w) = f (w) + ψ(w). Proximal coordinate descent methods <ref type="bibr" target="#b47">(Richtárik &amp; Takáč, 2014)</ref> solve problem (1) through iterative proximal gradient steps along each coordinate of F . Formally, given w ∈ R p and j ∈ [p], the j-th coordinate of w is updated as follows:</p><formula xml:id="formula_3">w + j = prox γj ψj w j -γ j ∇ j f (w t ) ,<label>(3)</label></formula><p>where γ j &gt; 0 is the step size and prox γj ψj (w) = arg min v∈R p 1 2 v -w 2 2 + γ j ψ j (v) is the proximal operator associated with ψ j <ref type="bibr" target="#b45">(Parikh &amp; Boyd, 2014)</ref>.</p><p>Update (3) only requires the computation of the j-th entry of the gradient. To satisfy differential privacy, we perturb this gradient entry with additive Gaussian noise of variance σ 2 j . The complete DP-CD procedure is shown in Algorithm 1. At each iteration, we pick a coordinate uniformly at random and update according to (3), albeit with noise addition (see line 7). For technical reasons related to our analysis, we use a periodic averaging scheme (line 9). This scheme is similar to DP-SVRG <ref type="bibr">(Johnson &amp; Zhang, 2013)</ref>, although no variance reduction is required since DP-CD computes coordinate gradients over the whole dataset.</p></div>
<div><head n="3.2">Privacy Guarantees</head><p>For Algorithm 1 to satisfy ( , δ)-DP, the noise scales σ = (σ 1 , . . . , σ p ) can be calibrated as given in Theorem 3.1.</p><formula xml:id="formula_4">Theorem 3.1. Assume (•; d) is L-component-Lipschitz ∀d ∈ X . Let ≤ 1 and δ &lt; 1/3. If σ 2 j = 12L 2 j T K log(1/δ) n 2 2</formula><p>for all j ∈ [p], then Algorithm 1 satisfies ( , δ)-DP.</p></div>
<div><head>Sketch of Proof. (Complete proof in Appendix B</head><p>). We track the privacy loss using Rényi differential privacy (RDP), which gives better guarantees than ( , δ)-DP for the composition of Gaussian mechanisms <ref type="bibr" target="#b40">(Mironov, 2017)</ref>. The j-th entry of ∇f has sensitivity</p><formula xml:id="formula_5">∆(∇ j f ) = ∆(∇ j )/n ≤ 2L j /n. By the Gaussian mechanism each iteration of DP- CD is (α, 2L 2 j α n 2 σ 2 j</formula><p>)-RDP for all α &gt; 1. The composition theorem for RDP gives a global RDP guarantee for DP-CD, that we convert to ( , δ)-DP using Proposition 3 of <ref type="bibr" target="#b40">Mironov (2017)</ref>. Choosing α carefully finally proves the result.</p><p>The dependence of the noise scales on , δ, n and T K (the number of updates) in Theorem 3.1 is standard in DP-ERM. However, the noise is calibrated to the loss function's component-Lipschitz constants. These can be much lower their global counterpart, the latter being used to calibrate the noise in DP-SGD algorithms. This will be crucial for DP-CD to achieve better utility than DP-SGD in some regimes.</p><p>Algorithm 1 Differentially Private Proximal Coordinate Descent Algorithm (DP-CD). Input: noise scales σ = (σ 1 , . . . , σ p ) for σ 1 , . . . , σ p &gt; 0; step sizes γ 1 , . . . , γ p &gt; 0; initial point w0 ∈ R p ; iteration budgets T, K &gt; 0.</p><p>1: for t = 0, . . . , T -1 do 2:</p><p>Set θ 0 = wt 3:</p><p>for k = 0, . . . , K -1 do 4:</p><p>Pick j from {1, . . . , p} uniformly at random 5:</p><p>Draw η j ∼ N (0, σ 2 j )</p><p>6:</p><p>Set</p><formula xml:id="formula_6">θ k+1 = θ k 7: Set θ k+1 j = prox γj ψj (θ k j -γ j (∇ j f (θ k ) + η j )) 8:</formula><p>end for 9:</p><p>Set wt+1 = 1 K K k=1 θ k 10: end for 11: return w priv = wT</p><p>We also note that, unlike DP-SGD, DP-CD does not rely on privacy amplification by subsampling <ref type="bibr" target="#b2">(Balle et al., 2018;</ref><ref type="bibr" target="#b41">Mironov et al., 2019)</ref>, and thereby avoids the approximations required by these schemes to bound the privacy loss.</p><p>Remark 3.2. Theorem 3.1 assumes ∈ (0, 1] to give a simple closed form for the noise scales. In practice we compute tighter values numerically using Rényi DP formulas directly (see Eq. 18 in Appendix B), removing the need for this assumption.</p></div>
<div><head n="3.3">Utility Guarantees</head><p>We now state our central result on the utility of DP-CD for the composite DP-ERM problem. As done in previous work, we use the asymptotic notation O to hide non-significant logarithmic factors. Non-asymptotic utility bounds can be found in Appendix C.</p><p>Theorem 3.3. Let (•; d) be a convex and L-component-Lipschitz loss function for all d ∈ X , and f be convex and M -component-smooth. Let ψ : R p → R be a convex and separable function. Let ≤ 1, δ &lt; 1/3 be the privacy budget. Let w * be a minimizer of F and F * = F (w * ). Let w priv ∈ R p be the output of Algorithm 1 with step sizes γ j = 1/M j , and noise scales σ 1 , . . . , σ p set as in Theorem 3.1 (with T and K chosen below) to ensure ( , δ)-DP. Then, the following holds:</p><formula xml:id="formula_7">1. For F convex, K = O R M √ pn L M -1</formula><p>, and T = 1, then:</p><formula xml:id="formula_8">E[F (w priv ) -F * ] = O p log(1/δ) n L M -1 R M ,</formula><p>where R M = max( F (w 0 ) -F (w * ), w 0 -w * M ) and more simply R M = w 0 -w * M when ψ = 0.</p></div>
<div><head n="2.">For</head><formula xml:id="formula_9">F µ M -strongly convex w.r.t. • M , K = O (p/µ M ),</formula><p>and</p><formula xml:id="formula_10">T = O (log(n µ M /p L M -1</formula><p>)), then:</p><formula xml:id="formula_11">E[F (w priv ) -F * ] = O p log(1/δ) n 2 2 L 2 M -1 µ M .</formula><p>Expectations are over the randomness of the algorithm.</p></div>
<div><head>Sketch of Proof. (Complete proof in Appendix C</head><p>). Existing analyses of CD fail to track the noise tightly across coordinates when adapted to the private setting. Contrary to these classical analyses, we prove a recursion on E θ k -w * 2 M , rather than on E F (θ k ) -F (w * ) . Our key technical result is a descent lemma (Lemma C.3) allowing us to obtain</p><formula xml:id="formula_12">E F (θ k+1 ) -F * - p -1 p E F (θ k ) -F * (4) ≤ E θ k -w * 2 M -E θ k+1 -w * 2 M + σ 2 M p .</formula><p>The above inequality shows that coordinate-wise updates leave a fraction p-1 p of the function "unchanged", while the remaining part decreases (up to additive noise). Importantly, all quantities are measured in M -norm. When summing (4) for k = 0, . . . , K -1, its left hand side simplifies and its right hand side is simplified as a telescoping sum:</p><formula xml:id="formula_13">1 p K k=1 E F (θ k ) -F * (5) ≤ E F ( wt ) -F * + E wt -w * 2 M + K p σ 2 M -1 ,</formula><p>where wt comes from θ 0 = wt . As wt+1 =</p><formula xml:id="formula_14">K k=1 θ k K and F is convex, we have F ( wt+1 ) -F * ≤ 1 K K k=1 F (θ k ) - F * .</formula><p>This proves the sub-linear convergence (up to an additive noise term) of the inner loop. The result in the convex case follows directly (since T = 1, only one inner loop is run). For strongly convex F , it further holds</p><formula xml:id="formula_15">that E wt -w * 2 M ≤ 2 µ M E[F ( wt ) -F (w * )]. Replacing in (5) with large enough K gives E F ( wt+1 ) -F * ≤ 1 2 E[F ( wt ) -F * ] + σ 2 M -1</formula><p>, and linear convergence (up to an additive noise term) follows. Finally, K and T are chosen to balance the "optimization" and the "privacy" errors.</p><p>Remark 3.4. Our novel convergence proof of CD is also useful in the non-private setting. In particular, we improve upon known convergence rates for inexact CD methods with additive error <ref type="bibr" target="#b53">(Tappenden et al., 2016)</ref>, under the hypothesis that gradients are noisy and unbiased. In their formalism, we have α = 0 and β = σ 2 M -1 /p. With our analysis, the algorithm requires 2pR 2 M /(ξ -pβ) (resp. 4p/µ M log((F (w 0 ) -F * )/(ξ -pβ))) iterations to achieve expected precision ξ &gt; pβ when F is convex (resp. µ Mstrongly-convex w.r.t. • M ), improving upon <ref type="bibr" target="#b53">Tappenden et al. (2016)</ref>'s results by a factor pβ/2R 2 M (resp. µ M /2). See Appendix C.3 for details. Moreover, unlike this prior work, our analysis does not require the objective to decrease at each iteration, which is essential to guarantee DP.</p><p>Our utility guarantees stated in Theorem 3.3 directly depend on precise coordinate-wise regularity measures of the objective function. In particular, the initial distance to optimal, the strong convexity parameter and the overall sensitivity of the loss function are measured in the norms • M and • M -1 (i.e., weighted by coordinate-wise smoothness constants or their inverse). In the remainder of this section, we thoroughly compare our utility results with existing ones for DP-SGD. We will show the optimality of our utility guarantees in Section 4.</p></div>
<div><head n="3.4">Comparison with DP-SGD and DP-SVRG</head><p>We now compare DP-CD with DP-SGD and DP-SVRG, for which <ref type="bibr" target="#b3">Bassily et al. (2014)</ref> and <ref type="bibr">Wang et al. (2017)</ref> proved utility guarantees. In this section, we assume that the loss function satisfies the hypotheses of Theorem 3.3, and is Λ-Lipschitz. We denote by µ I the strong convexity parameter of (•, d) w.r.t. • 2 and R I the equivalent of R M when M is the identity matrix I. As can be seen from Table 1, comparing DP-CD and DP-SGD boils down to comparing L M -1 R M with ΛR I for convex functions and L 2 M -1 /µ M with Λ 2 /µ I for strongly-convex functions. We compare these terms in two scenarios, depending on the distribution of coordinate-wise smoothness constants. To ease the comparison, we assume that R M = w 0 -w * M and R I = w 0 -w * I (which is notably the case when ψ = 0), and that F has a unique minimizer w * .</p><p>Balanced. When the smoothness constants M are all equal,</p><formula xml:id="formula_16">L M -1 R M = L 2 R I and L 2 M -1 /µ M = L 2 2 /µ I .</formula><p>This boils down to comparing L 2 to Λ. As Λ ≤ L 2 ≤ √ pΛ, DP-CD can be up to p times worse than DP-SGD. This can only happen when features are extremely corre-lated, which is generally not the case in machine learning. We show empirically in Section 6.2 that, even in balanced regimes, DP-CD can still significantly outperform DP-SGD.</p><p>Unbalanced. More favorable regimes exist when smoothness constants are imbalanced. To illustrate this, consider the case where the first coordinate of the loss function dominates others. There, M max = M 1 M min = M j and L max = L 1 L min = L j for all j = 1, so that L 2 1 /M 1 dominates the other terms of</p><formula xml:id="formula_17">L 2 M -1 . This yields L 2 M -1 ≈ L 2</formula><p>1 /M 1 ≈ Λ/M max , and µ M = µ I M min . Moreover, if the first coordinate of w * is already well estimated by w 0 (which is common for sparse models), then R M ≈ M min R I . We obtain that L M -1 R M ≈ M min /M max ΛR I for convex losses and</p><formula xml:id="formula_18">L M -1 µ M ≈ Mmin Mmax Λ 2</formula><p>µ I for strongly-convex ones. In both cases, DP-CD can perform arbitrarily better than DP-SGD, depending on the ratio between the smallest and largest coordinate-wise smoothness constants of the loss function. This is due to the inability of DP-SGD to adapt its step size to each coordinate. DP-CD thus converges quicker than DP-SGD on coordinates with smaller-scale gradients, requiring fewer accesses to the dataset, and in turn less noise addition. We give more details on this comparison in Appendix D, and complement it with an empirical evaluation on synthetic and real-world data in Section 6.</p></div>
<div><head n="4">Lower Bounds</head><p>We now prove a new lower bound on the error achievable for composite DP-ERM with L-component-Lipschitz loss functions. While our proof borrows some ideas from the lower bounds known for constrained ERM with Λ-Lipschitz losses <ref type="bibr" target="#b3">(Bassily et al., 2014)</ref>, deriving our lower bounds requires to address a number of specific challenges. First, we cannot use an 2 norm constraint as in <ref type="bibr" target="#b3">Bassily et al. (2014)</ref> in the design of the worst-case problem instances: we can only rely on separable regularizers. Second, imbalanced coordinate-wise Lipschitz constants prevent lowerbounding the distance between an arbitrary point and the solution. This leads us to revisit the construction of a "reidentifiable dataset" from <ref type="bibr" target="#b12">Bun et al. (2014)</ref> so that we have L-component-Lipschitzness while the sum of each column is large enough, which is crucial in our proof. The full proof is given in Appendix E.</p><formula xml:id="formula_19">Theorem 4.1. Let n, p &gt; 0, &gt; 0, δ = o( 1 n ), L 1 , . . . , L p &gt; 0, such that for all J ⊆ [p] of size at least p 75 , j∈J L 2 j = Ω( L<label>2</label></formula><p>2 ). Let X = p j=1 {±L j } and consider any ( , δ)-differentially private algorithm that outputs w priv . In each of the two following cases there exists a dataset D ∈ X n , a L-component-Lipschitz loss (•, d) for all d ∈ D and a regularizer ψ so that, with F the objective of (1) minimal at w * ∈ R p : Table <ref type="table">1</ref>. Utility guarantees for DP-CD, DP-SGD, and DP-SVRG for L-component-Lipschitz, Λ-Lipschitz loss.</p></div>
<div><head>Convex</head><p>Strongly-convex DP-CD (this paper)</p><formula xml:id="formula_20">O p log(1/δ) n L M -1 R M O p log(1/δ) n 2 2 L 2 M -1 µ M</formula><p>DP-SGD <ref type="bibr" target="#b3">(Bassily et al., 2014)</ref> DP-SVRG <ref type="bibr">(Wang et al., 2017</ref>)</p><formula xml:id="formula_21">O p log(1/δ) n ΛR I O p log(1/δ) n 2 2 Λ 2 µ I 1. If F is convex: E F (w priv ; D) -F (w * ) = Ω √ p L 2 w * 2 n . 2. If F is µ I -strongly-convex w.r.t. • 2 : E F (w priv ; D) -F (w * ) = Ω p L 2 2 µ I n 2 2 .</formula><p>We recover the lower bounds of <ref type="bibr" target="#b3">Bassily et al. (2014)</ref> for Λ-Lipschitz losses as a special case of ours by setting</p><formula xml:id="formula_22">L 1 = • • • = L p = Λ/ √ p.</formula><p>In this case, the loss function used in our proof is indeed</p><formula xml:id="formula_23">( p j=1 L 2 j ) 1/2 = Λ-Lipschitz.</formula><p>To relate these lower bounds to the performance of DP-CD, consider a suboptimal version of our algorithm where the step sizes are set to γ 1 = • • • = γ p = (max j M j ) -1 . In this setting, results from Theorem 3.3 still hold, and match the lower bounds from Theorem 4.1 up to logarithmic factors. We leave open the question of the optimality of DP-CD under the additional hypothesis of smoothness.</p><p>We note that the assumption on the sum of the L j 's over a set of indices J in Theorem 4.1 can be eliminated at the cost of an additional factor of L min /L max for convex losses and (L min /L max ) 2 for strongly-convex losses, making the bound looser. Although the aforementioned assumption may seem solely technical, we conjecture that better utility is possible when a few coordinate-wise Lipschitz constants dominate the others. We discuss this further in Section 8.</p></div>
<div><head n="5">DP-CD in Practice</head><p>We now discuss practical questions related to DP-CD. First, we show how to implement coordinate-wise gradient clipping using a single hyperparameter. Second, we explain how to privately estimate the smoothness constants. Finally, we discuss the possibility of standardizing the features and how this relates to estimating smoothness constants for the important problem of fitting generalized linear models.</p></div>
<div><head n="5.1">Coordinate-wise Gradient Clipping</head><p>To bound the sensitivity of coordinate-wise gradients, our analysis of Section 3 relies on the knowledge of Lipschitz constants for the loss function (•; d) that must hold for all possible data points d ∈ X , see inequality (2) and the discussion above it. This is classic in the analysis of DP optimization algorithms (see e.g., <ref type="bibr" target="#b3">Bassily et al., 2014;</ref><ref type="bibr">Wang et al., 2017)</ref>. In practice however, these Lipschitz constants can be difficult to bound tightly and often give largely pessimistic estimates of sensitivities, thereby making gradients overly noisy. To overcome this problem, the common practice in concrete deployments of DP-SGD algorithms is to clip per-sample gradients so that their norm does not exceed a fixed threshold parameter C &gt; 0 <ref type="bibr" target="#b0">(Abadi et al., 2016)</ref>:</p><formula xml:id="formula_24">clip(∇ (w), C) = min 1, C ∇ (w) 2 ∇ (w) .<label>(6)</label></formula><p>This effectively ensures that the sensitivity ∆(clip(∇ , C)) of the clipped gradient is bounded by 2C.</p><p>In DP-CD, gradients are released one coordinate at a time and should thus be clipped in a coordinate-wise fashion.</p><p>Using the same threshold for each coordinate would ruin the ability of DP-CD to account for imbalance across gradient coordinates, whereas tuning coordinate-wise thresholds as p individual hyperparameters {C j } p j=1 is impractical. Instead, we leverage the results of Theorem 3.3 to adapt them from a single hyperparameter. We first remark that our utility guarantees are invariant to the scale of the matrix M . After rescaling M to M = p tr(M ) M so that tr( M ) = tr(I) = p, as proposed by <ref type="bibr" target="#b47">Richtárik &amp; Takáč (2014)</ref>, the key quantity ∆ M -1 (∇ ) as defined in (2) appears in our utility bounds instead of L M -1 . This suggests to parameterize the j-th threshold as</p><formula xml:id="formula_25">C j = M j /tr(M )C for some C &gt; 0, ensuring that ∆ M -1 ({clip(∇ j , C j )} p j=1 ) ≤ 2C.</formula><p>The parameter C thus controls the overall sensitivity, allowing clipped DP-CD to perform p iterations for the same privacy budget as one iteration of clipped DP-SGD.</p></div>
<div><head n="5.2">Private Smoothness Constants</head><p>DP-CD requires the knowledge of the coordinate-wise smoothness constants M 1 , . . . , M p of f to set appropriate step sizes (see Theorem 3.3) and clipping thresholds (see above). 2 In most problems, the M j 's depend on the dataset D and must thus be estimated privately using a fraction of the overall privacy budget. Since f is an average of loss terms, its coordinate-wise smoothness constants are the average of those of (•, d) over d ∈ D. These per-sample quantities are easy to get for typical losses (see Section 5.3 for the case of linear models). Privately estimating M 1 , . . . , M p thus reduces to a classic private mean estimation problem for which many methods exist. For instance, assuming that the practitioner knows a crude upper bound on per-sample smoothness constants, he/she can compute the smoothness constants of the (•, d)'s, clip them to the pre-defined upper bounds, and privately estimate their mean using the Laplace mechanism (see Appendix F for details). We show numerically in Section 6 that dedicating 10% of the total budget to this strategy allows DP-CD to effectively exploit the imbalance across gradients' coordinates.</p></div>
<div><head n="5.3">Feature Standardization</head><p>CD algorithms are very popular to solve generalized linear models <ref type="bibr" target="#b25">(Friedman et al., 2010)</ref> and their regularized version (e.g., LASSO, logistic regression). For these problems, the coordinate-wise smoothness constants are M j ∝ 1 n X :,j 2 2 , where X :,j ∈ R n is the vector containing the value of the j-th feature. Therefore, standardizing the features to have zero mean and unit variance (a standard preprocessing step) makes coordinate-wise smoothness constants equal. However, this requires to compute the mean and variance of each feature in D, which is more costly than the smoothness constants to estimate privately. 3 Moreover, while our theory suggests that DP-CD may not be superior to DP-SGD when smoothness constants are all equal (see Section 3.4), the numerical results of Section 6 show that DP-CD often outperforms DP-SGD even when features are standardized.</p><p>Finally, we emphasize that standardization is not always possible. This notably happens when solving the problem at hand is a subroutine of another algorithm. For instance, the Iteratively Reweighted Least Squares (IRLS) algorithm <ref type="bibr" target="#b29">(Holland &amp; Welsch, 1977)</ref> finds the maximum likelihood estimate of a generalized linear model by solving a sequence of linear regression problems with reweighted features, proscribing standardization. Similar situations happen when using reweighted 1 methods for non-convex sparse regression <ref type="bibr" target="#b13">(Candès et al., 2008)</ref>, relying on convex (LASSO) solvers for the inner loop. DP-CD is thus a method of choice to serve as subroutine in private versions of these algorithms.</p><p>2 In fact, only Mj/ j M j is needed, as we tune the clipping threshold and scaling factor for the step sizes. See Section 6. 3 We note that the privacy cost of standardization is rarely accounted for in practical evaluations.  Relative error to non-private optimal for DP-CD (blue), DP-CD with privately estimated coordinate-wise smoothness constants (green), DP-SGD (orange) and DP-SCD (red, only applicable to the smooth case) on two imbalanced problems. The number of passes is tuned separately for each algorithm to achieve lowest error. We report min/mean/max values over 10 runs.</p></div>
<div><head n="6">Numerical Experiments</head><p>In this section, we assess the practical performance of DP-CD against (proximal) DP-SGD on LASSO 4 and 2regularized logistic regression 5 . On the latter problem, we also consider the dual private coordinate descent algorithm of <ref type="bibr" target="#b16">Damaskinos et al. (2021)</ref> (DP-SCD). For LASSO, we use the California dataset (Kelley Pace &amp; Barry, 1997), with n = 20, 640 records and p = 8 features as well as a synthetic dataset (coined "Sparse LASSO") with n = 1, 000 records and p = 1, 000 independent features that follow a standard normal distribution. The labels are then computed as a noisy sparse linear combination of a subset of 10 active features. For logistic regression, we consider the Electricity dataset (Electricity) with 45, 312 records and 8 features. On California and Electricity, we set = 1 and δ = 1/n 2 , which is generally seen as a rather high privacy regime. The Sparse LASSO dataset corresponds to a challenging setting for privacy (n = p), so we consider a low privacy regime with = 10, δ = 1/n 2 . Privacy accounting for DP-SGD is done by numerically evaluating the Rényi DP formula given by the sampled Gaussian mechanism <ref type="bibr" target="#b41">(Mironov et al., 2019)</ref>. Similarly for DP-CD, we do not use the closed-form formula of Theorem 3.1 but rather numerically evaluate the tighter Rényi DP formula given in Appendix B.</p><p>For DP-SGD, we use constant step sizes and standard gradient clipping. For DP-CD, we adapt the coordinate-wise clipping thresholds from one hyperparameter, as described in Section 5.1. Similarly, coordinate-wise step sizes are set to γ j = γ/M j , where γ is a hyperparameter. When the coordinate-wise smoothness constants are not all equal, we also consider DP-CD with privately computed M j 's, as described in Section 5.2. For each dataset and each algorithm, we simultaneously tune the clipping threshold, the number 4 i.e., (w, (x, y)) = (w x -y) 2 , ψ(w) = λ w 1 . 5 i.e., (w, (x, y)) = log(1+exp(-yw x)), ψ(w) = λ 2 w 2 2 .</p><p>of passes over the dataset and, for DP-CD and DP-SGD, the step sizes. After tuning these parameters, we report the relative error to the (non-private) optimal objective value. The complete tuning procedure is described in Appendix G.1, where we also give the best error for various numbers of passes for each algorithm and dataset. The code used to obtain all our results is available in a public repository<ref type="foot" target="#foot_1">6</ref> and in the supplementary material.</p></div>
<div><head n="6.1">Imbalanced Datasets</head><p>In the Electricity and California datasets, features are naturally imbalanced. DP-CD can exploit this through the use of coordinate-wise smoothness constants. We also consider a variant of DP-CD (DP-CD-P) which dedicates 10% of the privacy budget to estimate these constants (see Section 5.2) from a crude upper bound on each feature (twice their maximal absolute value). It then uses the resulting private smoothness constants in step sizes and clipping thresholds. Figure <ref type="figure" target="#fig_1">1</ref> shows that DP-CD outperforms DP-SGD and DP-SCD by an order of magnitude on both datasets, even when the smoothness constants are estimated privately.</p></div>
<div><head n="6.2">Balanced Datasets</head><p>To assess the performance of DP-CD when coordinatewise smoothness constants are balanced, we standardize the Electricity and California datasets (see Section 5.3). As standardization is done for all algorithms, we do not account for it in the privacy budget. On standardized datasets, coordinate-wise smoothness constants are all equal, removing the need of estimating them privately. We report the results in Figure <ref type="figure" target="#fig_3">2</ref>. Although our theory suggests that DP-CD may do worse than DP-SGD in balanced regimes, we observe that it still improves over DP-SGD (and DP-SCD) in practice. Similar observations hold in our challenging Sparse LASSO problem, where DP-SGD is barely able to make any progress. We believe these results are in part due to the beneficial effect of clipping in DP-CD, and the fact that DP-SGD relies on amplification by subsampling, for which privacy accounting is not perfectly tight. Additionally, CD methods are known to perform well on fitting linear models: our results show that this transfers well to private optimization.</p></div>
<div><head n="6.3">Running Time</head><p>The results above showed that DP-CD yields better utility than DP-SGD. We also observe that DP-CD tends to reach these results in up to 10 times fewer passes on the data than DP-SGD (see Appendix G.1 for detailed results). Additionally, when accounting for running time, DP-CD sig-nificantly outperforms DP-SGD: we refer to Appendix G.2 for the counterparts of Figure <ref type="figure" target="#fig_1">1</ref> and 2 as a function of the running time instead of the number of passes.</p><p>7 Related Work   Relative error to non-private optimal for DP-CD (blue), DP-SGD (orange) and DP-SCD (red, only applicable to the smooth case) on three balanced problems. The number of passes is tuned separately for each algorithm to achieve lowest error. We report min/mean/max values over 10 runs.</p><p>CD. Inexact CD was studied by <ref type="bibr" target="#b53">Tappenden et al. (2016)</ref>, but their analysis requires updates not to increase the objective, which is hardly compatible with DP. We obtain tighter results for inexact CD with noisy gradients (see Remark 3.4).</p><p>Private coordinate descent. <ref type="bibr" target="#b16">Damaskinos et al. (2021)</ref> introduced a CD method to privately solve the dual problem associated with generalized linear models with 2 regularization. Dual CD is tightly related to SGD, as each coordinate in the dual is associated with one data point. The authors briefly mention the possibility of performing primal coordinate descent but discard it on account of the seemingly large sensitivity of its updates. We show that primal DP-CD is in fact quite effective, and can be used to solve more general problems than considered by <ref type="bibr" target="#b16">Damaskinos et al. (2021)</ref>. Primal CD was successfully used by <ref type="bibr" target="#b10">Bellet et al. (2018)</ref> to privately learn personalized models from decentralized datasets. For the smooth objective they consider, each coordinate depends only on a subset of the full dataset, which directly yields low coordinate-wise sensitivity updates. In contrast, we introduce a general algorithm for composite DP-ERM, for which a novel utility analysis was required.</p></div>
<div><head n="8">Conclusion and Discussion</head><p>We presented the first differentially private proximal coordinate descent algorithm for composite DP-ERM. Using an original approach to analyze proximal CD with perturbed gradients, we derived optimal upper bounds on the privacyutility trade-off achieved by DP-CD. We also prove new lower bounds under a component-Lipschitzness assumption, and showed that DP-CD matches these bounds. Our results demonstrate that DP-CD strongly outperforms DP-SGD when gradients' coordinates are imbalanced. Numerical experiments show that DP-CD also performs very well in balanced regimes. The choice of coordinate-wise clipping thresholds is crucial for DP-CD to achieve good utility in practice, and we provided a simple rule to set them.</p><p>Although DP-CD already achieves good utility when most coordinates have small sensitivity, our lower bounds suggest that even better utility could be achieved by dynamically allocating more privacy budget to coordinates with largest sensitivities. A promising direction is to design DP-CD algorithms that leverage active set methods <ref type="bibr" target="#b63">(Yuan et al., 2010;</ref><ref type="bibr" target="#b36">Lewis &amp; Wright, 2016;</ref><ref type="bibr" target="#b44">Nutini et al., 2017;</ref><ref type="bibr" target="#b17">De Santis et al., 2016;</ref><ref type="bibr" target="#b39">Massias et al., 2018)</ref>, which could provide practical alternatives to recent DP-SGD approaches that use a subspace assumption <ref type="bibr" target="#b64">(Zhou et al., 2021;</ref><ref type="bibr" target="#b33">Kairouz et al., 2021)</ref>. Finally, we believe that adaptive clipping techniques <ref type="bibr" target="#b46">(Pichapati et al., 2019;</ref><ref type="bibr" target="#b54">Thakkar et al., 2021)</ref> may help to further improve the practical performance of DP-CD when coordinate-wise smoothness constants are more balanced.</p></div>
<div><head>A Lemmas on Sensitivity</head><p>In this section, we let X be the universe where the data is drawn from. To upper bound the sensitivities of a function's gradient, we start by recalling in Lemma A.1 that (coordinate) gradients are bounded by (coordinate-wise-)Lipschitz constants. We then link this upper bound with gradients' sensitivities in Lemma A.2.</p><p>Lemma A.1. Let : R p × X → R be convex and differentiable in its first argument, Λ &gt; 0 and L 1 , . . . , L p &gt; 0.</p><formula xml:id="formula_26">1. If (•; d) is Λ-Lipschitz for all d ∈ X , then ∇ (w; d) 2 ≤ Λ for all w ∈ R p and d ∈ X . 2. If (•; d) is L-component-Lipschitz for all d ∈ X , then |∇ j (w; d)| ≤ L j for all w ∈ R p , d ∈ X and j ∈ [p].</formula><p>Proof. Let d ∈ X . We start by proving the first statement. First, if ∇ (w; d) = 0, ∇ (w; d) 2 = 0 ≤ Λ and the result holds. Second, we focus on the case where ∇ (w; d) = 0. The convexity of gives, for w ∈ R p , d ∈ X :</p><formula xml:id="formula_27">(w + ∇ (w; d); d) ≥ (w; d) + ∇ (w; d), ∇ (w; d) = (w; d) + ∇ (w; d) 2 2 ,<label>(7)</label></formula><p>then, reorganizing the terms and using Λ-Lipschitzness of yields</p><formula xml:id="formula_28">∇ (w; d) 2 2 ≤ (w + ∇ (w; d); d) -(w; d) ≤ | (w + ∇ (w; d); d) -(w; d)| ≤ Λ ∇ (w; d) 2 ,<label>(8)</label></formula><p>and the result follows after dividing by ∇ (w; d) 2 . To prove the second statement, we set j ∈ [p], and w ∈ R p , and remark that if ∇ j (w; d) = 0, then |∇ j (w; d)| ≤ L j . When ∇ j (w; d) = 0, the convexity of yields</p><formula xml:id="formula_29">(w + ∇ j (w; d)e j ; d) ≥ (w; d) + ∇ (w; d), ∇ j (w; d)e j = (w; d) + ∇ j (w; d) 2 . (<label>9</label></formula><formula xml:id="formula_30">)</formula><p>Reorganizing the terms and using L-component-Lipschitzness of gives</p><formula xml:id="formula_31">∇ j (w; d) 2 ≤ (w + ∇ j (w; d)e j ; d) -(w; d) ≤ | (w + ∇ j (w; d)e j ; d) -(w; d)| ≤ L j |∇ j (w; d)| ,<label>(10)</label></formula><p>and we get the result after dividing by |∇ j (w; d)|.</p><p>Lemma A.2. Let : R p × X → R be convex and differentiable in its 1st argument, Λ &gt; 0 and L 1 , . . . , L p &gt; 0.</p><formula xml:id="formula_32">1. If (•; d) is Λ-Lipschitz for all d ∈ X , then ∆(∇ ) ≤ 2Λ. 2. If (•; d) is L-component-Lipschitz for all d ∈ X , then ∆(∇ j ) ≤ L j for all j ∈ [p].</formula><p>Proof. We start by proving the first statement. Let w, w ∈ R p , d, d ∈ X . From the triangle inequality and Lemma A.1, we get the following upper bounds:</p><formula xml:id="formula_33">∇ (w; d) -∇ (w ; d ) 2 ≤ |∇ (w; d)| + |∇ (w ; d )| ≤ 2Λ ,<label>(11)</label></formula><p>which is the claim of the first statement. To prove the second statement, we proceed similarly: the triangle inequality and Lemma A.1 give the following upper bounds:</p><formula xml:id="formula_34">|∇ j (w; d) -∇ j (w ; d )| ≤ |∇ j (w; d)| + |∇ j (w ; d )| ≤ 2L j ,<label>(12)</label></formula><p>which is the desired result.</p><p>We obtain the inequality (2) stated in Section 2 as a corollary.</p><formula xml:id="formula_35">Corollary A.3. Let L 1 , . . . , L p &gt; 0. Let (•; d) : R p → R be a convex, L-component-Lipschitz function for all d ∈ X . Then ∆ M -1 (∇ ) = p j=1 1 M j ∆(∇ j ) 2 1 2 ≤ p j=1 4 M j L 2 j 1 2 = 2 L M -1 .<label>(13)</label></formula></div>
<div><head>B Proof of Theorem 3.1</head><p>To track the privacy loss of an adaptive composition of K Gaussian mechanisms, we use Rényi Differential Privacy <ref type="bibr">(Mironov, 2017, RDP)</ref>. We note that similar results are obtained with zero Concentrated Differential Privacy <ref type="bibr" target="#b11">(Bun &amp; Steinke, 2016)</ref>. This flavor of differential privacy, gives tighter privacy guarantees in that setting, as it reduces the noise variance by a multiplicative factor of log(K/δ) in comparison to the usual advanced composition theorem of differential privacy <ref type="bibr" target="#b20">(Dwork et al., 2006)</ref>. Importantly, RDP can be translated back to differential privacy.</p><p>In this section, we recall the definition and main properties of zCDP. We denote by D the set of all datasets over a universe X and by F the set of possible outcomes of the randomized algorithms we consider.</p></div>
<div><head>B.1 Rényi Differential Privacy</head><p>We will use the Rényi divergence (Definition B.1), which gives a distribution-oriented vision of privacy. </p><formula xml:id="formula_36">D α (Y ||Z) = 1 α -1 log C Pr [Y = z] α Pr [Z = z] 1-α dz . (<label>14</label></formula><formula xml:id="formula_37">)</formula><p>We </p><formula xml:id="formula_38">D α (A(D)||A(D )) ≤ . (<label>15</label></formula><formula xml:id="formula_39">) Lemma B.3 (Mironov 2017, Proposition 3). If a randomized algorithm A : D → F is (α, )-RDP, then it is ( + log(1/δ) α-1 , δ)- differentially private for all 0 &lt; δ &lt; 1.</formula><p>Remark B.4. The above (α, )-RDP guarantees hold for multiple values of α, . As such, = (α) can be seen as a function of α, and Lemma B.3 ensures that the algorithm is ( , δ)-DP for</p><formula xml:id="formula_40">= min α&gt;1 (α) + log(1/δ) α -1 . (<label>16</label></formula><formula xml:id="formula_41">)</formula><p>We can now restate in Theorem B.5 the composition theorem of RDP, which is key in designing private iterative algorithms.</p><p>Theorem B.5 (Mironov 2017, Proposition 1). Let A 1 , . . . , A K : D → F be K &gt; 0 randomized algorithms, such that for</p><formula xml:id="formula_42">1 ≤ k ≤ K, A k is (α, k (α))-RDP</formula><p>, where these algorithms can be chosen adaptively (i.e., A k can use to the output of</p><formula xml:id="formula_43">A k for all k &lt; k). Let A : D → F K such that for D ∈ D, A(D) = (A 1 (D), . . . , A K (D)). Then A is α, K k=1 k (α) -RDP.</formula><p>Finally, we define the Gaussian mechanism (Definition B.6), as used in Algorithm 1, and restate in Lemma B.7 the privacy guarantees that it satisfies in terms of RDP.</p><p>Definition B.6 (Gaussian mechanism). Let f : D → R p , σ &gt; 0, and D ∈ D. The Gaussian mechanism for answering the query f is defined as:</p><formula xml:id="formula_44">M Gauss f (D; σ) = f (D) + N 0, σ 2 I p . (<label>17</label></formula><formula xml:id="formula_45">)</formula><p>Lemma B.7 (Mironov 2017, Corollary 3). The Gaussian mechanism with noise σ 2 is (α,</p><formula xml:id="formula_46">∆(f ) 2 α 2σ 2 )-RDP, where ∆(f ) = sup D,D f (D) -f (D ) 2 (for neighboring D, D ) is the sensitivity of f .</formula><p>Proof. The function h = f ∆(f ) has sensitivity 1, thus for any s &gt; 0, the Gaussian mechanism</p><formula xml:id="formula_47">M Gauss h (•; s) is (α, α 2σ 2 )-RDP (Mironov, 2017, Corollary 1). As f = ∆(f ) × h, we have M Gauss f (•; σ) = ∆(f ) × M Gauss h (•; σ ∆(f ) ). This mechanism is thus (α, ∆(f ) 2 α 2σ 2 )-RDP. Corollary B.8. Let 0 &lt; ≤ 1, 0 &lt; δ &lt; 1 3 . If a randomized algorithm A : D → F is (α, γα<label>2σ</label></formula><p>2 )-RDP with γ &gt; 0 and σ = √ 3γ log(1/δ) for all α &gt; 1, it is also ( , δ)-DP.</p><p>Proof. From Remark B.4 it holds that A is ( , δ)-DP with = min α&gt;1 γα</p><formula xml:id="formula_48">2σ 2 + log(1/δ) α-1</formula><p>. This minimum is attained when the derivative of the objective is zero, which is the case when</p><formula xml:id="formula_49">γ 2σ 2 = log(1/δ) (α-1) 2 , resulting in α = 1 + 2 log(1/δ)σ 2 γ . A is thus ( , δ)-DP with = γ 2σ 2 + γ log(1/δ) √ 2σ + γ log(1/δ) √ 2σ = γ 2σ 2 + 2γ log(1/δ) σ . (<label>18</label></formula><formula xml:id="formula_50">) Choosing σ = √ 3γ log(1/δ) now gives = 2 6 log(1/δ) + 2/3 ≤ (1/6 + 2/3) ≤ , (<label>19</label></formula><formula xml:id="formula_51">)</formula><p>where the first inequality comes from ≤ 1, thus 2 ≤ and δ &lt; 1/3 thus 1 log(1/δ) ≤ 1. The second inequality follows from 1/6 + 2/3 ≈ 0.983 &lt; 1.</p></div>
<div><head>B.2 Proof of Theorem 3.1</head><p>We are now ready to prove Theorem 3.1. From the privacy perspective, Algorithm 1 adaptively releases and post-processes a series of gradient coordinates protected by the Gaussian mechanism. We thus start by proving Lemma B.9, which gives an ( , δ)-differential privacy guarantee for the adaptive composition of K Gaussian mechanisms.</p><p>Lemma B.9. Let 0 &lt; ≤ 1, δ &lt; 1/3, K &gt; 0, p &gt; 0, and {f k : R p → R} k=K k=1 a family of K functions. The adaptive composition of K Gaussian mechanisms, with the k-th mechanism releasing f k with noise scale</p><formula xml:id="formula_52">σ k = ∆(f k ) √ 3K log(1/δ) is ( , δ)-differentially private.</formula><p>Proof. Let σ &gt; 0. Lemma B.7 guarantees that the k-th Gaussian mechanism with noise scale</p><formula xml:id="formula_53">σ k = ∆(f k )σ &gt; 0 is (α, α<label>2σ</label></formula><p>2 )-RDP. Then, the composition of these K mechanisms is, according to Theorem B.5, (α, kα 2σ 2 )-RDP. This can be converted to ( , δ)-DP via Corollary B.8 with γ = K, which gives</p><formula xml:id="formula_54">σ k = ∆(f k ) √ 3k log(1/δ) for k ∈ [K].</formula><p>We now restate Theorem 3.1 and prove it.</p><formula xml:id="formula_55">Theorem 3.1. Assume (•; d) is L-component-Lipschitz ∀d ∈ X . Let &lt; 1 and δ &lt; 1/3. If σ 2 j = 12L 2 j T K log(1/δ) n 2 2</formula><p>for all j ∈ [p], then Algorithm 1 satisfies ( , δ)-DP.</p><p>Proof. For j ∈ [1, p], ∇ j f in Algorithm 1 is released using the Gaussian mechanism with noise variance σ 2 j . The sensitivity of</p><formula xml:id="formula_56">∇ j f is ∆(∇ j f ) = ∆(∇j ) n ≤ 2Lj</formula><p>n . Note that T K gradients are released, and</p><formula xml:id="formula_57">σ 2 j = 12L 2 j T K log(1/δ) n 2 2 for j ∈ [1, p] ,</formula><p>thus by Lemma B.9 and the post-processing property of DP, Algorithm 1 is ( , δ)-differentially private.</p></div>
<div><head>C Proof of Utility (Theorem 3.3) C.1 Problem Statement</head><p>Let D ∈ X n be a dataset of n elements drawn from a universe X . Recall that we consider the following composite empirical risk minimization problem:</p><formula xml:id="formula_58">w * ∈ arg min w∈R p F (w; D) = 1 n n i=1 (w; d i ) =:f (w;D) +ψ(w) ,<label>(20)</label></formula><p>where (•, d) is convex, L-component-Lipschitz, and M -component-smooth for all d ∈ X , and ψ(w) = p j=1 ψ j (w j ) is convex and separable. We denote by F the complete objective function, and by f its smooth part. For readability, we omit the dependence on their second argument (i.e., the data) in the rest of this section.</p></div>
<div><head>C.2 Proof of Theorem 3.3</head><p>In this section, we prove our central theorem that guarantees the utility of the DP-CD algorithm. To this end, we start by proving a lemma that upper bounds the expected value of F (θ k+1 ) in Algorithm 1. Using this lemma, we prove sub-linear convergence for the inner loop of DP-CD. This gives the sub-linear convergence of our algorithm for convex losses. Under the additional hypothesis that F is strongly convex, we show that iterates of the outer loop of DP-CD converge linearly towards the (unique) minimum of F .</p><p>We recall that in Algorithm 1, iterates of the inner loop are denoted by θ 1 , . . . , θ K , and those of the outer loop by w1 , . . . , wT , with wt = 1 K K k=1 θ k for t &gt; 0. Algorithm 1 is randomized in two ways: when choosing the coordinate to update and when drawing noise. For convenience, we denote by E j [•] the expectation w.r.t. the choice of coordinate, by E η [•] the one w.r.t. the noise, and by E j,η [•] the expectation w.r.t. both. When no subscript is used, the expectation is taken over all random variables. We will also use the notation E j,η [•|θ k ] for the conditional expectation of a random variable, given a realization of θ k .</p></div>
<div><head>C.2.1 DESCENT LEMMA</head><p>We begin by proving Lemma C.1, which decomposes the change of a function F when updating its argument θ ∈ R p , in relation to a vector w ∈ R p , into two parts: one that remains fixed, corresponding to the unchanged entries of θ, and a second part corresponding to the objective decrease due to the update. At this point, the vector w is arbitrary, but we will later choose w to be a minimizer of F , that is a solution to (20).</p><p>Lemma C.1. Let , f, ψ, and F be defined as in Section C.1. Take a random variable θ ∈ R p and two arbitrary vectors w, g ∈ R p . Let a random variable j, taking its values uniformly randomly in [p], Choose γ 1 , . . . , γ p &gt; 0 and Γ = diag(γ 1 , . . . , γ p ). It holds that</p><formula xml:id="formula_59">E j [F (θ -γ j g j e j ) -F (w)|θ] - p -1 p (F (θ) -F (w)) ≤ 1 p f (θ) -f (w) + ∇f (θ), -Γg + 1 2 Γg 2 M + ψ(θ -Γg) -ψ(w) . (<label>21</label></formula><formula xml:id="formula_60">)</formula><p>Remark C.2. To avoid notational clutter, we will write γ j g j instead of γ j g j e j throughout this section.</p><p>Proof. We start the proof by finding an upper bound on E j [F (θ -γ j g j e j ) -F (w)|θ], using the M -component-smoothness of f :</p><formula xml:id="formula_61">E j [F (θ -γ j g j e j ) -F (w)|θ] = p j=1 1 p (F (θ -γ j g j ) -F (w)) (22) F =f +ψ = 1 p p j=1 f (θ -γ j g j ) -f (w) + ψ(θ -γ j g j ) -ψ(w) (23) f smooth ≤ 1 p p j=1 f (θ) + ∇f (θ), -γ j g j + 1 2 γ j g j 2 M -f (w) + ψ(θ -γ j g j ) -ψ(w) (24) = f (θ) -f (w) + 1 p p j=1 ∇f (θ), -γ j g j + 1 2 γ j g j 2 M + (ψ(θ -γ j g j ) -ψ(w)) (25) = f (θ) -f (w) + 1 p ∇f (θ), -Γg + 1 2p Γg 2 M + 1 p p j=1 (ψ(θ -γ j g j ) -ψ(w)) . (<label>26</label></formula><formula xml:id="formula_62">)</formula><p>The regularization terms can now be reorganized using the separability of ψ, as done by <ref type="bibr" target="#b47">(Richtárik &amp; Takáč, 2014)</ref>. Indeed, we notice that</p><formula xml:id="formula_63">p j=1 (ψ(θ -γ j g j ) -ψ(w)) = p j=1 ψ j (θ j -γ j g j ) -ψ j (w j ) + j =j ψ j (θ j ) -ψ(w j ) (27) = ψ(θ -Γg) -ψ(w) + (p -1)(ψ(θ) -ψ(w)) .<label>(28)</label></formula><p>Plugging ( <ref type="formula" target="#formula_63">28</ref>) in ( <ref type="formula" target="#formula_61">26</ref>) results in the following:</p><formula xml:id="formula_64">E j [F (θ -γ j g j e j ) -F (w)|θ] ≤ f (θ) -f (w) + 1 p ∇f (θ), -Γg + 1 2p Γg 2 M + 1 p (ψ(θ -Γg) -ψ(w)) + p -1 p (ψ(θ) -ψ(w)) (29) = 1 p f (θ) -f (w) + ∇f (θ), -Γg + 1 2 Γg 2 M + ψ(θ -Γg) -ψ(w) + p -1 p (f (θ) + ψ(θ) -f (w) -ψ(w)) ,<label>(30)</label></formula><p>which gives the lemma since F = f + ψ.</p><p>To exploit this result, we need to upper bound the right hand side of ( <ref type="formula" target="#formula_59">21</ref>) for the realizations of θ k in Algorithm 1. This is where our proof differs from classical convergence proofs for coordinate descent methods. Namely, we rewrite the right hand side of ( <ref type="formula" target="#formula_59">21</ref>) so as to obtain telescopic terms plus a bias term resulting from the addition of noise, as shown in Lemma C.3.</p><p>Lemma C.3. Let , f, ψ, and F defined as in Section C.1. For k &gt; 0, let θ k and θ k+1 be two consecutive iterates of the inner loop of Algorithm 1, γ 1 = 1 M1 , . . . , γ p = 1 Mp &gt; 0 the coordinate-wise step sizes (where M j are the coordinatewise smoothness constants of f ), and g j = 1 γj (θ k+1 j -θ k j ). Let w ∈ R p an arbitrary vector and σ 1 , . . . , σ p &gt; 0 the coordinate-wise noise scales given as input to Algorithm 1. It holds that</p><formula xml:id="formula_65">E j,η F (θ k+1 ) -F (w) θ k - p -1 p (F (θ k ) -F (w)) ≤ 1 2 θ k -w 2 Γ -1 -1 2 E j,η θ k+1 -w 2 Γ -1 θ k + 1 p σ 2 Γ ,<label>(31)</label></formula><p>where σ 2 Γ = p j=1 γ j σ 2 j and the expectations are taken over the random choice of j and η, conditioned upon the realization of θ k .</p><p>Proof. We define g the vector (g 1 , . . . , g p ) ∈ R p with g j = 1 γj (θ k+1 j -θ k j ) when coordinate j is chosen in Algorithm 1. We also denote by Γ = diag(γ 1 , . . . , γ p ) the diagonal matrix having the step sizes as its coefficients.</p><p>From Lemma C.1 with θ = θ k , w = w and g = g as defined above we obtain</p><formula xml:id="formula_66">E j F (θ k -γ j g j e j ) -F (w) θ k - p -1 p (F (θ k ) -F (w)) ≤ 1 p f (θ k ) -f (w) + ∇f (θ k ), -Γg + 1 2 Γg 2 M + ψ(θ k -Γg) -ψ(w) .<label>(32)</label></formula><p>We can upper bound the right hand term of (32) using the convexity of f and ψ:</p><formula xml:id="formula_67">f (θ k ) -f (w) + ∇f (θ k ), -Γg + 1 2 Γg 2 M + ψ(θ k -Γg) -ψ(w)<label>(33)</label></formula><formula xml:id="formula_68">≤ ∇f (θ k ), θ k -w + ∇f (θ k ), -Γg + 1 2 Γg 2 M + ∂ψ(θ k -Γg), θ k -Γg -w (34) = ∇f (θ k ) + ∂ψ(θ k -Γg), θ k -Γg -w + 1 2 Γg 2 M ,<label>(35)</label></formula><p>where we use the slight abuse of notation ∂ψ(θ k -Γg) to denote any vector in the subdifferential of ψ at the point θ k -Γg.</p><p>We now rewrite the dot product:</p><formula xml:id="formula_69">∇f (θ k ) + ∂ψ(θ k -Γg), θ k -Γg -w + 1 2 Γg 2 M (36) = g, θ k -Γg -w + 1 2 Γg 2 M + ∇f (θ k ) + ∂ψ(θ k -Γg) -g, θ k -Γg -w (37) = g, θ k -w -g 2 Γ + 1 2 g 2 Γ 2 M "descent" term + ∇f (θ k ) + ∂ψ(θ k -Γg) -g, θ k -Γg -w "noise" term ,<label>(38)</label></formula><p>where the second equality follows from g, -Γg = -g 2 Γ and Γg</p><formula xml:id="formula_70">2 M = g 2 Γ 2 M .</formula><p>We split (38) into two terms: a "descent" term and a "noise" term.</p><p>Rewriting the "descent" term. We first focus on the "descent" term. As γ j = 1 Mj for all j ∈ [p], it holds that γ<ref type="foot" target="#foot_2">2</ref> j M j = γ j which gives -g</p><formula xml:id="formula_71">2 Γ + 1 2 g 2 Γ 2 M = -g 2 Γ + 1 2 g 2 Γ = -1 2 g 2 Γ .</formula><p>We can now rewrite the "descent" term as a difference of two norms, materializing the distance to w, weighted by the inverse of the step sizes Γ -1 :</p><formula xml:id="formula_72">"descent" term = g, θ k -w - 1 2 g 2 Γ (39) = Γg, θ k -w Γ -1 - 1 2 Γg 2 Γ -1 (40) = 1 2 θ k -w 2 Γ -1 - 1 2 θ k -w 2 Γ -1 + Γg, θ k -w Γ -1 - 1 2 Γg 2 Γ -1 (41) = 1 2 θ k -w 2 Γ -1 - 1 2 θ k -Γg -w 2 Γ -1 ,<label>(42)</label></formula><p>where we factorized the norm to obtain the last inequality. We can rewrite (42) as an expectation over the random choice of the coordinate j (drawn uniformly in [p]), given the realizations of θ k and of the noise η (which determines g):</p><formula xml:id="formula_73">1 2 θ k -w 2 Γ -1 - 1 2 θ k -Γg -w 2 Γ -1 = p 2 ×   1 p p j=1 γ -1 j θ k j -w j 2 -γ -1 j θ k j -γ j g j -w j 2   (43) = p 2 × E j γ -1 j θ k j -w j 2 -γ -1 j θ k j -γ j g j -w j 2 θ k , η .<label>(44)</label></formula><p>Finally, we remark that γ -1</p><formula xml:id="formula_74">j θ k j -w j 2 -γ -1 j θ k j -γ j g j -w j 2 = θ k -w</formula><p>coordinate changes between the two vectors, and the squared norm</p><formula xml:id="formula_75">• 2 Γ -1 is separable. We thus obtain "descent" term = E j p 2 θ k -w 2 Γ -1 - p 2 θ k -γ j g j -w 2 Γ -1 θ k , η<label>(45)</label></formula><formula xml:id="formula_76">= p 2 θ k -w 2 Γ -1 - p 2 E j θ k+1 -w 2 Γ -1 θ k , η .<label>(46)</label></formula><p>Upper bounding the "noise" term. We now upper bound the "noise" term in (38). We first recall the definition of the noisy proximal update g j (line 7 of Algorithm 1), and define its non-noisy counterpart gj :</p><formula xml:id="formula_77">g j = γ -1 j prox γj ψj (θ k j -γ j (∇ j f (θ k ) + η j )) -θ k j (47) gj = γ -1 j prox γj ψj (θ k j -γ j (∇ j f (θ k )) -θ k j .<label>(48)</label></formula><p>For an update of the coordinate j ∈ [p], the optimality condition of the proximal operator gives, for η j the realization of the noise drawn at the current iteration when coordinate j is chosen:</p><formula xml:id="formula_78">0 ∈ θ k+1 j -θ k j + γ j (∇ j f (θ k ) + η j )) + 1 M j ∂ψ j (θ k j -γ j g j ) (49) = γ j × 1 γ j (θ k+1 j -θ k j ) + ∇ j f (θ k ) + η j + ∂ψ j (θ k j -γ j g j ) .<label>(50)</label></formula><p>As such, there exists a real number v j ∈ ∂ψ j (θ k j -γ j g j ) such that</p><formula xml:id="formula_79">g j = -1 γj (θ k+1 j -θ k j ) = ∇ j f (θ k ) + η j + v j .</formula><p>We denote by v ∈ R p the vector having this v j as j-th coordinate. Recall that ψ is separable, therefore v ∈ ∂ψ(θ k -Γg). The "noise" term of ( <ref type="formula" target="#formula_69">38</ref>) can be thus be rewritten using v:</p><formula xml:id="formula_80">"noise" term = ∇f (θ k ) + v -g, θ k -Γg -w = η, θ k -Γg -w ,<label>(51)</label></formula><p>and we now separate this term in two using g:</p><formula xml:id="formula_81">"noise" term = p j=1 η j (θ k j -γ j g j -w j ) = p j=1 η j (θ k j -γ j g j -w j ) + p j=1 η j (γ j g j -γ j g j ) .<label>(52)</label></formula><p>It is now time to consider the expectation with respect to the noise of these terms. First, as g j is not dependent on the noise anymore, it simply holds that</p><formula xml:id="formula_82">E η p j=1 η j (θ k j -γ j g j -w j ) | θ k = p j=1 E η [η j ] (θ k j -γ j g j -w j ) = 0 .<label>(53)</label></formula><p>The last step of our proof now takes care of the following term:</p><formula xml:id="formula_83">E η p j=1 η j (γ j g j -γ j g j ) | θ k ≤ E η γ j p j=1 η j ( g j -g j ) | θ k ≤ p j=1 γ j E η |η j | | g j -g j | θ k ,<label>(54)</label></formula><p>where each inequality comes from the triangle inequality. The non-expansiveness property of the proximal operator (see <ref type="bibr" target="#b45">Parikh &amp; Boyd (2014)</ref>, Section 2.3) is now key to our result, as it yields</p><formula xml:id="formula_84">| g j -g j | = γ -1 j prox γj ψj (θ k j -γ j (∇ j f (θ k ))) -prox γj ψj (θ k j -γ j (∇ j f (θ k ) + η j )) ≤ |η j | ,<label>(55)</label></formula><p>which directly gives, as E η η 2 j = σ 2 j (and</p><formula xml:id="formula_85">σ 2 Γ = p j=1 γ j σ 2 j ), p j=1 γ j E η |η j | | g j -g j | θ k ≤ p j=1 γ j E η [|η j | |η j |] = p j=1 γ j E η η 2 j = σ 2 Γ .<label>(56)</label></formula><p>We now have everything to prove the lemma by plugging ( <ref type="formula" target="#formula_85">56</ref>) and ( <ref type="formula" target="#formula_82">53</ref>) into expected value of (52), and then ( <ref type="formula" target="#formula_81">52</ref>) and ( <ref type="formula" target="#formula_72">42</ref>) back into (38) to obtain, after using the Tower property of conditional expectations:</p><formula xml:id="formula_86">1 p E j,η f (θ k ) -f (w) + ∇f (θ k ), -Γg + 1 2 Γg 2 M + ψ(θ k -Γg) -ψ(w) θ k (57) ≤ 1 p ("descent" term + "noise" term) (58) ≤ 1 2 θ k -w 2 Γ -1 - 1 2 E j,η θ k+1 -w 2 Γ -1 θ k + 1 p σ 2 Γ ,<label>(59)</label></formula><p>which is the result of the lemma. </p><formula xml:id="formula_87">. Letting wt+1 = 1 K K k=1 θ k , it holds that E F ( wt+1 ) -F (w * ) ≤ p( wt -w * 2 M + 2(F ( wt ) -F (w * ))) 2K + σ 2 M -1 . (<label>60</label></formula><formula xml:id="formula_88">)</formula><p>Remark C.5. The term F ( wt ) -F (w * ) essentially remains in the inequality due to the composite nature of F . When</p><formula xml:id="formula_89">ψ = 0, M -component-smoothness of f (•; d) (for d ∈ X ) gives f ( wt ) ≤ f (w * ) + ∇f (w * ), wt -w * + 1 2 wt -w * 2 M = f (w * ) + 1 2 wt -w * 2 M ,<label>(61)</label></formula><p>and the result of Lemma C.4 further simplifies as:</p><formula xml:id="formula_90">E F ( wt+1 ) -F (w * ) ≤ p wt -w * 2 M K + σ 2 M -1 .<label>(62)</label></formula><p>Proof. Summing Lemma C.3 for k = 0 to k = K and w = w * , taking expectation with respect to all choices of coordinate and random noise and using the tower property gives:</p><formula xml:id="formula_91">K-1 k=0 E F (θ k+1 ) -F (w * ) - p -1 p K-1 k=0 E (F (θ k ) -F (w * )) ≤ K-1 k=0 1 2 E θ k -w * 2 Γ -1 - 1 2 E θ k+1 -w * 2 Γ -1 + 1 p σ 2 Γ (63) = 1 2 E w0 -w * 2 Γ -1 - 1 2 E θ K -w * 2 Γ -1 + K p σ 2 Γ .<label>(64)</label></formula><p>Remark that</p><formula xml:id="formula_92">K-1 k=0 E F (θ k ) -F (w * ) = K k=1 E F (θ k ) -F (w * ) + (F ( w0 ) -F (w * )) -E F (θ K ) -F (w * )</formula><p>, then as E F (θ K ) -F (w * ) ≥ 0, we obtain a lower bound on the left hand side of (64):</p><formula xml:id="formula_93">K-1 k=0 E F (θ k+1 ) -F (w * ) -p-1 p K-1 k=0 E (F (θ k ) -F (w * )) ≥ 1 p K k=1 E F (θ k ) -F (w * ) -(F ( w0 ) -F (w * )) . (65) As wt+1 = 1 K K k=1 θ k , the convexity of F gives F ( wt+1 ) ≤ 1 K K k=1 F (θ k ) -F (w * ).</formula><p>Plugging this inequality into (65) and combining the result with (64) gives</p><formula xml:id="formula_94">F ( wt+1 ) -F (w * ) ≤ p( 1 2 w0 -w * 2 Γ -1 + F ( w0 ) -F (w * )) K + σ 2 Γ .<label>(66)</label></formula><p>C.2.4 STRONGLY CONVEX CASE Theorem 3.3 (Strongly-convex case). Let F be µ M -strongly convex w.r.t. • M and w * be the minimizer of F . The output w priv of DP-CD (Algorithm 1), starting from w0 ∈ R p with T &gt; 0, K = 2p(1 + 1/µ M ) and the σ j 's as in Theorem 3.1, satisfies:</p><formula xml:id="formula_95">F (w priv ) -F (w * ) ≤ F ( w0 ) -F (w * ) 2 T + 24p(1 + 1/µ M )T L 2 M -1 log(1/δ) n 2 2 . (<label>70</label></formula><formula xml:id="formula_96">)</formula><formula xml:id="formula_97">Setting T = log 2 32n 2 2 (F ( w0 )-F (w * )) p(1+1/µ M ) L 2 M -1 log(1/δ) yields: E F (w priv ) -F (w * ) ≤ 1 + log 2 (F ( w0 ) -F (w * ))n 2 2 24p(1 + 1/µ M ) L 2 M -1 log(1/δ) 24p(1 + 1/µ M ) L 2 M -1 log(1/δ) n 2 2 (71) = O p L 2 M -1 log(1/δ) µ M n 2 2 log 2 (F ( w0 ) -F (w * ))n µ M p L M -1 log(1/δ) . (<label>72</label></formula><formula xml:id="formula_98">)</formula><p>Proof. As F is µ M -strongly-convex with respect to norm • M , we obtain for any w ∈ R p , that F (w) ≥ F (w * ) +</p><formula xml:id="formula_99">µ M 2 w -w * 2 M . Therefore, F ( w0 ) -F (w * ) ≤ 2 µ M w0 -w * 2 M and Lemma C.4 gives, for 1 ≤ t ≤ T -1, F ( wt+1 ) -F (w * ) ≤ (1 + 1/µ M )p(F ( wt ) -F (w * )) K + σ 2 M .<label>(73)</label></formula><p>It remains to set K = 2p(1 + 1/µ M ) to obtain</p><formula xml:id="formula_100">F ( wt+1 ) -F (w * ) ≤ F ( wt ) -F (w * ) 2 + σ 2 M .<label>(74)</label></formula><p>Recursive application of this inequality gives</p><formula xml:id="formula_101">E F ( wT ) -F (w * ) ≤ F ( w0 ) -F (w * ) 2 T + T -1 t=0 1 2 t σ 2 M ≤ F ( w0 ) -F (w * ) 2 T + 2 σ 2 M ,<label>(75)</label></formula><p>where we upper bound the sum by the value of the complete series. It remains to replace σ 2 M by its value to obtain the result. Taking T = log 2</p><formula xml:id="formula_102">(F ( w0 )-F (w * ))n 2 2 24p(1+1/µ M ) L 2 M -1 log(1/δ) then gives E F ( wT ) -F (w * ) ≤ 1 + log 2 (F ( w0 ) -F (w * ))n 2 2 24p(1 + 1/µ M ) L 2 M -1 log(1/δ) 24p(1 + 1/µ M ) L 2 M -1 log(1/δ) n 2 2 (76) = O p L 2 M -1 log(1/δ) µ M n 2 2 log 2 (F ( w0 ) -F (w * ))n µ M p L M -1 log(1/δ) ,<label>(77)</label></formula><p>which is the result of our theorem.</p></div>
<div><head>C.3 Proof of Remark 1</head><p>We recall the notations of <ref type="bibr" target="#b53">Tappenden et al. (2016)</ref>. For θ ∈ R p , t ∈ R and j</p><formula xml:id="formula_103">∈ [p], let V j (θ, t) = ∇ j (θ)t + Mj 2 |t| 2 + ψ j (θ k j + t).</formula><p>For η ∈ R, we also define its noisy counterpart, V η j (θ, t) = (∇ j (θ) + η)t + Mj 2 |t| 2 + ψ j (θ k j + t). We aim at finding δ j such that for any θ k ∈ R p used in the inner loop of Algorithm 1:</p><formula xml:id="formula_104">E ηj V j (θ k , -γ j g j ) ≤ min g∈R V j (θ k , -γ j g) + δ j ,<label>(78)</label></formula><p>where the expectation is taken over the random noise η j , and -γ j g j = prox γj ψj (θ k j -γ j (∇ j f (θ k ) + η j )) -θ k j as defined in the analysis of Algorithm 1. We need to link the proximal operator we use in DP-CD with the quantity V ηj j that we just defined:</p><formula xml:id="formula_105">prox γj ψj (θ k j -γ j (∇ j f (θ k ) + η j )) = arg min v∈R 1 2 v -θ k j + γ j (∇ j f (θ k ) + η j ) 2 2 (79) = arg min v∈R γ j (∇ j f (θ k j ) + η j ), v -θ k j + 1 2 v -θ k j 2 2 + γ j ψ j (v) (80) = arg min v∈R ∇ j f (θ k ) + η j , v -θ k j + M j 2 v -θ k j 2 2 + ψ j (v) (81) = θ k j + arg min t∈R ∇ j f (θ k ) + η j , t + M j 2 t 2 2 + ψ j (θ k j + t) .<label>(82)</label></formula><p>Which means that -γ j g j = prox γj ψj (θ</p><formula xml:id="formula_106">k j -γ j (∇ j f (θ k )+η j ))-θ k j ∈ arg min t∈R V ηj j (θ k , t). Let -γ j g * j = prox γj ψj (θ k j - γ j ∇ j (θ k )) -θ k j be the non-noisy counterpart of -γ j g j . Since -γ j g j is a minimizer of V ηj j (θ k , •), it holds that V ηj j (θ k , -γ j g j ) ≤ ∇ j f (θ k ) + η j , -γ j g * j + M j 2 -γ j g * j 2 2 + ψ j (θ k j + -γ j g * j ) (83) = min t V j (θ k , t) + η j , -γ j g * j ,<label>(84)</label></formula><p>which can be rewritten as V j (θ k , -γ j g j ) ≤ min t V j (θ k , t) + η j , γ j (g j -g * j ) . Taking the expectation yields</p><formula xml:id="formula_107">E ηj V j (θ k , -γ j g j ) ≤ min t V j (θ k , t) + E ηj η j , γ j (g j -g * j ) .<label>(85)</label></formula><p>Finally, we remark that g j -g * j ≤ |γ j η j | and the non-expansiveness of the proximal operator gives</p><formula xml:id="formula_108">E ηj V j (θ k , -γ j g j ) ≤ min t V j (θ k , t) + γ j σ 2 j ,<label>(86)</label></formula><p>which implies an upper bound on the expectation of δ</p><formula xml:id="formula_109">j : E j,ηj [δ j ] = 1 p p j=1 E ηj [δ j ] ≤ 1 p p j=1 γ j σ 2 j = 1 p p</formula><p>j=1 σ 2 j /M j , when γ j = 1/M j . In the formalism of <ref type="bibr" target="#b53">Tappenden et al. (2016)</ref>, this amounts to setting α = 0 and β = 1 p σ 2 M -1 .</p><p>When n &gt; n * . We get the result in that case by augmenting the dataset D * that we constructed in the first part of this proof.</p><p>To do so, we follow the steps described by <ref type="bibr" target="#b3">Bassily et al. (2014)</ref> in the proof of their Lemma 5.1. The construction consists in choosing a vector c ∈ X , and adding n-n * 2 rows with c, and n-n * 2 rows with -c to the dataset D * . This results in a dataset D such that</p><formula xml:id="formula_110">n i=1 d i = Ω(n * L 2 ) = Ω( √ p L 2 )</formula><p>, since the contributions of rows -c and c (almost) cancel out. The theorem follows from observing that ( n * n α, β)-accuracy on this augmented dataset implies (α, β)-accuracy on the original dataset. As such, if an algorithm is both private and ( n * n α, β)-accurate on the dataset D , we get a contradiction, which gives the theorem as n * n = √ p n . Remark E.6. Without the assumption on the distribution of the L j 's, we can still get an inequality that resembles (103):</p><formula xml:id="formula_111">A(D aug L ) -q(D aug L ) 2 (100) ≥ j∈J 4L 2 j 9 ≥ 2 27</formula></div>
<div><head>Lmin</head><p>Lmax L 2 , with probability at least 1/3, and we get a result similar to Theorem E.5, except with an additional multiplicative factor L min /L max .</p></div>
<div><head>E.3 Lower Bound for Convex Functions</head><p>To prove a lower bound for our problem in the convex case, we let </p><formula xml:id="formula_112">L 1 , • • • , L p &gt;</formula><formula xml:id="formula_113">w * = arg min w∈R p F (w; D) = - 1 n w, n i=1 d i + n i=1 d i 2 βn w 2 2 ,<label>(105)</label></formula><p>To find the solution of (105), we look for w * so that the objective's gradient is zero, that is</p><formula xml:id="formula_114">w * = β n i=1 d i 2 n i=1 d i ,<label>(106)</label></formula><formula xml:id="formula_115">so that w * 2 = β n i=1 di 2 n i=1 d i 2 = β.</formula><p>To prove the lower bound, we remark that</p><formula xml:id="formula_116">F (w; D) -F (w * ; D) = - 1 n w -w * , n i=1 d i + n i=1 d i 2βn ( w 2 2 -w * 2 2 ) (107) = - 1 n w -w * , n i=1 d i β w * + n i=1 d i 2βn ( w 2 2 -w * 2 2 ) (108) = n i=1 d i βn w * -w, w * + 1 2 w 2 2 - 1 2 w * 2 2 (109) = n i=1 d i βn -w, w * + 1 2 w 2 2 + 1 2 w * 2 2 (110) = n i=1 d i 2βn w -w * 2 2 . (<label>111</label></formula><formula xml:id="formula_117">)</formula><p>At this point, we can proceed similarly to <ref type="bibr" target="#b3">Bassily et al. (2014)</ref> to relate this quantity to private estimation of one-way marginals. We let M = Ω(min(n L 2 , L 2 √ p/ )) and A be an ( , δ)-differentially private mechanism that outputs a private solution w priv to (105). Suppose, for the sake of contradiction, that for every dataset D with n i=1 d i 2 ∈ [M -1; M + 1], it holds with probability at least 2/3 that w priv -w * = Ω(β) .</p><p>(112)</p><p>We now derive from A a mechanism A to estimate one-way marginals. To do this, A runs A to obtain w priv and outputs M nβ w priv . We obtain that with probability at least 2/3,</p><formula xml:id="formula_118">A(D) -q(D) 2 = M nβ w priv - β M n i=1 d i 2 = Ω M n = Ω min L 2 , L 2 √ p n . (<label>113</label></formula><formula xml:id="formula_119">)</formula><p>where q(D) = 1 n n i=1 d i . This is in contradiction with Theorem E.5. We thus proved that w priv -w * = Ω(β), with probability at least 1/3. As a consequence, we now obtain that with probability at least 1/3,</p><formula xml:id="formula_120">F (w priv ; D) -F (w * ; D) = n i=1 d i 2βn w priv -w * 2 2 = Ω min L 2 β, β L 2 √ p n ,<label>(114)</label></formula><p>which gives the desired result on the expectation of F (w priv ; D) -F (w * ; D).</p><p>Finally, if we do not make any hypothesis on the L j 's distribution, we can directly use the non-augmented dataset constructed by <ref type="bibr" target="#b12">Bun et al. (2014)</ref> to prove Lemma E.4 (that is the dataset from Theorem E.5, rescaled but not augmented). The 2 -norm of the sum of this dataset is</p><formula xml:id="formula_121">n i=1 d j 2 = [M -1, M + 1] with M = Ω min Lmin Lmax n L 2 , Lmin Lmax √ p L 2</formula><p>. This holds since four columns of this dataset out of five have sum of ±nL j (for some j's), but no lower bound on the sum of the remaining columns can be derived. Thus, assuming (112) holds, then (113) can be rewritten as</p><formula xml:id="formula_122">A(D) -q(D) 2 = M nβ w priv - β M n i=1 d i 2 = Ω M n = Ω min L min L max L 2 , L min L max L 2 √ p n ,<label>(115)</label></formula><p>with probability at least 1/3, which is in contradiction with Remark E.6. We thus get an additional factor of L min /L max in the lower bound:  </p></div>
<div><head>G.2 Running Time</head><p>In this section, we report the running times of DP-CD and DP-SGD. We implemented DP-CD and DP-SGD in C++, with Python bindings 8 . The design matrix and the labels are kept in memory as dense matrices of the <software>Eigen library</software>. No special code optimization nor tricks is applied to the algorithms, except for the update of residuals at each iteration of DP-CD, which prevents from accessing the complete dataset at each step. All experiments were run on a laptop with 16GB of RAM and an Intel(R) Core(TM) i7-10610U CPU @ 1.80GHz.</p><p>Figure <ref type="figure">3</ref> shows the same experiments as in Figure <ref type="figure" target="#fig_1">1</ref> and Figure <ref type="figure" target="#fig_3">2</ref>, but as a function of the running time. In our implementation, DP-CD runs about 4 times as fast as DP-SGD for a given number of iterations (see Figure <ref type="figure">3a</ref> and Figure <ref type="figure">3b</ref> for 50 iterations).</p><p>On the three other plots, Figure <ref type="figure">3c</ref>, Figure <ref type="figure">3d</ref> and Figure <ref type="figure">3e</ref>, DP-CD yields better results in less iterations. DP-CD is thus particularly valuable in these scenarios: combined with its faster running time, it provides accurate results extremely fast.</p><p>For completeness, we provide in Table <ref type="table" target="#tab_4">3</ref> the full table of running time, corresponding to Table <ref type="table" target="#tab_3">2</ref> and<ref type="table" target="#tab_4">Figure 3</ref>. These results show that, for a given number of passes on the data, DP-CD consistently runs about 5 times faster than DP-SGD. </p></div><figure xml:id="fig_1"><head>Figure 1 .</head><label>1</label><figDesc>Figure1. Relative error to non-private optimal for DP-CD (blue), DP-CD with privately estimated coordinate-wise smoothness constants (green), DP-SGD (orange) and DP-SCD (red, only applicable to the smooth case) on two imbalanced problems. The number of passes is tuned separately for each algorithm to achieve lowest error. We report min/mean/max values over 10 runs.</figDesc></figure>
<figure xml:id="fig_3"><head>Figure 2 .</head><label>2</label><figDesc>Figure2. Relative error to non-private optimal for DP-CD (blue), DP-SGD (orange) and DP-SCD (red, only applicable to the smooth case) on three balanced problems. The number of passes is tuned separately for each algorithm to achieve lowest error. We report min/mean/max values over 10 runs.</figDesc></figure>
<figure xml:id="fig_4"><head>Definition B. 1 (</head><label>1</label><figDesc>Rényi divergence, van Erven &amp; Harremoës 2014). For two random variables Y and Z with values in the same domain C, the Rényi divergence is, for α &gt; 1,</figDesc></figure>
<figure xml:id="fig_5"><head /><label /><figDesc>0 and define a dataset D = {d 1 , . . . , d n } taking its values in a set X = p j=1 {±L j }. For β &gt; 0, we consider the problem (1) with the convex, smooth and L-component-Lipschitz loss function (w; d) = -w, d and the convex, separable regularizer ψ(w) =</figDesc></figure>
<figure xml:id="fig_6"><head>FF</head><label /><figDesc>(w priv ; D) -F (w * ; D) Bound for Strongly-Convex Functions To prove a lower bound for strongly-convex functions, we let µ I &gt; 0, L 1 , . . . , L p &gt; 0, andD = {d 1 , . . . , d n } ∈ p j=1 {± Lj 2µ I }.We consider the following problem, which fits in our setting: w * = arg min w∈R p i W is the (separable) characteristic function of the set W. Since ψ = i W is the characteristic function of a box-set, the proximal operator is equal to the projection on W and DP-CD iterates are thus guaranteed to remain in W. Therefore, regularity assumptions on f only need to hold on W, as pointed out in Remark 2.1. The loss function (w;d i ) = µ I 2 w -d i22 is L-component-Lipschitz on W since, for w ∈ W and j ∈ [p], the triangle inequality gives:|∇ j (w; d i )| ≤ µ I (|w j | + |d i,j |) ≤ µ I L j 2µ I + L j 2µ I ≤ L j . (118)This loss is also µ I -strongly convex w.r.t. 2 -norm since for w, w ∈ W, w -d i , w -w + w -w µ I -strong convexity since (w ;d i ) = µ I 2 w -d i 2 2 and ∇ (w ; d i ) = µ I (w -d i ).The minimum of the objective function in (117) is attained at w * = 1 n n i=1 d i = q(D) ∈ W. The excess risk of F is thus F (w; D) -F (w * ) 2 + w * , w * -w (122)</figDesc></figure>
<figure type="table" xml:id="tab_0"><head /><label /><figDesc><ref type="bibr" target="#b62">Zhang, 2014)</ref>. DP-SGD has become the standard approach to DP-ERM. In our work, we show that coordinatewise updates can have lower sensitivity than DP-SGD updates and propose a DP-CD algorithm achieving competitive results. A private variant of the Frank-Wolfe algorithm (DP-FW) was also proposed to solve constrained DP-ERM problems<ref type="bibr" target="#b52">(Talwar et al., 2015)</ref>. Although these algorithms achieve a good privacy-utility trade-off in theory, we are not aware of any empirical evaluation. DP-FW algorithms access gradients indirectly through a linear optimization oracle over a constrained set. Restricting to a constrained set is not necessary in DP-CD, allowing its use for a different family of problems.</figDesc><table><row><cell>Relative Error to Non-Private Opt</cell></row><row><cell>DP-ERM. Differentially Private Empirical Risk Minimiza-</cell></row><row><cell>tion was first studied by Chaudhuri et al. (2011), using</cell></row><row><cell>output perturbation (adding noise to the solution of the non-</cell></row><row><cell>private ERM problem) and objective perturbation (adding</cell></row><row><cell>noise to the ERM objective itself). Bassily et al. (2014) then</cell></row><row><cell>proposed DP-SGD and proved its near-optimality. Wang</cell></row><row><cell>et al. (2017) obtained faster convergence rates using a DP</cell></row><row><cell>version of the SVRG algorithm (Johnson &amp; Zhang, 2013;</cell></row><row><cell>Xiao &amp; Coordinate descent. Coordinate descent (CD) algorithms</cell></row><row><cell>have a long history in optimization. Luo &amp; Tseng (1992);</cell></row><row><cell>Tseng (2001); Tseng &amp; Yun (2009) have shown convergence</cell></row><row><cell>results for (block) CD algorithms for nonsmooth optimiza-</cell></row><row><cell>tion. Nesterov (2012) later proved a global non-asymptotic</cell></row><row><cell>1/k convergence rate for CD with random choice of coordi-</cell></row><row><cell>nates for a convex, smooth objective. Parallel, proximal vari-</cell></row><row><cell>ants were developed by Richtárik &amp; Takáč (2014); Fercoq</cell></row><row><cell>&amp; Richtárik (2015), while Hanzely et al. (2018) further con-</cell></row></table><note><p><p><p><p><p><p><p>DP-SCO. Recent work has also studied algorithms and utility guarantees for stochastic convex optimization under differential privacy constraints, a problem very similar to DP-ERM. Bassily et al. (2019) (following work from Hardt et al., 2016; Bassily et al., 2020) extended results known for DP-ERM to this setting, showing that the population risk of DP-SCO is asymptotically equivalent to the one of non-private SCO. Efficient algorithms for DP-SCO were proposed by Feldman et al. (2020); Wang et al. (2022), and Asi et al. (2021); Bassily et al. (2021) studied stochastic variants of DP-FW. As detailed by Dwork et al. (2015); Bassily et al. (2016); Jung et al. (</p>2021</p>) results from DP-ERM can be converted to DP-SCO. sidered non-separable non-smooth parts. Shalev-Shwartz &amp; Zhang (2013) introduced Dual CD algorithms for smooth ERM, showing performance similar to SVRG. We refer to</p><ref type="bibr" target="#b61">Wright (2015)</ref> </p>and</p><ref type="bibr" target="#b50">Shi et al. (2017)</ref> </p>for detailed reviews on</p></note></figure>
<figure type="table" xml:id="tab_2"><head /><label /><figDesc>C.2.2 CONVERGENCE LEMMALemma C.3 allows us to prove a result on the mean of K consecutive noisy coordinate-wise gradient updates, by simply summing it and rewriting the terms. This gives Lemma C.4, which is the key lemma of our proof. Lemma C.4. Assume (•, d) is convex, L-component-Lipschitz and M -component-smooth for all d ∈ X , ψ is convex and separable, such that F = f + ψ and w * is a minimizer of F . For t ∈ [T ], consider the K successive iterates θ 1 , . . . , θ K computed from the inner loop of Algorithm 1 starting from the point wt , with step sizes γ j = 1 Mj and noise scales σ j</figDesc><table /></figure>
<figure type="table" xml:id="tab_3"><head>Table 2 .</head><label>2</label><figDesc>Relative error to non-private optimal value of the objective function for different number of passes on the data. Results are reported for each dataset and for DP-CD and DP-SGD, after tuning step size and clipping hyperparameters. A star indicates the lowest error in each row.</figDesc><table><row><cell /><cell>Passes on data</cell><cell>2</cell><cell>5</cell><cell>10</cell><cell>20</cell><cell>50</cell></row><row><cell>Electricity (imbalanced)</cell><cell>DP-CD</cell><cell>0.1458 ± 6e-04</cell><cell>0.0842 ± 1e-03</cell><cell>0.0436 ± 2e-03</cell><cell>0.0147 ± 2e-03</cell><cell>0.0020 ± 1e-03*</cell></row><row><cell>= 1, δ = 1/n 2</cell><cell>DP-SGD</cell><cell>0.2047 ± 2e-02</cell><cell>0.1804 ± 2e-02</cell><cell>0.1766 ± 2e-02</cell><cell>0.1644 ± 2e-02</cell><cell>0.1484 ± 1e-02*</cell></row><row><cell>Electricity (balanced)</cell><cell>DP-CD</cell><cell>0.0186 ± 4e-04</cell><cell>0.0023 ± 4e-04</cell><cell>0.0013 ± 6e-04*</cell><cell>0.0013 ± 4e-04</cell><cell>0.0019 ± 8e-04</cell></row><row><cell>= 1, δ = 1/n 2</cell><cell>DP-SGD</cell><cell>0.0391 ± 1e-02</cell><cell>0.0189 ± 5e-03</cell><cell>0.0123 ± 4e-03</cell><cell>0.0106 ± 3e-03</cell><cell>0.0040 ± 2e-03*</cell></row><row><cell>California (imbalanced)</cell><cell>DP-CD</cell><cell>0.1708 ± 7e-03</cell><cell>0.1232 ± 1e-02</cell><cell>0.0598 ± 1e-02</cell><cell>0.0287 ± 5e-03</cell><cell>0.0124 ± 7e-03*</cell></row><row><cell>= 1, δ = 1/n 2</cell><cell>DP-SGD</cell><cell>0.2799 ± 9e-02</cell><cell>0.1863 ± 2e-02</cell><cell>0.1476 ± 2e-02</cell><cell>0.1094 ± 2e-02</cell><cell>0.1068 ± 2e-02*</cell></row><row><cell>California (balanced)</cell><cell>DP-CD</cell><cell>0.0007 ± 3e-04*</cell><cell>0.0011 ± 6e-04</cell><cell>0.0012 ± 5e-04</cell><cell>0.0010 ± 1e-04</cell><cell>0.0017 ± 1e-03</cell></row><row><cell>= 1, δ = 1/n 2</cell><cell>DP-SGD</cell><cell>0.0351 ± 2e-02</cell><cell>0.0226 ± 8e-03</cell><cell>0.0125 ± 3e-03</cell><cell>0.0087 ± 2e-03</cell><cell>0.0042 ± 1e-03*</cell></row><row><cell>Sparse LASSO</cell><cell>DP-CD</cell><cell>0.2498 ± 4e-02*</cell><cell>0.4702 ± 9e-02</cell><cell>0.5982 ± 4e-02</cell><cell>0.7160 ± 2e-02</cell><cell>0.7551 ± 0e+00</cell></row><row><cell>= 10, δ = 1/n 2</cell><cell>DP-SGD</cell><cell>0.7551 ± 0e+00</cell><cell>0.7551 ± 3e-09*</cell><cell>0.7551 ± 0e+00</cell><cell>0.7551 ± 0e+00</cell><cell>0.7551 ± 0e+00</cell></row></table></figure>
<figure type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc>Time of execution (in seconds) for different number of passes on the data (averaged over 10 runs). Results are reported for each dataset and for DP-CD and DP-SGD, after tuning step size and clipping hyperparameters.</figDesc><table><row><cell /><cell>Passes on data</cell><cell>2</cell><cell>5</cell><cell>10</cell><cell>20</cell><cell>50</cell></row><row><cell>Electricity (imbalanced)</cell><cell>DP-CD</cell><cell>0.0128 ± 1e-03</cell><cell>0.0274 ± 1e-03</cell><cell>0.0500 ± 1e-03</cell><cell>0.0980 ± 7e-04</cell><cell>0.2457 ± 2e-03</cell></row><row><cell>= 1, δ = 1/n 2</cell><cell>DP-SGD</cell><cell>0.0663 ± 2e-03</cell><cell>0.1722 ± 1e-02</cell><cell>0.3321 ± 1e-02</cell><cell>0.6729 ± 1e-02</cell><cell>1.8588 ± 2e-01</cell></row><row><cell>Electricity (balanced)</cell><cell>DP-CD</cell><cell>0.0121 ± 7e-04</cell><cell>0.0281 ± 3e-03</cell><cell>0.0529 ± 2e-03</cell><cell>0.1062 ± 6e-03</cell><cell>0.2577 ± 2e-03</cell></row><row><cell>= 1, δ = 1/n 2</cell><cell>DP-SGD</cell><cell>0.0686 ± 4e-03</cell><cell>0.1768 ± 1e-02</cell><cell>0.3578 ± 2e-02</cell><cell>0.6787 ± 2e-02</cell><cell>1.6766 ± 2e-02</cell></row><row><cell>California (imbalanced)</cell><cell>DP-CD</cell><cell>0.0029 ± 9e-05</cell><cell>0.0065 ± 8e-05</cell><cell>0.0130 ± 1e-04</cell><cell>0.0258 ± 1e-04</cell><cell>0.0647 ± 2e-04</cell></row><row><cell>= 1, δ = 1/n 2</cell><cell>DP-SGD</cell><cell>0.0269 ± 1e-03</cell><cell>0.0665 ± 1e-03</cell><cell>0.1318 ± 2e-03</cell><cell>0.2628 ± 3e-03</cell><cell>0.6476 ± 8e-03</cell></row><row><cell>California (balanced)</cell><cell>DP-CD</cell><cell>0.0031 ± 2e-04</cell><cell>0.0065 ± 2e-04</cell><cell>0.0132 ± 1e-04</cell><cell>0.0262 ± 2e-04</cell><cell>0.0649 ± 3e-04</cell></row><row><cell>= 1, δ = 1/n 2</cell><cell>DP-SGD</cell><cell>0.0261 ± 7e-04</cell><cell>0.0641 ± 5e-04</cell><cell>0.1295 ± 2e-03</cell><cell>0.2592 ± 4e-03</cell><cell>0.6469 ± 7e-03</cell></row><row><cell>Sparse LASSO</cell><cell>DP-CD</cell><cell>0.0244 ± 6e-04</cell><cell>0.0760 ± 6e-04</cell><cell>0.1614 ± 4e-03</cell><cell>0.3213 ± 5e-04</cell><cell>0.6598 ± 1e-02</cell></row><row><cell>= 10, δ = 1/n 2</cell><cell>DP-SGD</cell><cell>0.0718 ± 3e-03</cell><cell>0.1788 ± 4e-03</cell><cell>0.3654 ± 7e-03</cell><cell>0.7292 ± 2e-02</cell><cell>1.8110 ± 3e-02</cell></row></table><note><p><p>8 </p>The code is available at https://gitlab.inria.fr/pmangold1/private-coordinate-descent/.</p></note></figure>
			<note place="foot" n="1" xml:id="foot_0"><p>In fact, our privacy guarantees hold even if all intermediate iterates are released (not just the final model).</p></note>
			<note place="foot" n="6" xml:id="foot_1"><p>https://gitlab.inria.fr/pmangold1/ private-coordinate-descent/</p></note>
			<note place="foot" n="2" xml:id="foot_2"><p>Γ -1 -θ k -γ j g j -w 2 Γ -1 , as only one</p></note>
			<note place="foot" n="7" xml:id="foot_3"><p>Recall that step sizes for CD algorithms are coordinate-wise, and thus larger than in SGD algorithms. We empirically verify that the best step size always lies strictly inside the considered interval for both DP-CD and DP-SGD.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>The authors would like to thank the anonymous reviewers who provided useful feedback on previous versions of this work, which helped to improve the paper.</p><p>This work was supported in part by the <rs type="funder">Inria Exploratory Action FLAMED</rs> and by the <rs type="funder">French National Research Agency (ANR)</rs> through grant <rs type="grantNumber">ANR-20-CE23-0015</rs> (Project <rs type="projectName">PRIDE</rs>) and <rs type="grantNumber">ANR-20-CHIA-0001-01</rs> (<rs type="projectName">Chaire IA CaMeLOt</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_94zSSMc">
					<idno type="grant-number">ANR-20-CE23-0015</idno>
					<orgName type="project" subtype="full">PRIDE</orgName>
				</org>
				<org type="funded-project" xml:id="_ZHPUaxp">
					<idno type="grant-number">ANR-20-CHIA-0001-01</idno>
					<orgName type="project" subtype="full">Chaire IA CaMeLOt</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div><p>We conclude the proof by using the fact that Γ j = M -1 j for all j ∈ [p], thus</p></div>
<div><head>C.2.3 CONVEX CASE</head><p>Theorem 3.3 (Convex case). Let w * be a minimizer of F and R 2 M = max( w0 -w * 2 M , F ( w0 ) -F (w * )). The output w priv of DP-CD (Algorithm 1), starting from w0 ∈ R p with T = 1, K &gt; 0 and the σ j 's as in Theorem 3.1, satisfies:</p><p>yields:</p><p>Proof. In the convex case, we iterate only once in the inner loop (since T = 1). As such, w priv = w1 , and applying Lemma C.4 with wt+1 = w1 , w t = w0 and σ j chosen as in Theorem 3.1 gives the result.</p><p>then gives</p><p>and the result follows from 2 √ 8 + 12 √ 8 ≈ 8.48 &lt; 9.</p><p>Convex functions. When the objective function F is convex, we use Lemma C.4 to obtain, since σ 2</p><p>Therefore, when F is convex, we get F (w 1 ) -F (w * ) ≤ ξ, for ξ &gt; βp, as long as</p><p>In comparison, <ref type="bibr">Tappenden et al. (2016, Theorem 5.1 therein)</ref> gives convergence to ξ &gt; 2pR 2 M β when K ≥</p><p>.</p><p>We thus gain a factor βp/2R 2 M in utility. Importantly, our utility upper bound does not depend on initialization in that setting, whereas the one of <ref type="bibr" target="#b53">Tappenden et al. (2016)</ref> does.</p><p>Strongly-convex functions. When the objective function F is µ M -strongly-convex w.r.t. to • M , then from (75) we obtain, as long as</p><p>and</p><p>. In comparison, <ref type="bibr">Tappenden et al. (2016, Theorem 5.2 therein)</ref> shows convergence to ξ &gt; βp</p><p>. We thus gain a factor µ M /2 in utility.</p></div>
<div><head>D Comparison with DP-SGD</head><p>In this section, we provide more details on the arguments of Section 3.4, where we suppose that is L-component-Lipschitz and Λ-Lipschitz. To ease the comparison, we assume that R M = w 0 -w * M , which is notably the case in the smooth setting with ψ = 0 (see Remark C.2).</p><p>Balanced. We start by the scenario where coordinate-wise smoothness constants are balanced and all equal to</p><p>We then consider the convex and strongly-convex functions separately:</p><p>which means that f is M µ M -strongly-convex with respect to • 2 . This gives</p><p>In light of the results summarized in Table <ref type="table">1</ref>, it remains to compare L 2 = p j=1 L 2 j with Λ, for which it holds that Λ ≤ p j=1 L 2 j ≤ √ pΛ, which is our result.</p><p>Unbalanced. When smoothness constants are disparate, we discuss the case where</p><p>• one coordinate of the gradient dominates the others: we assume without loss of generality that the dominating coordinate is the first one. It holds that M 1 =: M max M min =: M j , for all j = 1 and L 1 =: L max L min =: L j , for all j = 1 such that</p><p>As L 1 dominates the other component-Lipschitz constants, most of the variation of the loss comes from its first coordinate. This implies that L 1 is close to the global Lipschitz constant Λ of . As such, it holds that</p><p>• the first coordinate of w0 is already very close to its optimal value so that</p><p>We can now easily compare DP-CD with DP-SGD in this scenario. First, if is convex, then L M -1 R M ≈ Mmin Mmax ΛR I . Second, when is strongly-convex, we observe that for x, y ∈ R p ,</p><p>which implies that when</p><p>This yields, under our hypotheses,</p><p>In both cases, DP-CD can get arbitrarily better than DP-SGD, and gets better as the ratio M max /M min increases.</p><p>The two hypotheses we describe above are of course very restrictive. However, it gives some insight about when and why DP-CD can outperform DP-SGD. Our numerical experiments in Section 6 confirm this analysis, even in less favorable cases.</p></div>
<div><head>E Proof of Lower Bounds</head><p>To prove lower bounds on the utility of L-component-Lipschitz functions, we extend the proof of <ref type="bibr" target="#b3">Bassily et al. (2014)</ref> to our setting (that is, L-component-Lipschitz functions and unconstrained composite optimization). There are three main difficulties in adapting their proof:</p><p>• First, the optimization problem (1) is not constrained. We stress that while convex constraints can be enforced using the regularizer ψ (using the characteristic function of a convex set), its separable nature only allows box constraints. In contrast, <ref type="bibr" target="#b3">Bassily et al. (2014)</ref> rely on an 2 -norm constraint to obtain their lower bounds.</p><p>• Second, Lemma 5.1 of <ref type="bibr" target="#b3">Bassily et al. (2014)</ref> must be extended to our L-component-Lipschitz setting. To do so, we consider datasets with points in p j=1 {-L j , L j } rather than {-1/ √ p, 1/ √ p} p , and carefully adapt the construction of the dataset D so that</p><p>)), which is essential to prove our lower bounds.</p><p>• Third, the lower bounds of <ref type="bibr" target="#b3">Bassily et al. (2014)</ref> rely on fingerprinting codes, and in particular on the result of <ref type="bibr" target="#b12">Bun et al. (2014)</ref> which uses such codes to prove that (when n is smaller than some n * we describe later) differential privacy is incompatible with precisely and simultaneously estimating all p counting queries defined over the columns of the dataset D. In our construction, since all columns of D now have different scales, we need an additional hypothesis on the repartition of the L j 's, (i.e., that j∈J L 2 j = Ω( L 2 ) for all J ⊆ [p] of a given size), which is not required in existing lower bounds (where all columns have equal scale).</p></div>
<div><head>E.1 Counting Queries and Accuracy</head><p>We start our proof by recalling and extending to our setting the notions of counting queries (Definition E.1) and accuracy (Definition E.2), as described by <ref type="bibr" target="#b12">Bun et al. (2014)</ref>. The main feature of our definitions is that we allow the set X to have different scales for each of its coordinates, and that we account for this scale in the definition of accuracy. We denote by conv(X ) the convex hull of a set X . Definition E.1 (Counting query). Let n &gt; 0. A counting query on X is a function q : X n → conv(X ) defined using a predicate q : X → X . The evaluation of the query q over a dataset D ∈ X n is defined as the arithmetic mean of q on D:</p><p>In our proof, we will use a specific class of queries: one-way marginals (Definition E.3), that compute the arithmetic mean of a dataset along one of its column.</p><p>Definition E.3 (One-way marginals). Let X = p j=1 {-L j ; L j } or X = {0, L j } p . The family of one-way marginals on X is defined by queries with predicates q j (x) = x j for x ∈ X . For a dataset D ∈ X n of size n, we thus have</p></div>
<div><head>E.2 Lower Bound for One-Way Marginals</head><p>We can now restate a key result from <ref type="bibr" target="#b12">Bun et al. (2014)</ref>, which shows that there exists a minimal number n * of records needed in a dataset to allow achieving both accuracy and privacy on the estimation of one-way marginals on X = ({0, 1} p ) n . This lemma relies on the construction of re-identifiable distribution (see <ref type="bibr">Bun et al. 2014, Definition 2.10)</ref>. One can then use this distribution to find a dataset on which a private algorithm can not be accurate (see <ref type="bibr">Bun et al. 2014, Lemma 2.11)</ref>.</p><p>Lemma E.4 <ref type="bibr">(Bun et al. 2014, Corollary 3.6)</ref>. For &gt; 0 and p &gt; 0, there exists a number n * = Ω( √ p ) such that for all n ≤ n * , there exists no algorithm that is both (1/3, 1/75)-accurate and ( , o 1 n )-differentially private for the estimation of one-way marginals on ({0, 1} p ) n .</p><p>To leverage this result in our setting of private empirical risk minimization, we start by extending it to queries on X = p j=1 {-L j ; L j }. Before stating the main theorem of this section (Theorem E.5), we describe a procedure χ L : ({0, 1} p ) n → X 3n (with L 1 , . . . , L p &gt; 0), that takes as input a dataset D ∈ ({0, 1} p ) n and outputs an augmented and rescaled version. This procedure is crucial to our proof and is defined as follows. First, it adds 2n rows filled with 1's to D, which ensures that the sum of each column of D is Θ(n) (which gives the lower bound on M in Theorem E.5). Then it rescales each of these columns by subtracting 1/2 to each coefficient and multiplying the j-th column of D (j ∈ [p]) by 2L j . The resulting dataset D aug L = χ L (D) is a set of 3n points with values in X = p j=1 {-L j , L j }, with the property that, for all j ∈ [p], 3nL j ≥ n i=1 (D aug L ) i,j ≥ nL j . For D ∈ ({0, 1} p ) n , we show how to reconstruct q j (χ L (D)) from q j (D) in Claim 1. Claim 1. Let n ∈ N, j ∈ [p], L j &gt; 0 and q j the j-th one-way marginal on datasets D with p columns such that for</p><p>where we use the slight abuse of notation by denoting the one-way marginals q j : X 3n → conv(X ) and q j : ({0, 1} p ) n → [0, 1] p in the same way.</p><p>Proof. Let D ∈ ({0, 1} p ) n , and let D aug ∈ ({0, 1} p ) 3n constructed by adding 2n rows of 1's at the end of D. Let D aug L = χ L (D). We remark that</p><p>Then, we link q j (D aug ) with q j (D aug L ):</p><p>combining ( <ref type="formula">97</ref>) and ( <ref type="formula">98</ref>) gives the result.</p><p>Differentially Private Coordinate Descent for Composite Empirical Risk Minimization Theorem E.5. Let n, p ∈ N, and L 1 , . . . , L p &gt; 0. Assume that for all subsets J ⊆ [p] of size at least p 75 , j∈J L 2 j = Ω( L 2 ). Define X = p j=1 {-L j ; +L j }, and let q j : X → {-L j , L j } be the predicate of the j-th one-way marginal on X . Take &gt; 0 and δ = o( 1 n ). There exists a number M = Ω min n L 2 , √ p L 2 such that for every ( , δ)-differentially private algorithm A, there exists a dataset</p><p>such that, with probability at least 1/3 over the randomness of A:</p><p>, and define the set of queries Q composed of p queries q j (D) = 1 n n i=1 d i,j for j ∈ [p]. Let A be a ( , δ)-differentially-private randomized algorithm. Let α, β ∈ [0, 1]. We will show that there exists a dataset D such that</p><p>When n ≤ n * . Assume, for the sake of contradiction, that A : X 3n → conv(X ) is ( 1 3 α, β)-accurate for Q. Then, for each dataset D ∈ X 3n , we have</p><p>Importantly, for all D ∈ ({0, 1}) p ) n , the randomized algorithm A satisfies (100) for the dataset</p><p>Using Claim 1, the results of A and be linked to the ones of A, as</p><p>Therefore, if A satisfies (100) and (101), then A : ({0, 1} p ) n → [0, 1] p satisfies, for all D ∈ ({0, 1} p ) n ,</p><p>which is exactly the definition of (α, β)-accuracy for A. Remark that since A is only a post-processing of A, without additional access to the dataset itself, A is itself ( , δ)-differentially-private. We have thus constructed an algorithm that is both accurate and private for n ≤ n * , which contradicts the result of Lemma E.4 when β = 1 75 . This proves the existence of a dataset D ∈ ({0, 1} p ) n such that for D aug L = χ L (D), A(D aug L ) is not ( 1 3 α, β)-accurate on Q, which means that with probability at least 1/3, there exists a subset J ⊆</p><p>where the second inequality comes from the fact that |J | ≥ βp = p 75 and our hypothesis on j∈J L 2 j . Notice that when</p><p>√ p , we recover the result of <ref type="bibr" target="#b3">Bassily et al. (2014)</ref>, since L 2 = 1 it holds with probability at least 1/3 that</p><p>and in that case, since all L j 's are equal, it indeed holds that j∈J L 2 j = Ω( L 2 ). Finally, we remark that the sum of each column of D aug L is n i=1 d i,j ≥ nL j , and as such, we have</p><p>It remains to apply Theorem E.5 to obtain that, with probability at least 1/3,</p><p>which gives the lower bound on the expected value of F (w priv ; D) -F (w * ). Note that without the additional assumption on the distribution of the L j 's, Remark E.6 directly gives the result with an additional multiplicative factor (L min /L max ) 2 :</p><p>with probability at least 1/3.</p></div>
<div><head>F Private Estimation of Smoothness Constants</head><p>In this section, we explain how a fraction of the budget of DP can be used to estimate the coordinate-wise smoothness constants, which are essential to the good performance of DP-CD on imbalanced problems. Let f be defined as the average loss over the dataset D as in problem (1). We denote by M (i) j the j-th component-smoothness constant of (•, d i ), where d i is the i-th point in D. The j-th smoothness constant of the function f is thus the average of all these constants:</p><p>Assuming that the practitioner knows an approximate upper bound b j over the M (i) j 's, they can enforce it by clipping M (i) j to b j for each i ∈ [n]. The sensitivity of the average of the clipped M (i) j 's is thus 2b j /n. One can then compute an estimate of M 1 , . . . , M p under -DP using the Laplace mechanism as follows:</p><p>where the factor p in noise scale comes from using the simple composition theorem <ref type="bibr" target="#b19">(Dwork &amp; Roth, 2014)</ref>, and Lap(λ) is a sample drawn in a Laplace distribution of mean zero and scale λ. The computed constant can then directly be used in DP-CD, allocating the remaining budgetto the optimization procedure.</p></div>
<div><head>G Additional Experimental Details and Results</head><p>G.1 Hyperparameter Tuning DP-SGD and DP-CD both depend on three hyperparameters: step size, clipping threshold and number of passes on data. For DP-CD, step sizes are adapted from a parameter as described in Section 6, and clipping thresholds as well (see Section 5.1).</p><p>For DP-SGD, the step size is given by γ/β, where γ is the hyperparameter and β is the problem's global smoothness constant (which we consider given), and the clipping threshold is used directly to clip gradients along their 2 -norm.</p><p>We simultaneously tune these three hyperparameters for each algorithm across the following grid:</p><p>• step size: 10 logarithmically-spaced values between 10 -6 and 1 for DP-SGD, and between 10 -2 and 10 for DP-CD. 7</p><p>• clipping threshold: 100 logarithmically-spaced values, between 10 -3 and 10 6 .</p><p>• number of passes: 5 values <ref type="bibr">(2, 5, 10, 20 and 50)</ref>.</p><p>We run each algorithm on each dataset 5 times on each combination of hyperparameter values. We then keep the set of hyperparameters that yield the lowest value of the objective at the last iterate, averaged across the 5 runs.</p><p>In Table <ref type="table">2</ref>, we report the best relative error (in comparison to optimal objective value) at the last iterate, averaged over five runs, for each dataset, algorithm, and total number of passes on the data. As such, each cell of this table corresponds to the best value obtained after tuning the step size and clipping hyperparameters for a given number of passes. Figure <ref type="figure">3</ref>. Relative error to non-private optimal for DP-CD (blue, round marks), DP-CD with privately estimated coordinate-wise smoothness constants (green, + marks) and DP-SGD (orange, triangle marks) on five problems. We report average, minimum and maximum values over 10 runs for each algorithm, as a function of the algorithm running time (in seconds).</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep Learning with Differential Privacy</title>
		<author>
			<persName><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">B</forename><surname>Mcmahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Mironov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Talwar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security</title>
		<meeting>the 2016 ACM SIGSAC Conference on Computer and Communications Security</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="308" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Private stochastic convex optimization: Optimal rates in 1 geometry</title>
		<author>
			<persName><forename type="first">H</forename><surname>Asi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Feldman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Koren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Talwar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page" from="393" to="403" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Privacy Amplification by Subsampling: Tight Analyses via Couplings and Divergences</title>
		<author>
			<persName><forename type="first">B</forename><surname>Balle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Barthe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gaboardi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Private Empirical Risk Minimization: Efficient Algorithms and Tight Error Bounds</title>
		<author>
			<persName><forename type="first">R</forename><surname>Bassily</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Thakurta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 55th Annual Symposium on Foundations of Computer Science</title>
		<meeting><address><addrLine>Philadelphia, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014-10">2014. October 2014</date>
			<biblScope unit="page" from="464" to="473" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Algorithmic stability for adaptive data analysis</title>
		<author>
			<persName><forename type="first">R</forename><surname>Bassily</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Nissim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Steinke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Stemmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ullman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Forty-Eighth Annual ACM Symposium on Theory of Computing, STOC '16</title>
		<meeting>the Forty-Eighth Annual ACM Symposium on Theory of Computing, STOC '16<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2016-06">June 2016</date>
			<biblScope unit="page" from="1046" to="1059" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Private Stochastic Convex Optimization with Optimal Rates</title>
		<author>
			<persName><forename type="first">R</forename><surname>Bassily</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Feldman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Talwar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Guha Thakurta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title />
		<author>
			<persName><surname>Curran Associates</surname></persName>
		</author>
		<author>
			<persName><surname>Inc</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Stability of Stochastic Gradient Descent on Nonsmooth Convex Losses</title>
		<author>
			<persName><forename type="first">R</forename><surname>Bassily</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Feldman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Guzmán</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Talwar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="4381" to="4391" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title />
		<author>
			<persName><surname>Curran Associates</surname></persName>
		</author>
		<author>
			<persName><surname>Inc</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Non-Euclidean Differentially Private Stochastic Convex Optimization</title>
		<author>
			<persName><forename type="first">R</forename><surname>Bassily</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Guzman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nandi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLT</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="474" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Personalized and Private Peer-to-Peer Machine Learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bellet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Guerraoui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Taziki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tommasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018-03">March 2018</date>
			<biblScope unit="page" from="473" to="481" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Concentrated Differential Privacy: Simplifications, Extensions, and Lower Bounds</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Steinke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Theory of Cryptography</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Hirt</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Smith</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="635" to="658" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Fingerprinting codes and the price of approximate differential privacy</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ullman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vadhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">STOC</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Enhancing sparsity by reweighted l 1 minimization</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Candès</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">B</forename><surname>Wakin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Boyd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Fourier Anal. Applicat</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">5-6</biblScope>
			<biblScope unit="page" from="877" to="905" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Coordinate Descent Method for Large-scale L2-loss Linear Support Vector Machines</title>
		<author>
			<persName><forename type="first">K.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-J</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1369" to="1398" />
			<date type="published" when="2008-06">June 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Differentially Private Empirical Risk Minimization</title>
		<author>
			<persName><forename type="first">K</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Monteleoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Sarwate</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">29</biblScope>
			<biblScope unit="page" from="1069" to="1109" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Differentially Private Stochastic Coordinate Descent</title>
		<author>
			<persName><forename type="first">G</forename><surname>Damaskinos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Mendler-Dünner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Guerraoui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Parnell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021-05">May 2021</date>
			<biblScope unit="volume">35</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A fast active set block coordinate descent algorithm for 1 -regularized least squares</title>
		<author>
			<persName><forename type="first">M</forename><surname>De Santis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lucidi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Rinaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Optim</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="781" to="809" />
			<date type="published" when="2016-01">January 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Differential Privacy</title>
		<author>
			<persName><forename type="first">C</forename><surname>Dwork</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Automata, Languages and Programming</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Bugliesi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Preneel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">V</forename><surname>Sassone</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">I</forename><surname>Wegener</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The Algorithmic Foundations of Differential Privacy</title>
		<author>
			<persName><forename type="first">C</forename><surname>Dwork</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends® in Theoretical Computer Science</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="211" to="407" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Calibrating Noise to Sensitivity in Private Data Analysis</title>
		<author>
			<persName><forename type="first">C</forename><surname>Dwork</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Mcsherry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Nissim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Theory of Cryptography</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Halevi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Rabin</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="265" to="284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Preserving Statistical Validity in Adaptive Data Analysis</title>
		<author>
			<persName><forename type="first">C</forename><surname>Dwork</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Feldman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pitassi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Reingold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Forty-Seventh Annual ACM Symposium on Theory of Computing, STOC '15</title>
		<meeting>the Forty-Seventh Annual ACM Symposium on Theory of Computing, STOC '15<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2015-06">June 2015</date>
			<biblScope unit="page" from="117" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<ptr target="https://www.openml.org/d/151" />
		<title level="m">Electricity Dataset</title>
		<imprint>
			<publisher>Electricity</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Private stochastic convex optimization: Optimal rates in linear time</title>
		<author>
			<persName><forename type="first">V</forename><surname>Feldman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Koren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Talwar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual ACM SIGACT Symposium on Theory of Computing</title>
		<meeting>the 52nd Annual ACM SIGACT Symposium on Theory of Computing<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2020-06">June 2020</date>
			<biblScope unit="page" from="439" to="449" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Accelerated, parallel and proximal coordinate descent</title>
		<author>
			<persName><forename type="first">O</forename><surname>Fercoq</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Richtárik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Optim</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1997" to="2013" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Regularization Paths for Generalized Linear Models via Coordinate Descent</title>
		<author>
			<persName><forename type="first">J</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Statistical Software</title>
		<idno type="ISSN">1548-7660</idno>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="22" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Variance reduction via gradient sketching</title>
		<author>
			<persName><forename type="first">F</forename><surname>Hanzely</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mishchenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Richtárik</surname></persName>
		</author>
		<author>
			<persName><surname>Sega</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems, NIPS'18</title>
		<meeting><address><addrLine>Red Hook, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates Inc</publisher>
			<date type="published" when="2018-12">December 2018</date>
			<biblScope unit="page" from="2086" to="2097" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Variance reduced coordinate descent with acceleration: New method with a surprising application to finite-sum problems</title>
		<author>
			<persName><forename type="first">F</forename><surname>Hanzely</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Richtárik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="page" from="4039" to="4048" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Train faster, generalize better: Stability of stochastic gradient descent</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2016-06">June 2016</date>
			<biblScope unit="page" from="1225" to="1234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Robust regression using iteratively reweighted least-squares</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">W</forename><surname>Holland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Welsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications in Statistics -Theory and Methods</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="813" to="827" />
			<date type="published" when="1977-01">January 1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Accelerating stochastic gradient descent using predictive variance reduction</title>
		<author>
			<persName><forename type="first">R</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">C</forename><forename type="middle">J C</forename><surname>Burges</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">26</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title />
		<author>
			<persName><surname>Curran Associates</surname></persName>
		</author>
		<author>
			<persName><surname>Inc</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A new analysis of differential pri-vacy&amp;#x2019;s generalization guarantees</title>
		<author>
			<persName><forename type="first">C</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ligett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Neel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sharifi-Malvajerdi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shenfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual ACM SIGACT Symposium on Theory of Computing</title>
		<meeting>the 53rd Annual ACM SIGACT Symposium on Theory of Computing<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2021-06">June 2021</date>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note>invited paper</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Nearly) Dimension Independent Private ERM with Ada-Grad Rates via Publicly Estimated Subspaces</title>
		<author>
			<persName><forename type="first">P</forename><surname>Kairouz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Diaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Thakurta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Proceedings of Machine Learning Research</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Belkin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Kpotufe</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">134</biblScope>
			<biblScope unit="page" from="2717" to="2746" />
			<date type="published" when="2021">2021</date>
			<publisher>PMLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Efficient Greedy Coordinate Descent for Composite Problems</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Karimireddy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Koloskova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">U</forename><surname>Stich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jaggi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 22nd International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019-04">April 2019</date>
			<biblScope unit="page" from="2887" to="2896" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Sparse spatial autoregressions</title>
		<author>
			<persName><forename type="first">R</forename><surname>Kelley Pace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Barry</surname></persName>
		</author>
		<idno type="DOI">10.1016/S0167-7152(96)00140-X</idno>
	</analytic>
	<monogr>
		<title level="j">Statistics &amp; Probability Letters</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<date type="published" when="1997-05">May 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A proximal method for composite minimization</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Wright</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematical Programming</title>
		<imprint>
			<biblScope unit="volume">158</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="501" to="546" />
			<date type="published" when="2016-07">July 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Blockwise coordinate descent procedures for the multi-task lasso, with applications to neural semantic basis discovery</title>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Palatucci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2009-06">June 2009</date>
			<biblScope unit="page" from="649" to="656" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">On the convergence of the coordinate descent method for convex differentiable minimization</title>
		<author>
			<persName><forename type="first">Z.-Q</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Tseng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Optim. Theory Appl</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="7" to="35" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">a Fast Solver for the Lasso with Dual Extrapolation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Massias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Salmon</surname></persName>
		</author>
		<author>
			<persName><surname>Celer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="3315" to="3324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Differential Privacy</title>
		<author>
			<persName><forename type="first">I</forename><surname>Mironov</surname></persName>
		</author>
		<author>
			<persName><surname>Renyi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.07476</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE 30th Computer Security Foundations Symposium (CSF)</title>
		<imprint>
			<date type="published" when="2017-08">2017. August 2017</date>
			<biblScope unit="page" from="263" to="275" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">enyi Differential Privacy of the Sampled Gaussian Mechanism</title>
		<author>
			<persName><forename type="first">I</forename><surname>Mironov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Talwar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.10530</idno>
		<imprint>
			<date type="published" when="2019-08">August 2019</date>
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Efficiency of coordinate descent methods on huge-scale optimization problems</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Nesterov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Optim</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="341" to="362" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Coordinate Descent Converges Faster with the Gauss-Southwell Rule Than Random Selection</title>
		<author>
			<persName><forename type="first">J</forename><surname>Nutini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Laradji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Friedlander</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Koepke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2015-06">June 2015</date>
			<biblScope unit="page" from="1632" to="1641" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Let's Make Block Coordinate Descent Go Fast: Faster Greedy Rules, Message-Passing, Active</title>
		<author>
			<persName><forename type="first">J</forename><surname>Nutini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Laradji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schmidt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.08859</idno>
	</analytic>
	<monogr>
		<title level="m">Set Complexity, and Superlinear Convergence</title>
		<imprint>
			<date type="published" when="2017-12">December 2017</date>
		</imprint>
	</monogr>
	<note>math</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Proximal Algorithms</title>
		<author>
			<persName><forename type="first">N</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Boyd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends in Optimization</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="127" to="239" />
			<date type="published" when="2014-01">January 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Adaptive Clipping for Private SGD</title>
		<author>
			<persName><forename type="first">V</forename><surname>Pichapati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">T</forename><surname>Suresh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Reddi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><surname>Adaclip</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.07643</idno>
		<imprint>
			<date type="published" when="2019-10">October 2019</date>
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Iteration complexity of randomized block-coordinate descent methods for minimizing a composite function</title>
		<author>
			<persName><forename type="first">P</forename><surname>Richtárik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Takáč</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematical Programming</title>
		<imprint>
			<biblScope unit="volume">144</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="1" to="38" />
			<date type="published" when="2014-04">April 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Block Coordinate Relaxation Methods for Nonparametric Wavelet Denoising</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sardy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Bruce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Tseng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computational and Graphical Statistics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="361" to="379" />
			<date type="published" when="2000-06">June 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Stochastic dual coordinate ascent methods for regularized loss</title>
		<author>
			<persName><forename type="first">S</forename><surname>Shalev-Shwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="567" to="599" />
			<date type="published" when="2013-02">February 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<author>
			<persName><forename type="first">H.-J</forename><forename type="middle">M</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.00040</idno>
		<title level="m">A Primer on Coordinate Descent Algorithms</title>
		<imprint>
			<date type="published" when="2017-01">January 2017</date>
		</imprint>
	</monogr>
	<note>math, stat</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Membership Inference Attacks Against Machine Learning Models</title>
		<author>
			<persName><forename type="first">R</forename><surname>Shokri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Stronati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Shmatikov</surname></persName>
		</author>
		<idno type="DOI">10.1109/SP.2017</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE Symposium on Security and Privacy (SP)</title>
		<imprint>
			<date type="published" when="2017-05">2017. May 2017</date>
			<biblScope unit="page" from="3" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Nearly Optimal Private LASSO</title>
		<author>
			<persName><forename type="first">K</forename><surname>Talwar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Guha Thakurta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Inexact Coordinate Descent: Complexity and Preconditioning</title>
		<author>
			<persName><forename type="first">R</forename><surname>Tappenden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Richtárik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gondzio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Optim. Theory Appl</title>
		<imprint>
			<biblScope unit="volume">170</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="144" to="176" />
			<date type="published" when="2016-07">July 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Differentially Private Learning with Adaptive Clipping</title>
		<author>
			<persName><forename type="first">O</forename><surname>Thakkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">B</forename><surname>Mcmahan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Convergence of a block coordinate descent method for nondifferentiable minimization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Tseng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Optim. Theory Appl</title>
		<imprint>
			<biblScope unit="volume">109</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="475" to="494" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Block-coordinate gradient descent method for linearly constrained nonsmooth separable optimization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Tseng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Optim. Theory Appl</title>
		<imprint>
			<biblScope unit="volume">140</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">513</biblScope>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Rényi divergence and Kullback-Leibler divergence</title>
		<author>
			<persName><forename type="first">T</forename><surname>Van Erven</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Harremoës</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3797" to="3820" />
			<date type="published" when="2014-07">July 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Differentially private empirical risk minimization revisited: Faster and more general</title>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">I</forename><surname>Guyon</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">U</forename><forename type="middle">V</forename><surname>Luxburg</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title />
		<author>
			<persName><surname>Curran Associates</surname></persName>
		</author>
		<author>
			<persName><surname>Inc</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Differentially private SGD with non-smooth losses</title>
		<author>
			<persName><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied and Computational Harmonic Analysis</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="page" from="306" to="336" />
			<date type="published" when="2022-01">January 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Coordinate descent algorithms</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Wright</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematical Programming</title>
		<imprint>
			<biblScope unit="volume">151</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="34" />
			<date type="published" when="2015-06">June 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">A Proximal Stochastic Gradient Method with Progressive Variance Reduction</title>
		<author>
			<persName><forename type="first">L</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Optim</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="2057" to="2075" />
			<date type="published" when="2014-01">January 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">A Comparison of Optimization Methods and Software for Large-scale L1-regularized Linear Classification</title>
		<author>
			<persName><forename type="first">G.-X</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-J</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="3183" to="3234" />
			<date type="published" when="2010-12">December 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Bypassing the ambient dimension: Private SGD with gradient subspace identification</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Banerjee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</teiCorpus></text>
</TEI>