<?xml version='1.0' encoding='utf-8'?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" version="1.1" xsi:schemaLocation="http://www.tei-c.org/ns/1.0 http://api.archives-ouvertes.fr/documents/aofr-sword.xsd">
  <teiHeader>
    <fileDesc>
      <titleStmt>
        <title>HAL TEI export of hal-04003126</title>
      </titleStmt>
      <publicationStmt>
        <distributor>CCSD</distributor>
        <availability status="restricted">
          <licence target="http://creativecommons.org/licenses/by/">Attribution</licence>
        </availability>
        <date when="2024-04-20T14:18:49+02:00" />
      </publicationStmt>
      <sourceDesc>
        <p part="N">HAL API platform</p>
      </sourceDesc>
    </fileDesc>
  </teiHeader>
  <text>
    <body>
      <listBibl>
        <biblFull>
          <titleStmt>
            <title xml:lang="en">Deep metric learning for visual servoing: when pose and image meet in latent space</title>
            <author role="aut">
              <persName>
                <forename type="first">Samuel</forename>
                <surname>Felton</surname>
              </persName>
              <idno type="halauthorid">2173453-0</idno>
              <affiliation ref="#struct-1092623" />
              <affiliation ref="#struct-105160" />
            </author>
            <author role="aut">
              <persName>
                <forename type="first">Élisa</forename>
                <surname>Fromont</surname>
              </persName>
              <idno type="halauthorid">2362950-0</idno>
              <affiliation ref="#struct-491653" />
              <affiliation ref="#struct-105160" />
            </author>
            <author role="aut">
              <persName>
                <forename type="first">Eric</forename>
                <surname>Marchand</surname>
              </persName>
              <idno type="halauthorid">8608-0</idno>
              <affiliation ref="#struct-1092623" />
              <affiliation ref="#struct-105160" />
            </author>
            <editor role="depositor">
              <persName>
                <forename>Eric</forename>
                <surname>Marchand</surname>
              </persName>
              <email type="md5">2d150c976ed389781f52875aab6f43c2</email>
              <email type="domain">irisa.fr</email>
            </editor>
            <funder ref="#projanr-49611" />
          </titleStmt>
          <editionStmt>
            <edition n="v1" type="current">
              <date type="whenSubmitted">2023-02-23 22:16:08</date>
              <date type="whenModified">2024-01-03 15:01:41</date>
              <date type="whenReleased">2023-02-24 09:56:44</date>
              <date type="whenProduced">2023-05-29</date>
              <date type="whenEndEmbargoed">2023-02-23</date>
              <ref type="file" target="https://inria.hal.science/hal-04003126/document">
                <date notBefore="2023-02-23" />
              </ref>
              <ref type="file" subtype="author" n="1" target="https://inria.hal.science/hal-04003126/file/ICRA23_1860_FI.pdf">
                <date notBefore="2023-02-23" />
              </ref>
            </edition>
            <respStmt>
              <resp>contributor</resp>
              <name key="110655">
                <persName>
                  <forename>Eric</forename>
                  <surname>Marchand</surname>
                </persName>
                <email type="md5">2d150c976ed389781f52875aab6f43c2</email>
                <email type="domain">irisa.fr</email>
              </name>
            </respStmt>
          </editionStmt>
          <publicationStmt>
            <distributor>CCSD</distributor>
            <idno type="halId">hal-04003126</idno>
            <idno type="halUri">https://inria.hal.science/hal-04003126</idno>
            <idno type="halBibtex">felton:hal-04003126</idno>
            <idno type="halRefHtml">&lt;i&gt;ICRA 2023 - IEEE International Conference on Robotics and Automation&lt;/i&gt;, May 2023, London, United Kingdom. pp.741-747, &lt;a target="_blank" href="https://dx.doi.org/10.1109/ICRA48891.2023.10160963"&gt;&amp;#x27E8;10.1109/ICRA48891.2023.10160963&amp;#x27E9;&lt;/a&gt;</idno>
            <idno type="halRef">ICRA 2023 - IEEE International Conference on Robotics and Automation, May 2023, London, United Kingdom. pp.741-747, &amp;#x27E8;10.1109/ICRA48891.2023.10160963&amp;#x27E9;</idno>
            <availability status="restricted">
              <licence target="http://creativecommons.org/licenses/by/">Attribution</licence>
            </availability>
          </publicationStmt>
          <seriesStmt>
            <idno type="stamp" n="UNIV-RENNES1">Université de Rennes 1</idno>
            <idno type="stamp" n="CNRS">CNRS - Centre national de la recherche scientifique</idno>
            <idno type="stamp" n="INRIA">INRIA - Institut National de Recherche en Informatique et en Automatique</idno>
            <idno type="stamp" n="UNIV-UBS">Université de Bretagne Sud</idno>
            <idno type="stamp" n="INSA-RENNES">Institut National des Sciences Appliquées de Rennes</idno>
            <idno type="stamp" n="INRIA-RENNES">INRIA Rennes - Bretagne Atlantique</idno>
            <idno type="stamp" n="IRISA">Irisa</idno>
            <idno type="stamp" n="IRISA_SET">IRISA_SET</idno>
            <idno type="stamp" n="INRIA_TEST">INRIA - Institut National de Recherche en Informatique et en Automatique</idno>
            <idno type="stamp" n="TESTALAIN1">TESTALAIN1</idno>
            <idno type="stamp" n="CENTRALESUPELEC">Ecole CentraleSupélec</idno>
            <idno type="stamp" n="INRIA2">INRIA 2</idno>
            <idno type="stamp" n="TDS-MACS">Réseau de recherche en Théorie des Systèmes Distribués, Modélisation, Analyse et Contrôle des Systèmes</idno>
            <idno type="stamp" n="UR1-HAL">Publications labos UR1 dans HAL-Rennes 1</idno>
            <idno type="stamp" n="UR1-MATH-STIC">UR1 - publications Maths-STIC</idno>
            <idno type="stamp" n="UR1-UFR-ISTIC">UFR ISTIC Informatique et électronique</idno>
            <idno type="stamp" n="TEST-UR-CSS">TEST Université de Rennes CSS</idno>
            <idno type="stamp" n="UNIV-RENNES">Université de Rennes</idno>
            <idno type="stamp" n="INRIA-RENGRE">INRIA-RENGRE</idno>
            <idno type="stamp" n="HYAIAI">Hybrid Approaches for Interpretable Artificial Intelligence</idno>
            <idno type="stamp" n="ANR">ANR</idno>
            <idno type="stamp" n="UR1-MATH-NUM">Pôle UnivRennes - Mathématiques - Numérique </idno>
            <idno type="stamp" n="CYBERSCHOOL">CyberSchool - Ecole universitaire de recherche en Cybersécurité</idno>
          </seriesStmt>
          <notesStmt>
            <note type="audience" n="2">International</note>
            <note type="invited" n="0">No</note>
            <note type="popular" n="0">No</note>
            <note type="peer" n="1">Yes</note>
            <note type="proceedings" n="1">Yes</note>
          </notesStmt>
          <sourceDesc>
            <biblStruct>
              <analytic>
                <title xml:lang="en">Deep metric learning for visual servoing: when pose and image meet in latent space</title>
                <author role="aut">
                  <persName>
                    <forename type="first">Samuel</forename>
                    <surname>Felton</surname>
                  </persName>
                  <idno type="halauthorid">2173453-0</idno>
                  <affiliation ref="#struct-1092623" />
                  <affiliation ref="#struct-105160" />
                </author>
                <author role="aut">
                  <persName>
                    <forename type="first">Élisa</forename>
                    <surname>Fromont</surname>
                  </persName>
                  <idno type="halauthorid">2362950-0</idno>
                  <affiliation ref="#struct-491653" />
                  <affiliation ref="#struct-105160" />
                </author>
                <author role="aut">
                  <persName>
                    <forename type="first">Eric</forename>
                    <surname>Marchand</surname>
                  </persName>
                  <idno type="halauthorid">8608-0</idno>
                  <affiliation ref="#struct-1092623" />
                  <affiliation ref="#struct-105160" />
                </author>
              </analytic>
              <monogr>
                <meeting>
                  <title>ICRA 2023 - IEEE International Conference on Robotics and Automation</title>
                  <date type="start">2023-05-29</date>
                  <date type="end">2023-06-02</date>
                  <settlement>London</settlement>
                  <country key="GB">United Kingdom</country>
                </meeting>
                <imprint>
                  <publisher>IEEE</publisher>
                  <biblScope unit="pp">741-747</biblScope>
                </imprint>
              </monogr>
              <idno type="doi">10.1109/ICRA48891.2023.10160963</idno>
            </biblStruct>
          </sourceDesc>
          <profileDesc>
            <langUsage>
              <language ident="en">English</language>
            </langUsage>
            <textClass>
              <classCode scheme="halDomain" n="spi.auto">Engineering Sciences [physics]/Automatic</classCode>
              <classCode scheme="halTypology" n="COMM">Conference papers</classCode>
              <classCode scheme="halOldTypology" n="COMM">Conference papers</classCode>
              <classCode scheme="halTreeTypology" n="COMM">Conference papers</classCode>
            </textClass>
            <abstract xml:lang="en">
              <p>We propose a new visual servoing method that controls a robot's motion in a latent space. We aim to extract the best properties of two previously proposed servoing methods: we seek to obtain the accuracy of photometric methods such as Direct Visual Servoing (DVS), as well as the behavior and convergence of pose-based visual servoing (PBVS). Photometric methods suffer from limited convergence area due to a highly non-linear cost function, while PBVS requires estimating the pose of the camera which may introduce some noise and incurs a loss of accuracy. Our approach relies on shaping (with metric learning) a latent space, in which the representations of camera poses and the embeddings of their respective images are tied together. By leveraging the multimodal aspect of this shared space, our control law minimizes the difference between latent image representations thanks to information obtained from a set of pose embeddings. Experiments in simulation and on a robot validate the strength of our approach, showing that the sought out benefits are effectively found.</p>
            </abstract>
            <particDesc>
              <org type="consortium">Robotex2.0</org>
            </particDesc>
          </profileDesc>
        </biblFull>
      </listBibl>
    </body>
    <back>
      <listOrg type="structures">
        <org type="researchteam" xml:id="struct-1092623" status="VALID">
          <idno type="RNSR">201822637G</idno>
          <orgName>Sensor-based and interactive robotics</orgName>
          <orgName type="acronym">RAINBOW</orgName>
          <date type="start">2018-01-01</date>
          <desc>
            <address>
              <addrLine>Campus de Beaulieu35042 Rennes cedex</addrLine>
              <country key="FR" />
            </address>
            <ref type="url">https://www.inria.fr/equipes/rainbow</ref>
          </desc>
          <listRelation>
            <relation active="#struct-419153" type="direct" />
            <relation active="#struct-300009" type="indirect" />
            <relation active="#struct-1092618" type="direct" />
            <relation active="#struct-490899" type="indirect" />
            <relation active="#struct-105160" type="indirect" />
            <relation active="#struct-117606" type="indirect" />
            <relation active="#struct-301232" type="indirect" />
            <relation active="#struct-172265" type="indirect" />
            <relation active="#struct-247362" type="indirect" />
            <relation active="#struct-411575" type="indirect" />
            <relation name="UMR6074" active="#struct-441569" type="indirect" />
            <relation active="#struct-481355" type="indirect" />
            <relation active="#struct-302102" type="indirect" />
          </listRelation>
        </org>
        <org type="institution" xml:id="struct-105160" status="VALID">
          <idno type="ROR">https://ror.org/015m7wh34</idno>
          <orgName>Université de Rennes</orgName>
          <orgName type="acronym">UR</orgName>
          <desc>
            <address>
              <addrLine>Campus de Beaulieu, 263 avenue Général Leclerc, CS 74205, 35042 RENNES CEDEX</addrLine>
              <country key="FR" />
            </address>
            <ref type="url">https://www.univ-rennes.fr/</ref>
          </desc>
        </org>
        <org type="researchteam" xml:id="struct-491653" status="VALID">
          <idno type="RNSR">201622044W</idno>
          <orgName>Large Scale Collaborative Data Mining</orgName>
          <orgName type="acronym">LACODAM</orgName>
          <desc>
            <address>
              <addrLine>Campus de Beaulieu 35042 Rennes cedex</addrLine>
              <country key="FR" />
            </address>
            <ref type="url">https://www.inria.fr/equipes/lacodam</ref>
          </desc>
          <listRelation>
            <relation active="#struct-419153" type="direct" />
            <relation active="#struct-300009" type="indirect" />
            <relation active="#struct-491231" type="direct" />
            <relation active="#struct-490899" type="indirect" />
            <relation active="#struct-105160" type="indirect" />
            <relation active="#struct-117606" type="indirect" />
            <relation active="#struct-301232" type="indirect" />
            <relation active="#struct-172265" type="indirect" />
            <relation active="#struct-247362" type="indirect" />
            <relation active="#struct-411575" type="indirect" />
            <relation name="UMR6074" active="#struct-441569" type="indirect" />
            <relation active="#struct-481355" type="indirect" />
            <relation active="#struct-302102" type="indirect" />
          </listRelation>
        </org>
        <org type="laboratory" xml:id="struct-419153" status="VALID">
          <idno type="RNSR">198018249C</idno>
          <idno type="ROR">https://ror.org/04040yw90</idno>
          <orgName>Inria Rennes – Bretagne Atlantique</orgName>
          <desc>
            <address>
              <addrLine>Campus de beaulieu35042 Rennes cedex</addrLine>
              <country key="FR" />
            </address>
            <ref type="url">http://www.inria.fr/centre/rennes</ref>
          </desc>
          <listRelation>
            <relation active="#struct-300009" type="direct" />
          </listRelation>
        </org>
        <org type="institution" xml:id="struct-300009" status="VALID">
          <idno type="ROR">https://ror.org/02kvxyf05</idno>
          <orgName>Institut National de Recherche en Informatique et en Automatique</orgName>
          <orgName type="acronym">Inria</orgName>
          <desc>
            <address>
              <addrLine>Domaine de VoluceauRocquencourt - BP 10578153 Le Chesnay Cedex</addrLine>
              <country key="FR" />
            </address>
            <ref type="url">http://www.inria.fr/en/</ref>
          </desc>
        </org>
        <org type="department" xml:id="struct-1092618" status="VALID">
          <orgName>RÉALITÉ VIRTUELLE, HUMAINS VIRTUELS, INTERACTIONS ET ROBOTIQUE</orgName>
          <orgName type="acronym">IRISA-D5</orgName>
          <date type="start">2022-01-01</date>
          <desc>
            <address>
              <country key="FR" />
            </address>
            <ref type="url">https://www.irisa.fr/departements/signaux-image-numerique-robotique</ref>
          </desc>
          <listRelation>
            <relation active="#struct-490899" type="direct" />
            <relation active="#struct-105160" type="indirect" />
            <relation active="#struct-117606" type="indirect" />
            <relation active="#struct-301232" type="indirect" />
            <relation active="#struct-172265" type="indirect" />
            <relation active="#struct-247362" type="indirect" />
            <relation active="#struct-300009" type="indirect" />
            <relation active="#struct-411575" type="indirect" />
            <relation name="UMR6074" active="#struct-441569" type="indirect" />
            <relation active="#struct-481355" type="indirect" />
            <relation active="#struct-302102" type="indirect" />
          </listRelation>
        </org>
        <org type="laboratory" xml:id="struct-490899" status="VALID">
          <idno type="IdRef">026386909</idno>
          <idno type="ISNI">0000 0001 2298 7270</idno>
          <idno type="RNSR">200012163A</idno>
          <idno type="ROR">https://ror.org/00myn0z94</idno>
          <orgName>Institut de Recherche en Informatique et Systèmes Aléatoires</orgName>
          <orgName type="acronym">IRISA</orgName>
          <date type="start">2017-01-01</date>
          <desc>
            <address>
              <addrLine>Avenue du général LeclercCampus de Beaulieu 35042 RENNES CEDEX</addrLine>
              <country key="FR" />
            </address>
            <ref type="url">http://www.irisa.fr</ref>
          </desc>
          <listRelation>
            <relation active="#struct-105160" type="direct" />
            <relation active="#struct-117606" type="direct" />
            <relation active="#struct-301232" type="indirect" />
            <relation active="#struct-172265" type="direct" />
            <relation active="#struct-247362" type="direct" />
            <relation active="#struct-300009" type="direct" />
            <relation active="#struct-411575" type="direct" />
            <relation name="UMR6074" active="#struct-441569" type="direct" />
            <relation active="#struct-481355" type="direct" />
            <relation active="#struct-302102" type="indirect" />
          </listRelation>
        </org>
        <org type="institution" xml:id="struct-117606" status="VALID">
          <idno type="ROR">https://ror.org/04xaa4j22</idno>
          <orgName>Institut National des Sciences Appliquées - Rennes</orgName>
          <orgName type="acronym">INSA Rennes</orgName>
          <desc>
            <address>
              <addrLine>20, avenue des Buttes de Coësmes - CS 70839 - 35708 Rennes cedex 7</addrLine>
              <country key="FR" />
            </address>
            <ref type="url">http://www.insa-rennes.fr/</ref>
          </desc>
          <listRelation>
            <relation active="#struct-301232" type="direct" />
          </listRelation>
        </org>
        <org type="regroupinstitution" xml:id="struct-301232" status="VALID">
          <idno type="IdRef">162105150</idno>
          <orgName>Institut National des Sciences Appliquées</orgName>
          <orgName type="acronym">INSA</orgName>
          <desc>
            <address>
              <country key="FR" />
            </address>
          </desc>
        </org>
        <org type="institution" xml:id="struct-172265" status="VALID">
          <idno type="ROR">https://ror.org/04ed7fw48</idno>
          <orgName>Université de Bretagne Sud</orgName>
          <orgName type="acronym">UBS</orgName>
          <desc>
            <address>
              <addrLine>BP 92116 - 56321 Lorient cedex</addrLine>
              <country key="FR" />
            </address>
            <ref type="url">http://www.univ-ubs.fr/</ref>
          </desc>
        </org>
        <org type="institution" xml:id="struct-247362" status="VALID">
          <idno type="ROR">https://ror.org/03rxtdc22</idno>
          <orgName>École normale supérieure - Rennes</orgName>
          <orgName type="acronym">ENS Rennes</orgName>
          <desc>
            <address>
              <addrLine>Campus de Ker Lann - avenue Robert Schuman - 35170 Bruz</addrLine>
              <country key="FR" />
            </address>
            <ref type="url">http://www.ens-rennes.fr</ref>
          </desc>
        </org>
        <org type="institution" xml:id="struct-411575" status="VALID">
          <idno type="IdRef">184443237</idno>
          <idno type="ROR">https://ror.org/019tcpt25</idno>
          <orgName>CentraleSupélec</orgName>
          <desc>
            <address>
              <addrLine>3, rue Joliot Curie,Plateau de Moulon,91192 GIF-SUR-YVETTE Cedex</addrLine>
              <country key="FR" />
            </address>
            <ref type="url">http://www.centralesupelec.fr</ref>
          </desc>
        </org>
        <org type="regroupinstitution" xml:id="struct-441569" status="VALID">
          <idno type="IdRef">02636817X</idno>
          <idno type="ISNI">0000000122597504</idno>
          <idno type="ROR">https://ror.org/02feahw73</idno>
          <orgName>Centre National de la Recherche Scientifique</orgName>
          <orgName type="acronym">CNRS</orgName>
          <date type="start">1939-10-19</date>
          <desc>
            <address>
              <country key="FR" />
            </address>
            <ref type="url">https://www.cnrs.fr/</ref>
          </desc>
        </org>
        <org type="institution" xml:id="struct-481355" status="VALID">
          <idno type="IdRef">202743233</idno>
          <idno type="ROR">https://ror.org/030hj3061</idno>
          <orgName>IMT Atlantique</orgName>
          <orgName type="acronym">IMT Atlantique</orgName>
          <date type="start">2017-01-01</date>
          <desc>
            <address>
              <addrLine>Campus Brest : Technopôle Brest-Iroise CS 8381829238 BREST Cedex 3 -Campus Nantes : 4, rue Alfred Kastler- La chantrerie 44300 NANTES -Campus Rennes :  2 Rue de la Châtaigneraie, 35510 CESSON SEVIGNE</addrLine>
              <country key="FR" />
            </address>
            <ref type="url">https://www.imt-atlantique.fr</ref>
          </desc>
          <listRelation>
            <relation active="#struct-302102" type="direct" />
          </listRelation>
        </org>
        <org type="regroupinstitution" xml:id="struct-302102" status="VALID">
          <idno type="ROR">https://ror.org/025vp2923</idno>
          <orgName>Institut Mines-Télécom [Paris]</orgName>
          <orgName type="acronym">IMT</orgName>
          <date type="start">2012-03-01</date>
          <desc>
            <address>
              <addrLine>37-39 Rue Dareau, 75014 Paris</addrLine>
              <country key="FR" />
            </address>
            <ref type="url">http://www.mines-telecom.fr/</ref>
          </desc>
        </org>
        <org type="department" xml:id="struct-491231" status="VALID">
          <orgName>GESTION DES DONNÉES ET DE LA CONNAISSANCE</orgName>
          <orgName type="acronym">IRISA-D7</orgName>
          <desc>
            <address>
              <country key="FR" />
            </address>
            <ref type="url">https://www.irisa.fr/fr/departements/d7-gestion-donnees-connaissance</ref>
          </desc>
          <listRelation>
            <relation active="#struct-490899" type="direct" />
            <relation active="#struct-105160" type="indirect" />
            <relation active="#struct-117606" type="indirect" />
            <relation active="#struct-301232" type="indirect" />
            <relation active="#struct-172265" type="indirect" />
            <relation active="#struct-247362" type="indirect" />
            <relation active="#struct-300009" type="indirect" />
            <relation active="#struct-411575" type="indirect" />
            <relation name="UMR6074" active="#struct-441569" type="indirect" />
            <relation active="#struct-481355" type="indirect" />
            <relation active="#struct-302102" type="indirect" />
          </listRelation>
        </org>
      </listOrg>
      <listOrg type="projects">
        <org type="anrProject" xml:id="projanr-49611" status="VALID">
          <idno type="anr">ANR-10-EQPX-0044</idno>
          <orgName>ROBOTEX</orgName>
          <desc>Réseau national de plateformes robotiques d'excellence</desc>
          <date type="start">2010</date>
        </org>
      </listOrg>
    </back>
  <teiCorpus>
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep metric learning for visual servoing: when pose and image meet in latent space</title>
			</titleStmt>
			<publicationStmt>
				<publisher />
				<availability status="unknown"><licence /></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Samuel</forename><surname>Felton</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Élisa</forename><surname>Fromont</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Eric</forename><surname>Marchand</surname></persName>
						</author>
						<title level="a" type="main">Deep metric learning for visual servoing: when pose and image meet in latent space</title>
					</analytic>
					<monogr>
						<imprint>
							<date />
						</imprint>
					</monogr>
					<idno type="MD5">4BFEE5C8C89AE25DF8B3573D731B5BDB</idno>
					<idno type="DOI">10.1109/ICRA48891.2023.10160963</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-04-12T14:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid" />
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div><p>We propose a new visual servoing method that controls a robot's motion in a latent space. We aim to extract the best properties of two previously proposed servoing methods: we seek to obtain the accuracy of photometric methods such as Direct Visual Servoing (DVS), as well as the behavior and convergence of pose-based visual servoing (PBVS). Photometric methods suffer from limited convergence area due to a highly non-linear cost function, while PBVS requires estimating the pose of the camera which may introduce some noise and incurs a loss of accuracy. Our approach relies on shaping (with metric learning) a latent space, in which the representations of camera poses and the embeddings of their respective images are tied together. By leveraging the multimodal aspect of this shared space, our control law minimizes the difference between latent image representations thanks to information obtained from a set of pose embeddings. Experiments in simulation and on a robot validate the strength of our approach, showing that the sought out benefits are effectively found.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div><head>I. INTRODUCTION</head><p>Visual servoing (VS) is the task of controlling the motion of a robot in order to reach a desired goal or a desired pose using only visual information extracted from an image stream <ref type="bibr" target="#b6">[7]</ref>. The camera can be mounted on the robot's end effector or directly observing the robot. Visual servoing usually requires the extraction and the tracking of visual information (usually geometric features) from the image in order to design the control law. The choice of features is a crucial aspect of VS, as it impacts the servoing behaviour (namely the convergence to the target pose and the trajectory in 3D space). Geometric features are usually split into two categories. The first one, Image-Based VS (IBVS), uses 2D primitives in the image space, such as points <ref type="bibr" target="#b27">[28]</ref>, lines <ref type="bibr" target="#b0">[1]</ref> or moments <ref type="bibr" target="#b5">[6]</ref>, in order to create the control law. The second category, named Pose-Based VS (PBVS) <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b5">[6]</ref>, uses image information to estimate the camera pose, which can be used to directly control the robot's motion. The PBVS control law is often seen as the most practical and optimal one: if the camera pose is perfectly estimated, then the scheme is globally convergent and the trajectory is the shortest path to the goal pose, both in translation and rotation.</p><p>In all cases, geometric features must be extracted, as well as matched between current and desired images. As this is an error-prone process, another way of performing VS has developed. By using photometric features, such as raw pixel intensities, feature extraction is avoided <ref type="bibr" target="#b8">[9]</ref>. In Direct VS (DVS), the difference between pixel intensities is minimized, which leads to very accurate positioning, but with Authors are with Univ Rennes, Inria, CNRS, Irisa, Rennes, France Email: {samuel.felton, elisa.fromont, eric.marchand}@irisa.fr an unpredictable trajectory and a small convergence domain, since the cost function to minimize is highly non-linear. To alleviate the latter problem, a solution is to represent images by lower dimensional features that better correlate to the pose. Multiple representations have been studied: <ref type="bibr" target="#b1">[2]</ref> expresses an image with photometric moments. In <ref type="bibr" target="#b21">[22]</ref>, servoing is performed in the frequency domain, where only the smoothly varying low-frequency information is preserved. A similar, learning-based approach was proposed in <ref type="bibr" target="#b20">[21]</ref>, where principal component analysis is used to project on a subspace, which maximally preserves the information of an image set. Inspired by this work, we previously proposed AEVS <ref type="bibr" target="#b10">[11]</ref>, projecting images in the latent space of an autoencoder.</p><p>Between all these VS methods, a trade-off becomes apparent: the easier feature extraction is, the harder servoing becomes. On one end of the spectrum lies DVS, with no extraction but a highly non-linear cost function. On the other end, PBVS requires estimating the pose (from a 3D model of the scene/object or directly from data) but the cost function is smooth. Pose estimation also has the drawback of introducing some noise into the robot's trajectory. However, as camera relocalization is a fundamental task of many computer vision applications, it has become a topic of interest for the deep learning community, that seeks to replace or support classical geometric approaches with a neural network. One of the first works was <software ContextAttributes="created">PoseNet</software> <ref type="bibr" target="#b16">[17]</ref>, that uses a Convolutional Neural Network (CNN) to regress the camera pose in a scene (position + orientation) from a single RGB image. This process was repurposed for VS, with multiple works such as <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b30">[31]</ref> which employ a CNN to estimate the pose difference between current and desired images. This difference is then fed to the PBVS control law in order to move the robot closer to the target pose. Servoing is then fully dependent on this estimation, and <ref type="bibr" target="#b32">[33]</ref> notes that in the case of orientation regression, trained networks may still yield a large error, as the rotation space is not smooth and continuous. Furthermore, In some cases, the difference between translational and rotational motions may be nonobservable.</p><p>Our work proposes to replace the standard pose regression approach with metric learning. In metric learning, we seek to learn a similarity function between samples that is dependent on the task to be accomplished. These approaches can be used to group images of the same concept (high similarity), while pushing away images of different concepts (low similarity). This principle can be applied to many tasks, including classification <ref type="bibr" target="#b12">[13]</ref>, object tracking <ref type="bibr" target="#b9">[10]</ref> or camera relocalization <ref type="bibr" target="#b2">[3]</ref>.</p><p>We propose to use metric learning to shape the latent space in which we perform VS. We aim to learn a space in which the similarity between two learned representations correlates with the distance between their respective camera poses. This representation space can be seen as an intermediate manifold between poses and images on which we project both modalities. Doing so, we propose a new servoing control law that combines the optimal behavior of PBVS with the accuracy and simplicity of photometric methods. In Section II, we give an overview of VS, detailing the generic minimization framework, as well as presenting how PBVS and visual servoing in an autoencoder latent space are achieved. With this knowledge, we introduce our method in Section III, that constrains the latent space to have good properties for VS via metric learning. By leveraging the multimodal aspect of this shared space, our control law minimizes image error thanks to information obtained from a set of pose projections. Finally, In Section IV, we present simulated and real-world experiments that validate our approach and illustrate its properties.</p></div>
<div><head>II. RELATED WORKS A. Visual servoing framework</head><p>Visual servoing aims to reach a desired pose r * , from its arbitrary, current pose r. Many robotics tasks, such as navigation, tracking, object picking can be viewed through this prism, e.g. navigation is a succession of positioning tasks. VS thus seeks to solve an optimization problem, finding the pose closest to r * that minimizes an error function e: r = arg min r e(r).</p><p>(</p><formula xml:id="formula_0">)<label>1</label></formula><p>If e is well designed and e = 0, then r = r * . Since the pose r may be unknown during servoing, e is defined as e = s(r)s * , which is the difference between what the camera sees at the current pose s(r) = s, and what it should see at the desired pose s * . As stated before, the choice of features s is important as it conditions the 3D trajectory, the convergence domain -how far can r be from r * before VS diverges -and the final accuracy -how close is r to r * . Features may lie in the image space (IBVS), in the 3D world (PBVS) or in photometric space (e.g. considering pixel intensities <ref type="bibr" target="#b8">[9]</ref>). In all cases, the relationship between the variation (in time) of the features s and the camera velocity v must be established:</p><formula xml:id="formula_1">ṡ = L s v<label>(2)</label></formula><p>where L s = ∂s ∂r is called the interaction matrix. By inverting this equation, we can then define the control law that best minimizes the error e:</p><formula xml:id="formula_2">v = -λL + s e<label>(3)</label></formula><p>where λ is a gain parameter L + s the pseudo-inverse of L s . The computed velocity v can be used to move the robot's end-effector closer to the desired pose r * . VS operates in a closed loop, with the minimization of e being performed in an iterative manner.</p></div>
<div><head>B. Deep learning for pose-based VS</head><p>A pose r = t θu ⊤ is an element of SE(3) = R 3 × SO(3) the group that represents rigid transformations, combining translation t and rotation θu, where u is the axis around which to rotate and θ is the angle of the rotation. It may also be represented as a homogeneous matrix o T c , that expresses the pose of camera F c (or any other frame) in a reference frame F o (such as one given by the origin of a scene object o). The displacement between two poses ∆r = c * T c can be computed as c * T c = c * T o o T c . Deep Learning (DL) has previously been used to estimate the camera pose r, given a single image I. The first major work to accomplish this was <software ContextAttributes="created">PoseNet</software> <ref type="bibr" target="#b16">[17]</ref>. Given a dataset of images and their associated poses (expressed in a common frame), a CNN is trained to minimize the following loss function:</p><formula xml:id="formula_3">L pose (t, q) = ∥ t -t∥ 2 + β∥q -q∥ 2<label>(4)</label></formula><p>where t, q are the predicted position and orientation of the camera and t, q is the ground truth. The orientation q can be expressed as a unit quaternion, or as an axis-angle θu. β is an important hyperparameter, that balances the learning of both translation and orientation. This weighting is required as the two quantities are on different scales, and it must carefully be tweaked in order to get sensible results. In <ref type="bibr" target="#b15">[16]</ref>, the authors introduced a multi-task loss that automatically learns the weighting, improving upon the manual tuning of β. Given two images, it is also possible to regress the pose difference ∆r <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b30">[31]</ref>. ∆r can then be plugged into the PBVS control law <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b6">[7]</ref> to compute v. The interaction matrix of a pose expressed in a fixed frame F c * (also valid for F o ) is defined as <ref type="bibr" target="#b6">[7]</ref>:</p><formula xml:id="formula_4">L r = c * R c 0 0 L θu<label>(5)</label></formula><p>with c * R c a rotation matrix and L θu the interaction matrix defined in <ref type="bibr" target="#b7">[8]</ref>. If ∆r is perfectly estimated, then the control law v = -λL -1 r ∆r ensures that the 3D trajectory is a geodesic both in translation and rotation.</p></div>
<div><head>C. Servoing in latent space</head><p>In <ref type="bibr" target="#b10">[11]</ref>, we introduced AEVS, a method to perform VS in the latent space of an autoencoder (AE). This AE is a neural network that learns a projection from an image to a lower dimensional representation (encoder), as well as the inverse mapping (decoder). This approach is similar to PCA-based VS <ref type="bibr" target="#b20">[21]</ref>, except that AEs learn non-linear projections, which PCA cannot. The autoencoding objective is the minimization of the reconstruction error. Considering two embeddings z I , z I * , the control law is of the form:</p><formula xml:id="formula_5">v = -λL + z I (z I -z I * )<label>(6)</label></formula><p>where L z I is computed analytically by applying the chain rule, finding L z I is the composition of the encoder Jacobian and the interaction matrix of the input image, detailed in <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b19">[20]</ref>:</p><formula xml:id="formula_6">L z I = ∂z I ∂I ∂I ∂r .</formula><p>Although this process is applied to images, the same reasoning can be applied to an encoder for any type of inputs, as long as the interaction matrix of the input can be defined. While this approach improves upon other photometric dimensionality reduction schemes, it has some drawbacks. First, the training objective is weakly correlated to pose estimation. This makes it hard to know whether the latent space of a trained AE will give good results when applied to VS. Second, the interaction matrix L z I depends on the interaction matrix of DVS, which is known to lead to unpredictable trajectories due to a highly non-linear cost function <ref type="bibr" target="#b8">[9]</ref>. Finally, AEVS requires the camera's intrinsic calibration, as well as an estimate of the depth: a very coarse estimation works, but degrades the trajectory.</p></div>
<div><head>D. Metric learning</head><p>Metric learning aims to learn a similarity function between two compared inputs. The similarity measure is defined not in the input space (e.g. comparing pixel intensities) but rather from the factors that underlie the variations of the data. As an example, metric learning may learn to group images of the same subject (e.g. class, landmark or person) while enforcing a large margin between dissimilar concepts. Metric learning is often coupled to nearest neighbor search. A majority of the metric learning algorithms focuses on binary supervision to learn a meaningful metric: either two samples are similar or they are not. This is best seen in the common triplet loss <ref type="bibr" target="#b12">[13]</ref> </p><formula xml:id="formula_7">L triplet (x a , x p , x n ) = max(d(x a , x p ) -d(x a , x n ) + ϵ, 0)</formula><p>that compares an anchor representation x a with similar and dissimilar samples x p , x n , where ϵ is a margin parameter that gives the separation threshold between positive and negative data points. It is also possible to learn smoothly varying similarity functions with continuous metric learning, as studied in <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b2">[3]</ref>. An application of continuous metric learning that is closely tied to our work can be found in <ref type="bibr" target="#b2">[3]</ref>. This approach learns a feature space where the Euclidean distance between two representations is directly correlated with the overlap between two images, i.e. how much of the scene is visible in both cameras. An additional pose difference regressor is then trained to obtain a better estimate for the camera relocalization task. This network compares the query with its nearest neighbors. Another work closely related to ours is <ref type="bibr" target="#b14">[15]</ref> which learns a feature space that is equivariant to camera motion. This approach uses discrete metric learning and a discrete number of motion patterns (e.g. move forward, rotate left/right) is learned.</p><p>Using latent representations also allows projecting multiple modalities of the same data to compare them with a single metric, in a common space. In <ref type="bibr" target="#b26">[27]</ref>, authors transform text and images and map them to a shared space. Metric learning then allows for the retrieval of images that match a given textual query. Similarly, <ref type="bibr" target="#b24">[25]</ref> embeds point clouds, textual tasks, and trajectories in the same space, so that the best fitting trajectory may be selected given a task. Multiple modalities can also be used in audio processing, by either creating a music sample-tag association <ref type="bibr" target="#b29">[30]</ref>, or an acousticlinguistic relationship <ref type="bibr" target="#b25">[26]</ref>.</p></div>
<div><head>III. METRIC LEARNING FOR VISUAL SERVOING</head><p>This section presents our novel approach to Visual Servoing, that leverages deep metric learning in order to frame VS in a learned space. We first detail the reasoning behind our method, then present our training procedure, that shapes the latent space. Finally, we describe how VS is performed in this new latent space. In this paper, our goal is to propose a latent space servoing scheme with a behavior similar to PBVS, overcoming the limitations of other deep learning-based methods. To do so, we propose to create a multimodal latent space Z, in which both pose and image representations are mapped. A pose r ∈ SE(3) maps to an image I ∈ I via a camera. A pose r j maps to an embedding z rj , while the image acquired at r j is noted z Ij . The relationship between latent space and images/poses is illustrated in Figure <ref type="bibr" target="#b0">(1)</ref> We argue that for the best VS behavior, the distance between two embeddings should be equal to the distance between their underlying poses:</p><formula xml:id="formula_8">d Z (z j , z k ) = d SE(3) (r j , r k ), where d Z is the Euclidean distance: d Z (z j , z k ) = ∥z j -z k ∥ 2<label>(7)</label></formula><p>The distance constraint holds true, whether z j , z k are image or pose projections, i.e. z j = z Ij or z j = z rj . If this property is perfectly met, it follows that:</p><p>• for a given image I j , acquired at pose r j , d Z (z Ij , z rj ) = 0. This is similar to an absolute pose regression objective; • for two images I j , I k acquired at r j , r k , the constraint</p><formula xml:id="formula_9">d Z (z Ij , z I k ) = d SE(3) (r j , r k</formula><p>) is akin to estimating the relative pose difference between r j and r k from the images;</p><formula xml:id="formula_10">• finally, d Z (z Ij , z I k ) = d Z (z rj , z r k ).</formula><p>As the two cost functions are the same, using the interaction matrix linked to a pose representation in a servoing context is valid for the minimization of d Z (z Ij , z I k ).</p><p>We thus seek to learn a space that is equivariant to 3D motion. This is ideal for VS, as we wish for features that have a strong and straightforward relation to pose. Moreover, we seek to explicitly learn a space that is invariant to perturbations P ∈ P, such as lighting changes or occlusions (i.e. z I+P = z I ).</p></div>
<div><head>B. Learning an SE(3)-equivariant space</head><p>To learn the space Z, we propose to use two distinct, parallel neural networks. The first is ϕ : SE(3) → Z, that maps a pose r to an embedding z r = ϕ(r). The second model ψ : I → Z, maps an image I to its latent representation z I = ψ(I).</p><p>In order to shape Z, we devise our loss function that is based on the distances between latent representations. While most metric learning approaches focus on clustering problems, we require our distances to be continuously meaningful as in <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b17">[18]</ref>. Instead of comparing the embeddings of specific tuples (r j , I j , r k , I k ), we adopt a full-batch approach, comparing a representation with every other in the batch. To do so, we leverage the distance matrices in SE(3) and in the latent space, viewing an embedding batch as a fully connected graph. By using this dense approach, we encourage the latent representations to position themselves with respect to every neighbor, providing a more stable training signal for the models ϕ and ψ.</p><p>To train the pose encoder ϕ, Our first loss seeks to enforce the equivariance with SE(3) when considering pose projections. As SE(3) has no true distance metric, a weighting between translation t and rotation θu distances must be introduced to create a pseudo-metric d SE(3) , as in <ref type="bibr" target="#b15">[16]</ref>. We propose to define the weightings from the data and normalize the translational and rotational distances by their average value in the dataset. To compute the translation/rotation distances between two poses r j , r k , we first compute the pose difference ∆r = (∆t, ∆θu) we then define a single dataset-aware metric on SE(3), as</p><formula xml:id="formula_11">d SE(3) (r j , r k ) = 1 w t ∥∆t∥ 2 + 1 w θu ∆θ<label>(8)</label></formula><p>where w t , w θu are averages of translational and rotational distances, computed on a subset of the dataset. With d SE(3) defined, we introduce our first loss. Considering a batch of B samples, we seek to minimize</p><formula xml:id="formula_12">L ϕ,SE(3) = 1 B 2 B j=0 B k=0 (d SE(3) (r j , r k )-∥z rj -z r k ∥ 2 ) 2 (9)</formula><p>This loss is fairly straightforward to minimize, as ϕ has access to the full pose information, and its main task is to transform the combination of translational and rotational metrics into a single euclidean distance. Of course, since poses are not available during VS, we require ψ to project an image to the same embedding as its associated pose, as well as match the distances with other poses. This is modeled by:</p><formula xml:id="formula_13">L ψ,ϕ = 1 B 2 B j=0 B k=0 (∥z Ij -z r k ∥ 2 -∥z rj -z r k ∥ 2 ) 2 (10)</formula><p>By transitivity, L ψ,ϕ also minimizes</p><formula xml:id="formula_14">(d SE(3)(rj ,r k ) -∥z Ij - z I k ∥ 2 ) 2 .</formula><p>To learn invariance to perturbations, we incorporate perturbed samples in the image batch. These noisy samples are exploited in L ψ,ϕ . Moreover, the image representations associated to a single pose r j (the original image and its P perturbed versions) are compared, and their distance to each other minimized:</p><formula xml:id="formula_15">L Pj = 1 P 2 P m=0 P n=0 ∥z Ij +Pm -z Ij +Pn ∥ 2<label>(11)</label></formula><p>To obtain our final training objective, we sum up the losses</p><formula xml:id="formula_16">L = L ϕ,SE(3) + L ψ,ϕ + j L Pj (<label>12</label></formula><formula xml:id="formula_17">)</formula><p>The impact of each loss is visualized in Figure <ref type="bibr" target="#b1">(2)</ref>. It can be seen that the objectives designed above act as pushpull forces, moving the embeddings to respect the distance constraints, as well as minimize the influence of perturbations. By comparing a representation with every neighbor, we ensure that a single iteration forces an embedding towards a more stable location.</p><formula xml:id="formula_18">z Ij z rj z r k z r l d Z(z I j ,zr k ) d Z(zr j ,zr k ) P d SE(3) (rj, r k ) d SE(3) (rj, r l ) -∇L ψ,ϕ -∇L ϕ,SE<label>(3)</label></formula><p>-∇L P,j Fig. <ref type="figure">2</ref>: Shaping the latent space with metric learning. We compare the representations of a sample j with their neighbors. While illustrating only the gradients for j, the loss is applied to every other sample.</p></div>
<div><head>C. Training policy</head><p>To constitute our batches of data, we randomly sample N = 64 poses from the dataset, and add their M = 3 closest neighbors found in a subset of the data to ensure that the constraints are met both globally and locally. For each image, we also generate P = 2 perturbations. Comparing to AEVS <ref type="bibr" target="#b10">[11]</ref>, not using a decoder (discarded for VS) network allows for larger batches. The perturbations consist in adding Gaussian noise, changing the brightness of the image and performing random erasing <ref type="bibr" target="#b31">[32]</ref>. The image/pose generation process is the same as the one presented in <ref type="bibr" target="#b10">[11]</ref>, leveraging simulation to create data cheaply and efficiently.</p><p>We set the dimensionality of the latent space so that Z = R 32 . The image encoder ψ is a ResNet-34 <ref type="bibr" target="#b11">[12]</ref>, with the modifications of <ref type="bibr" target="#b10">[11]</ref>, i.e. replacing batch normalization <ref type="bibr" target="#b13">[14]</ref> with weight normalization <ref type="bibr" target="#b22">[23]</ref> and the end average pooling with a group convolution. The pose-embedding network ϕ is a 5-layer perceptron of dimensions 6 → 32 → 64 → 128 → 64 → 32, with ReLU activations after each hidden layer. The networks are jointly trained for 50 epochs, on a dataset of 100K samples. Training takes around 8h with a Quadro RTX 6000. Gradient descent is performed with the Adam optimizer <ref type="bibr" target="#b18">[19]</ref> and a learning rate of 10 -4 . With our networks trained, we can finally perform VS in the latent space.</p></div>
<div><head>D. Visual servoing in latent space</head><p>Our approach to VS is two-pronged, based on the multimodal nature of Z. We first embed the current and desired images z I = ψ(I), z I * = ψ(I * ) in Z to obtain e = z Iz I * .</p><p>To minimize e, we require an interaction matrix, that gives us the control directions. Before VS starts, we project a set of N poses {z r1 , ...z r N } = {ϕ(r 1 ), ..., ϕ(r N )} in the latent space, along with their interaction matrix, thanks to the method described in Section II-C. The latent interaction matrix is then defined as L zr j = ∂zr j ∂rj L rj , with ∂zr j ∂rj the Jacobian of ϕ with respect to r j (computed via forward propagation) and L rj given by Equation ( <ref type="formula" target="#formula_4">5</ref>). Because we minimize L ψ,ϕ (Equation ( <ref type="formula">10</ref>)), We can approximate the interaction matrix at z I as an interpolation of its neighbors' interaction matrices. We thus have a K-Nearest Neighbors (KNN) regression problem, defined as:</p><formula xml:id="formula_19">L zr = K j=0 α j L zr j with α j = ∥z rj -z I ∥ 2 K k=0 ∥z r k -z I ∥ 2<label>(13)</label></formula><p>To obtain the pose embeddings set, we generate poses on a 6D grid (displacements in translation and rotation), and oversample near z I * . In our experiments, we thus create a set of 1M pose representations that can be used for KNN. To be computationally tractable, we store them in a KD-tree <ref type="bibr" target="#b4">[5]</ref>. Other, smarter, less memory demanding sampling strategies, based on the values of both z I and z I * , are possible. We however found that this straightforward approach works well in practice. Unlike AEVS, the interaction matrices of the pose representations (and thus the network Jacobian) are computed offline. Fitting the pieces together, the final control law is simply</p><formula xml:id="formula_20">v = -λL + zr (z I -z I * )<label>(14)</label></formula><p>In the next section, we explore the behavior of this VS scheme, both in simulation and on a 6DOF robot.</p></div>
<div><head>IV. EXPERIMENTS AND RESULTS</head></div>
<div><head>A. Simulation validation</head><p>We start our experimental validation with a large scale experiment: we run 500 VS tasks with multiple methods.</p><p>The initial positions are chosen with a "look-at/look-from" scenario: we sample camera positions in a volume of dimensions 1.2m × 1.2m × 0.3m, centered on the desired position. we then sample the focal (look-at) points of the cameras on the planar scene, with a distance to the desired focal points between 8cm and 32cm. The scene is a poster of dimensions 80cm × 60cm, and we set the desired camera elevation to 60cm. From the camera position and focal point, we build the camera orientation, to which we add a rotation around the optical axis ∈ [-120 • , 120 • ]. The average initial pose error is 47cm ± 16cm, 74 • ± 28 • .</p><p>We experiment with multiple methods, the first one being a pure photometric scheme (DVS) <ref type="bibr" target="#b8">[9]</ref>. We also compare with AEVS <ref type="bibr" target="#b10">[11]</ref>, as well as a PBVS visual servoing approach which uses a CNN-based pose regression approach, as developed in <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b3">[4]</ref> (referred as PBVS-CNN). To ease training and improve results, we adopt the automatic weighting loss of <ref type="bibr" target="#b15">[16]</ref> that balances translation and rotation errors in Equation ( <ref type="formula" target="#formula_3">4</ref>). For both AEVS and PBVS-CNN, we use the network and data described in Section III-C. For our method, we explore different numbers of neighbors K. We first report the percentage of samples that convergence for each method. A sample is defined as having converged if the VS method significantly reduces (by at least 90%) the initial positioning error and the final velocities are close to 0. For those samples, we measure the end positioning error, as well as the Absolute Pose Error (APE), averaged over all iterations of a trajectory, which describes how far the method strays from the geodesic of PBVS. The other included metrics are the mean length ratios: the length of trajectory of the observed method divided by the length of the PBVS trajectory. As can be seen in Table <ref type="table" target="#tab_1">I</ref>, our method is able to converge reliably and accurately, with a positioning error that is comparable to photometric methods in the case of clean target images I * (noted ✓in the table), while having a far larger domain of convergence. We can observe that a larger value for K leads to a more stable and accurate positioning. Looking at the results of the PBVS-CNN, it can be seen that the end positioning error is subpar (3.3cm, 1.71 • on average). The trajectory statistics (APE and length ratio) show that our method is far closer to the behavior of PBVS, compared to AEVS or DVS. This is made explicit in Figure <ref type="figure">(3b</ref>). The overall statistics when considering cases where I * is perturbed (noted ✗) highlight that our method better handles variations in lighting and occlusions. While both convergence and accuracy degrade, they remain above that of the pose estimator, even on clean images.</p><p>Next, we explore the servoing behavior in the latent space. We perform 8 trajectories, where the initial errors are displacements on the x, y axes, with an initial error of 20cm. We then visualize the trajectories in the latent space by projecting in a 2D-subspace with PCA (explained variance ≈ 96%). As can be seen in Figure (3a), the minimization of e in the latent space leads to nearly straight lines in the latent space. The error between pose embeddings also correlates well with the error from image representations.   </p></div>
<div><head>B. Robot experiment</head><p>We also deploy our method on a 6DoF gantry robot and study its behavior on a large motion. For the experiment, shown in Figure <ref type="bibr" target="#b3">(4)</ref>, the initial displacement is ∆r 0 = (-8.16cm, 7.27cm, -29.98cm, 24.86 • , -10.16 • , 135.06 • ), with a large error in the image (Figure <ref type="figure" target="#fig_2">(4c</ref>)) and run our method for 1.2k iterations. Note that this motion (and thus the image) is far from what is seen during training. As servoing progresses, the error in the latent space (Figure (4f)) is quickly minimized. The final positioning error is ∆r f inal = (0.09cm, 0.08cm, -0.04cm, 0.08 • , -0.12 • , -0.01 • ), and the resulting error in image space is low (Figure <ref type="figure" target="#fig_2">(4d)</ref>). As shown in Figures <ref type="figure" target="#fig_2">(4e,</ref><ref type="figure" target="#fig_2">4g,</ref><ref type="figure" target="#fig_2">4h</ref>), the trajectory starts with some unwanted motion on the x, y axes (compensated by y/x rotations), probably due to the fact that I lies outside the training domain. However, our method recovers and exhibits a smooth decrease in positioning error.</p></div>
<div><head>V. CONCLUSION</head><p>In this paper, we proposed a method that allows visual servoing from both image and pose representations in a common latent space. This new visual servoing scheme combines the accuracy of photometric methods with the behavior of pose-based approaches. Our experiments show strong results, with a large convergence domain and accurate positioning. In future works, we plan to extend our method to deal with more than two modalities (i.e. adding depth or segmentation information), so that the visual features at the current and desired poses may be drawn from different domains. In addition, we believe that it is possible to reduce data requirements by including some form of self-supervised learning. For instance, small motions could be used to warp an image and generate new weakly labeled samples. Supervision would then be used to learn representations tied to large motions, while smaller displacements would be taken into account with self-supervision.</p></div><figure xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Fig.1: The proposed latent space for visual servoing. Both images and poses are projected in Z, where they can be compared.</figDesc></figure>
<figure xml:id="fig_1"><head>c * tc 2 Fig. 3 :</head><label>23</label><figDesc>Fig. 3: (a) PCA projection of trajectories z I I * in the latent space, for 2D motions. Circles show the error for the pose embeddings z r -z r * for various distances. (b) 3D trajectories of different methods on a single example.</figDesc></figure>
<figure xml:id="fig_2"><head>Fig. 4 :</head><label>4</label><figDesc>Fig. 4: First experiment: (a) Starting image I. (b) Desired image I * . (c) Starting image difference I-I * . (d) Final image difference. (e) Pose difference rr * . (f) Error in the latent space z Iz I * . (g) Camera velocities v. (h) 3D trajectory.</figDesc></figure>
<figure type="table" xml:id="tab_1"><head>TABLE I :</head><label>I</label><figDesc>Comparative results of different VS methods on 500 trajectories. Error and trajectory statistics are reported for cases that converge. For metrics marked with ↑, higher is better. Lower is better when marked ↓.</figDesc><table /></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Visual servoing from lines</title>
		<author>
			<persName><forename type="first">N</forename><surname>Andreff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Espiau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Horaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. Journal of Robotics Research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="669" to="699" />
			<date type="published" when="2002-08">August 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A Direct Dense Visual Servoing Approach using Photometric Moments</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bakthavatchalam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Tahri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Chaumette</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Robotics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1226" to="1239" />
			<date type="published" when="2018-10">October 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Relocnet: Continuous metric learning relocalisation using neural nets</title>
		<author>
			<persName><forename type="first">V</forename><surname>Balntas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Prisacariu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="751" to="767" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Training deep neural networks for visual servoing</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Bateux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Marchand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leitner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Chaumette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Corke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. on Robotics and Automation, ICRA'18</title>
		<meeting><address><addrLine>Brisbane, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-05">May 2018</date>
			<biblScope unit="page" from="3307" to="3314" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Multidimensional binary search trees used for associative searching</title>
		<author>
			<persName><surname>Jl</surname></persName>
		</author>
		<author>
			<persName><surname>Bentley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="509" to="517" />
			<date type="published" when="1975">1975</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Image moments: a general and useful set of features for visual servoing</title>
		<author>
			<persName><forename type="first">F</forename><surname>Chaumette</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Robotics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="713" to="723" />
			<date type="published" when="2004-08">August 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">servo control, Part I: Basic approaches</title>
		<author>
			<persName><forename type="first">F</forename><surname>Chaumette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hutchinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Magazine</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="82" to="90" />
			<date type="published" when="2006-12">December 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">2 1/2 D visual servoing: a possible solution to improve image-based and position-based visual servoings</title>
		<author>
			<persName><forename type="first">F</forename><surname>Chaumette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Malis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. on Robotics and Automation</title>
		<meeting><address><addrLine>San Francisco, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000-04">April 2000</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="630" to="635" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Visual servoing set free from image processing</title>
		<author>
			<persName><forename type="first">C</forename><surname>Collewet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Marchand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Chaumette</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. on Robotics and Automation, ICRA'08</title>
		<meeting><address><addrLine>Pasadena, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-05">May 2008</date>
			<biblScope unit="page" from="81" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Triplet loss in siamese network for object tracking</title>
		<author>
			<persName><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="459" to="474" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Visual Servoing in Autoencoder Latent Space</title>
		<author>
			<persName><forename type="first">S</forename><surname>Felton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Brault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Fromont</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Marchand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="3234" to="3241" />
			<date type="published" when="2022-04">April 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep metric learning using triplet network</title>
		<author>
			<persName><forename type="first">E</forename><surname>Hoffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ailon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International workshop on similarity-based pattern recognition</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="84" to="92" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML'15</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning image representations tied to ego-motion</title>
		<author>
			<persName><forename type="first">D</forename><surname>Jayaraman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1413" to="1421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Geometric loss functions for camera pose regression with deep learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="page" from="6555" to="6564" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Posenet: A convolutional network for real-time 6-dof camera relocalization</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Grimes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision, ICCV</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2938" to="2946" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep metric learning beyond binary supervision</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kwak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2288" to="2297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Control camera and light source positions using image gradient information</title>
		<author>
			<persName><forename type="first">E</forename><surname>Marchand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. on Robotics and Automation, ICRA'07</title>
		<meeting><address><addrLine>Roma, Italia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-04">April 2007</date>
			<biblScope unit="page" from="417" to="422" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Subspace-based visual servoing</title>
		<author>
			<persName><forename type="first">E</forename><surname>Marchand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="2699" to="2706" />
			<date type="published" when="2019-07">July 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Direct visual servoing in the frequency domain</title>
		<author>
			<persName><forename type="first">E</forename><surname>Marchand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="620" to="627" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Weight normalization: A simple reparameterization to accelerate training of deep neural networks</title>
		<author>
			<persName><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
			<biblScope unit="page" from="901" to="909" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Exploring convolutional networks for end-to-end visual servoing</title>
		<author>
			<persName><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Pandya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Krishna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. on Robotics and Automation, ICRA'17</title>
		<imprint>
			<date type="published" when="2017-05">May 2017</date>
			<biblScope unit="page" from="3817" to="3823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep multimodal embedding: Manipulating novel objects with point-clouds, language and trajectories</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2794" to="2801" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Google's next-generation real-time unit-selection synthesizer using sequence-tosequence lstm-based autoencoders</title>
		<author>
			<persName><forename type="first">V</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Agiomyrgiannakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Silen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1143" to="1147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning deep structure-preserving image-text embeddings</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="5005" to="5013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Dynamic visual servo control of robots. an adaptive image based approach</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">E</forename><surname>Weiss</surname></persName>
		</author>
		<idno>CMU-RI-TR-84-16</idno>
		<imprint>
			<date type="published" when="1984-04">April 1984</date>
		</imprint>
		<respStmt>
			<orgName>Carnegie-Mellon University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Relative end-effector control using cartesian position-based visual servoing</title>
		<author>
			<persName><forename type="first">W</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hulls</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Bell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Robotics and Automation</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="684" to="696" />
			<date type="published" when="1996-10">October 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Multimodal metric learning for tag-based music retrieval</title>
		<author>
			<persName><forename type="first">M</forename><surname>Won</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Oramas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Nieto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Gouyon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Serra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="591" to="595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Siamese convolutional neural network for sub-millimeter-accurate camera pose estimation and visual servoing</title>
		<author>
			<persName><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Pham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<date type="published" when="2019-11">2019. 11 2019</date>
			<biblScope unit="page" from="935" to="941" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Random erasing data augmentation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI conference on artificial intelligence</title>
		<meeting>the AAAI conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="13001" to="13008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">On the continuity of rotation representations in neural networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5745" to="5753" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</teiCorpus></text>
</TEI>