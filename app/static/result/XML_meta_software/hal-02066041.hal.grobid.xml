<?xml version='1.0' encoding='utf-8'?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" version="1.1" xsi:schemaLocation="http://www.tei-c.org/ns/1.0 http://api.archives-ouvertes.fr/documents/aofr-sword.xsd">
  <teiHeader>
    <fileDesc>
      <titleStmt>
        <title>HAL TEI export of hal-02066041</title>
      </titleStmt>
      <publicationStmt>
        <distributor>CCSD</distributor>
        <availability status="restricted">
          <licence target="http://creativecommons.org/licenses/by/">Attribution</licence>
        </availability>
        <date when="2024-04-19T18:27:12+02:00" />
      </publicationStmt>
      <sourceDesc>
        <p part="N">HAL API platform</p>
      </sourceDesc>
    </fileDesc>
  </teiHeader>
  <text>
    <body>
      <listBibl>
        <biblFull>
          <titleStmt>
            <title xml:lang="en">Learning How to Correct a Knowledge Base from the Edit History</title>
            <author role="aut">
              <persName>
                <forename type="first">Thomas</forename>
                <forename type="middle">Pellissier</forename>
                <surname>Tanon</surname>
              </persName>
              <email type="md5">306d387b66e726f678c5738e1722b560</email>
              <email type="domain">pellissier-tanon.fr</email>
              <idno type="idhal" notation="string">ttanon</idno>
              <idno type="idhal" notation="numeric">18226</idno>
              <idno type="halauthorid" notation="string">1263215-18226</idno>
              <idno type="ORCID">https://orcid.org/0000-0002-0620-6486</idno>
              <idno type="GOOGLE SCHOLAR">https://scholar.google.fr/citations?user=jIrS0PQAAAAJ&amp;hl=fr</idno>
              <affiliation ref="#struct-484335" />
            </author>
            <author role="aut">
              <persName>
                <forename type="first">Camille</forename>
                <surname>Bourgaux</surname>
              </persName>
              <email type="md5">aff0be14a3592b621b6ffddb43408ce3</email>
              <email type="domain">ens.fr</email>
              <idno type="idhal" notation="string">camille-bourgaux</idno>
              <idno type="idhal" notation="numeric">184719</idno>
              <idno type="halauthorid" notation="string">22200-184719</idno>
              <idno type="ARXIV">https://arxiv.org/a/bourgaux_c_1</idno>
              <idno type="ORCID">https://orcid.org/0000-0002-8806-6682</idno>
              <affiliation ref="#struct-478610" />
            </author>
            <author role="aut">
              <persName>
                <forename type="first">Fabian</forename>
                <forename type="middle">M.</forename>
                <surname>Suchanek</surname>
              </persName>
              <email type="md5">ea5bf13de9841dd0a0a6f9e33eb04e13</email>
              <email type="domain">suchanek.name</email>
              <idno type="idhal" notation="string">fabian-suchanek</idno>
              <idno type="idhal" notation="numeric">12540</idno>
              <idno type="halauthorid" notation="string">1052285-12540</idno>
              <idno type="IDREF">https://www.idref.fr/203477707</idno>
              <idno type="ORCID">https://orcid.org/0000-0001-7189-2796</idno>
              <idno type="GOOGLE SCHOLAR">djtZhi8AAAAJ</idno>
              <affiliation ref="#struct-484335" />
            </author>
            <editor role="depositor">
              <persName>
                <forename>Thomas</forename>
                <surname>Pellissier Tanon</surname>
              </persName>
              <email type="md5">306d387b66e726f678c5738e1722b560</email>
              <email type="domain">pellissier-tanon.fr</email>
            </editor>
            <funder ref="#projanr-44439" />
          </titleStmt>
          <editionStmt>
            <edition n="v1" type="current">
              <date type="whenSubmitted">2019-03-13 10:23:57</date>
              <date type="whenModified">2024-04-19 16:18:58</date>
              <date type="whenReleased">2019-03-25 09:57:10</date>
              <date type="whenProduced">2019-05-13</date>
              <date type="whenEndEmbargoed">2019-03-13</date>
              <ref type="file" target="https://imt.hal.science/hal-02066041/document">
                <date notBefore="2019-03-13" />
              </ref>
              <ref type="file" subtype="greenPublisher" n="1" target="https://imt.hal.science/hal-02066041/file/2019-WWW-corhist.pdf">
                <date notBefore="2019-03-13" />
              </ref>
            </edition>
            <respStmt>
              <resp>contributor</resp>
              <name key="604918">
                <persName>
                  <forename>Thomas</forename>
                  <surname>Pellissier Tanon</surname>
                </persName>
                <email type="md5">306d387b66e726f678c5738e1722b560</email>
                <email type="domain">pellissier-tanon.fr</email>
              </name>
            </respStmt>
          </editionStmt>
          <publicationStmt>
            <distributor>CCSD</distributor>
            <idno type="halId">hal-02066041</idno>
            <idno type="halUri">https://imt.hal.science/hal-02066041</idno>
            <idno type="halBibtex">tanon:hal-02066041</idno>
            <idno type="halRefHtml">&lt;i&gt;World Wide Web Conference&lt;/i&gt;, May 2019, San Francisco, United States. &lt;a target="_blank" href="https://dx.doi.org/10.1145/3308558.3313584"&gt;&amp;#x27E8;10.1145/3308558.3313584&amp;#x27E9;&lt;/a&gt;</idno>
            <idno type="halRef">World Wide Web Conference, May 2019, San Francisco, United States. &amp;#x27E8;10.1145/3308558.3313584&amp;#x27E9;</idno>
            <availability status="restricted">
              <licence target="http://creativecommons.org/licenses/by/">Attribution</licence>
            </availability>
          </publicationStmt>
          <seriesStmt>
            <idno type="stamp" n="INSTITUT-TELECOM">Institut Mines Télécom</idno>
            <idno type="stamp" n="ENS-PARIS">Ecole Normale Supérieure de Paris</idno>
            <idno type="stamp" n="CNRS">CNRS - Centre national de la recherche scientifique</idno>
            <idno type="stamp" n="INRIA">INRIA - Institut National de Recherche en Informatique et en Automatique</idno>
            <idno type="stamp" n="INRIA-ROCQ">INRIA Paris - Rocquencourt</idno>
            <idno type="stamp" n="TELECOM-PARISTECH" corresp="INSTITUT-TELECOM">Télécom Paris</idno>
            <idno type="stamp" n="PARISTECH">ParisTech</idno>
            <idno type="stamp" n="TESTALAIN1">TESTALAIN1</idno>
            <idno type="stamp" n="INRIA2">INRIA 2</idno>
            <idno type="stamp" n="PSL">Université Paris sciences et lettres</idno>
            <idno type="stamp" n="INRIA-PSL">INRIA-PSL</idno>
            <idno type="stamp" n="LTCI" corresp="TELECOM-PARISTECH">Laboratoire Traitement et Communication de l'Information</idno>
            <idno type="stamp" n="INFRES" corresp="TELECOM-PARISTECH">Département Informatique et Réseaux</idno>
            <idno type="stamp" n="DIG" corresp="TELECOM-PARISTECH">Equipe Data, Intelligence and Graphs</idno>
            <idno type="stamp" n="INSTITUTS-TELECOM" corresp="INSTITUT-TELECOM">composantes instituts telecom </idno>
            <idno type="stamp" n="ENS-PSL" corresp="PSL">École normale supérieure - PSL</idno>
            <idno type="stamp" n="ANR">ANR</idno>
            <idno type="stamp" n="DIENS" corresp="ENS-PARIS">Département d'informatique de l'ENS-PSL</idno>
          </seriesStmt>
          <notesStmt>
            <note type="audience" n="2">International</note>
            <note type="invited" n="0">No</note>
            <note type="popular" n="0">No</note>
            <note type="peer" n="1">Yes</note>
            <note type="proceedings" n="1">Yes</note>
          </notesStmt>
          <sourceDesc>
            <biblStruct>
              <analytic>
                <title xml:lang="en">Learning How to Correct a Knowledge Base from the Edit History</title>
                <author role="aut">
                  <persName>
                    <forename type="first">Thomas</forename>
                    <forename type="middle">Pellissier</forename>
                    <surname>Tanon</surname>
                  </persName>
                  <email type="md5">306d387b66e726f678c5738e1722b560</email>
                  <email type="domain">pellissier-tanon.fr</email>
                  <idno type="idhal" notation="string">ttanon</idno>
                  <idno type="idhal" notation="numeric">18226</idno>
                  <idno type="halauthorid" notation="string">1263215-18226</idno>
                  <idno type="ORCID">https://orcid.org/0000-0002-0620-6486</idno>
                  <idno type="GOOGLE SCHOLAR">https://scholar.google.fr/citations?user=jIrS0PQAAAAJ&amp;hl=fr</idno>
                  <affiliation ref="#struct-484335" />
                </author>
                <author role="aut">
                  <persName>
                    <forename type="first">Camille</forename>
                    <surname>Bourgaux</surname>
                  </persName>
                  <email type="md5">aff0be14a3592b621b6ffddb43408ce3</email>
                  <email type="domain">ens.fr</email>
                  <idno type="idhal" notation="string">camille-bourgaux</idno>
                  <idno type="idhal" notation="numeric">184719</idno>
                  <idno type="halauthorid" notation="string">22200-184719</idno>
                  <idno type="ARXIV">https://arxiv.org/a/bourgaux_c_1</idno>
                  <idno type="ORCID">https://orcid.org/0000-0002-8806-6682</idno>
                  <affiliation ref="#struct-478610" />
                </author>
                <author role="aut">
                  <persName>
                    <forename type="first">Fabian</forename>
                    <forename type="middle">M.</forename>
                    <surname>Suchanek</surname>
                  </persName>
                  <email type="md5">ea5bf13de9841dd0a0a6f9e33eb04e13</email>
                  <email type="domain">suchanek.name</email>
                  <idno type="idhal" notation="string">fabian-suchanek</idno>
                  <idno type="idhal" notation="numeric">12540</idno>
                  <idno type="halauthorid" notation="string">1052285-12540</idno>
                  <idno type="IDREF">https://www.idref.fr/203477707</idno>
                  <idno type="ORCID">https://orcid.org/0000-0001-7189-2796</idno>
                  <idno type="GOOGLE SCHOLAR">djtZhi8AAAAJ</idno>
                  <affiliation ref="#struct-484335" />
                </author>
              </analytic>
              <monogr>
                <meeting>
                  <title>World Wide Web Conference</title>
                  <date type="start">2019-05-13</date>
                  <date type="end">2019-05-17</date>
                  <settlement>San Francisco</settlement>
                  <country key="US">United States</country>
                </meeting>
                <imprint>
                  <biblScope unit="serie">Proceedings of the 2019 World Wide Web Conference (WWW ’19)</biblScope>
                </imprint>
              </monogr>
              <idno type="doi">10.1145/3308558.3313584</idno>
            </biblStruct>
          </sourceDesc>
          <profileDesc>
            <langUsage>
              <language ident="en">English</language>
            </langUsage>
            <textClass>
              <keywords scheme="author">
                <term xml:lang="en">Data cleaning</term>
                <term xml:lang="en">Rule mining</term>
                <term xml:lang="en">Wikidata</term>
                <term xml:lang="en">History</term>
                <term xml:lang="en">Knowledge base</term>
              </keywords>
              <classCode scheme="halDomain" n="info.info-ai">Computer Science [cs]/Artificial Intelligence [cs.AI]</classCode>
              <classCode scheme="halDomain" n="info.info-db">Computer Science [cs]/Databases [cs.DB]</classCode>
              <classCode scheme="halTypology" n="COMM">Conference papers</classCode>
              <classCode scheme="halOldTypology" n="COMM">Conference papers</classCode>
              <classCode scheme="halTreeTypology" n="COMM">Conference papers</classCode>
            </textClass>
            <abstract xml:lang="en">
              <p>The curation of a knowledge base is a crucial but costly task. In this work, we propose to take advantage of the edit history of the knowledge base in order to learn how to correct constraint violations. Our method is based on rule mining, and uses the edits that solved some violations in the past to infer how to solve similar violations in the present. The experimental evaluation of our method on Wikidata shows significant improvements over baselines.</p>
            </abstract>
          </profileDesc>
        </biblFull>
      </listBibl>
    </body>
    <back>
      <listOrg type="structures">
        <org type="laboratory" xml:id="struct-484335" status="VALID">
          <idno type="IdRef">162384270</idno>
          <idno type="ISNI">0000 0000 9194 9502</idno>
          <idno type="RNSR">200319327Z</idno>
          <idno type="ROR">https://ror.org/057er4c39</idno>
          <orgName>Laboratoire Traitement et Communication de l'Information</orgName>
          <orgName type="acronym">LTCI</orgName>
          <date type="start">2017-01-01</date>
          <desc>
            <address>
              <addrLine>Télécom Paris 19 Place Marguerite Perey 91120 PALAISEAU</addrLine>
              <country key="FR" />
            </address>
            <ref type="url">https://www.telecom-paris.fr/fr/recherche/laboratoires/laboratoire-traitement-et-communication-de-linformation-ltci</ref>
          </desc>
          <listRelation>
            <relation active="#struct-302102" type="direct" />
            <relation active="#struct-1048346" type="direct" />
          </listRelation>
        </org>
        <org type="researchteam" xml:id="struct-478610" status="VALID">
          <idno type="RNSR">201622223R</idno>
          <orgName>Value from Data</orgName>
          <orgName type="acronym">VALDA </orgName>
          <date type="start">2016-12-01</date>
          <desc>
            <address>
              <addrLine>ENS Paris</addrLine>
              <country key="FR" />
            </address>
            <ref type="url">https://www.inria.fr/equipes/valda</ref>
          </desc>
          <listRelation>
            <relation active="#struct-25027" type="direct" />
            <relation active="#struct-59704" type="indirect" />
            <relation active="#struct-564132" type="indirect" />
            <relation active="#struct-300009" type="indirect" />
            <relation name="UMR8548" active="#struct-441569" type="indirect" />
            <relation active="#struct-454310" type="direct" />
          </listRelation>
        </org>
        <org type="regroupinstitution" xml:id="struct-302102" status="VALID">
          <idno type="ROR">https://ror.org/025vp2923</idno>
          <orgName>Institut Mines-Télécom [Paris]</orgName>
          <orgName type="acronym">IMT</orgName>
          <date type="start">2012-03-01</date>
          <desc>
            <address>
              <addrLine>37-39 Rue Dareau, 75014 Paris</addrLine>
              <country key="FR" />
            </address>
            <ref type="url">http://www.mines-telecom.fr/</ref>
          </desc>
        </org>
        <org type="institution" xml:id="struct-1048346" status="VALID">
          <idno type="IdRef">026375273</idno>
          <idno type="ISNI">0000 0001 2108 2779</idno>
          <idno type="ROR">https://ror.org/01naq7912</idno>
          <orgName>Télécom Paris</orgName>
          <date type="start">2019-06-12</date>
          <desc>
            <address>
              <addrLine>19 Place Marguerite Perey 91120 Palaiseau </addrLine>
              <country key="FR" />
            </address>
            <ref type="url">https://www.telecom-paris.fr</ref>
          </desc>
        </org>
        <org type="regrouplaboratory" xml:id="struct-25027" status="VALID">
          <idno type="IdRef">148034055</idno>
          <idno type="RNSR">199812876J</idno>
          <orgName>Département d'informatique - ENS Paris</orgName>
          <orgName type="acronym">DI-ENS</orgName>
          <date type="start">1999-01-01</date>
          <desc>
            <address>
              <addrLine>École normale supérieure 45 rue d'Ulm F-75230 Paris Cedex 05</addrLine>
              <country key="FR" />
            </address>
            <ref type="url">http://www.di.ens.fr/</ref>
          </desc>
          <listRelation>
            <relation active="#struct-59704" type="direct" />
            <relation active="#struct-564132" type="indirect" />
            <relation active="#struct-300009" type="direct" />
            <relation name="UMR8548" active="#struct-441569" type="direct" />
          </listRelation>
        </org>
        <org type="institution" xml:id="struct-59704" status="VALID">
          <idno type="IdRef">031738419</idno>
          <idno type="ISNI">0000000123532622</idno>
          <idno type="ROR">https://ror.org/05a0dhs15</idno>
          <orgName>École normale supérieure - Paris</orgName>
          <orgName type="acronym">ENS-PSL</orgName>
          <date type="start">1985-07-24</date>
          <desc>
            <address>
              <addrLine>45, Rue d'Ulm - 75230 Paris cedex 05</addrLine>
              <country key="FR" />
            </address>
            <ref type="url">https://www.ens.psl.eu/</ref>
          </desc>
          <listRelation>
            <relation active="#struct-564132" type="direct" />
          </listRelation>
        </org>
        <org type="regroupinstitution" xml:id="struct-564132" status="VALID">
          <idno type="IdRef">241597595</idno>
          <idno type="ISNI">0000 0004 1784 3645</idno>
          <orgName>Université Paris Sciences et Lettres</orgName>
          <orgName type="acronym">PSL</orgName>
          <desc>
            <address>
              <addrLine>60 rue Mazarine 75006 Paris</addrLine>
              <country key="FR" />
            </address>
            <ref type="url">https://www.psl.eu/</ref>
          </desc>
        </org>
        <org type="institution" xml:id="struct-300009" status="VALID">
          <idno type="ROR">https://ror.org/02kvxyf05</idno>
          <orgName>Institut National de Recherche en Informatique et en Automatique</orgName>
          <orgName type="acronym">Inria</orgName>
          <desc>
            <address>
              <addrLine>Domaine de VoluceauRocquencourt - BP 10578153 Le Chesnay Cedex</addrLine>
              <country key="FR" />
            </address>
            <ref type="url">http://www.inria.fr/en/</ref>
          </desc>
        </org>
        <org type="regroupinstitution" xml:id="struct-441569" status="VALID">
          <idno type="IdRef">02636817X</idno>
          <idno type="ISNI">0000000122597504</idno>
          <idno type="ROR">https://ror.org/02feahw73</idno>
          <orgName>Centre National de la Recherche Scientifique</orgName>
          <orgName type="acronym">CNRS</orgName>
          <date type="start">1939-10-19</date>
          <desc>
            <address>
              <country key="FR" />
            </address>
            <ref type="url">https://www.cnrs.fr/</ref>
          </desc>
        </org>
        <org type="laboratory" xml:id="struct-454310" status="VALID">
          <idno type="IdRef">241614864</idno>
          <idno type="RNSR">196718247G</idno>
          <orgName>Inria de Paris</orgName>
          <date type="start">2016-03-10</date>
          <desc>
            <address>
              <addrLine>2 rue Simone Iff -CS 42112 -75589 Paris Cedex 12</addrLine>
              <country key="FR" />
            </address>
            <ref type="url">http://www.inria.fr/centre/paris</ref>
          </desc>
          <listRelation>
            <relation active="#struct-300009" type="direct" />
          </listRelation>
        </org>
      </listOrg>
      <listOrg type="projects">
        <org type="anrProject" xml:id="projanr-44439" status="VALID">
          <idno type="anr">ANR-16-CE23-0007</idno>
          <orgName>DICOS</orgName>
          <desc>Découverte de schémas complexes dans les bases de connaissances</desc>
          <date type="start">2016</date>
        </org>
      </listOrg>
    </back>
  <teiCorpus>
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning How to Correct a Knowledge Base from the Edit History</title>
				<funder ref="#_jKKgRtn">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher />
				<availability status="unknown"><licence /></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Thomas</forename><forename type="middle">Pellissier</forename><surname>Tanon</surname></persName>
							<email>ttanon@enst.fr</email>
							<affiliation key="aff0">
								<orgName type="institution">Télécom ParisTech</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Camille</forename><surname>Bourgaux</surname></persName>
							<email>camille.bourgaux@ens.fr</email>
							<affiliation key="aff1">
								<orgName type="laboratory">DI ENS</orgName>
								<orgName type="institution" key="instit1">CNRS</orgName>
								<orgName type="institution" key="instit2">ENS</orgName>
								<orgName type="institution" key="instit3">PSL Univ. &amp; Inria</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Fabian</forename><surname>Suchanek</surname></persName>
							<email>suchanek@enst.fr</email>
							<affiliation key="aff2">
								<orgName type="institution">Télécom ParisTech</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Learning How to Correct a Knowledge Base from the Edit History</title>
					</analytic>
					<monogr>
						<imprint>
							<date />
						</imprint>
					</monogr>
					<idno type="MD5">707AB0FBCBC61D6588B863B574D831FB</idno>
					<idno type="DOI">10.1145/3308558.3313584</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-04-12T14:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid" />
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>knowledge base</term>
					<term>history</term>
					<term>data cleaning</term>
					<term>rule mining</term>
					<term>Wikidata</term>
				</keywords>
			</textClass>
			<abstract>
<div><p>The curation of a knowledge base is a crucial but costly task. In this work, we propose to take advantage of the edit history of the knowledge base in order to learn how to correct constraint violations. Our method is based on rule mining, and uses the edits that solved some violations in the past to infer how to solve similar violations in the present. The experimental evaluation of our method on <software>Wikidata</software> shows significant improvements over baselines.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div><p>birth date". If the rule is taken as an ontological rule, then it would just infer that Spinoza has some birth date. If the rule is taken as a constraint, in contrast, the KB would be considered incorrect. Constraints are thus similar in spirit to database integrity constraints. In practice, constraints often have exceptions. Therefore, it is useful to allow data that does not respect them (in <software>Wikidata</software>, e.g., constraint violations are simply flagged). Nonetheless, by design, most of the constraint violations are not exceptions but actual errors and proposing to repair them is a good starting point when it comes to improving KB quality.</p><p>In this paper, we aim at learning how to repair constraint violations. Our goal is to help a KB editor by suggesting how to clean the data locally (providing a solution to a particular constraint violation) or globally (providing rules that can be automatically applied to all constraint violations of a given form once validated by the editor). To do that, we take advantage of the edit history of the KB. We use it to mine correction rules that express how different kinds of constraint violations are usually solved. To the best of our knowledge, this is the first work that builds on past users corrections in order to infer possible new ones. We validate our framework experimentally on <software>Wikidata</software>, for which the whole edit history of more than 700 millions edits is available. Our experiments show substantial improvements over baselines. More concretely, our contributions are as follows:</p><p>• a formal definition of the problem of correction rule mining,</p><p>• a dataset of more than 67M past corrections for ten different kinds of <software ContextAttributes="used">Wikidata</software> constraints (13k constraints in total), <ref type="foot" target="#foot_0">1</ref>• a correction rule mining algorithm, together with an implementation for <software ContextAttributes="used">Wikidata</software>, <software ContextAttributes="used">CorHist</software>,<ref type="foot" target="#foot_1">2</ref> • a suggestion tool for users to correct data based on our mined correction rules,<ref type="foot" target="#foot_2">3</ref> • an experimental evaluation based both on the prediction of the corrections in the history and on user validation of the suggested local corrections.</p><p>facts are not necessarily false. They thus classically have only "correctness" constraints, such as disjointness or functionality axioms (corresponding to special cases of denial constraints and equality generating dependencies in databases).</p><p>To express also completeness constraints, several works propose to use description logics, with varying semantics <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b36">37]</ref>. Another possibility is to use queries that should or should not hold as constraints (see e.g., <ref type="bibr" target="#b23">[24]</ref> for methods for writing constraint queries in SPARQL). Other approaches define constraint languages to specify conditions for RDF graph <ref type="bibr" target="#b8">[9]</ref> validation, such as SHACL <ref type="bibr" target="#b21">[22]</ref> or ShEx <ref type="bibr" target="#b7">[8]</ref>. It has been argued in <ref type="bibr" target="#b29">[30]</ref> that description logics under the closed world assumption are also suitable for constraint checking in RDF, which can then be implemented with SPARQL queries. In our work, we follow a similar path, using description logic axioms as constraints for RDFS KBs, because it corresponds best to what we observe in current real-world KBs.</p><p>Contrary to the above works, we do not aim at expressing constraints, but at repairing their violations. The correction rules we learn for this purpose are similar in spirit to active integrity constraints <ref type="bibr" target="#b10">[11]</ref>, which specify for each constraint a set of possible repair actions. This type of constraints has recently been applied to description logic KBs as well <ref type="bibr" target="#b31">[32]</ref>. Conditioned active integrity constraints add conditions for choosing among the possible actions, and we propose, in a similar spirit, to take into account the context of the constraint violation to correct it. Different from the existing work <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b31">32]</ref>, our goal is to mine correction rules automatically from the edit history of the KB.</p><p>Knowledge base cleaning. Several recent approaches have dealt with the interactive cleaning of KBs. The proposed methods detect when a constraint is violated, compute the responsible facts, and then interact with the user to find out how to update the KB. The goal is then to minimize the number of questions the user has to answer. This is done in various ways, which include taking into account the dependencies among the facts to check or the interaction between several constraints violations to define heuristics to choose the best question to ask the user <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6]</ref>.</p><p>Other approaches to improve the quality of a KB rely on statistics, clustering, or structural aspects of the KBs. The work of <ref type="bibr" target="#b30">[31]</ref> uses statistics to add missing types to the KB, and to detect wrong statements. The work of <ref type="bibr" target="#b24">[25]</ref> exploits the observation that cycles in the KB often contain wrong "IsA" relations. Again other approaches <ref type="bibr" target="#b0">[1]</ref> use crowdsourcing to detect Linked Data quality issues. We refer the reader to Section 7.2 of <ref type="bibr" target="#b0">[1]</ref> for a recent overview of approaches for data quality assessment.</p><p>Our method also exploits KB constraints. However, it differs from the above in that it learns the corrections automatically from the edit history. It thus taps a source of knowledge that has so far not been exploited.</p><p>Rule learning. Mining logical rules by finding correlations in a dataset is a well-established research topic. In particular, learning patterns in the data can be used for completing KBs <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b35">36]</ref>. An algorithm for learning conjunctive patterns from a KB enriched with a set of rules is described in <ref type="bibr" target="#b19">[20]</ref>. Methods similar to association rule mining have also been used for induction of new ontological rules from a KB <ref type="bibr" target="#b32">[33]</ref>. A more recent trend is to use embedding-based models for KB completion. A comparison between these models and usual rule learning approaches is reported in <ref type="bibr" target="#b26">[27]</ref> and significant recent works in this area include <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b39">40]</ref>.</p><p>In this paper, we use a vanilla rule mining algorithm inspired by <ref type="bibr" target="#b13">[14]</ref>. Our contribution is not the rule mining per se, but the application of rule mining to the edit history of a KB in order to mine correction rules. This avenue has, to the best of our knowledge, never been investigated.</p></div>
<div><head n="3">PRELIMINARIES</head><p>In this work, we use description logics (DL) <ref type="bibr" target="#b3">[4]</ref> as KB language and as constraint language, because they are the foundation of the Semantic Web standard OWL <ref type="bibr" target="#b15">[16]</ref>.</p><p>Syntax. We assume a set N C of concept names (unary predicates, also called classes), a set N R of role names (binary predicates, also called properties), and a set N I of individuals (also called constants). An ABox (dataset) is a set of concept or role assertions of the form</p><formula xml:id="formula_0">A(a) or R(a, b), where A ∈ N C , R ∈ N R , a, b ∈ N I . A TBox (ontology)</formula><p>is a set of axioms whose form depends on the DL L in question, and expresses relationships between concepts and roles (e.g., concept or role hierarchies, role domains and ranges...). A knowledge base (KB) K = T ∪ A is the union of an ABox A and a TBox T .</p><p>In this work, we assume that T is a flat QL TBox <ref type="bibr" target="#b22">[23]</ref>, i.e., that L differs from the standard RDF Schema (RDFS) <ref type="bibr" target="#b16">[17]</ref> only by allowing inverse roles in role inclusions. More precisely, T can contain concept inclusions of the form A 1 ⊑ A 2 (subclass), ∃P ⊑ A (domain or range), and role inclusions P 1 ⊑ P 2 (subproperty), where</p><formula xml:id="formula_1">A (i) ∈ N C and P (i) := R | R -with R ∈ N R .</formula><p>A KB can also be written as a set of RDF triples ⟨s, p, o⟩ where s is the subject, p is the property, and o the object, using special properties to translate concept membership and relationships between concepts and roles <ref type="bibr" target="#b28">[29]</ref>. A concept assertion A(a) is written as ⟨a, rdf:type, A⟩, and a role assertion R(a, b) as ⟨a, R, b⟩. Flat QL TBox axioms can also be represented by single triples. For example, A 1 ⊑ A 2 is written as ⟨A 1 , rdfs:subClassOf, A 2 ⟩ and ∃R -⊑ A is written as ⟨R, rdfs:range, A⟩.</p><p>Semantics. We recall the standard semantics of DL KBs. An interpretation has the form I = (∆ I , • I ), where ∆ I is a non-empty set and • I is a function that injectively maps each a ∈ N I to a I ∈ ∆ I (unique name assumption), ⊤ to ∆ I , each A ∈ N C to A I ⊆ ∆ I , and each R ∈ N R to R I ⊆ ∆ I ×∆ I . The function • I is straightforwardly extended to general concepts and roles, e.g.</p><formula xml:id="formula_2">(¬B) I = ∆ I \ B I , (R -) I = {(c, d) | (d, c) ∈ R I }, {a 1 , . . . , a n } = {a I 1 , . . . , a I n }, (∃P • B) I = {c | ∃d : (c, d) ∈ P I , d ∈ B I }, (B 1 ⊓ B 2 ) I = B I 1 ∩ B I 2 , (B 1 ⊔ B 2 ) I = B I 1 ∪ B I 2 .</formula><p>An interpretation I satisfies an inclusion G ⊑ H , if G I ⊆ H I ; it satisfies an axiom (func P) if P I is functional; it satisfies an axiom (trans P) if P I is transitive; and it satisfies A(a)</p><formula xml:id="formula_3">(resp. R(a, b)), if a I ∈ A I (resp. (a I , b I ) ∈ R I ). We write I |= α if I satisfies the DL axiom α. An interpretation I is a model of K = T ∪ A if I satisfies all axioms in K. A KB is consistent if it has a model. A KB K entails a DL axiom α if I |= α for every model I of K. Queries. A conjunctive query (CQ) takes the form q(ì x) = ∃ì yψ (ì x, ì y),</formula><p>where ψ is a conjunction of atoms of the form A(t) or R(t, t ′ ) or of equalities t = t ′ , where t, t ′ are individual names or variables from ì x ∪ ì y. If ì x = ∅, q is a Boolean CQ (BCQ). A BCQ q is satisfied by an interpretation I, written I |= q, if there is a homomorphism π mapping the variables and individual names of q into ∆ I such that: π (a) = a I for every a ∈ N I , π (t) ∈ A I for every concept atom A(t) in ψ , (π (t), π (t ′ )) ∈ R I for every role atom R(t, t ′ ) in ψ , and π (t) = π (t ′ ) for every t = t ′ in ψ . We also consider as BCQs the queries true and false which are respectively always and never satisfied by an interpretation. A BCQ q is entailed from K, written K |= q, iff q is satisfied by every model of K. A tuple of constants ì a is a (certain) answer to a CQ q(ì x) if K |= q(ì a) where q(ì a) is the BCQ obtained by replacing the variables from ì x by the constants ì a.</p><p>We denote by answers(q(ì x), K) the set of answers of q(ì x) over K.</p><p>A union of CQs (UCQ) is a disjunction of CQs and has as answers the union of the answers of the CQs it contains.</p><p>Canonical model. It is well-known that a flat QL KB K has a canonical model I K such that for every BCQ q, K |= q iff I K |= q. The domain of I K is the set of individual names that occur in K and <ref type="bibr" target="#b22">[23]</ref>.</p><formula xml:id="formula_4">A I K = {a | A |= B(a), T |= B ⊑ A} for every A ∈ N C and R I K = {(a, b) | A |= P(a, b), T |= P ⊑ R} for every R ∈ N R</formula></div>
<div><head n="4">CONSTRAINTS</head><p>This section defines the constraints that can be imposed on a KB, and relates the problem of checking that a KB complies with these constraints to CQ answering over this KB.</p><p>Defining constraints. In this work, we consider two types of constraints: consistency constraints (which express that some statements are contradictory), and completeness constraints (which impose that certain statements should hold in the KB as soon as some others do). While violations of consistency constraints can only be solved by removing statements, those of completeness constraints can also be solved by adding statements. Definition 1 (Constraint): Constraints are built from complex concepts and roles defined by the following grammar rules: blahP</p><formula xml:id="formula_5">:= R | R - blahB := ⊤ | A | B ⊓ B | B ⊔ B | ∃P • {a 1 , . . . , a n } | ∃P • B where R ∈ N R , A ∈ N C , a 1 , . . . , a n ∈ N I .</formula><p>A consistency constraint is a concept inclusion of the form B 1 ⊑ ¬B 2 or of the form B ⊑ {a 1 , . . . , a n }, a role inclusion of the form P 1 ⊑ ¬P 2 , or a functionality axiom of the form (func P).</p><p>A completeness constraint is a concept inclusion of the form B 1 ⊑ B 2 , a role inclusion of the form P 1 ⊑ P 2 , or a transitivity axiom of the form (trans P).</p><p>This definition of constraints covers the "constraining" versions of the majority of the most popular DL axioms used on the Web of Data according to the ranking done by <ref type="bibr" target="#b14">[15]</ref>.</p><p>We assume without loss of generality that concepts of the form B 1 ⊔ B 2 or {a 1 , . . . , a n } with n &gt; 1 appear only on the right side of inclusions, and not at all in negative inclusions of the form B 1 ⊑ ¬B 2 . For example, we assume that ∃P</p><formula xml:id="formula_6">• (B 1 ⊔ B 2 ) ⊑ C is rewritten as ∃P • B 1 ⊑ C, ∃P • B 2 ⊑ C, and B ⊑ ¬∃P • {a 1 , . . . , a n } is rewritten as B ⊑ ¬∃P • {a 1 }, . . . , B ⊑ ¬∃P • {a n }.</formula><p>As usual, we abbreviate ∃P • ⊤ as ∃P. Example 1: As a running example, we consider the following KB K = T ∪ A and set of constraints C inspired from <software>Wikidata</software>. Our TBox T expresses that human beings and deities are persons. Our ABox A provides information on several individuals. Our constraints C state that there are three possible genders (consistency constraint), that those who have a mother or are a mother must be persons or animals, that a mother must have gender female, and that if a has mother b, then b must have child a (completeness constraints). T = { Human ⊑ Person, Deity ⊑ Person } A = { Deity(Zeus), Deity(Rhea), hasGender(Zeus, masculine), hasGender(Rhea, female), hasMother(Zeus, Rhea), hasChild(Rhea, Zeus), Human(Spinoza), hasMother(Spinoza, Marques)</p><formula xml:id="formula_7">} C = { Γ 0 = ∃hasGender -⊑ {male, female, nonbinary}, Γ 1 = ∃hasMother ⊑ Person ⊔ Animal, Γ 2 = ∃hasMother -⊑ Person ⊔ Animal, Γ 3 = ∃hasMother -⊑ ∃hasGender • {female}, Γ 4 = hasMother ⊑ hasChild -} ◁ We say that a KB K satisfies a constraint Γ ∈ C if I K |= Γ, where I K is the canonical model of K. Otherwise, K violates Γ.</formula><p>Example 2: In our running example, the KB K satisfies Γ 1 since ∃hasMother I K = {Zeus, Spinoza} and I K |= Person(Zeus) and I K |= Person(Spinoza). However, it violates Γ 2 because I K ̸ |= Person(Marques) ∨ Animal(Marques) while Marques ∈ ∃hasMother -I K . It violates Γ 3 and Γ 4 for similar reasons. Finally, {hasGender(Zeus, masculine), Γ 0 } has no model because of the unique name assumption (which enforces that the interpretation of masculine differs from those of male, female and nonbinary). Hence, I K cannot be a model of Γ 0 . Thus, K violates Γ 0 . ◁ Note the semantic difference between the constraints and the axioms of the TBox: The axiom (Human ⊑ Person) in the TBox makes every human an answer to the query asking for persons. In contrast, if we had put the axiom in the set of constraints, it would have required all human beings in the KB to be explicitly marked as persons. As another example, consider the axiom (func hasBirthdate) (which says that everyone can have at most one birth date). If this axiom appears in the TBox, it renders the KB inconsistent whenever a person is given two distinct dates of birth. This has severe consequences on the reasoning capabilities, since everything is entailed from an inconsistent KB. If this axiom is in the set of constraints, in contrast, then distinct dates of birth lead only to the violation of the constraint. This gives us relevant information without having any impact on the usability of the KB.</p><p>Table <ref type="table">1</ref>: Translation of DL axioms into rules. Variables that appear in the right column and not in the left one are fresh.</p><formula xml:id="formula_8">π (⊤, x) = true π (A, x) = A(x) π ({a 1 , . . . , a n }, x) = x = a 1 ∨ • • • ∨ x = a n π (R, x, y) = R(x, y) π (R -, x, y) = π (R, y, x) π (B 1 ⊓ B 2 , x) = π (B 1 , x) ∧ π (B 2 , x) π (B 1 ⊔ B 2 , x) = π (B 1 , x) ∨ π (B 2 , x) π (∃P • B, x) = ∃y(π (P, x, y) ∧ π (B, y)) π (B ⊑ C) = π (B, x) → π (C, x) π (P ⊑ Q) = π (P, x, y) → π (Q, x, y) π (B ⊑ ¬C) = π (B, x) ∧ π (C, x) → false π (P ⊑ ¬Q) = π (P, x, y) ∧ π (Q, x, y) → false π (func P) = π (P, x, y) ∧ π (P, x, z) → y = z π (trans P) = π (P, x, y) ∧ π (P, y, z) → π (P, x, z)</formula><p>The following proposition shows that this transformation is sound and that the rule body and head can be rewritten as CQ and UCQ.</p><p>Proposition 1: For every constraint Γ ∈ C, π (Γ) can be rewritten as a rule</p><formula xml:id="formula_9">Γ(ì x) : b(ì x) → h(ì x) where b(ì x) is a CQ, h(ì x) is a UCQ, and for every flat QL KB K, K satisfies Γ iff answers(b(ì x), K) ⊆ answers(h(ì x), K).</formula><p>Proof. By our assumptions on the form of the concepts that occur in the left side of the inclusions or in the right side of a negative inclusion, b(ì x) := ∃ì yφ(ì x, ì y) is a CQ. It is easy to show by structural induction that for every concept B (resp. role P), π (B, x) (resp. π (P, x, y)) can be written as a UCQ q(x) (resp. q(x, y)) and that answers(q(x), K) = B I K (resp. answers(q(x, y), K)</p><formula xml:id="formula_10">= P I K ). If Γ is a completeness constraint of the form B 1 ⊑ B 2 (resp. P 1 ⊑ P 2 ),</formula><p>or a consistency constraint of the form B ⊑ {a 1 , . . . , a n }, the result follows immediately since K satisfies</p><formula xml:id="formula_11">Γ iff I K |= Γ. If Γ is a consistency constraint of the form B 1 ⊑ ¬B 2 (resp. P 1 ⊑ ¬P 2 ), answers(b(ì x), K) = B I K 1 ∩ B I K 2 (resp. answers(b(ì x), K) = P I K 1 ∩ P I K</formula><p>2 ) and is empty iff Γ is satisfied. Since in this case h(ì x) := false, answers(h(ì x), K) = ∅, and the desired relation holds.</p><p>If Γ is of the form (func P), since the answers of h(x, y, z) := y = z over K are all possible tuples of the form (a, b, b), P I K is functional iff answers(b(x, y, z), K) ⊆ answers(h(x, y, z), K). Finally, if Γ is of the form (trans P), b(x, y, z) := π (P, x, y) ∧ π (P, y, z) and h(x, y, z) := π (P, x, z), and it is easy to see that</p><formula xml:id="formula_12">P I K is transitive iff answers(b(x, y, z), K) ⊆ answers(h(x, y, z), K). □ Constraint violations. A constraint instance Γ(ì a) of a constraint Γ(ì x</formula><p>) is obtained by replacing the variables ì x by the individual names ì a in Γ(ì x). This notion allows us to define constraint violations: Definition 2 (Constraint violation): A violation of a constraint Γ(ì x) in K is a minimal subset V ⊆ K such that there exists ì a such that V violates Γ(ì a) and K violates Γ(ì a). We denote by Violations(K, Γ(ì x)) the set of violations of Γ(ì x).</p><p>In this definition, the requirement that K violates Γ(ì a) may seem superfluous. Yet, if Γ is a completeness constraint, it may be the case that some V ⊆ K violates Γ(ì a), while K satisfies it.</p><p>Example 4: In our running example, it is easy to see that the subset V 0 = {hasGender(Zeus, masculine)} is a violation of Γ 0 . Consider now V = {hasMother(Spinoza, Marques)}. V is a violation of Γ 2 , Γ 3 and Γ 4 . Indeed, it violates Γ 2 (Marques), Γ 3 (Marques), and Γ 4 (Spinoza, Marques) and K does not satisfy any of these constraint instances. However, even if</p><formula xml:id="formula_13">V violates Γ 1 (Spinoza), V is not a vi- olation of Γ 1 because {Human(Spinoza), Human ⊑ Person} ⊆ K satisfies the head of Γ 1 (Spinoza). ◁ If a constraint instance Γ(ì a) is violated, ì a ∈ answers(b(ì x), K</formula><p>) and ì a answers(h(ì x), K), so its violations are the minimal subsets of K responsible for ì a ∈ answers(b(ì x), K). The next proposition relates constraint violations and justifications. A justification (also known as an explanation, axiom pinpointing, or MinAs) for the entailment of a BCQ is a minimal subset of the KB that entails the BCQ <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b33">34]</ref>. Proposition 2:</p><formula xml:id="formula_14">If K violates Γ(ì a) : b(ì a) → h(ì a), a subset V ⊆ K is a violation of Γ(ì a) iff V is a justification of K |= b(ì a).</formula></div>
<div><head n="5">CORRECTIONS</head><p>We now turn to correcting constraint violations.</p><p>Solutions. We will make use of atomic modifications of the KB to define solutions to constraint violations. Definition 3 (Atomic modification): An atomic modification of a KB K is a pair m = (M + , M -) of two sets of assertions or L-axioms that takes one of the following forms: Addition: m = ({⟨s, p, o⟩}, ∅), where ⟨s, p, o⟩ K Deletion: m = (∅, {⟨s, p, o⟩}), where ⟨s, p, o⟩ ∈ K Replacement: m = ({⟨s, p, o⟩}, {⟨s ′ , p ′ , o ′ ⟩}), where ⟨s, p, o⟩ K, ⟨s ′ , p ′ , o ′ ⟩ ∈ K, and ⟨s, p, o⟩ differs from ⟨s ′ , p ′ , o ′ ⟩ in exactly one component.</p><p>Thus, an atomic modification consists of two sets M + and M -, each of which is either the empty set or a singleton set. M + will be added to the KB, and M -will be removed from the KB. Since the sets contain at most one triple, we slightly abuse the notation and identify the singletons with their elements (e.g., we will denote the addition of ⟨s, p, o⟩ simply by (⟨s, p, o⟩, ∅)). A replacement is equivalent to a sequence of a deletion and an addition. We chose to keep it as an atomic modification because it corresponds to common knowledge base curation tasks, such as correcting an erroneous object for a given subject and predicate, or fixing a predicate misuse. Atomic modifications can be used to solve a constraint violation, as follows: Definition 4 (Solution): A solution to a violation V of a constraint instance Γ(ì a) in K is an atomic modification (M + , M -) such that there exists Note also that every constraint violation has at least one solution, which consists of the deletion of any of its elements. Solutions may also be additions or replacements, as in the following example: Example 5: In our running example, the deletion (∅, hasGender (Zeus, masculine)) and the replacement (hasGender(Zeus, male), hasGender(Zeus, masculine)) are two possible solutions to V 0 for Γ 0 (masculine).</p><formula xml:id="formula_15">K ′ ⊆ K such that (V ∪ K ′ ∪ M + ) \ M -satisfies Γ(ì a). We call (M + , M -) a solution to V for Γ(ì a) in K. Note that Γ(ì a) can still be violated in (K ∪ M + ) \ M -if K contains other violations of Γ(ì a) for which (M + , M -) is not a solution. For example, if Γ(a) : ∃xR(a, x) ∧ A(a)</formula><p>The deletion (∅, hasMother(Spinoza, Marques)) is a solution to V for the three constraint instances Γ 2 (Marques), Γ 3 (Marques) and Γ 4 (Spinoza, Marques). The additions (Human(Marques), ∅), (hasGender(Marques, female), ∅) and (hasChild(Marques, Spinoza), ∅) are solutions to V for respectively Γ 2 (Marques), Γ 3 (Marques) and Γ 4 (Spinoza, Marques). ◁ Good solutions. Our goal is to find "good" solutions to constraint violations, i.e., solutions that make the KB as close to the real world as possible. The basic requirement for a "good" solution is that it deletes only erroneous facts, and that it adds only true facts. We also prefer replacements over deletions as long as they fulfill this condition. For instance, in our running example, the replacement (hasGender(Zeus, male), hasGender(Zeus, masculine)) is better than the deletion (∅, hasGender(Zeus, masculine)), because it corrects erroneous information instead of simply erasing it.</p><p>In some cases, there may be no "good" solution that consists of a single atomic modification. Consider for example a completeness constraint of the form A ⊑ ∃P • B violated by {A(a)}. If A(a) is true, we should actually add both P(a, b) and B(b) for some b. We choose to define solutions as atomic modifications nevertheless to simplify the problem by reducing the size of possible solutions. This is a limitation of our approach since we will not be able to learn solutions that are not atomic. However, we will still be able to learn to add B(b) to solve the aforementioned constraint violation in the case where P(a, b) is already present.</p><p>The main difficulty in finding good solutions to constraint violations is that we do not have access to an oracle that knows the validity of all facts. This is the problem that all KB cleaning approaches face (cf. Section 2). Our idea is to exploit the history of the KB modifications to learn how to correct constraint violations. Definition 5 (Edit history): The edit history of a KB is a sequence of KBs</p><formula xml:id="formula_16">(K i ) 0≤i ≤p = (T i ∪A i ) 0≤i ≤p such that K i+1 = (K i ∪M + i )\M - i , where (M + i , M - i )</formula><p>is an atomic modification. The edit history allows us to pinpoint how constraint violations have been corrected in the past. In order to avoid learning from vandalism or mistakes, we consider only those corrections that have not been reversed: Definition 6 (Past correction): A past correction is a solution (M + , M -) to a violation V of a constraint instance Γ(ì a) in K i such that there exist B and D such that the current KB</p><formula xml:id="formula_17">K p = (K i ∪ B) \ D with M + ⊆ B, M -⊆ D, M + ∩ D = ∅, M -∩ B = ∅.</formula><p>Intuitively, (B, D) corresponds to the sequence of additions and deletions that leads from K i to the current state of the KB K p , that contains the solution, and that does not "undo" it.</p><p>Relevant past corrections. During the history of a KB, users can change not just the assertions of the KB, but also the TBox. However, the TBox is typically much smaller and more stable than the ABox. Therefore, the edit history of the TBox is not a rich ground for correction rule mining. Moreover, we are interested in learning solutions that correct constraint violations in the current KB K p . We thus consider only those past corrections that would have been corrections also under the current TBox. For example, assume that the TBox contained C ⊑ B. Assume that C(a) was added to correct a violation of the constraint A ⊑ B. If, in the meantime, the inclusion C ⊑ B has been removed, we do not want to learn from this past correction. The following definition formalizes these requirements. Definition 7 (Relevant Past Correction): A relevant past correction (M + , M -) to a violation V of a constraint instance Γ(ì a) in K i is a past correction such that (i) M + ∪ M -contains only assertions, and (ii)</p><formula xml:id="formula_18">(V ∩ A i ) ∪ T p contains a violation V ′ of Γ(ì a) such that (M + , M -) is also a solution to V ′ in A i ∪ T p .</formula><p>We will now see how we can use the relevant past corrections to mine correction rules.</p></div>
<div><head n="6">FROM HISTORY TO CORRECTION RULES</head><p>In this section, we propose an approach based on rule mining to learn correction rules for building solutions to constraint violations.</p></div>
<div><head n="6.1">Extraction of the Relevant Past Corrections</head><p>Algorithm 1 constructs the set of relevant past corrections from the KB history. It consists of three main steps. First, it constructs patterns to spot KB modifications that could be part of a relevant past correction. Then it uses these patterns to extract atomic modifications that solved some violation in the past. Finally, the relevant past corrections are obtained by pruning those that have been reversed. </p><formula xml:id="formula_19">T p )} if Γ is a completeness constraint then Patterns(Γ) ∪= ({(A(ì x), _) | A(ì x) ∈ h ′ (ì x), h ′ (ì x) ∈ rewrite(h(ì x), T p )}) // Extract past corrections for 0 ≤ i ≤ p -1 do if (M + i , M - i ) such that K i+1 = (K i ∪ M + i ) \ M - i matches some pattern in Patterns(Γ) then PCDataset ∪= {⟨(M + i , M - i ), Γ(ì a), V, i⟩ | V ∈ Violations(K i , Γ(ì a)) \ Violations(K i+1 , Γ(ì a))} // Remove reversed past corrections for ⟨(M + i , M - i ), Γ(ì a), V, i⟩ ∈ PCDataset do if M + i ⊈ K p or M - i ∩ K p ∅ then PCDataset \= {⟨(M + i , M - i ), Γ(ì a), V, i⟩}</formula><p>Let us explain our algorithm with our running example. Consider the constraint Γ 0 (x) : ∃yhasGender(y, x) → x = male ∨ x = female ∨ x = nonbinary. Assume that ⟨Zeus, hasGender, masculine⟩ was added between K 1 and K 2 , but then replaced by ⟨Zeus, hasGender, male⟩ between K 100 and K 101 .</p><p>The first goal of the algorithm is to find out that the removal of ⟨Zeus, hasGender, masculine⟩ between K 100 and K 101 (as part of the replacement) may be part of a relevant past correction. We call this deletion a correction seed. Formally, a correction seed is a deletion (∅, M -) or an addition (M + , ∅) such that (i) there exists 0 Γ 0 (masculine) {⟨Zeus, hasGender, masculine⟩} 100 (resp. M + i = M + ) and (ii) there exists a KB T p ∪ D, where D is a set of assertions, such that T p ∪ D contains a violation V of some constraint instance and (∅, M -) (resp. (M + , ∅)) is a solution to V. Looking for correction seeds instead of computing the constraint violations for all constraints on all KB versions has the advantage of significantly reducing the search space.</p><formula xml:id="formula_20">≤ i ≤ p -1 such that K i+1 = (K i ∪ M + i ) \ M - i with M - i = M -</formula><p>To find such correction seeds efficiently, the first step of the algorithm precomputes for each constraint a set of atomic modification patterns that the possible correction seeds would match. In the example there would be only one pattern: the deletion pattern (_, ⟨?, hasGender, ?⟩), where _ can be anything so that it matches both the deletion of ⟨?, hasGender, ?⟩ and its replacements. Since we only consider past corrections that involve assertions, and want them to be relevant for the current TBox, computing the correction seed patterns can be done via query rewriting of the CQs in the body b(ì x) and the head h(ì x) of the constraint w.r.t. T p . Indeed, if T is a flat QL TBox, any CQ q(ì x) can be rewritten w.r.t. T into a UCQ q ′ (ì x) such that for every ABox A, answering q(ì x) over T ∪ A amounts to answering q ′ (ì x) over A <ref type="bibr" target="#b22">[23]</ref>. Each atom that occurs in the rewriting of the body of a constraint corresponds to a deletion pattern, and each atom that occurs in the rewriting of the head of a completeness constraint corresponds to an addition pattern. We collect the patterns for the constraint Γ in the set Patterns(Γ). Note that it is not possible to solve a consistency constraint with an addition, which is why such constraints have only deletion patterns.</p><p>The second step of the algorithm verifies, for each correction seed, whether it solved some constraint violation in the past -i.e., whether K i contains some violations of some constraint instances that are not in K i+1 . If so, the modification between K i and K i+1 is a solution that solved these violations in K i . In the example we would have found the violation {⟨Zeus, hasGender, masculine⟩} of Γ 0 (masculine) in K 100 , which is not in K 101 . So we would have extracted that (⟨Zeus, hasGender, male⟩, ⟨Zeus, hasGender, masculine⟩) is a solution that solved the violation {⟨Zeus, hasGender, masculine⟩} of Γ 0 (masculine) in K 100 . We store this information as a tuple in the relevant past corrections dataset (the <software>PCDataset</software>), as shown in Table 2. Finding the constraint instances violated in K i or K i+1 is done via CQ answering (Proposition 1), and computing their violations amounts to computing BCQ justifications (Proposition 2).</p><p>The final step of the algorithm removes corrections that have been reversed. The result is thus the set of relevant past corrections.</p></div>
<div><head n="6.2">Correction Rule Mining</head><p>Correction rules. The previous algorithm has given us a list of relevant past corrections (the <software ContextAttributes="used">PCDataset</software>, exemplified in Table <ref type="table" target="#tab_0">2</ref>). We now present our approach to mine correction rules from this dataset and the KB history. Definition 8 (Correction rule): A correction rule is of the form r := [Γ(ì x)] : E(ì x, ì y, ì z) → (M + (ì x, ì y), M -(ì x, ì y)), where</p><p>• Γ(ì x) is a constraint that can be partially instantiated, i.e., some of its variables have been replaced by constants,</p><formula xml:id="formula_21">• (M + (ì x, ì y), M -(ì x, ì y)</formula><p>) is a pair of sets of at most one triple, • E(ì x, ì y, ì z) is a set of atoms called the context of the violation such that M -(ì x, ì y) ⊆ E(ì x, ì y, ì z),</p><p>and both</p><formula xml:id="formula_22">(M + (ì x, ì y), M -(ì x, ì y)) and E(ì x, ì y, ì z) are built from N C ∪ N R ∪ {rdf:type} ∪ N I ∪ ì x ∪ ì y ∪ ì z.</formula><p>A correction rule can be applied to a KB K when there exist tuples of constants ì a, ì b such that K violates Γ(ì a) (recall that this can be decided via CQ answering by Proposition 1) and K |= ∃ì zE(ì a, ì b, ì z). The result of the rule application is then</p><formula xml:id="formula_23">(M + (ì a, ì b), M -(ì a, ì b)).</formula><p>Note that while the variables from E(ì x, ì y, ì z) that do not appear in Γ(ì x) or in the head of r can be existentially quantified, those that occur in the head of r have to be free: they have to be mapped to individuals occurring in the KB in order to construct the result.</p><p>Example 6: In our running example, we would like to learn the following correction rules: r 1 := [Γ 0 (masculine)] : {hasGender(y, masculine)} → (hasGender(y, male), hasGender(y, masculine)) r 2 := [Γ 2 (x)] : {hasMother(y, x), Human(y)}</p></div>
<div><head>→ (Human(x), ∅)</head><p>The context of the second rule says that if x is the mother of a human, then x must also be a human. The rule obtained by replacing Human by Animal would express how to solve a violation of Γ 2 in the context where y is an animal. ◁</p><p>Mining correction rules. We mine correction rules with Algorithm 2. This algorithm is an adaptation of the algorithm in <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref> to our context, where we learn rules not from a KB but from the <software ContextAttributes="used">PCDataset</software> and the KB history. We first adapt the definitions of the confidence and support from <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref> to our case. The support of the body of a correction rule r for a constraint Γ is the number of violations of Γ stored in the <software ContextAttributes="used">PCDataset</software> that could have been corrected by applying r . Such violations are associated with an instance Γ(ì a) of the partially instantiated Γ(ì x) that appears in r and with an index i such that K i |= ∃ì zE(ì a, ì b, ì z) for some ì b. These two conditions imply that r could be applied to the KB K i . Moreover, we need to check that the result of applying r to K i actually gives a solution to V. </p><formula xml:id="formula_24">BSup = {V | ⟨_, Γ(ì a), V, i⟩ ∈ PCDataset, ∃ ì b K i |= ∃ì zE(ì a, ì b, ì z)</formula><p>and the result of the application of r to K i is a solution to V}.</p><p>The support of the rule r measures when the past correction is exactly the result of the application of the rule in the cases where it could be applied. Formally, sup rule (r ) = |RSup|, where</p><formula xml:id="formula_25">RSup = {V | ⟨(M + (ì a, ì b), M -(ì a, ì b)), Γ(ì a), V, i⟩ ∈ PCDataset,</formula><p>and</p><formula xml:id="formula_26">K i |= ∃ì zE(ì a, ì b, ì z)}.</formula><p>Finally, the confidence of a correction rule r is conf (r ) = sup rule (r )</p><p>sup bod (r ) .</p></div>
<div><head>Algorithm 2 Correction rule mining</head><p>Input: <software>PCDataset</software>, (K i ) 0≤i ≤p , minsup, minconf , θ Output: correction rules // Generate basic rules</p><formula xml:id="formula_27">BasicR := ∅ for all ⟨(M + (ì a, ì b), M -(ì a, ì b)), Γ(ì a), V, i⟩ ∈ PCDataset do r 0 := [Γ(ì a)] : M -(ì a, ì b) → (M + (ì a, ì b), M -(ì a, ì b)) BasicR ∪= {σ (r 0 ) | C ⊆ ì a ∪ ì b, σ : C ↣ Var, sup rule (σ (r 0 )) ≥ minsup, conf (σ (r 0 )) ≥ minconf }</formula><p>// Refine the context part of the rules q := [], q.enqueueAll(BasicR) while q is not empty do r := q.dequeue()</p><p>Output r for all operators op do for all r ′ ∈ op(r ) do if sup rule (r ′ ) ≥ minsup and conf (r ′ ) ≥ conf (r ) + θ then q.enqueue(r ′ )</p><p>Algorithm 2 shows our mining algorithm. It takes as input the <software>PCDataset</software> computed by Algorithm 1, the KB history, a minimum support threshold, a minimum confidence threshold, and a regularization threshold θ . These thresholds are chosen empirically (see Section 7.3). The algorithm produces correction rules (Definition 8).</p><p>For this purpose, it first generates a trivial rule r 0 for each entry of the <software>PCDataset</software>. This rule has as context simply the deletion part of the constraint past correction. This trivial rule is then transformed into several more general rules, which we call basic rules, each of which is obtained from r 0 by replacing some of the constants by variables. Formally, the algorithm uses all partial substitutions σ from constants to distinct fresh variables. It retains only those basic rules that meet the minimum support and confidence thresholds.</p><p>In the second step, the algorithm incrementally refines each rule by building up its context part E(ì x, ì y, ì z). This works similarly to the mining algorithm of <ref type="bibr" target="#b13">[14]</ref>: Each refinement step adds one atom built from the KB concept and role names and the variables and constants that appear in the rule, plus at most one fresh variable. For this purpose, the algorithm uses the operators defined in <ref type="bibr" target="#b13">[14]</ref>.</p><p>If the resulting rule meets the minimum support threshold, and improves the confidence by at least θ , the rule is retained. Note that Algorithm 2 outputs only rules that would have been returned by the algorithm of <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref> if evaluated with our confidence function. It prunes more rules because of the use of the θ and the minconf thresholds that are also used to do an early pruning of the rules during the context construction. Algorithm 2 can be easily parallelized by running it independently on each constraint and/or having multiple workers working on the same queue.</p><p>Applying correction rules. When all rules have been mined, they are sorted by decreasing confidence, breaking ties by help of the support (as it is done in <ref type="bibr" target="#b25">[26]</ref> to build classifiers from rules). This set of rules then forms a program that can be used to fix constraint violations as follows. Given a violation V of a constraint Γ in K, choose the first rule r in the program that is relevant for Γ (i.e., that contains [Γ(ì x)] where Γ(ì x) is a partially instantiated version of Γ). Then check whether r can be applied to V. The correction is the result of the rule application.</p><p>Example 7: Assume we mined the rules r 1 and r 2 of the preceding example with confidence 0.9 and 0.8 respectively, and another rule r 3 := [Γ 0 (x)] : {hasGender(x, y)} → (∅, hasGender(x, y)) with confidence 0.5. The correction program is (r 1 , r 2 , r 3 ). To correct a violation of Γ 0 , i.e. a wrong value for the hasGender property, the program first checks whether r 1 is applicable. If so, it replaces masculine by male. Otherwise, it falls back to r 3 and removes the wrong value. To correct a violation of Γ 2 , it ignores r 1 that is not related to Γ 2 and either applies r 2 if the context matches or does nothing. ◁</p></div>
<div><head n="7">EXPERIMENTS ON WIKIDATA</head><p>This section describes <software>CorHist</software>, which implements the framework introduced for <software ContextAttributes="used">Wikidata</software>, and presents its experimental evaluation.</p></div>
<div><head n="7.1">Wikidata</head><p><software ContextAttributes="used">Wikidata</software> is a generalist collaborative knowledge base. The project started in 2012, and as of July 2018, it has collected more than 500M statements about 50M entities. The data about each entity is stored in a versioned JSON blob, and there are more than 700M revisions. <software ContextAttributes="used">Wikidata</software> encodes facts not in plain RDF triples but in a reified representation, in which each main ⟨s, p, o⟩ triple can be annotated with qualifiers and provenance information <ref type="bibr" target="#b37">[38]</ref>. <software ContextAttributes="used">Wikidata</software> knows the property instanceOf which is similar to rdf:type. It does not have a formally defined TBox, but knows properties such as subClassOf, subPropertyOf, and inverseOf. However, only the property subClassOf is used to flag the constraint violations. Therefore, we use only this property in our TBox, which thus contains simple concept inclusions.</p><p>We consider the set C of constraints built from ten types of <software ContextAttributes="used">Wikidata</software> property constraints (see Table <ref type="table">3</ref>). They are the top <software ContextAttributes="used">Wikidata</software> property constraints that can be expressed in DL, covering the majority of the most used constraints, as well as 71% of <software ContextAttributes="used">Wikidata</software> property constraints. The remaining constraints are mainly about string format validation with regular expressions (52% of the remaining constraints) and about qualifiers (31% of them).</p></div>
<div><head n="7.2">Dataset Construction</head><p>We stored the RDF version <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b17">18]</ref> of the <software ContextAttributes="used">Wikidata</software> edit history in an RDF quad store. We used named graphs for the global state of <software ContextAttributes="used">Wikidata</software> after each revision, and for the triple additions and deletions. Our dataset stores 390M annotated triples about 49M items extracted from the July 1st, 2018 full database dump.</p><p>We extracted the relevant past corrections as explained in Section 6.1. <software ContextAttributes="used">Wikidata</software> revisions do not correspond exactly to atomic modifications in our sense. For example, <software ContextAttributes="used">Wikidata</software> bots are able to change multiple unrelated facts about the same entity at the same Table <ref type="table">3</ref>: <software ContextAttributes="used">Wikidata</software> property constraints. R is the property for which the constraint is given. A constraint has several lines when it uses a property whose set of values may be specified or not. ♯constr. is the total number of constraints of the given type in <software ContextAttributes="used">Wikidata</software>. ♯triples is the sum for all these constraints of the numbers of triples with the property R on which they apply. ♯violations is the number of violations for this constraint in <software ContextAttributes="used">Wikidata</software> on July 1st, 2018. ♯past cor. is the number of past corrections we extracted from <software ContextAttributes="used">Wikidata</software> history. t.o. indicates that we were not able to extract all past corrections because of timeout so that we sample them (we then indicate the number of corrections we extracted).</p></div>
<div><head>Name in Wikidata DL form</head><p>Rule form ♯constr. ♯triples ♯violations ♯past cor.</p><formula xml:id="formula_28">Type 4 ∃R ⊑ A 1 ⊔ • • • ⊔ A n ∃yR(x, y) → A 1 (x) ∨ • • • ∨ A n (x) 2575 249M 3465k t.o.(&gt;16M) Value type 4 ∃R -⊑ A 1 ⊔ • • • ⊔ A n ∃yR(y, x) → A 1 (x) ∨ • • • ∨ A n (x) 696 67M 3062k t.o.(&gt;19M) One-of ∃R -⊑ {a 1 , . . . , a n } ∃yR(y, x) → x = a 1 ∨ • • • ∨ x = a n 104 3.6M 4k 14k Item requires ∃R ⊑ ∃R ′ • {a 1 , . . . , a n } ∃yR(x, y) → R ′ (x, a 1 ) ∨ • • • ∨ R ′ (x, a n ) 3102 255M 3710k t.o.(&gt;15M) statement ∃R ⊑ ∃R ′ ∃yR(x, y) → ∃zR ′ (x, z) Value requires ∃R -⊑ ∃R ′ • {a 1 , . . . , a n } ∃yR(y, x) → R ′ (x, a 1 ) ∨ • • • ∨ R ′ (x, a n ) 243 85M 1345k t.o.(&gt;6M) statement -⊑ ∃R ′ ∃yR(y, x) → ∃zR ′ (x, z) Conflict with ∃R ⊑ ¬∃R ′ • {a 1 , . . . , a n } ∃yR(x, y) ∧ (R ′ (x, a 1 ) ∨ • • • ∨ R ′ (x, a n )) → false 601 449M 142k 465k ∃R ⊑ ¬∃R ′ ∃yzR(x, y) ∧ R ′ (x, z) → false Inverse/Symmetric 5 R ⊑ R ′- R(x, y) → R ′ (y, x) 146 6M 409k 2989k Single value (func R) R(x, y) ∧ R(x, z) → y = z 2772 85M 334k 389k Distinct values (func R -) R(y, x) ∧ R(z, x) → y = z 2728 56M 189k 7432k</formula><p>time. <software>Wikidata</software> users also sometimes prefer to delete a statement then add another one with the same property instead of directly modifying the value, in order to clear the existing qualifiers and references. Therefore, we artificially created a replacement modification for every deletion with a neighboring addition by the same user, which shares at least two components of the triple (analogously for additions). For example, if the correction seed is the deletion of ⟨Zeus, hasGender, masculine⟩, and if this revision or a neighboring one adds ⟨Zeus, hasGender, male⟩, then we consider this a replacement. However, if the same revision added the triple ⟨Zeus, hasMother, Rhea⟩, then we would not consider this a replacement, because it does not share two components with the first one. Since the TBox consists of simple concept inclusions and the constraint bodies contain only roles, the deletion patterns for correction seeds correspond directly to the atoms of the constraint body. In the same vein, only atoms in the head of the Type or Value type constraints need to be rewritten. To find the constraint violations solved by a correction seed, we make use of the fact that the correction seed allows us to know the constraint instance Γ(ì a), and we look for matches of the constraint instance body.</p><p>To speed up the execution for the four constraint types which have the highest numbers of past corrections, Type, Value type, Item requires statement and Value requires statement, we did not extract all the past corrections but sample them as follows. We compute only the relevant past corrections that where applied between K i and K i+1 where i is a multiple of s := max(1, N /10 6 ) with N the number of triples with the property R of the constraint at hand. This sampling allows us to get a sufficient ground for rule mining 4 The <software>Wikidata</software> constraint Type can be qualified to modify its meaning. We ignore these cases, which are marginal: they concern less than 6% of the Type constraints. The same goes analogously for Value type. 5 Inverse and Symetric are two distinct kinds of constraints in <software ContextAttributes="used">Wikidata</software> but we treat them together since Symetric is actually a special case of Inverse.</p><p>for each constraint. In practice, it affects only the most frequent 0.9% of Type, 2% of Value type, 0.5% of Item requires statement, and 3% of Value requires statement constraints.</p></div>
<div><head n="7.3">Mining Rules</head><p>The output of our method is a set of correction rules that form a program (Section 6.2). To evaluate such a program, we apply it to each of the constraint violations stored in the <software>PCDataset</software>, using the associated stage of the KB to evaluate the part of the context which is not the deletion part of the correction. Then we check whether the correction we compute is exactly the same as the one associated to the constraint violation in the <software ContextAttributes="used">PCDataset</software>. The precision p of the program is given by the fraction of the corrections computed by the program that are actually the same as those that have been applied. The recall r of the program is the fraction of the constraint violations stored in <software ContextAttributes="used">PCDataset</software> for which the program gives some correction. The F1 score is</p><formula xml:id="formula_29">F 1 = 2 p •r p+r .</formula><p><software>CorHist</software> mines rules as explained in Section 6.2. In order to decrease the computation time, we only allow one atom p(s, o) in E(ì x, ì y, ì z) \ Body(ì x, ì y), where Body(ì x, ì y) corresponds to the part of the context that matches part of the constraint body, such that s is a variable of ì x ∪ ì y and o is a fresh variable or a constant.</p><p>Rules were mined per constraint. For each constraint, we split the set of extracted past corrections into a 70% training set, a 10% cross-validation set, and a 20% test set. The training set is used to mine the rules, the cross-validation set is used to determine the confidence threshold that maximizes the F1 score of the obtained program, and the test set is used to evaluate the final program.</p><p>Table <ref type="table" target="#tab_1">4</ref> gives examples of rules mined by <software ContextAttributes="used">CorHist</software>. Several of these rules show the crucial importance of the instantiation of the constraint and/or of the context to be able to choose the correction. For instance, the rule for the Single value constraint uses the fact that an entity involved in a property "member of sport team" is probably a human being, and thus that if it has several values for the functional property "sex or gender" and one of them is a value reserved for non-human organisms in <software ContextAttributes="used">Wikidata</software>, this value is probably wrong. In the same vein, the rule for the Item requires statement constraint recognizes that an entity has a heritage designation that is specific to Sweden ("monument in Fornminnesregistret"), to conclude that its country is Sweden. The rules also propose fixes to misused predicates: in <software ContextAttributes="used">Wikidata</software>, the property "manner of death" is intended for the general circumstances of a person's death (such as "accident"), while the property "cause of death" is intended to give more precise causes (such as "traffic accident").</p></div>
<div><head n="7.4">Evaluation against the Test Set</head><p>Table <ref type="table" target="#tab_2">5</ref> presents the results of the evaluation of the mined programs against the test set. We computed both the micro and macro average of the precision, recall and F1 score per kind of constraint. The micro average aggregates over the whole set of relevant past corrections for the given kind of constraint, whereas the macro average computes the scores for each constraint of this given kind, and then computes the average. Both numbers are important: The micro average gives more weight to correction rules that fix many violations. It thus measures the overall impact of the correction rules on the dataset. However, if few rules had a large impact, then it would be easier to formulate these rules by hand. Our method, in contrast, can also find rules that by themselves solve less violations, but together contribute a large mass of corrections. To illustrate this, we also report the macro average: It measures the average performance across different constraints. We compare our approach with two baselines: The first one, called "delete", is the most basic one and uses the fact that all <software ContextAttributes="used">Wikidata</software> constraint bodies contain an atom of the form R(x, y) and the TBox contains only concept inclusions, so that all constraint violations contain an assertion that matches R(x, y). The "delete" baseline simply deletes this assertion. For the completeness constraints we define an additional baseline, "add", which tries to add a new triple to solve the constraint violation. For Inverse and Symmetric constraints this baseline adds the missing reverse edge and performs very well. For Item requires statement, Value requires statement, Type and Value type, it adds the missing triple only if it is possible to know the expected value from the constraint rule. For example, for Type constraints of the form ∃yR(x, y) → A(x) it applies the correction (A(x), ∅). Value type constraints are handled in the same way. However, this baseline is not able to figure out what is the relevant addition correction for a constraint of the form ∃yR(x, y) → A 1 (x) ∨ A 2 (x) because there is no way to know a priori if A 1 or A 2 should be added. For Item requires statement constraints of the form ∃yR(x, y) → R ′ (x, a), the "add" baseline applies (R ′ (x, a), ∅) (similarly for Value requires statement constraints). However, it cannot find a correction if there are multiple a i .  As shown in Table <ref type="table" target="#tab_2">5</ref>, the precision of our approach significantly outperforms the two baselines -often by a very high margin. Regarding the recall, we manage to keep a reasonable, and sometimes even good, recall (see best F1 scores in Table <ref type="table" target="#tab_2">5</ref>), except for Single and Distinct value. The very low recall obtained for these two kinds of constraints is easily explainable because they are mostly used on predicates that link <software ContextAttributes="used">Wikidata</software> to other databases (91% of the Single and 95% of the Distinct constraints), and we cannot get meaningful information about the target database to mine corrections.</p></div>
<div><head n="7.5">User Evaluation</head><p>To see whether our corrections are accepted by the community, we designed a user study. We created a tool that suggests our corrections to <software ContextAttributes="created">Wikidata</software> users for validation (available at https://tools. wmflabs.org/wikidata-game/distributed/#game=43). The user can choose a constraint type, and the tool then suggests corrections for random violations of constraints of this type (Figure <ref type="figure" target="#fig_1">1</ref>). The violations for which corrections are suggested are provided by query.wikidata.org, which limits their number for performance reasons. For each proposed correction, the user has to choose between three options: apply the proposed correction to <software ContextAttributes="created">Wikidata</software>, tag it as wrong, or get another correction to review. We ran the experiment for 3 months and 47 <software ContextAttributes="created">Wikidata</software> users participated.</p><p>Table <ref type="table" target="#tab_3">6</ref> presents the results. The number of corrections reviewed is highly unbalanced between the kinds of constraints, mainly because a few users evaluate a lot of suggestions, and have a predilection for some kinds of constraints. It is thus difficult to draw conclusions for those kinds of constraints for which very few corrections have been evaluated. However, we can still make some interesting observations. In particular, the proposed corrections marked as wrong give us insights about possible weaknesses of our approach.</p><p>For the constraints which got a significant number of evaluations, our approach seems to perform well for Inverse and Symmetric, Conflict with and Value requires statement constraints, with approval rates above 80%. The other approval rates are lower. This is partly due to biases in the data. For example, when a gender is missing, our approach proposes the value "male" by default, because of the overrepresentation of men in <software>Wikidata</software>. Another issue is the quality of the constraints, which in <software ContextAttributes="created">Wikidata</software> are sometimes questionable or difficult to understand (e.g., an incomplete set of possible types or values for completeness or One-of constraints).</p><p>However, even lower approval scores do not mean that our approach would be useless: Psychological research <ref type="bibr" target="#b11">[12]</ref> shows that people find it much easier to choose from given options than to come up with an answer by themselves. The actual time needed to come up with an answer may vary, but if it takes just 3 times longer to come up with an answer than to accept or reject our proposed correction, then achieving a precision of 40% is already useful: If we have a precision of 40%, and if a free-form answer takes time t, then the expected answer time with our tool is 40%× 1  3 ×t +60%× 4 3 ×t &lt; t.</p></div>
<div><head n="8">CONCLUSION AND FUTURE WORK</head><p>We have introduced the problem of learning how to fix constraint violations from a KB history. We have also presented a method based on rule mining to this end. Our experimental evaluation on <software>Wikidata</software> shows significant improvement over baselines. Our tool is live on <software ContextAttributes="used">Wikidata</software> and has already allowed users to correct more than 23k constraint violations. While our evaluation focused on <software ContextAttributes="used">Wikidata</software> for which the whole edit history was available, we believe that our method can be applied in other settings, for example using edits done during the partial cleaning of an automatically extracted KB.</p><p>For future work, it would be interesting to evaluate the impact of parameters such as the size of the context part of the correction rule in terms of rule quality. We also plan to extend the learning dataset with external knowledge (such as other KBs), or with information extracted from other sources (for instance from Wikipedia). We believe that this will allow finding even more precise correction rules, thus making KBs ever more precise and more useful.</p></div><figure xml:id="fig_0"><head /><label /><figDesc>→ false, and K = {R(a, b), R(a, c), A(a)}, the deletion of R(a, b) is a solution to the violation {R(a, b), A(a)}, but {R(a, c), A(a)} still violates Γ(a).</figDesc></figure>
<figure xml:id="fig_1"><head>Algorithm 1</head><label>1</label><figDesc>Construction of PCDataset Input: set of constraints C, current TBox T p , history (K i ) 0≤i ≤p Output: set of relevant past corrections PCDataset // Construct correction seed patterns for all Γ ∈ C such that Γ(ì x) : b(ì x) → h(ì x) do Patterns(Γ) := {(_, A(ì x))|A(ì x)∈b ′ (ì x), b ′ (ì x)∈rewrite(b(ì x),</figDesc></figure>
<figure xml:id="fig_2"><head /><label /><figDesc>For example, consider the case where PCDataset contains both ⟨(∅, R(a, b))), Γ(a), {R(a, b), A(a)}, i⟩ and ⟨(∅, R(a, c))), Γ(a), {R(a, c), A(a)}, j⟩ for Γ(a) : ∃xR(a, x) ∧ A(a) → false. Both violations count for the support of the body of [Γ(a)] : R(a, x) → (∅, R(a, x)) but only the second one counts for the support of the body of [Γ(a)] : R(a, c) → (∅, R(a, c)), even in the case where K i |= R(a, c). Formally, sup bod (r ) = |BSup| where</figDesc></figure>
<figure xml:id="fig_3"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Example of a replacement correction suggested by CorHist for a violation of the constraint ∃countryOfCitizenship ⊑ ¬∃sexOrGender • {maleOrg, femaleOrg}.</figDesc><graphic coords="11,53.80,83.69,510.24,89.24" type="bitmap" /></figure>
<figure type="table" xml:id="tab_0"><head>Table 2 :</head><label>2</label><figDesc>Dataset PCDataset of relevant past corrections extracted for our running example. hasGender, male⟩}, {⟨Zeus, hasGender, masculine⟩})</figDesc><table><row><cell>Relevant past correction</cell><cell>Constraint instance</cell><cell>Violation</cell><cell>KB index</cell></row><row><cell>({⟨Zeus,</cell><cell /><cell /><cell /></row></table></figure>
<figure type="table" xml:id="tab_1"><head>Table 4 :</head><label>4</label><figDesc>Example of mined rules.</figDesc><table><row><cell>Constr. type</cell><cell>Constraint Γ</cell><cell>Correction rule</cell></row><row><cell>Type</cell><cell>∃isAListOf ⊑ List</cell><cell>[Γ(s)] : isAListOf(s, o) ∧ WikiDisambiguationPage(s) → (∅, isAListOf(s, o))</cell></row><row><cell>Value type</cell><cell cols="2">∃foundInTax -⊑ Taxon [Γ(human)] : foundInTax(s, human) ∧ hasPart(s, v) → (foundInTax(s, homoSapiens), foundInTax(s, human))</cell></row><row><cell>One-of</cell><cell>∃mannerDeath -⊑ {. . . }</cell><cell>[Γ(trafficAcc)] : mannerDeath(s, trafficAcc) → (causeDeath(s, trafficAcc), mannerDeath(s, trafficAcc))</cell></row><row><cell cols="2">Item req. stm. ∃heritageStatus ⊑ ∃country</cell><cell>[Γ(s)] : heritageStatus(s, monumentInFornminnesregistret) → (country(s, Sweden), ∅)</cell></row><row><cell cols="2">Val. req. stm. ∃residence -⊑ ∃country</cell><cell>[Γ(s)] : diplomaticRelation(s, v) → (country(s, s), ∅)</cell></row><row><cell>Conflict</cell><cell>∃filmplID ⊑ ¬∃filmplFilmID</cell><cell>[Γ(s)] : filmplID(s, o) → (∅, filmplID(s, o))</cell></row><row><cell>Inv./Sym.</cell><cell>geneticAssoc ⊑ geneticAssoc -</cell><cell>[Γ(s, o)] :→ (geneticAssoc(o, s), ∅)</cell></row><row><cell>Single val.</cell><cell>(func sexOrGender)</cell><cell>[Γ(s)] : sexOrGender(s, maleOrg) ∧ sportsTeam(s, v) → (∅, sexOrGender(s, maleOrg))</cell></row><row><cell>Distinct val.</cell><cell>(func ncbiLocusTag -)</cell><cell>[Γ(s)] : ncbiLocusTag(o, s) ∧ molecularFunction(o, v) → (∅, ncbiLocusTag(o, s))</cell></row></table></figure>
<figure type="table" xml:id="tab_2"><head>Table 5 :</head><label>5</label><figDesc>Evaluation of the correction rules mined by CorHist with a minimal support of 10, a minimal confidence between 0.5 and 1 and a regularization threshold of 0.05, and comparison with the baselines. Best precision and F1 scores in bold.</figDesc><table><row><cell /><cell /><cell cols="3">Micro average</cell><cell cols="2">Macro average</cell></row><row><cell>Constraint type</cell><cell /><cell cols="5">Prec. Rec. F1 Prec. Rec. F1</cell></row><row><cell>Type 6</cell><cell>add</cell><cell cols="5">0.35 0.33 0.34 0.29 0.55 0.38</cell></row><row><cell /><cell cols="2">delete 0.04</cell><cell>1</cell><cell cols="2">0.07 0.13</cell><cell>1 0.23</cell></row><row><cell /><cell cols="6">CorHist 0.84 0.70 0.76 0.91 0.39 0.55</cell></row><row><cell>Value type 6</cell><cell>add</cell><cell cols="5">0.09 0.21 0.13 0.33 0.58 0.42</cell></row><row><cell /><cell cols="2">delete 0.01</cell><cell>1</cell><cell cols="2">0.02 0.16</cell><cell>1 0.27</cell></row><row><cell /><cell cols="6">CorHist 0.81 0.61 0.70 0.89 0.51 0.65</cell></row><row><cell>One-of</cell><cell cols="2">delete 0.26</cell><cell>1</cell><cell cols="2">0.42 0.35</cell><cell>1 0.52</cell></row><row><cell /><cell cols="6">CorHist 0.77 0.83 0.80 0.95 0.37 0.53</cell></row><row><cell>Item requires</cell><cell>add</cell><cell cols="5">0.99 0.14 0.25 0.80 0.27 0.40</cell></row><row><cell>statement 6</cell><cell cols="5">delete 0.017 1 0.033 0.10</cell><cell>1 0.19</cell></row><row><cell /><cell cols="6">CorHist 0.94 0.35 0.51 0.95 0.32 0.47</cell></row><row><cell>Value requires</cell><cell>add</cell><cell cols="5">0.44 20e-6 41e-6 0.33 0.12 0.18</cell></row><row><cell>statement 6</cell><cell cols="6">delete 0.041 1 0.079 0.082 1 0.15</cell></row><row><cell /><cell cols="6">CorHist 0.91 0.53 0.67 0.94 0.37 0.53</cell></row><row><cell>Conflict with</cell><cell cols="2">delete 0.39</cell><cell>1</cell><cell cols="2">0.56 0.39</cell><cell>1 0.56</cell></row><row><cell /><cell cols="6">CorHist 0.92 0.55 0.69 0.91 0.41 0.57</cell></row><row><cell>Inverse/Sym.</cell><cell>add</cell><cell>0.91</cell><cell>1</cell><cell cols="2">0.95 0.77</cell><cell>1 0.86</cell></row><row><cell /><cell cols="3">delete 0.072 1</cell><cell cols="2">0.12 0.14</cell><cell>1 0.24</cell></row><row><cell /><cell cols="6">CorHist 0.92 1 7 0.96 0.90 0.84 0.87</cell></row><row><cell>Single value</cell><cell cols="2">delete 0.34</cell><cell>1</cell><cell cols="2">0.51 0.42</cell><cell>1 0.59</cell></row><row><cell /><cell cols="6">CorHist 0.95 0.093 0.17 0.96 0.078 0.14</cell></row><row><cell cols="6">Distinct values delete 0.036 1 0.070 0.42</cell><cell>1 0.59</cell></row><row><cell /><cell cols="6">CorHist 0.99 0.020 0.039 0.93 0.12 0.21</cell></row></table></figure>
<figure type="table" xml:id="tab_3"><head>Table 6 :</head><label>6</label><figDesc>Human evaluation of the suggested corrections.</figDesc><table><row><cell>Constraint type</cell><cell cols="4">Suggested "Apply" "Wrong" Approval</cell></row><row><cell>Type</cell><cell>9908</cell><cell>252</cell><cell>312</cell><cell>0.45</cell></row><row><cell>Value type</cell><cell>2374</cell><cell>195</cell><cell>208</cell><cell>0.48</cell></row><row><cell>One-of</cell><cell>239</cell><cell>14</cell><cell>47</cell><cell>0.23</cell></row><row><cell>Item requires stm.</cell><cell>41</cell><cell>8</cell><cell>32</cell><cell>0.2</cell></row><row><cell>Value requires stm.</cell><cell>1024</cell><cell>790</cell><cell>178</cell><cell>0.82</cell></row><row><cell>Conflict with</cell><cell>3254</cell><cell>1717</cell><cell>203</cell><cell>0.89</cell></row><row><cell>Inverse/Symmetric</cell><cell>28138</cell><cell>20247</cell><cell>1720</cell><cell>0.92</cell></row><row><cell>Single value</cell><cell>3264</cell><cell>41</cell><cell>71</cell><cell>0.37</cell></row><row><cell>Distinct values</cell><cell>921</cell><cell>8</cell><cell>23</cell><cell>0.26</cell></row></table></figure>
			<note place="foot" n="1" xml:id="foot_0"><p>available at https://doi.org/10.6084/m9.figshare.7712720</p></note>
			<note place="foot" n="2" xml:id="foot_1"><p>available at https://github.com/Tpt/corhist</p></note>
			<note place="foot" n="3" xml:id="foot_2"><p>available at https://tools.wmflabs.org/wikidata-game/distributed/#game=43</p></note>
			<note place="foot" xml:id="foot_3"><p>Checking constraints. We show that our setting allows us to check constraint satisfaction via CQ answering. For this purpose, we use a function π , which maps each constraint Γ ∈ C to a rule of the form ∃ì yφ(ì x, ì y) → ∃ì zφ ′ (ì x, ì z). This function is defined recursively as shown in Table1. The left side of the rule is called the body and its right side the head. Example 3: In our running example, we obtain the following rules: Γ 0 (x) : ∃yhasGender(y, x) →x = male∨x = female∨x = nonbinary Γ 1 (x) : ∃yhasMother(x, y) → Person(x) ∨ Animal(x) Γ 2 (x) : ∃yhasMother(y, x) → Person(x) ∨ Animal(x) Γ 3 (x) : ∃yhasMother(y, x) → ∃z(hasGender(x, z) ∧ z = female) Γ 4 (x, y) : hasMother(x, y) → hasChild(y, x) ◁</p></note>
			<note place="foot" n="6" xml:id="foot_4"><p>Computed from a sample of the set of relevant past corrections. One Type constraint, six Value type constraints and one Val. req. stm constraint were omitted due to time-out.</p></note>
			<note place="foot" n="7" xml:id="foot_5"><p>The actual value is greater than 0.995 and rounded to 1 for consistency.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGMENTS</head><p>Partially supported by the grant <rs type="grantNumber">ANR-16-CE23-0007-01</rs> ("<rs type="projectName">DICOS</rs>").</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_jKKgRtn">
					<idno type="grant-number">ANR-16-CE23-0007-01</idno>
					<orgName type="project" subtype="full">DICOS</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Detecting Linked Data quality issues via crowdsourcing: A DBpedia study</title>
		<author>
			<persName><forename type="first">Maribel</forename><surname>Acosta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amrapali</forename><surname>Zaveri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elena</forename><surname>Simperl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitris</forename><surname>Kontokostas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabian</forename><surname>Flöck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jens</forename><surname>Lehmann</surname></persName>
		</author>
		<idno type="DOI">10.3233/SW-160239</idno>
		<ptr target="https://doi.org/10.3233/SW-160239" />
	</analytic>
	<monogr>
		<title level="j">Semantic Web</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="303" to="335" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">User-guided Repairing of Inconsistent Knowledge Bases</title>
		<author>
			<persName><forename type="first">Abdallah</forename><surname>Arioua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angela</forename><surname>Bonifati</surname></persName>
		</author>
		<idno type="DOI">10.5441/002/edbt.2018.13</idno>
		<ptr target="https://doi.org/10.5441/002/edbt.2018.13" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21th International Conference on Extending Database Technology, EDBT 2018</title>
		<meeting>the 21th International Conference on Extending Database Technology, EDBT 2018<address><addrLine>Vienna, Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-03-26">2018. March 26-29, 2018</date>
			<biblScope unit="page" from="133" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Cleaning Data with Constraints and Experts</title>
		<author>
			<persName><forename type="first">Ahmad</forename><surname>Assadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tova</forename><surname>Milo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Slava</forename><surname>Novgorodov</surname></persName>
		</author>
		<idno type="DOI">10.1145/3201463.3201464</idno>
		<ptr target="https://doi.org/10.1145/3201463.3201464" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st International Workshop on the Web and Databases</title>
		<meeting>the 21st International Workshop on the Web and Databases<address><addrLine>Houston, TX, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-06-10">2018. June 10, 2018</date>
			<biblScope unit="page" from="1" to="1" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m">The Description Logic Handbook: Theory, Implementation, and Applications</title>
		<editor>
			<persName><forename type="first">Franz</forename><surname>Baader</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Diego</forename><surname>Calvanese</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Deborah</forename><forename type="middle">L</forename><surname>Mcguinness</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Daniele</forename><surname>Nardi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Peter</forename><forename type="middle">F</forename><surname>Patel-Schneider</surname></persName>
		</editor>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">QOCO: A Query Oriented Data Cleaning System with Oracles</title>
		<author>
			<persName><forename type="first">Moria</forename><surname>Bergman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tova</forename><surname>Milo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Slava</forename><surname>Novgorodov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wang-Chiew</forename><surname>Tan</surname></persName>
		</author>
		<idno type="DOI">10.14778/2824032.2824096</idno>
		<ptr target="https://doi.org/10.14778/2824032.2824096" />
	</analytic>
	<monogr>
		<title level="j">PVLDB</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1900" to="1903" />
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Query-Driven Repairing of Inconsistent DL-Lite Knowledge Bases</title>
		<author>
			<persName><forename type="first">Meghyn</forename><surname>Bienvenu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Camille</forename><surname>Bourgaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">François</forename><surname>Goasdoué</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Fifth International Joint Conference on Artificial Intelligence, IJCAI 2016</title>
		<meeting>the Twenty-Fifth International Joint Conference on Artificial Intelligence, IJCAI 2016<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-07">2016. July 2016</date>
			<biblScope unit="page" from="957" to="964" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">DBpedia -A crystallization point for the Web of Data</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Bizer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jens</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georgi</forename><surname>Kobilarov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sören</forename><surname>Auer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Becker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Cyganiak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Hellmann</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.websem.2009.07.002</idno>
		<ptr target="https://doi.org/10.1016/j.websem.2009.07.002" />
	</analytic>
	<monogr>
		<title level="j">Journal of Web Semantics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="154" to="165" />
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Semantics and Validation of Shapes Schemas for RDF</title>
		<author>
			<persName><forename type="first">Iovka</forename><surname>Boneva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emilio</forename><forename type="middle">Labra</forename><surname>José</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">G</forename><surname>Gayo</surname></persName>
		</author>
		<author>
			<persName><surname>Prud'hommeaux</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-68288-4_7</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-68288-4_7" />
	</analytic>
	<monogr>
		<title level="m">The Semantic Web -ISWC 2017 -16th International Semantic Web Conference</title>
		<meeting><address><addrLine>Vienna, Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-10-21">2017. October 21-25, 2017</date>
			<biblScope unit="page" from="104" to="120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">RDF 1.1 Concepts and Abstract Syntax</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Cyganiak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Wood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Lanthaler</surname></persName>
		</author>
		<ptr target="http://www.w3.org/TR/2014/REC-rdf11-concepts-20140225/" />
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Introducing Wikidata to the Linked Data Web</title>
		<author>
			<persName><forename type="first">Fredo</forename><surname>Erxleben</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Günther</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Krötzsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Mendez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denny</forename><surname>Vrandečić</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-11964-9_4</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-11964-9_4" />
	</analytic>
	<monogr>
		<title level="m">The Semantic Web -ISWC 2014 -13th International Semantic Web Conference</title>
		<meeting><address><addrLine>Riva del Garda, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-10-19">2014. October 19-23, 2014</date>
			<biblScope unit="page" from="50" to="65" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Active integrity constraints</title>
		<author>
			<persName><forename type="first">Sergio</forename><surname>Flesca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergio</forename><surname>Greco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ester</forename><surname>Zumpano</surname></persName>
		</author>
		<idno type="DOI">10.1145/1013963.1013977</idno>
		<ptr target="https://doi.org/10.1145/1013963.1013977" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th International ACM SIGPLAN Conference on Principles and Practice of Declarative Programming</title>
		<meeting>the 6th International ACM SIGPLAN Conference on Principles and Practice of Declarative Programming<address><addrLine>Verona, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004-08-26">2004. 24-26 August 2004</date>
			<biblScope unit="page" from="98" to="107" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Multiple-choice and short-answer exam performance in a college classroom</title>
		<author>
			<persName><forename type="first">C</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K Laurie</forename><surname>Funk</surname></persName>
		</author>
		<author>
			<persName><surname>Dickson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Teaching of Psychology</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="273" to="277" />
			<date type="published" when="2011">2011. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Fast rule mining in ontological knowledge bases with AMIE+</title>
		<author>
			<persName><forename type="first">Luis</forename><surname>Galárraga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christina</forename><surname>Teflioudi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katja</forename><surname>Hose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabian</forename><forename type="middle">M</forename><surname>Suchanek</surname></persName>
		</author>
		<idno type="DOI">10.1007/s00778-015-0394-1</idno>
		<ptr target="https://doi.org/10.1007/s00778-015-0394-1" />
	</analytic>
	<monogr>
		<title level="j">VLDB J</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="707" to="730" />
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">AMIE: association rule mining under incomplete evidence in ontological knowledge bases</title>
		<author>
			<persName><forename type="first">Luis</forename><surname>Antonio Galárraga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christina</forename><surname>Teflioudi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katja</forename><surname>Hose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabian</forename><forename type="middle">M</forename><surname>Suchanek</surname></persName>
		</author>
		<idno type="DOI">10.1145/2488388.2488425</idno>
		<ptr target="https://doi.org/10.1145/2488388.2488425" />
	</analytic>
	<monogr>
		<title level="m">22nd International World Wide Web Conference, WWW '13</title>
		<meeting><address><addrLine>Rio de Janeiro, Brazil</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-05-13">2013. May 13-17, 2013</date>
			<biblScope unit="page" from="413" to="422" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">OWL: Yet to arrive on the Web of Data?</title>
		<author>
			<persName><forename type="first">Birte</forename><surname>Glimm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><surname>Hogan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Krötzsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Axel</forename><surname>Polleres</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW2012 Workshop on Linked Data on the Web</title>
		<meeting><address><addrLine>Lyon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-04-16">2012. 16 April, 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">OWL 2 Web Ontology Language Profiles</title>
		<author>
			<persName><forename type="first">Cuenca</forename><surname>Bernardo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boris</forename><surname>Grau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Motik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Achille</forename><surname>Horrocks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carsten</forename><surname>Fokoue</surname></persName>
		</author>
		<author>
			<persName><surname>Lutz</surname></persName>
		</author>
		<ptr target="https://www.w3.org/TR/owl2-profiles/" />
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">RDF Schema 1</title>
		<author>
			<persName><forename type="first">Ramanathan</forename><surname>Guha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Brickley</surname></persName>
		</author>
		<ptr target="http://www.w3.org/TR/2014/REC-rdf-schema-20140225/" />
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Reifying RDF: What Works Well With Wikidata?</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Hernández</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><surname>Hogan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Krötzsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th International Workshop on Scalable Semantic Web Knowledge Base Systems co-located with 14th International Semantic Web Conference (ISWC 2015)</title>
		<meeting>the 11th International Workshop on Scalable Semantic Web Knowledge Base Systems co-located with 14th International Semantic Web Conference (ISWC 2015)<address><addrLine>Bethlehem, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-10-11">2015. October 11, 2015</date>
			<biblScope unit="page" from="32" to="47" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Rule Learning from Knowledge Graphs Guided by Embedding Models</title>
		<author>
			<persName><forename type="first">Thinh</forename><surname>Vinh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daria</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohamed</forename><forename type="middle">H</forename><surname>Stepanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evgeny</forename><surname>Gad-Elrab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gerhard</forename><surname>Kharlamov</surname></persName>
		</author>
		<author>
			<persName><surname>Weikum</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-00671-6_5</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-00671-6_5" />
	</analytic>
	<monogr>
		<title level="m">The Semantic Web -ISWC 2018 -17th International Semantic Web Conference</title>
		<meeting><address><addrLine>Monterey, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-10-08">2018. October 8-12, 2018</date>
			<biblScope unit="page" from="72" to="90" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The role of semantics in mining frequent patterns from knowledge bases in description logics with rules</title>
		<author>
			<persName><forename type="first">Joanna</forename><surname>Józefowska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Agnieszka</forename><surname>Lawrynowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomasz</forename><surname>Lukaszewski</surname></persName>
		</author>
		<idno type="DOI">10.1017/S1471068410000098</idno>
		<ptr target="https://doi.org/10.1017/S1471068410000098" />
	</analytic>
	<monogr>
		<title level="j">TPLP</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="251" to="289" />
			<date type="published" when="2010">2010. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Finding All Justifications of OWL DL Entailments</title>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Kalyanpur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bijan</forename><surname>Parsia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Horridge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evren</forename><surname>Sirin</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-540-76298-0_20</idno>
		<ptr target="https://doi.org/10.1007/978-3-540-76298-0_20" />
	</analytic>
	<monogr>
		<title level="m">The Semantic Web, 6th International Semantic Web Conference, 2nd Asian Semantic Web Conference, ISWC 2007 + ASWC 2007</title>
		<meeting><address><addrLine>Busan, Korea</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-11-11">2007. November 11-15, 2007</date>
			<biblScope unit="page" from="267" to="280" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">Holger</forename><surname>Knublauch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitris</forename><surname>Kontokostas</surname></persName>
		</author>
		<ptr target="https://www.w3.org/TR/shacl/" />
		<title level="m">Shapes Constraint Language (SHACL)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">An Introduction to Description Logics and Query Rewriting</title>
		<author>
			<persName><forename type="first">Roman</forename><surname>Kontchakov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Zakharyaschev</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-10587-1_5</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-10587-1_5" />
	</analytic>
	<monogr>
		<title level="m">Reasoning Web. Reasoning on the Web in the Big Data Era -10th International Summer School 2014</title>
		<meeting><address><addrLine>Athens, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-09-08">2014. September 8-13, 2014</date>
			<biblScope unit="page" from="195" to="244" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Test-driven evaluation of linked data quality</title>
		<author>
			<persName><forename type="first">Dimitris</forename><surname>Kontokostas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Westphal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sören</forename><surname>Auer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Hellmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jens</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roland</forename><surname>Cornelissen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amrapali</forename><surname>Zaveri</surname></persName>
		</author>
		<idno type="DOI">10.1145/2566486.2568002</idno>
		<ptr target="https://doi.org/10.1145/2566486.2568002" />
	</analytic>
	<monogr>
		<title level="m">23rd International World Wide Web Conference, WWW '14</title>
		<meeting><address><addrLine>Seoul, Republic of Korea</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-04-07">2014. April 7-11, 2014</date>
			<biblScope unit="page" from="747" to="758" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Graph-Based Wrong IsA Relation Detection in a Large-Scale Lexical Taxonomy</title>
		<author>
			<persName><forename type="first">Jiaqing</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanghua</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seung-Won</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haixun</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence</title>
		<meeting>the Thirty-First AAAI Conference on Artificial Intelligence<address><addrLine>San Francisco, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-02-04">2017. February 4-9, 2017</date>
			<biblScope unit="page" from="1178" to="1184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Integrating Classification and Association Rule Mining</title>
		<author>
			<persName><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wynne</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth International Conference on Knowledge Discovery and Data Mining (KDD-98)</title>
		<meeting>the Fourth International Conference on Knowledge Discovery and Data Mining (KDD-98)<address><addrLine>New York City, New York, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998-08-27">1998. August 27-31, 1998</date>
			<biblScope unit="page" from="80" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Fine-Grained Evaluation of Rule-and Embedding-Based Systems for Knowledge Graph Completion</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Meilicke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manuel</forename><surname>Fink</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Ruffinelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rainer</forename><surname>Gemulla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heiner</forename><surname>Stuckenschmidt</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-00671-6_1</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-00671-6_1" />
	</analytic>
	<monogr>
		<title level="m">The Semantic Web -ISWC 2018 -17th International Semantic Web Conference</title>
		<meeting><address><addrLine>Monterey, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-10-08">2018. October 8-12, 2018</date>
			<biblScope unit="page" from="3" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Bridging the gap between OWL and relational databases</title>
		<author>
			<persName><forename type="first">Boris</forename><surname>Motik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Horrocks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ulrike</forename><surname>Sattler</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.websem.2009.02.001</idno>
		<ptr target="https://doi.org/10.1016/j.websem.2009.02.001" />
	</analytic>
	<monogr>
		<title level="j">J. Web Sem</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="74" to="89" />
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">OWL 2 Web Ontology Language Mapping to RDF Graphs</title>
		<author>
			<persName><forename type="first">Boris</forename><surname>Motik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Patel-Schneider</surname></persName>
		</author>
		<ptr target="https://www.w3.org/TR/owl-mapping-to-rdf/" />
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Using Description Logics for RDF Constraint Checking and Closed-World Recognition</title>
		<author>
			<persName><forename type="first">F</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName><surname>Patel-Schneider</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence</title>
		<meeting>the Twenty-Ninth AAAI Conference on Artificial Intelligence<address><addrLine>Austin, Texas, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-01-25">2015. January 25-30, 2015</date>
			<biblScope unit="page" from="247" to="253" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Improving the Quality of Linked Data Using Statistical Distributions</title>
		<author>
			<persName><forename type="first">Heiko</forename><surname>Paulheim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Bizer</surname></persName>
		</author>
		<idno type="DOI">10.4018/ijswis.2014040104</idno>
		<ptr target="https://doi.org/10.4018/ijswis.2014040104" />
	</analytic>
	<monogr>
		<title level="j">Int. J. Semantic Web Inf. Syst</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="63" to="86" />
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Repairing ABoxes through Active Integrity Constraints</title>
		<author>
			<persName><forename type="first">Christos</forename><surname>Rantsoudis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Feuillade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Herzig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Workshop on Description Logics</title>
		<meeting>the 30th International Workshop on Description Logics<address><addrLine>Montpellier, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-07-18">2017. July 18-21, 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">General Terminology Induction in OWL</title>
		<author>
			<persName><forename type="first">Viachaslau</forename><surname>Sazonau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Uli</forename><surname>Sattler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gavin</forename><surname>Brown</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-25007-6_31</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-25007-6_31" />
	</analytic>
	<monogr>
		<title level="m">The Semantic Web -ISWC 2015 -14th International Semantic Web Conference</title>
		<meeting><address><addrLine>Bethlehem, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-10-11">2015. October 11-15, 2015</date>
			<biblScope unit="page" from="533" to="550" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Non-Standard Reasoning Services for the Debugging of Description Logic Terminologies</title>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Schlobach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronald</forename><surname>Cornet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI-03, Proceedings of the Eighteenth International Joint Conference on Artificial Intelligence</title>
		<meeting><address><addrLine>Acapulco, Mexico</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003-08-09">2003. August 9-15, 2003</date>
			<biblScope unit="page" from="355" to="362" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Yago: a core of semantic knowledge</title>
		<author>
			<persName><forename type="first">Fabian</forename><forename type="middle">M</forename><surname>Suchanek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gjergji</forename><surname>Kasneci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gerhard</forename><surname>Weikum</surname></persName>
		</author>
		<idno type="DOI">10.1145/1242572.1242667</idno>
		<ptr target="https://doi.org/10.1145/1242572.1242667" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th International Conference on World Wide Web, WWW 2007</title>
		<meeting>the 16th International Conference on World Wide Web, WWW 2007<address><addrLine>Banff, Alberta, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-05-08">2007. May 8-12, 2007</date>
			<biblScope unit="page" from="697" to="706" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Completeness-Aware Rule Learning from Knowledge Graphs</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Pellissier Tanon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daria</forename><surname>Stepanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Razniewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paramita</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gerhard</forename><surname>Weikum</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-68288-4_30</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-68288-4_30" />
	</analytic>
	<monogr>
		<title level="m">The Semantic Web -ISWC 2017 -16th International Semantic Web Conference</title>
		<meeting><address><addrLine>Vienna, Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-10-21">2017. October 21-25, 2017</date>
			<biblScope unit="page" from="507" to="525" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Integrity Constraints in OWL</title>
		<author>
			<persName><forename type="first">Jiao</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evren</forename><surname>Sirin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deborah</forename><forename type="middle">L</forename><surname>Mcguinness</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2010</title>
		<meeting>the Twenty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2010<address><addrLine>Atlanta, Georgia, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-07-11">2010. July 11-15, 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Wikidata: a free collaborative knowledgebase</title>
		<author>
			<persName><forename type="first">Denny</forename><surname>Vrandečić</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Krötzsch</surname></persName>
		</author>
		<idno type="DOI">10.1145/2629489</idno>
		<ptr target="https://doi.org/10.1145/2629489" />
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="78" to="85" />
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Embedding Entities and Relations for Learning and Inference in Knowledge Bases</title>
		<author>
			<persName><forename type="first">Bishan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6575</idno>
		<ptr target="http://arxiv.org/abs/1412.6575" />
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Differentiable Learning of Logical Rules for Knowledge Base Reasoning</title>
		<author>
			<persName><forename type="first">Fan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-04-09">2017. 4-9 December 2017</date>
			<biblScope unit="page" from="2316" to="2325" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</teiCorpus></text>
</TEI>