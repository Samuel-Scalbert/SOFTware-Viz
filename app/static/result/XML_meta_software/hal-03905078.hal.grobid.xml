<?xml version='1.0' encoding='utf-8'?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" version="1.1" xsi:schemaLocation="http://www.tei-c.org/ns/1.0 http://api.archives-ouvertes.fr/documents/aofr-sword.xsd">
  <teiHeader>
    <fileDesc>
      <titleStmt>
        <title>HAL TEI export of hal-03905078</title>
      </titleStmt>
      <publicationStmt>
        <distributor>CCSD</distributor>
        <availability status="restricted">
          <licence target="http://creativecommons.org/licenses/by/4.0/">Distributed under a Creative Commons Attribution 4.0 International License</licence>
        </availability>
        <date when="2024-04-23T09:32:07+02:00" />
      </publicationStmt>
      <sourceDesc>
        <p part="N">HAL API platform</p>
      </sourceDesc>
    </fileDesc>
  </teiHeader>
  <text>
    <body>
      <listBibl>
        <biblFull>
          <titleStmt>
            <title xml:lang="en">Differentially Private Federated Learning on Heterogeneous Data</title>
            <author role="aut">
              <persName>
                <forename type="first">Maxence</forename>
                <surname>Noble</surname>
              </persName>
              <idno type="halauthorid">2385265-0</idno>
              <affiliation ref="#struct-89626" />
            </author>
            <author role="aut">
              <persName>
                <forename type="first">Aurélien</forename>
                <surname>Bellet</surname>
              </persName>
              <email type="md5">7c92d2fc696e1875415477238a601d34</email>
              <email type="domain">inria.fr</email>
              <ptr type="url" target="http://researchers.lille.inria.fr/abellet/" />
              <idno type="idhal" notation="string">aurelien-bellet</idno>
              <idno type="idhal" notation="numeric">9877</idno>
              <idno type="halauthorid" notation="string">30290-9877</idno>
              <idno type="ORCID">https://orcid.org/0000-0003-3440-1251</idno>
              <idno type="GOOGLE SCHOLAR">https://scholar.google.fr/citations?user=j8svx3IAAAAJ</idno>
              <idno type="IDREF">https://www.idref.fr/17653136X</idno>
              <idno type="ARXIV">https://arxiv.org/a/bellet_a_1</idno>
              <affiliation ref="#struct-432650" />
            </author>
            <author role="aut">
              <persName>
                <forename type="first">Aymeric</forename>
                <surname>Dieuleveut</surname>
              </persName>
              <email type="md5">7001b13cdb9e64d83b049377e6e70aa9</email>
              <email type="domain">polytechnique.edu</email>
              <idno type="idhal" notation="string">aymeric-dieuleveut</idno>
              <idno type="idhal" notation="numeric">1109167</idno>
              <idno type="halauthorid" notation="string">851620-1109167</idno>
              <idno type="ORCID">https://orcid.org/0009-0005-1848-1724</idno>
              <idno type="GOOGLE SCHOLAR">https://scholar.google.fr/citations?user=ge-OinUAAAAJ</idno>
              <affiliation ref="#struct-89626" />
            </author>
            <editor role="depositor">
              <persName>
                <forename>Aurélien</forename>
                <surname>Bellet</surname>
              </persName>
              <email type="md5">7c92d2fc696e1875415477238a601d34</email>
              <email type="domain">inria.fr</email>
            </editor>
            <funder ref="#projanr-52219" />
          </titleStmt>
          <editionStmt>
            <edition n="v1" type="current">
              <date type="whenSubmitted">2022-12-17 19:05:42</date>
              <date type="whenModified">2024-01-24 09:54:24</date>
              <date type="whenReleased">2022-12-19 08:33:20</date>
              <date type="whenProduced">2022</date>
              <date type="whenEndEmbargoed">2022-12-17</date>
              <ref type="file" target="https://inria.hal.science/hal-03905078/document">
                <date notBefore="2022-12-17" />
              </ref>
              <ref type="file" subtype="author" n="1" target="https://inria.hal.science/hal-03905078/file/noble22a.pdf">
                <date notBefore="2022-12-17" />
              </ref>
              <ref type="externalLink" target="http://arxiv.org/pdf/2111.09278" />
            </edition>
            <respStmt>
              <resp>contributor</resp>
              <name key="179317">
                <persName>
                  <forename>Aurélien</forename>
                  <surname>Bellet</surname>
                </persName>
                <email type="md5">7c92d2fc696e1875415477238a601d34</email>
                <email type="domain">inria.fr</email>
              </name>
            </respStmt>
          </editionStmt>
          <publicationStmt>
            <distributor>CCSD</distributor>
            <idno type="halId">hal-03905078</idno>
            <idno type="halUri">https://inria.hal.science/hal-03905078</idno>
            <idno type="halBibtex">noble:hal-03905078</idno>
            <idno type="halRefHtml">&lt;i&gt;Proceedings of The 25th International Conference on Artificial Intelligence and Statistics (AISTATS)&lt;/i&gt;, 2022, Virtual, Spain</idno>
            <idno type="halRef">Proceedings of The 25th International Conference on Artificial Intelligence and Statistics (AISTATS), 2022, Virtual, Spain</idno>
          </publicationStmt>
          <seriesStmt>
            <idno type="stamp" n="X">Ecole Polytechnique</idno>
            <idno type="stamp" n="CNRS">CNRS - Centre national de la recherche scientifique</idno>
            <idno type="stamp" n="INRIA">INRIA - Institut National de Recherche en Informatique et en Automatique</idno>
            <idno type="stamp" n="INRIA-LILLE">INRIA Lille - Nord Europe</idno>
            <idno type="stamp" n="X-CMAP" corresp="X">Centre de mathématiques appliquées (CMAP)</idno>
            <idno type="stamp" n="X-DEP" corresp="X">Polytechnique</idno>
            <idno type="stamp" n="X-DEP-MATHA" corresp="X-DEP">Département de mathématiques appliquées</idno>
            <idno type="stamp" n="INRIA_TEST">INRIA - Institut National de Recherche en Informatique et en Automatique</idno>
            <idno type="stamp" n="TESTALAIN1">TESTALAIN1</idno>
            <idno type="stamp" n="CMAP">Centre de Mathématiques Appliquées</idno>
            <idno type="stamp" n="CRISTAL">Centre de Recherche en Informatique, Signal et Automatique de Lille (CRISTAL)</idno>
            <idno type="stamp" n="INRIA2">INRIA 2</idno>
            <idno type="stamp" n="CRISTAL-MAGNET" corresp="CRISTAL">CRISTAL-MAGNET</idno>
            <idno type="stamp" n="UNIV-LILLE">Université de Lille</idno>
            <idno type="stamp" n="IP_PARIS">Institut Polytechnique de Paris</idno>
            <idno type="stamp" n="ANR">ANR</idno>
          </seriesStmt>
          <notesStmt>
            <note type="audience" n="2">International</note>
            <note type="invited" n="0">No</note>
            <note type="popular" n="0">No</note>
            <note type="peer" n="1">Yes</note>
            <note type="proceedings" n="1">Yes</note>
          </notesStmt>
          <sourceDesc>
            <biblStruct>
              <analytic>
                <title xml:lang="en">Differentially Private Federated Learning on Heterogeneous Data</title>
                <author role="aut">
                  <persName>
                    <forename type="first">Maxence</forename>
                    <surname>Noble</surname>
                  </persName>
                  <idno type="halauthorid">2385265-0</idno>
                  <affiliation ref="#struct-89626" />
                </author>
                <author role="aut">
                  <persName>
                    <forename type="first">Aurélien</forename>
                    <surname>Bellet</surname>
                  </persName>
                  <email type="md5">7c92d2fc696e1875415477238a601d34</email>
                  <email type="domain">inria.fr</email>
                  <ptr type="url" target="http://researchers.lille.inria.fr/abellet/" />
                  <idno type="idhal" notation="string">aurelien-bellet</idno>
                  <idno type="idhal" notation="numeric">9877</idno>
                  <idno type="halauthorid" notation="string">30290-9877</idno>
                  <idno type="ORCID">https://orcid.org/0000-0003-3440-1251</idno>
                  <idno type="GOOGLE SCHOLAR">https://scholar.google.fr/citations?user=j8svx3IAAAAJ</idno>
                  <idno type="IDREF">https://www.idref.fr/17653136X</idno>
                  <idno type="ARXIV">https://arxiv.org/a/bellet_a_1</idno>
                  <affiliation ref="#struct-432650" />
                </author>
                <author role="aut">
                  <persName>
                    <forename type="first">Aymeric</forename>
                    <surname>Dieuleveut</surname>
                  </persName>
                  <email type="md5">7001b13cdb9e64d83b049377e6e70aa9</email>
                  <email type="domain">polytechnique.edu</email>
                  <idno type="idhal" notation="string">aymeric-dieuleveut</idno>
                  <idno type="idhal" notation="numeric">1109167</idno>
                  <idno type="halauthorid" notation="string">851620-1109167</idno>
                  <idno type="ORCID">https://orcid.org/0009-0005-1848-1724</idno>
                  <idno type="GOOGLE SCHOLAR">https://scholar.google.fr/citations?user=ge-OinUAAAAJ</idno>
                  <affiliation ref="#struct-89626" />
                </author>
              </analytic>
              <monogr>
                <meeting>
                  <title>Proceedings of The 25th International Conference on Artificial Intelligence and Statistics (AISTATS)</title>
                  <date type="start">2022</date>
                  <settlement>Virtual</settlement>
                  <country key="ES">Spain</country>
                </meeting>
                <imprint />
              </monogr>
              <idno type="arxiv">2111.09278</idno>
            </biblStruct>
          </sourceDesc>
          <profileDesc>
            <langUsage>
              <language ident="en">English</language>
            </langUsage>
            <textClass>
              <classCode scheme="halDomain" n="info.info-lg">Computer Science [cs]/Machine Learning [cs.LG]</classCode>
              <classCode scheme="halDomain" n="stat.ml">Statistics [stat]/Machine Learning [stat.ML]</classCode>
              <classCode scheme="halTypology" n="COMM">Conference papers</classCode>
              <classCode scheme="halOldTypology" n="COMM">Conference papers</classCode>
              <classCode scheme="halTreeTypology" n="COMM">Conference papers</classCode>
            </textClass>
            <abstract xml:lang="en">
              <p>Federated Learning (FL) is a paradigm for large-scale distributed learning which faces two key challenges: (i) training efficiently from highly heterogeneous user data, and (ii) protecting the privacy of participating users. In this work, we propose a novel FL approach (DP-SCAFFOLD) to tackle these two challenges together by incorporating Differential Privacy (DP) constraints into the popular SCAFFOLD algorithm. We focus on the challenging setting where users communicate with a "honest-but-curious" server without any trusted intermediary, which requires to ensure privacy not only towards a third party observing the final model but also towards the server itself. Using advanced results from DP theory and optimization, we establish the convergence of our algorithm for convex and non-convex objectives. Our paper clearly highlights the trade-off between utility and privacy and demonstrates the superiority of DP-SCAFFOLD over the state-ofthe-art algorithm DP-FedAvg when the number of local updates and the level of heterogeneity grows. Our numerical results confirm our analysis and show that DP-SCAFFOLD provides significant gains in practice.</p>
            </abstract>
          </profileDesc>
        </biblFull>
      </listBibl>
    </body>
    <back>
      <listOrg type="structures">
        <org type="laboratory" xml:id="struct-89626" status="VALID">
          <idno type="IdRef">197373461</idno>
          <idno type="RNSR">199719340P</idno>
          <orgName>Centre de Mathématiques Appliquées - Ecole Polytechnique</orgName>
          <orgName type="acronym">CMAP</orgName>
          <desc>
            <address>
              <addrLine>École Polytechnique Route de Saclay 91128 Palaiseau Cedex</addrLine>
              <country key="FR" />
            </address>
            <ref type="url">http://www.cmap.polytechnique.fr/</ref>
          </desc>
          <listRelation>
            <relation active="#struct-300340" type="direct" />
            <relation name="UMR7641" active="#struct-441569" type="direct" />
          </listRelation>
        </org>
        <org type="researchteam" xml:id="struct-432650" status="VALID">
          <idno type="RNSR">201321079K</idno>
          <orgName>Machine Learning in Information Networks</orgName>
          <orgName type="acronym">MAGNET</orgName>
          <date type="start">2015-01-01</date>
          <desc>
            <address>
              <country key="FR" />
            </address>
            <ref type="url">http://www.inria.fr/equipes/magnet</ref>
          </desc>
          <listRelation>
            <relation active="#struct-104752" type="direct" />
            <relation active="#struct-300009" type="indirect" />
            <relation active="#struct-410272" type="direct" />
            <relation name="UMR9189" active="#struct-120930" type="indirect" />
            <relation name="UMR9189" active="#struct-374570" type="indirect" />
            <relation name="UMR9189" active="#struct-441569" type="indirect" />
          </listRelation>
        </org>
        <org type="institution" xml:id="struct-300340" status="VALID">
          <idno type="IdRef">027309320</idno>
          <idno type="ROR">https://ror.org/05hy3tk52</idno>
          <orgName>École polytechnique</orgName>
          <orgName type="acronym">X</orgName>
          <date type="start">1794-03-11</date>
          <desc>
            <address>
              <addrLine>École polytechnique, 91128 Palaiseau Cedex</addrLine>
              <country key="FR" />
            </address>
            <ref type="url">https://www.polytechnique.edu/</ref>
          </desc>
        </org>
        <org type="regroupinstitution" xml:id="struct-441569" status="VALID">
          <idno type="IdRef">02636817X</idno>
          <idno type="ISNI">0000000122597504</idno>
          <idno type="ROR">https://ror.org/02feahw73</idno>
          <orgName>Centre National de la Recherche Scientifique</orgName>
          <orgName type="acronym">CNRS</orgName>
          <date type="start">1939-10-19</date>
          <desc>
            <address>
              <country key="FR" />
            </address>
            <ref type="url">https://www.cnrs.fr/</ref>
          </desc>
        </org>
        <org type="laboratory" xml:id="struct-104752" status="VALID">
          <idno type="RNSR">200818245B</idno>
          <idno type="ROR">https://ror.org/04eej9726</idno>
          <orgName>Inria Lille - Nord Europe</orgName>
          <desc>
            <address>
              <addrLine>Parc Scientifique de la Haute Borne 40, avenue Halley Bât.A, Park Plaza 59650 Villeneuve d'Ascq</addrLine>
              <country key="FR" />
            </address>
            <ref type="url">http://www.inria.fr/lille/</ref>
          </desc>
          <listRelation>
            <relation active="#struct-300009" type="direct" />
          </listRelation>
        </org>
        <org type="institution" xml:id="struct-300009" status="VALID">
          <idno type="ROR">https://ror.org/02kvxyf05</idno>
          <orgName>Institut National de Recherche en Informatique et en Automatique</orgName>
          <orgName type="acronym">Inria</orgName>
          <desc>
            <address>
              <addrLine>Domaine de VoluceauRocquencourt - BP 10578153 Le Chesnay Cedex</addrLine>
              <country key="FR" />
            </address>
            <ref type="url">http://www.inria.fr/en/</ref>
          </desc>
        </org>
        <org type="laboratory" xml:id="struct-410272" status="VALID">
          <idno type="IdRef">18388695X</idno>
          <idno type="RNSR">201521249L</idno>
          <idno type="ROR">https://ror.org/05vrs3189</idno>
          <orgName>Centre de Recherche en Informatique, Signal et Automatique de Lille - UMR 9189</orgName>
          <orgName type="acronym">CRIStAL</orgName>
          <date type="start">2015-01-01</date>
          <desc>
            <address>
              <addrLine>Université de Lille - Campus scientifique - Bâtiment ESPRIT - Avenue Henri Poincaré - 59655 Villeneuve d’Ascq</addrLine>
              <country key="FR" />
            </address>
            <ref type="url">https://www.cristal.univ-lille.fr/</ref>
          </desc>
          <listRelation>
            <relation name="UMR9189" active="#struct-120930" type="direct" />
            <relation name="UMR9189" active="#struct-374570" type="direct" />
            <relation name="UMR9189" active="#struct-441569" type="direct" />
          </listRelation>
        </org>
        <org type="institution" xml:id="struct-120930" status="VALID">
          <idno type="IdRef">256304629</idno>
          <idno type="ISNI">0000000122034461</idno>
          <idno type="ROR">https://ror.org/01x441g73</idno>
          <orgName>Centrale Lille</orgName>
          <desc>
            <address>
              <addrLine>École Centrale de Lille - Cité Scientifique - CS 20048 59651 Villeneuve d'Ascq Cedex</addrLine>
              <country key="FR" />
            </address>
            <ref type="url">https://centralelille.fr/</ref>
          </desc>
        </org>
        <org type="regroupinstitution" xml:id="struct-374570" status="VALID">
          <idno type="IdRef">223446556</idno>
          <idno type="ISNI">0000 0001 2242 6780</idno>
          <idno type="ROR">https://ror.org/02kzqn938</idno>
          <idno type="Wikidata">Q3551621</idno>
          <orgName>Université de Lille</orgName>
          <desc>
            <address>
              <addrLine>EPE Université de Lille. -- 42 rue Paul Duez, 59000 Lille</addrLine>
              <country key="FR" />
            </address>
            <ref type="url">https://www.univ-lille.fr/</ref>
          </desc>
        </org>
      </listOrg>
      <listOrg type="projects">
        <org type="anrProject" xml:id="projanr-52219" status="VALID">
          <idno type="anr">ANR-20-CE23-0015</idno>
          <orgName>PRIDE</orgName>
          <desc>Apprentissage automatique décentralisé et préservant la vie privée</desc>
          <date type="start">2020</date>
        </org>
      </listOrg>
    </back>
  <teiCorpus>
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Differentially Private Federated Learning on Heterogeneous Data</title>
				<funder ref="#_SmbFken #_UEb5sw9 #_kuGZGs5">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher />
				<availability status="unknown"><licence /></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Maxence</forename><surname>Noble</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Aurélien</forename><surname>Bellet</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Aymeric</forename><surname>Dieuleveut</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Centre de Mathématiques Appliquées Ecole Polytechnique</orgName>
								<orgName type="department" key="dep2">Inria</orgName>
								<orgName type="institution">Institut Polytechnique de Paris Univ. Lille</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">CNRS</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="laboratory">UMR 9189</orgName>
								<orgName type="institution">Centrale Lille</orgName>
								<address>
									<addrLine>-CRIStAL</addrLine>
									<postCode>F-59000</postCode>
									<settlement>Lille</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">Centre de Mathématiques Appliquées Ecole Polytechnique</orgName>
								<orgName type="institution">Institut Polytechnique de Paris</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Differentially Private Federated Learning on Heterogeneous Data</title>
					</analytic>
					<monogr>
						<imprint>
							<date />
						</imprint>
					</monogr>
					<idno type="MD5">AC3748E4E315B6781A8610151AF6D7FD</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-04-12T14:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid" />
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div><p>Federated Learning (FL) is a paradigm for large-scale distributed learning which faces two key challenges: (i) training efficiently from highly heterogeneous user data, and (ii) protecting the privacy of participating users. In this work, we propose a novel FL approach (DP-<software>SCAFFOLD</software>) to tackle these two challenges together by incorporating Differential Privacy (DP) constraints into the popular <software ContextAttributes="used">SCAFFOLD</software> algorithm. We focus on the challenging setting where users communicate with a "honest-but-curious" server without any trusted intermediary, which requires to ensure privacy not only towards a third party observing the final model but also towards the server itself. Using advanced results from DP theory and optimization, we establish the convergence of our algorithm for convex and non-convex objectives. Our paper clearly highlights the trade-off between utility and privacy and demonstrates the superiority of <software ContextAttributes="used">DP-</software><software ContextAttributes="used">SCAFFOLD</software> over the state-ofthe-art algorithm DP-<software ContextAttributes="used">FedAvg</software> when the number of local updates and the level of heterogeneity grows. Our numerical results confirm our analysis and show that <software ContextAttributes="used">DP-SCAFFOLD</software> provides significant gains in practice.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div><p>). Compared to machine learning in the cloud, the promise of FL is to avoid the costs of moving data and to mitigate privacy concerns. Yet, this promise can only be fulfilled if two key challenges are addressed. First, FL algorithms must be able to efficiently deal with the high heterogeneity of data across users, which stems from the fact that each local dataset reflects the usage and production patterns specific to a given user. Heterogeneous data may prevent FL algorithms from converging unless they use a large number of communication rounds between the users and the server, which is often considered as a bottleneck in FL <ref type="bibr">(Khaled et al., 2020;</ref><ref type="bibr">Karimireddy et al., 2020b)</ref>. Second, when training data contains sensitive or confidential information, FL algorithms must provide rigorous privacy guarantees to ensure that the server (or a third party) cannot accurately reconstruct this information from model updates shared by users <ref type="bibr" target="#b15">(Geiping et al., 2020)</ref>. The widely recognized way to quantify such guarantees is Differential Privacy (DP) <ref type="bibr" target="#b10">(Dwork and Roth, 2014)</ref>.</p><p>Since the seminal <software ContextAttributes="used">FedAvg</software> algorithm proposed by <ref type="bibr">McMahan et al. (2017)</ref>, a lot of effort has gone into addressing these two challenges separately. FL algorithms like <software ContextAttributes="used">SCAFFOLD</software> <ref type="bibr">(Karimireddy et al., 2020b)</ref> and <ref type="bibr"><software>FedProx</software> (Li et al., 2020a)</ref> can better deal with heterogeneous data, while versions of <software ContextAttributes="used">FedAvg</software> with Differential Privacy (DP) guarantees have been proposed based on the addition of random noise to the model updates <ref type="bibr">(McMahan et al., 2018;</ref><ref type="bibr" target="#b16">Geyer et al., 2017;</ref><ref type="bibr" target="#b21">Triastcyn and Faltings, 2019</ref>). Yet, we are not aware of any approach designed to tackle data heterogeneity while ensuring differential privacy, or of any work studying the associated trade-offs. This appears to be a challenging problem: on the one hand, data heterogeneity can hurt the privacy-utility trade-off of DP-FL algorithms (by requiring more communication rounds and thus more noise). On the other hand, it is not clear how to extend existing heterogeneous FL algorithms to satisfy DP and what the resulting privacyutility trade-off would be in theory and in practice.</p><p>Our work precisely aims to tackle the issue of data heterogeneity in the context of FL under DP constraints. We aim to protect the privacy of any user's data against a honest-but-curious server observing all user updates, and against a third party observing only the final model. We present DP-<software ContextAttributes="used">SCAFFOLD</software>, a novel differential private FL algorithm for training a global model from heterogeneous data based on <software ContextAttributes="used">SCAFFOLD</software> <ref type="bibr">(Karimireddy et al., 2020b)</ref> augmented with the addition of noise in the local model updates. Our convergence analysis leverages a particular initialization of the algorithm, and controls a different set of quantities than in the original proof.</p><p>Relying on recent tools for tightly keeping track of the privacy loss of the subsampled Gaussian mechanism <ref type="bibr" target="#b24">(Wang et al., 2020)</ref> under Rényi Differential Privacy (RDP) <ref type="bibr">(Mironov, 2017)</ref>, we formally characterize the privacy-utility trade-off of DP-<software ContextAttributes="used">FedAvg</software>, considered as the state-of-the-art DP-FL algorithm <ref type="bibr" target="#b16">(Geyer et al., 2017)</ref>, and DP-<software ContextAttributes="used">SCAFFOLD</software> in convex and nonconvex regimes. Our results show the superiority of <software ContextAttributes="used">DP-SCAFFOLD</software> over DP-<software ContextAttributes="used">FedAvg</software> when the number of local updates is large and/or the level of heterogeneity is high. Finally, we provide experiments on simulated and real-world data which confirm our theoretical findings and show that the gains achieved by <software ContextAttributes="used">DP-SCAFFOLD</software> are significant in practice.</p><p>The rest of the paper is organized as follows. Section 2 reviews some background and related work on FL, data heterogeneity and privacy. Section 3 describes the problem setting and introduces DP-<software>SCAFFOLD</software>. In Section 4, we provide theoretical guarantees on both privacy and utility for DP-<software ContextAttributes="used">SCAFFOLD</software> and DP-<software ContextAttributes="used">FedAvg</software>. Finally, Section 5 presents the results of our experiments and we conclude with some perspectives for future work in Section 6.</p></div>
<div><head n="2">RELATED WORK</head><p>Federated learning &amp; heterogeneity. The baseline FL algorithm <software ContextAttributes="used">FedAvg</software> <ref type="bibr">(McMahan et al., 2017)</ref> is known to suffer from instability and convergence issues in heterogeneous settings, related to device variability or non-identically distributed data <ref type="bibr">(Khaled et al., 2020)</ref>. In the last case, these issues stem from a userdrift in the local updates, which occurs even if all users are available or full-batch gradients are used <ref type="bibr">(Karimireddy et al., 2020b)</ref>. Several FL algorithms have been proposed to better tackle heterogeneity. <software ContextAttributes="used">FedProx</software> <ref type="bibr">(Li et al., 2020a)</ref> features a proximal term in the objective function of local updates. However, it is often numerically outperformed by <ref type="bibr"><software>SCAFFOLD</software> (Karimireddy et al., 2020b)</ref>, which relies on variance reduction through control variates. In a nutshell, the update direction of the global model at the server (c) and the update direction of each user i's local model (c i ) are estimated and combined in local Stochastic Gradient Descent (SGD) steps (c -c i ) to correct the user-drift (see Section 3.3 for more details). <ref type="bibr">MIME (Karimireddy et al., 2020a</ref>) also focuses on client heterogeneity and improves on <software ContextAttributes="used">SCAFFOLD</software> by using the stochastic gradient evaluated on the global model as the local variate c i and the synchronized full-batch gradient as the global control variate c. However, computing full-batch gradients is very costly in practice. Similarly, incorporating DP noise into <software ContextAttributes="used">FedDyn</software> <ref type="bibr" target="#b1">(Acar et al., 2021)</ref>, which is based on the exact minimization of a proxy function, is not straightforward. On the other hand, the adaptation of <software ContextAttributes="used">SCAFFOLD</software> to DP-<software ContextAttributes="used">SCAFFOLD</software> is more natural as control variates only depend on stochastic gradients and thus do not degrade the privacy level throughout the iterations (see details in Section 4.1).</p><p>Extension to other optimization schemes: While Fed-Opt <ref type="bibr">(Reddi et al., 2020)</ref> generalizes <software ContextAttributes="used">FedAvg</software> by using different optimization methods locally (e.g., Adam (Kingma and Ba, 2014), AdaGrad <ref type="bibr" target="#b7">(Duchi et al., 2011)</ref>, etc., instead of vanilla local SGD steps) or a different aggregation on the central server, these methods may also suffer from user-drift. Their main objective is to improve the convergence rate <ref type="bibr" target="#b23">(Wang et al., 2021)</ref> without focusing on heterogeneity. We thus choose to focus on the simplest algorithm to highlight the impact of DP and heterogeneity.</p><p>Federated learning &amp; differential privacy. Even if datasets remain decentralized in FL, the privacy of users may still be compromised by the fact that the server (which may be "honest-but-curious") or a third party has access to model parameters that are exchanged during or after training <ref type="bibr" target="#b14">(Fredrikson et al., 2015;</ref><ref type="bibr" target="#b20">Shokri et al., 2017;</ref><ref type="bibr" target="#b15">Geiping et al., 2020)</ref>. Differential Privacy (DP) <ref type="bibr" target="#b10">(Dwork and Roth, 2014)</ref> provides a robust mathematical way to quantify the information that an algorithm A leaks about its input data. DP relies on a notion of neighboring datasets, which in the context of FL may refer to pairs of datasets differing by one user <ref type="bibr">(user-level DP)</ref> or by one data point of one user (record-level DP).</p><p>Definition 2.1 (Differential Privacy, <ref type="bibr" target="#b10">Dwork and Roth, 2014)</ref>. Let , δ &gt; 0. A randomized algorithm A : X n → Y is ( , δ)-DP if for all pairs of neighboring datasets D, D and every subset S ⊂ Y, we have:</p><formula xml:id="formula_0">P[A(D) ∈ S] ≤ e P[A(D ) ∈ S] + δ.</formula><p>The privacy level is controlled by the parameters and δ (the lower, the more private). A standard building block to design DP algorithms is the Gaussian mech-anism <ref type="bibr" target="#b10">(Dwork and Roth, 2014)</ref>, which adds Gaussian noise to the output of a non-private computation. The variance of the noise is calibrated to the sensitivity of the computation, i.e., the worst-case change (measured in 2 norm) in its output on two neighboring datasets. The design of private ML algorithms heavily relies on the Gaussian mechanism to randomize intermediate data-dependent computations (e.g. gradients). The privacy guarantees of the overall procedure are then obtained via composition <ref type="bibr" target="#b11">(Dwork et al., 2010;</ref><ref type="bibr">Kairouz et al., 2015)</ref>. Recent theoretical tools like Rényi Differential Privacy (Mironov, 2017) (see Appendix B) allow to obtain tighter privacy bounds for the Gaussian mechanism under composition and data subsampling <ref type="bibr" target="#b24">(Wang et al., 2020)</ref>.</p><p>In the context of FL, the output of an algorithm A in the sense of Definition 2.1 contains all information observed by the party we aim to protect against. Some work considered a trusted server and thus only protect against a third party who observes the final model. In this setting, <ref type="bibr">McMahan et al. (2018)</ref> introduced DP-<software ContextAttributes="used">FedAvg</software> and DP-<software ContextAttributes="used">FedSGD</software> (i.e., DP-<software ContextAttributes="used">FedAvg</software> with a single local update), which was also proposed independently by <ref type="bibr" target="#b16">Geyer et al. (2017)</ref>. These algorithms extend <software ContextAttributes="used">FedAvg</software> and <software ContextAttributes="used">FedSGD</software> by having the server add Gaussian noise to the aggregated user updates. <ref type="bibr" target="#b21">Triastcyn and Faltings (2019)</ref> used a relaxation of DP known as Bayesian DP to provide sharper privacy loss bounds. However, these papers do not discuss the theoretical trade-off between utility and privacy. Some recent work by <ref type="bibr" target="#b25">Wei et al. (2020)</ref> has formally examined this trade-off for DP-<software ContextAttributes="used">FedSGD</software>, providing a utility guarantee for strongly convex loss functions. However, they do not consider multiple local updates. Some papers also considered the setting with a "honest-but-curious" server, where users must randomize their updates locally before sharing them. This corresponds to a stronger version of DP, referred to as Local Differential Privacy (LDP) <ref type="bibr" target="#b8">(Duchi et al., 2013;</ref><ref type="bibr" target="#b26">Zhao et al., 2021;</ref><ref type="bibr" target="#b9">Duchi et al., 2018)</ref>. DP-<software ContextAttributes="used">FedAvg</software> and DP-<software ContextAttributes="used">FedSGD</software> can be easily adapted to this setting by pushing the Gaussian noise addition to the users, which induces a cost in utility. <ref type="bibr" target="#b26">Zhao et al. (2021)</ref> consider DP-<software ContextAttributes="used">FedSGD</software> in this setting but do not provide any utility analysis. <ref type="bibr">Girgis et al. (2021b)</ref> provide utility and compression guarantees for variants of DP-<software ContextAttributes="used">FedSGD</software> in an intermediate model where a trusted shuffler between the server and the users randomly permutes the user contributions, which is known to amplify privacy <ref type="bibr" target="#b3">(Balle et al., 2019;</ref><ref type="bibr" target="#b5">Cheu et al., 2019;</ref><ref type="bibr" target="#b17">Ghazi et al., 2019;</ref><ref type="bibr" target="#b12">Erlingsson et al., 2019)</ref>. However, both of these studies do not consider multiple local updates, which is key to reduce the number of communication rounds. <ref type="bibr">Li et al. (2020b)</ref> consider the server as "honest-but-curious" but does not ensure end-to-end privacy to the users. <ref type="bibr">Finally, Hu et al. (2020)</ref> present a personalized DP-FL approach as a way to tackle data heterogeneity, but it is limited to linear models. Summary. To the best of our knowledge, there exists no FL approach designed to tackle data heterogeneity under DP constraints, or any study of existing DP-FL algorithms capturing the impact of data heterogeneity on the privacy-utility trade-off.</p></div>
<div><head n="3">DP-SCAFFOLD</head><p>In this section, we first describe the framework that we consider for FL and DP, before giving a detailed description of DP-<software>SCAFFOLD</software>. A table summarizing all notations is provided in Appendix A.</p></div>
<div><head n="3.1">Federated Learning Framework</head><p>We consider a setting with a central server and M users. Each user i ∈ [M ], holds a private local dataset</p><formula xml:id="formula_1">D i = {d i 1 , ..., d i R } ⊂ X R , composed of R</formula><p>observations living in a space X . We denote by D := D 1 ... D M the disjoint union of all user datasets. Each dataset D i is supposed to be independently sampled from distinct distributions. The objective is to solve the following empirical risk minimization problem over parameter x:</p><formula xml:id="formula_2">min x∈R d F (x) := 1 M M i=1 F i (x),</formula><p>where</p><formula xml:id="formula_3">F i (x) := 1 R R j=1 f i (x, d i j</formula><p>) is the empirical risk on user i, and for all x ∈ R d and d ∈ X , f i (x, d) is the loss of the model x on observation d. We denote by ∇f i (x, d i j ) the gradient of the loss f i computed on a sample d i j ∈ D i , and by extension, for any</p><formula xml:id="formula_4">S i ⊂ D i , ∇f i (x, S i ) := 1 |Si| j∈Si ∇f i (x, d i j )</formula><p>is the averaged mini-batch gradient. We note that our results can easily be adapted to optimize any weighted average of the loss functions and to imbalanced local datasets.</p></div>
<div><head n="3.2">Privacy Model</head><p>We aim at controlling the information leakage from individual datasets D i in the updates shared by the users. For simplicity, our analysis focuses on recordlevel DP with respect to (w.r.t) the joint dataset D. We thus consider the following notion of neighborhood: D, D ∈ X M R are neighboring datasets (denoted ||D -D || ≤ 1) if they differ by at most one record, that is if there exists at most one i ∈ [M ] such that D i and D i differ by one record. We want to ensure privacy (or quantify privacy level) (i) towards a third party observing the final model and (ii) towards an honest-but-curious server. Our DP budget is set in advance and denoted by ( , δ), and corresponds to the Algorithm 1: <software>DP-SCAFFOLD</software>(T, K, l, s, σ g , C)</p><p>Server Input: initial x 0 , initial c 0 i-th User Input: initial c 0 i Output: x T 1 for t = 1, ..., T do 2 User subsampling by the server:</p><formula xml:id="formula_5">3 Sample C t ⊂ [M ] of size lM 4 Server sends (x t-1 , c t-1 ) to users i ∈ C t 5 for user i ∈ C t do 6 Initialize model: y 0 i ← x t-1 7 for k = 1, ..., K do 8</formula><p>Data subsampling by user i:</p><formula xml:id="formula_6">Sample S k i ⊂ D i of size sR 9 for sample j ∈ S k i do</formula><p>Compute gradient:</p><formula xml:id="formula_7">g ij ← ∇f i (y k-1 i , d i j ) Clip gradient: gij ← g ij / max 1, ||g ij || 2 /C</formula><p>Add DP noise to local gradients:</p><formula xml:id="formula_8">Hk i ← 1 sR j∈S k i gij + 2C sR N (0, σ 2 g ) y k i ← y k-1 i -η l ( Hk i -c t-1 i + c t-1 ) ct i ← c t-1 i -c t-1 + 1 Kη l (x t-1 -y K i ) (∆y t i , ∆c t i ) ← (y K i -x t-1 , ct i -c t-1 i )</formula><p>User i sends to server (∆y t i , ∆c t i )</p><formula xml:id="formula_9">c t i ← ct i 19</formula><p>Server aggregates:</p><formula xml:id="formula_10">20 (∆x t , ∆c t ) ← 1 lM i∈C t (∆y t i , ∆c t i ) 21 x t ← x t-1 + η g ∆x t , c t ← c t-1 + l∆c t</formula><p>desired level of privacy towards a third party observing the final model (or any model during the training process).<ref type="foot" target="#foot_0">1</ref> We will also report the corresponding (weaker) DP guarantees towards the server.</p></div>
<div><head n="3.3">Description of DP-SCAFFOLD</head><p>We now explain how our algorithm <software>DP-SCAFFOLD</software> is constructed. <software ContextAttributes="used">DP-SCAFFOLD</software> proceeds similarly as standard FL algorithms like <software ContextAttributes="used">FedAvg</software>: all users perform a number of local updates K, before communicating with the central server. We denote T the number of communication rounds. As <software ContextAttributes="used">SCAFFOLD</software>, <software ContextAttributes="used">DP-SCAFFOLD</software> relies on the use of control variates that are updated throughout the iterations of the algorithm: (i) on the server side (c, downloaded by the users) and (ii) on the user side ({c i } i∈[M ] , uploaded to the server).</p><p>At any round t ∈ [T ], a subset C t of users with cardinality lM is uniformly selected by the server, where l is the user sampling ratio. Each user i ∈ C t downloads the global model x t-1 held by the central server and performs K local updates on their local copy y i of the model (with step-size η l ≥ 0), starting from y 0 i = x t-1 . At iteration k ∈ [K], user i ∈ C t samples an independent sR -mini-batch of data S k i ⊂ D i , where s is the data sampling ratio. Given a clipping parameter C &gt; 0, for all j ∈ S k i , the gradient ∇f i (y k-1 i , d i j ) is computed and clipped at threshold C <ref type="bibr" target="#b0">(Abadi et al., 2016)</ref>, giving gij . The resulting average stochastic gradient</p><formula xml:id="formula_11">H k i (y k-1 i</formula><p>) is made private w.r.t. D i using Gaussian noise calibrated to the 2 -sensitivity S = 2C/sR and to the scale σ g (a parameter which will depend on the privacy budget), giving Hk</p><formula xml:id="formula_12">i (y k-1 i ) such that Hk i (y k-1 i ) := H k i (y k-1 i ) + SN (0, σ 2 g ).</formula><p>Finally, we update the model y k-1 i (omitting index t):</p><formula xml:id="formula_13">y k i ← y k-1 i -η l Hk i (y k-1 i ) "noisy" gradient + c t-1 -c t-1 i drift correction ,<label>(1)</label></formula><p>using the control variates which are updated at the end of each inner loop:</p><formula xml:id="formula_14">c t i ← c t-1 i -c t-1 + 1 Kη l (x t-1 -y K i ) = 1 K K k=1 Hk i (y k-1 i ).</formula><p>After K local iterations, each user communicates (y K i -x t-1 ) and (c t i -c t-1 i ) to the central server, and updates the global model with step-size η g , as described in Step 21 of Alg. 1.</p><p>From the privacy point of view, the updates (∆y i , ∆c i ) that are transmitted to the server are private w.r.t. D (proved in Section 4.1), thus making private (x, c) w.r.t D by postprocessing.</p><p>The complete pseudo-code is given in Algorithm 1. Subsampling steps, which amplify privacy <ref type="bibr">(Kasiviswanathan et al., 2011)</ref>, are highlighted in red, and steps specifically related to DP are highlighted in yellow. Setting σ g = 0 and C = ∞ recovers the classical <software ContextAttributes="used">SCAFFOLD</software> algorithm, and removing control variates (i.e., setting c t i to 0 for all t ∈ [T ], i ∈ [M ]) recovers DP-<software ContextAttributes="used">FedAvg</software>, which we describe in Appendix A (Algorithm 2) for completeness.</p><p>Intuition for control variates. In <software>SCAFFOLD</software>, the local control variate c i converges to the local gradient ∇f i (x * ) at the optimal, while c approximates </p><formula xml:id="formula_15">= 1 M M i=1 c 0 i , with c 0 i = 1 K K k=1</formula><p>Hk i (x 0 ) (perturbed by DP-noise), without updating the global model. Note that as we leverage user sampling in the privacy analysis, the server cannot communicate with all users at a single round and the users have to be randomly picked to ensure privacy. We prove the convergence of <software>DP-SCAFFOLD-</software>warm in Section 4.2 (assuming that every user participated to the warm-start phase). Our experiments in Section 5 are conducted with this version of <software ContextAttributes="used">DP-SCAFFOLD</software>.</p><p>User-level privacy. Our framework can easily be adapted to user-level privacy, by setting S = 2C/s.</p></div>
<div><head n="4">THEORETICAL ANALYSIS</head><p>We first provide the analysis of the privacy level in Section 4.1, then analyze utility in Section 4.2.</p></div>
<div><head n="4.1">Privacy</head><p>We first establish that the setting of our algorithms DP-<software>SCAFFOLD</software> and DP-<software ContextAttributes="used">FedAvg</software> enables a fair comparison in terms of privacy.</p><p>Claim 4.1. For a given noise scale σ g &gt; 0, x t has the same level of privacy at any round t ∈ [T ] in DP-<software>SCAFFOLD</software>(-warm) and DP-<software ContextAttributes="used">FedAvg</software> after the server aggregation.</p><p>This claim can be proved by induction, see Appendix B. Consequently, the analysis of privacy is similar for DP-<software ContextAttributes="used">FedAvg</software> or DP-<software ContextAttributes="used">SCAFFOLD</software>. Theorem 4.1 gives the order of magnitude of σ g (same for DP-<software ContextAttributes="used">FedAvg</software> and DP-<software ContextAttributes="used">SCAFFOLD</software>) to ensure DP towards the server or any third party. Similar to previous work (see e.g., <ref type="bibr">Girgis et al., 2021b)</ref>, the results presented below consider the following regime, as it allows to obtain simple closed forms for the privacy guarantees in Theorem 4.1.</p><p>Assumption 1. We consider a noise level σ g , a privacy budget &gt; 0 and a data-subsampling ratio s s.t.: (i) s = o(1), (ii)</p><p>&lt; 1 and (iii) σ g = Ω(s K/ log(2T l/δ)) (high privacy regime).</p><p>Note that our analysis does not require Assumption 1, but the resulting expressions and the dependency on the key parameters are then difficult to interpret. This assumption is actually not used in our experiments, where we compute the privacy loss numerically using the complete formulas from our proof. Under Assumption 1, set σ g = Ω s lT K log(2T l/δ) log(2/δ)/ √ M . Then, for DP-<software>SCAFFOLD</software>(-warm) and DP-<software ContextAttributes="used">FedAvg</software>, x T is (1) O( ), δ -DP towards a third party, (2) O( s ), δ s -DP towards the server, where s = M/l and δ s = δ 2 ( 1 l + 1).</p><p>Sketch of proof. We here summarize the main steps of the proof. Let σ g be a given DP noise level. Our proof stands for the privacy analysis over a query function of sensitivity 1 (since calibration is made with constant S in Section 3.2). We denote GM(σ g ) the corresponding Gaussian mechanism. We first provide the result for any third party.</p><p>We combine the following steps:</p><p>• Data-subsampling with Rényi DP.  <ref type="bibr">(Mironov, 2017)</ref>, we get that for any δ &gt; 0, the whole mechanism is ( a (α, δ ), δ )-DP where</p><formula xml:id="formula_16">a (α, δ ) = O Ks 2 α lM σ 2 g + log(1/δ ) α-1 .</formula><p>• User-subsampling with DP. In order to get explicit bounds (that may not be optimal), we then use classical DP tools to estimate an upper DP bound T after T rounds. By combining amplification by subsampling results <ref type="bibr">(Kasiviswanathan et al., 2011) over users and</ref><ref type="bibr">strong composition (Kairouz et al., 2015)</ref> (with Assumption 1-(ii)) over communication rounds, we finally get that, for any δ &gt; 0, x T is ( T (α, δ , δ ), T lδ +δ )-DP where T (α, δ , δ ) = O(l a (α, δ ) T log(1/δ )).</p><p>• Fixing parameters. Considering our final privacy budget δ for any third party, we fix δ := δ/2T l and δ := δ/2. Following the method of the Moments Accountant <ref type="bibr" target="#b0">(Abadi et al., 2016)</ref>, we minimize the bound on T w.r.t. α &gt; 1, which gives that T = O(˜ ) where</p><formula xml:id="formula_17">˜ = l T log(2/δ) s K log(2T l/δ) σ g √ lM + Ks 2 lM σ 2 g .</formula><p>Finally, under Assumption 1-(iii), the second term is bounded by the first one. We then invert the formula of this upper bound of ˜ to express σ g as a function of a given privacy budget , proving the first statement.</p><p>To prove the second statement, we recall that the server has access to individual contributions before aggregation (which prevents a reduction by a factor lM of the variance) and that it knows the selected users at each round, which cancels the user-sampling effect (factor l). We refer to Appendix B for the full proof as well as the non-asymptotic (tighter) formulas.</p><p>Remarks on privacy accounting. The RDP analysis conducted to handle data subsampling allows to limit the impact of K in the expression of σ g . A standard analysis would require a noise level increased by an extra factor O( log(T Kls/δ)). On the other hand, we tracked the privacy loss over the communication rounds using standard strong composition <ref type="bibr" target="#b11">(Dwork et al., 2010)</ref>, which gives a closed-form expression but is often sub-optimal in practice. In our experiments, we use RDP upper bounds to calibrate σ g more tightly.</p><p>We refer to Appendix B for more details.</p><p>Extension to other frameworks. Instead of the Gaussian mechanism, other randomizers could be applied, possibly to the per-example gradients. The privacy analysis would be then similar to ours as long as a tight RDP bound on the subsampling of this mechanism is provided (see the work of <ref type="bibr" target="#b24">Wang et al., 2020</ref>, for more details). Otherwise, classic DP results for composition and subsampling must be used instead. Besides this, our analysis could be extended to the use of a shuffler between the users and the server to amplify privacy guarantees. For instance, one could use a recent RDP result for shuffled subsampled pure DP mechanisms (Girgis et al., 2021a, Theorem 1).</p></div>
<div><head n="4.2">Utility</head><p>We denote by • the Euclidean 2 -norm. We assume that F is bounded from below by F * = F (x * ), for an x * ∈ R d . Furthermore, we make standard assumptions on the functions</p><formula xml:id="formula_18">(F i ) i∈[M ] . Assumption 2. For all i ∈ [M ], F i is differentiable and ν-smooth (i.e., ∇F i is ν-Lipschitz).</formula><p>We also make the following assumption on the stochastic gradients and data sampling.</p><formula xml:id="formula_19">Assumption 3. For any iteration t ∈ [T ], k ∈ [K], 1. the stochastic gradient ∇f i (y k-1 i , d i j ) is condi- tionally unbiased, i.e., E d i j [∇f i (y k-1 i , d i j )|y k-1 i ] = ∇F i (y k-1 i</formula><p>).</p></div>
<div><head n="2.">the stochastic gradient has bounded variance, i.e., for any y</head><formula xml:id="formula_20">∈ R d , E d i j [ ∇f i (y, d i j ) -∇F i (y) 2 ] ≤ ς 2 . 3. there exists a clipping constant C independent of i, j such that ∇f i (y k-1 i , d i j ) ≤ C.</formula><p>The first condition is naturally satisfied when d i j is uniformly sampled in <ref type="bibr">[R]</ref>. The second condition is classical in the literature, and can be relaxed to only assume that the noise is bounded at the optimal point x * <ref type="bibr">(Gower et al., 2019)</ref>. Remark that consequently, the variance of a mini-batch of size sR uniformly sampled over D i is upper bounded by ς 2 /sR. Finally, the third point ensures that we can safely ignore the impact of gradient clipping.</p><p>Lastly, to obtain a convergence guarantee for DP-<software>FedAvg</software> (but not for DP-<software ContextAttributes="used">SCAFFOLD</software>), we use Assumption 4 on the data-heterogeneity, which bounds gradients ∇f i towards ∇f .</p></div>
<div><head>Assumption 4 (Bounded Gradient dissimilarity).</head><p>There exist constants G ≥ 0 and B ≥ 1 such that:</p><formula xml:id="formula_21">∀x ∈ R d , 1 M M i=1 ||∇F i (x)|| 2 ≤ G 2 + B 2 ||∇F (x)|| 2 .</formula><p>Quantifying the heterogeneity between users by controlling the difference between the local gradients and the global one is classical in federated optimization (e.g. <ref type="bibr">Kairouz et al., 2021)</ref>. We can now state a utility result in the convex case, by considering σ * g := s lT K log(2T l/δ) log(2/δ)/ √ M (order of magnitude of noise scale to approximately ensure endto-end ( , δ)-DP w.r.t. D according to Theorem 4.1). This result is extended to the strongly convex and nonconvex cases in Appendix C. Theorem 4.2 (Utility result -convex case). Assume that for all</p><formula xml:id="formula_22">i ∈ [M ], F i is convex. Let x 0 ∈ R d and denote D 0 := ||x 0 -x * ||.</formula><p>Under Assumptions 2 and 3, we consider the sequence of iterates (x t ) t≥0 of Algorithm 1 (DP-<software>SCAFFOLD</software>) and Algorithm 2 (DP-<software ContextAttributes="used">FedAvg</software>), starting from x 0 , and with DP noise σ g := σ * g . Then there exist step-sizes (η g , η l ) and weights (w t ) t∈[T ] such that the expected excess of loss E[F (x T )] -F * , where x T = T t=1 w t x t , is bounded by:</p><p>• For DP-<software>FedAvg</software>, under Assumption 4:</p><formula xml:id="formula_23">O D0C d log(T l/δ) log(1/δ) M R privacy bound + ςD0 √ sRlM KT + B 2 νD 2 0 T + GD0 √ 1 -l √ lM T + D 4/3 0 ν 1/3 G 2/3 T 2/3</formula><p>optimization bound .</p><p>• For DP-<software>SCAFFOLD</software>-warm:</p><formula xml:id="formula_24">O D0C d log(T l/δ) log(1/δ) M R privacy bound + ςD0 √ sRlM KT + νD 2 0 l 2/3 T optimization bound</formula><p>.</p><p>The two bounds given in Theorem 4.2 consist of three and two terms respectively:</p><p>1. A classical convergence rate resulting from (nonprivate) first order optimization, highlighted in green.</p><p>The dominant part, as T → ∞, is ςD0 √ sRlM KT . This term is inversely proportional to the square root of the total number of iterations T K times the average number of gradients computed per iteration lM × sR, and increases proportionally to the stochastic gradients' standard deviation ς and the initial distance to the optimal point D 0 .</p><p>2. An extra term, in blue, showing that heterogeneity hinders the convergence of DP-<software>FedAvg</software>, for which Assumption 4 is required. Here, as T → ∞, the dominant term in it is GD0 √ 1-l √ lM T , except if the user sampling ratio l = 1, then the dominating term becomes</p><formula xml:id="formula_25">D 4/3 0 ν 1/3 G 2/3 T 2/3</formula><p>. Both these terms do not decrease with the number of local iterations K, and increase with heterogeneity constant G. This extra term for DP-<software>FedAvg</software> highlights the superiority of <software ContextAttributes="used">DP-SCAFFOLD</software> over DP-<software ContextAttributes="used">FedAvg</software> under data heterogeneity. 3. Lastly, an additional term showing the impact of DP appears. This term is diverging with the number of iterations T , which results in the privacy-utility trade-off on T . Moreover, this term decreases proportionally to the whole number of data records M R. It also outlines the cost of DP since it sublinearly grows with the size of the model d and dramatically increases inversely to the DP budget .</p><p>Take-away messages. Our analysis highlights that: (i) <software>DP-</software><software ContextAttributes="used">SCAFFOLD</software> improves on DP-<software ContextAttributes="used">FedAvg</software> in the presence of heterogeneity; and (ii) increasing the number of local updates K is very profitable to <software ContextAttributes="used">DP-</software><software ContextAttributes="used">SCAFFOLD</software>, as it improves the dominating optimization bound without degrading the privacy bound. These aspects are numerically confirmed in Section 5.</p></div>
<div><head>Sketch of proof and originality.</head><p>To establish Theorem 4.2, we adapt the proof of Theorems V and VII in <ref type="bibr">Karimireddy et al. (2020b)</ref>. However, we consider a weakened assumption on stochastic gradients due the addition of Gaussian noise in the local updates. Consequently, in order to limit the impact of this additional noise, we change the quantity (Lyapunov function) that is controlled during the proof: we combine the squared distance to the optimal point x t -x * 2 to a control of the lag at iteration t,</p><formula xml:id="formula_26">1 KM K k=1 M i=1 E α t i,k-1 -x t 2</formula><p>; where we ensure that our control variates (c t i ) i∈[M ] at iteration t correspond to noisy stochastic gradients measured at points</p><formula xml:id="formula_27">(α t i,k-1 ) i∈[M ] , that is, c t i = 1 K K k=1 Hk i α t i,k-1 . 3 This proof is detailed in Appendix C.</formula><p>Relying directly on the result in <ref type="bibr">Karimireddy et al. (2020b)</ref> would require to devote a large fraction (e.g., half) of the privacy budget to the initialization phase to obtain a reasonable bound. Such a strategy did not perform well in experiments.</p><p>On the warm-start strategy. To obtain the utility result, we have to ensure that initial users' controls c 0 i are set as follows:</p><formula xml:id="formula_28">c 0 i = 1 K K k=1</formula><p>Hk i (x 0 ) (notations of Alg. 1). Our theoretical result thus only holds for the DP-<software>SCAFFOLD</software>-warm version. However, we observed in our experiments that <software ContextAttributes="used">DP-SCAFFOLD</software> (which uses initial user control variates equal to 0) led to the same results as DP-<software ContextAttributes="used">SCAFFOLD</software>-warm.</p><p>Extension to other local randomizers. Our utility analysis would easily extend to any unbiased mechanism with explicit variance (see Appendix C).</p></div>
<div><head n="5">EXPERIMENTS</head><p>Experimental setup. In our experiments, 4 we perform federated classification with two models: (i) logistic regression (LogReg) for synthetic and real-world data, and (ii) a deep neural network with one hidden layer (DNN ) (see Appendix D.1 for the precise architecture) on real-world data. We fix the global step-size η g = 1, local step-size η l = η 0 /sK where η 0 is carefully tuned (see Appendix D.1), and use a 2 -regularization parameter set to 5.10 -3 . Regarding privacy, we fix δ = 1/M R in all experiments. Then, for each setting, once the parameters related to sampling and number of iterations are fixed, we calculate the corresponding privacy bound by using non-asymptotic upper bounds from RDP theory (see Section 4.1). Details on the clipping heuristic are given in Appendix D.1. We report average results over 3 random runs.</p><p>Datasets. For synthetic data, we follow the data generation setup of <ref type="bibr">Li et al. (2020a)</ref>, which enables to control heterogeneity between users' local models and between users' data distributions, respectively with parameters α and β (the higher, the more heterogeneous). Note that the setting (α, β) = (0, 0) still creates heterogeneity and does not lead to i.i.d. data across users. Our data is generated from a logistic regression design with 10 classes, with input dimension d = 40. We consider M = 100 users, each holding 3 In contrast, the proof in the convex case in Karimireddy et al.</p><formula xml:id="formula_29">(2020b) controls 1 M M i=1 E c t i -∇fi(x * ) 2 . 4 Code available on Github.</formula><p>R = 5000 samples. We compare three levels of heterogeneity: (α, β) ∈ {(0, 0), (1, 1), (5, 5)}. Details on data generation are given in Appendix D.2.</p><p>We also conduct experiments on the EMNIST-'balanced' dataset <ref type="bibr" target="#b6">(Cohen et al., 2017)</ref>, which consists of 47 balanced classes (letters and digits) containing all together 131, 600 samples. The dataset is divided into M = 40 users, who each have R = 2500 data records. Heterogeneity is controlled by parameter γ. For γ% similar data, we allocate to each user γ% i.i.d. data and the remaining (100 -γ)% by sorting according to the label <ref type="bibr">(Hsu et al., 2019)</ref>, which corresponds to '<software ContextAttributes="used">FEMNIST</software>' (Federated EMNIST ). For experiments involving DNN, we rather use the seminal MNIST dataset, which features 60, 000 samples labeled by one of the 10 balanced classes. All of the samples are allocated between M = 60 users (thus R = 1000). For both datasets, we consider heterogeneity levels γ ∈ {0%, 10%, 100%}.</p><p>We split each dataset in train/test sets with proportion 80%/20%. Features are first standardized, then each data point is normalized to have unit L2 norm.</p><p>Superiority of <software ContextAttributes="used">DP-SCAFFOLD</software>. We first study the performance of different algorithms under varying levels of data heterogeneity and number of local updates. We set subsampling parameters to l = 0.2 and s = 0.2 for all of the datasets and fix the noise level σ g = 60 for synthetic data, and σ g = 30 for real-world data. We compare 6 algorithms: <software ContextAttributes="used">FedAvg</software>, <software ContextAttributes="used">FedSGD</software> (<software ContextAttributes="used">FedAvg</software> with Ks = 1), <software ContextAttributes="used">SCAFFOLD</software>(-warm), with and without DP. The results for LogReg (convex objective) with T = 400 are shown in Figure <ref type="figure">1</ref> for synthetic data and Figure <ref type="figure">2</ref> (top row) for <software ContextAttributes="used">FEMNIST</software>. Figure <ref type="figure">2</ref> (bottom row) shows results for DNN (non-convex objective) with T = 100 on MNIST data. We report in the figure caption the corresponding privacy bound for the last iterate with respect to a third party.</p><p>In both convex and non-convex settings, DP-<software ContextAttributes="used">SCAFFOLD</software> clearly outperforms DP-<software ContextAttributes="used">FedAvg</software> and DP-<software ContextAttributes="used">FedSGD</software> under data heterogeneity. The performance gap also increases with the number K of local updates, see Figure <ref type="figure">1</ref>. These results confirm our theoretical results: they show that the control variates of DP-<software ContextAttributes="used">SCAFFOLD</software> are robust to noise, and allow to overcome the limitations of DP-<software ContextAttributes="used">FedAvg</software> under high heterogeneity and many local updates.</p><p>Trade-offs between parameters. In <software ContextAttributes="used">DP-SCAFFOLD</software>, a fixed guarantee can be achieved by different combinations of values for K, T and σ g , as shown in Theorem 4.2). We propose to empirically observe these trade-offs on synthetic data under a high privacy regime ( = 3). The sampling parameters are fixed to l = 0.05, s = 0.2. Given σ g and K, we calculate the maximal value of T such that the privacy bound is still maintained after T communication rounds. Table <ref type="table">1</ref> shows the test accuracy obtained after these iterations for a high heterogeneity setting (α, β) = (5, 5).</p><p>Our results highlight the trade-off between T and K (which relates to hardware and communication constraints in real deployments) to achieve some given performance. Indeed, if K is too large, T has to be chosen very low to ensure the desired privacy, leading to poor accuracy. For instance, with K = 40, T cannot exceed 90, and the resulting accuracy thus barely reaches 22%, even with low private noise. On the other hand, if we set K too low, <software>DP-SCAFFOLD</software> does not converge despite a high value of T , since it does not take advantage of the local updates. Moreover, we can observe another dimension of the trade-off involving σ g . It seems that better performance can be achieved by setting σ g relatively low, although it implies to choose a smaller T . This trade-off is evidenced by the fact that the accuracy achieved in the first two rows (σ g = 10 and σ g = 20) is quite similar, showing that σ g and T compensate each other.</p><p>Other results. Appendix D.3 shows results with other metrics and heterogeneity levels, higher privacy regimes, and presents additional experiments on the effect of sampling parameters l and s (and the tradeoff with T ) on privacy and convergence.</p></div>
<div><head n="6">CONCLUSION</head><p>Our paper introduced a novel FL algorithm, <software ContextAttributes="used">DP-SCAFFOLD</software>, to tackle data heterogeneity under DP constraints, and showed that it improves over the baseline DP-<software ContextAttributes="used">FedAvg</software> from both the theoretical and empirical point of view. In particular, our theoretical analysis highlights an interesting trade-off between the parameters of the problem, involving a term of heterogeneity in DP-<software ContextAttributes="used">FedAvg</software> which does not appear in the rate of <software ContextAttributes="used">DP-SCAFFOLD</software>. As future work, we aim at providing additional experiments with deep learning models and various sizes of local datasets across users, for more realistic use-cases. Besides, our paper opens other perspectives. <software ContextAttributes="used">DP-</software><software ContextAttributes="used">SCAFFOLD</software> may be improved by incorporating other ML techniques such as momentum. On the experimental side, a larger number of samples and a more precise tuning of the trade-off between T , K and subsampling parameters may dramatically improve the utility for real-world cases under a given privacy budget. From a theoretical perspective, investigating an adaptation of our approach to a personalized FL setting <ref type="bibr" target="#b13">(Fallah et al., 2020;</ref><ref type="bibr">Sattler et al., 2020;</ref><ref type="bibr">Marfoq et al., 2021)</ref>, where formal privacy guarantees have seldom been studied (at the exception of <ref type="bibr" target="#b4">Bellet et al., 2018;</ref><ref type="bibr">Hu et al., 2020)</ref>, is a direction of interest.</p><p>Table <ref type="table">1</ref>: Test Accuracy (%) For DP-<software ContextAttributes="used">SCAFFOLD</software> on Synthetic Data, With = 3, l = 0.05, s = 0.2, (α, β) = (5, 5). </p><formula xml:id="formula_30">σ g K = 1 K = 5 K = 10 K = 20 K =</formula></div>
<div><head>ORGANIZATION OF THE APPENDIX</head><p>This appendix is organized as follows. Appendix A summarizes the main notations and provides the detailed DP-<software>FedAvg</software> algorithm for completeness. Appendix B provides details on our privacy analysis. Appendix C gives the full proofs of our utility results for the convex, strongly convex and non-convex cases. Finally, Appendix D provides more details on the experiments of Section 5, as well as additional results.  Compute gradient:</p></div>
<div><head>A ADDITIONAL INFORMATION</head></div>
<div><head>A.1 Table of Notations</head><formula xml:id="formula_31">1 R R j=1 f i (•, d i j )) F global objective function ( 1 M M i=1 F i ) x t ∈</formula><formula xml:id="formula_32">g ij ← ∇f i (y k-1 i , d i j ) Clip gradient: gij ← g ij / max 1, ||g ij || 2 /C</formula><p>Add DP noise to local gradients: x t ← x t-1 + η g ∆x t</p><formula xml:id="formula_33">Hk i ← 1 sR j∈S k i gij + 2C sR N (0, σ 2 g ) y k i ← y k-1 i -η l Hk i ∆y t i ← y K i -x t-</formula></div>
<div><head>B DETAILS ON PRIVACY ANALYSIS</head><p>In this section, we provide the proof of our privacy results. We start by recalling standard differential privacy results on composition and amplification by subsampling in Section B.1. Section B.2 reviews recent results in Rényi Differential Privacy (RDP) which allow to obtain tighter privacy bounds. We then formally state and prove Claim 4.1 in Section B.3. Finally, we provide the proof of our main result (Theorem 4.1) in Section B.4.</p></div>
<div><head>B.1 Reminders on Differential Privacy</head><p>In the following, we denote by D ∈ X n to a dataset of size n. Two datasets D, D ∈ X n are said to be neighboring (denoted by ||D -D || ≤ 1) if they differ by at most one element.</p><p>Composition. Let M 1 (•; A 1 ), ..., M T (•; A T ) be a sequence of T adaptive DP mechanisms where A t stands for the auxiliary input to the t-th mechanism, which may depend on the outputs of previous mechanisms (M t ) t &lt;t . The ability to choose the sequences of mechanisms adaptively is crucial for the design of iterative machine learning Remark. When stating theoretical results, is typically approximated by O( T log(1/δ )) when &lt;&lt; 1.</p><p>Privacy amplification by subsampling. A key result in DP is that applying a private algorithm on a random subsample of the dataset amplifies privacy guarantees <ref type="bibr">(Kasiviswanathan et al., 2011)</ref>. In this work, we are interested in subsampling without replacement. Definition B.1 (Subsampling without replacement). The subsampling procedure Samp n,m : X n → X m (where m ∈ N, with m ≤ n) takes D as input and chooses uniformly among its elements a subset D of m elements. We may also denote Samp n,m as Samp q where q = m/n in the rest of the paper. = log(1 + q(e -1)), δ = qδ, q = m/n.</p><p>Remark. In theoretical results, is often approximated by O(q ) when &lt;&lt; 1.</p></div>
<div><head>B.2 Rényi Differential Privacy</head><p>Abadi et al. ( <ref type="formula">2016</ref>) demonstrated in practice that the privacy bounds provided by standard ( , δ)-DP theory (see Section B.1) often overestimate the actual privacy loss. In order to better express inequalities on the tails of the output distributions of private algorithms, we introduce the privacy loss random variable <ref type="bibr" target="#b10">(Dwork and Roth, 2014;</ref><ref type="bibr" target="#b0">Abadi et al., 2016;</ref><ref type="bibr" target="#b24">Wang et al., 2020)</ref>. Given a random mechanism M , let M (D) and M (D ) be the distributions of the output when M is run on D and D respectively. The privacy loss L M D,D is defined as:</p><formula xml:id="formula_34">L M D,D (θ) := log M (D)(θ) M (D )(θ)</formula><p>where θ ∼ M (D).</p><p>(2)</p><p>The interpretation of this quantity is easy to understand: ( , δ)-DP ensures that the absolute value of the privacy loss is bounded by with probability at least (1 -δ) for all pairs of neighboring datasets D and D (Dwork and Roth, 2014, Lemma 3.17).</p><p>We will reason on the Cumulant Generating Function (CGF) of the privacy loss, denoted K M , rather than on the privacy loss L M itself. This CGF is expressed as follows for any λ &gt; 0:</p><formula xml:id="formula_35">K M (D, D , λ) := E θ∼M (D) e λL M D,D (θ) = E θ∼M (D) M (D)(θ) M (D )(θ) λ ,</formula><p>which is also equivalent to:</p><formula xml:id="formula_36">K M (D, D , λ) = E θ∼M (D ) M (D)(θ) M (D )(θ) λ+1 .<label>(3)</label></formula><p>By the property of the moment generating function, K M (D, D , •) fully determines the distribution of the privacy loss random variable L M D,D . We also define K M (λ) := sup ||D-D ||≤1 K M (D, D , λ), which is the upper bound on the CGF for any pair of neighboring datasets.</p><p>We can now introduce Rényi Differential Privacy (RDP), which generalizes DP using the Rényi divergence D α . Definition B.2 (Rényi Differential Privacy, Mironov, 2017). For any α ∈ (1, ∞) and any &gt; 0, a mechanism M : X n → Y is said to be (α, )-RDP, if for all neighboring datasets D and D ,</p><formula xml:id="formula_37">D α (M (D)||M (D )) := 1 α -1 log E θ∼M (D ) M (D)(θ) M (D )(θ) α ≤ . (<label>4</label></formula><formula xml:id="formula_38">)</formula><p>Given a mechanism M and a RDP parameter α, we can thus determine from Definition B.2 the lowest value of the -RDP bound, denoted M (α), such that M is (α, M (α))-RDP. Indeed, M (α) is such that:</p><formula xml:id="formula_39">M (α) = inf ∈ε(M )</formula><p>where ε(M ) := { &gt; 0 : sup</p><formula xml:id="formula_40">||D-D ||≤1 D α (M (D)||M (D )) ≤ }.</formula><p>The obvious similarity between Eq. (3) and Eq. ( <ref type="formula" target="#formula_37">4</ref>) shows the link between the CGF and the notion of RDP. Indeed, for any α ∈ (1, ∞), it is easy to see that (α -1) M (α) is equal to K M (λ) where λ + 1 = α (restated in Lemma B.3).</p><formula xml:id="formula_41">Lemma B.3 (Equivalence RDP-CGF). Any mechanism M is (λ + 1, K M (λ)/λ)-RDP for all λ &gt; 0.</formula><p>We now recall how we can convert RDP guarantees into standard DP guarantees.</p><p>Lemma B.4 (RDP to DP conversion, Mironov, 2017). If M is ( , α)-RDP, then M is ( +log(1/δ)/(α-1), δ)-DP for any 0 &lt; δ &lt; 1.</p><p>Given Lemma B.4 and Lemma B.3, it is possible to find the smallest from some fixed parameter δ or the smallest δ from some fixed parameter so as to achieve ( , δ)-DP:</p><formula xml:id="formula_42">(δ) = min λ&gt;0 log(1/δ) + K M (λ) λ ,<label>(5) δ</label></formula><formula xml:id="formula_43">( ) = min λ&gt;0 e K M (λ)-λ .<label>(6)</label></formula><p>Moreover, λ → K M (λ)/λ is monotonous (Van Erven and Harremos, 2014, Theorem 3) and λ → K M (λ) is convex <ref type="bibr">(Van Erven and Harremos, 2014, Theorem 11)</ref>. This last property enables to bound K M by a linear interpolation between the values of K M evaluated at integers, as stated below:</p><formula xml:id="formula_44">∀λ &gt; 0, K M (λ) ≤ (1 -λ + λ )K M ( λ ) + (λ -λ )K M ( λ ).<label>(7)</label></formula><p>Therefore, Problem ( <ref type="formula" target="#formula_42">5</ref>) is quasi-convex and Problem ( <ref type="formula" target="#formula_43">6</ref>) is log-convex, and both can be solved if we know the expression of K M (λ) for any λ &gt; 0.</p><p>We provide below other useful results from RDP theory, which we will use in our privacy analysis.</p><p>Lemma B.5 (RDP Composition, Mironov, 2017). Let α ∈ (1, ∞). Let M 1 and M 2 be two mechanisms such that M 1 is (α, 1 )-RDP and M 2 , which takes the output of M 1 as auxiliary input, is (α, 2 )-RDP. Then the composed mechanism</p><formula xml:id="formula_45">M 2 • M 1 is (α, 1 + 2 )-RDP. Lemma B.6 (RDP Gaussian mechanism, Mironov, 2017). If f : X n → R d has 2 -sensitivity 1, then the Gaussian mechanism G f (•) := f (•) + N (0, σ 2 g I d ) is (α, α/2σ 2 g )-RDP</formula><p>for any α &gt; 1. Lemma B.7 (RDP for subsampled Gaussian mechanism, <ref type="bibr" target="#b24">Wang et al., 2020)</ref>. Let α ∈ N with α ≥ 2 and 0 &lt; q &lt; 1 be a subsampling ratio. Suppose f :</p><formula xml:id="formula_46">X n → R d has 2 -sensitivity equal to 1. Let G f (•) := G f • Samp q (•) be a subsampled Gaussian mechanism. Then G f is (α, (α, σ 2 g ))-RDP where (α, σ 2 g ) ≤ 1 α -1 log 1 + 2q 2 α 2 min{2(e 1/σ 2 g -1), e 1/σ 2 g } + α j=3</formula><p>2q j α j e j(j-1)/2σ 2 g .</p></div>
<div><head>Remark.</head><p>By considering q = o(1), the dominant term in the upper bound of (α, σ 2 g ) comes from the term of the sum of the order of q 2 . In particular, when σ 2 g is large (i.e. high privacy regime), the term min{2(e 1/σ 2 g -1), e 1/σ 2 g } simplifies to 2(e 1/σ 2 g -1) ≤ 4/σ 2 g . This thus simplifies the whole upper bound to O(αq 2 /σ 2 g ).</p></div>
<div><head>B.3 Proof of Claim 4.1</head><p>We restate below a more formal version of Claim 4.1 along with its proof. For any t ∈ [T ], we define subversions of algorithms DP-<software>SCAFFOLD</software> (Alg. 1) and DP-<software ContextAttributes="used">FedAvg</software> (Alg. 2), which stop at round t and reveal an output, either to the server or to a third party:</p><p>• To the server. We assume that the sampling of users C t is known by the server. Formally, we define A t DP-<software>SCAFFOLD</software> , which outputs (reveals) {y t i , c t i } i∈C t , and A t DP-<software ContextAttributes="used">FedAvg</software> , which outputs {y t i } i∈C t (those quantities being private w.r.t. {D i } i∈C t ).</p><p>• To a third party. We define Ãt DP-<software>SCAFFOLD</software> , which outputs (x t , c t ) and Ãt DP-<software ContextAttributes="used">FedAvg</software> , which outputs x t (those quantities being private w.r.t. D).</p><p>In both privacy models, DP-<software>SCAFFOLD</software> and DP-<software ContextAttributes="used">FedAvg</software> can be seen as T adaptive compositions of these subalgorithms.</p><p>Claim B.1 (Formal version of Claim 4.1). For any t ∈ [T ], the following holds:</p><p>• A t DP-<software>SCAFFOLD</software> and A t DP-<software ContextAttributes="used">FedAvg</software> have the same level of privacy (towards the server),</p><p>• Ãt DP-<software>SCAFFOLD</software> and Ãt DP-<software ContextAttributes="used">FedAvg</software> have the same level of privacy (towards a third party).</p><p>Proof. We prove the claim by reasoning by induction on the number of communication rounds t. We only give the proof for the first statement (including the DP-<software>SCAFFOLD</software>-warm version). The second one can be proved in a similar manner.</p><p>First, consider t = 1. For any i ∈ C t , control variates c 0 i are either all set to 0 (DP-<software>SCAFFOLD</software>), or c 0 i are at least as private as y 1 i (DP-<software ContextAttributes="used">SCAFFOLD</software>-warm). The level of privacy for A 1 DP-<software ContextAttributes="used">SCAFFOLD</software> is thus fully determined by the level of privacy of {y 1 i } i∈C t , which is the same as A 1 DP-<software ContextAttributes="used">FedAvg</software> . Therefore the claim is true for t = 1. Then, let t ∈ [T ] and suppose that the claim is verified for all t &lt; t. Let i ∈ C t and first consider A t DP-<software ContextAttributes="used">SCAFFOLD</software> . The update of the i-th user model (see Eq. 1) at round t shows that an additional information leakage may come from the correction (c t-1 -c t-1 i ), or more precisely from c t-1 i since c t-1 is known by the server. By assumption of induction, c t-1 i is also known by the server. Therefore, using the post-processing property of DP, the y t i as updated in DP-<software ContextAttributes="used">SCAFFOLD</software> is as private w.r.t. D i as the y t i as updated in DP-<software ContextAttributes="used">FedAvg</software>. Besides this, the update of the i-th control variate fully depends on the local updates of y t i through the average of the DP-noised stochastic gradients calculated over the local iterations. Therefore, considering all the contributions from C t , A t DP-<software ContextAttributes="used">FedAvg</software> and A t DP-<software ContextAttributes="used">SCAFFOLD</software> have the same level of privacy.</p></div>
<div><head>B.4 Proof of Theorem 4.1</head><p>Preliminaries. Lemma B.7 only gives an upper bound of the RDP privacy for a subsampled Gaussian mechanism when α ∈ N with α ≥ 2. However we will need to optimize our privacy bound w.r.t. α ∈ R with α &gt; 1. We thus use Lemma B.3 and the convexity of the CGF (see Eq. 7) to generalize this upper bound to the following result.</p><p>Let α ∈ R with α &gt; 1. Under the same assumptions as in Lemma B.7,</p><formula xml:id="formula_47">G f is (α, (α, σ 2 g ))-RDP with (α, σ 2 g ) ≤ (1 -α + α ) α -1 α -1 ( α , σ 2 g ) + (α -α ) α -1 α -1 ( α , σ 2 g ),<label>(8)</label></formula><p>where (•, σ 2 g ) admits the upper bound given in Lemma B.7.</p><p>Details of the proof. Our privacy analysis assumes that the query function has sensitivity 1, since the calibration of the Gaussian noise is locally adjusted in our algorithms with the constant S = 2C/sR (see Section 3.2). We simply denote by G the Gaussian mechanism with variance σ 2 g , which is (α, α/2σ 2 g )-RDP (Lemma B.6). Below, we first prove privacy guarantees towards a third party observing only the final result, and then deduce the guarantees towards the honest-but-curious server.</p><p>Step 1: data subsampling. Let t ∈ [T ] be an arbitrary round. We first provide an upper bound a for the privacy loss after the aggregation by the server of the lM individual contributions (line 20 in Alg. 1), thanks to the local addition of noise.</p><p>Let i ∈ C t , α &gt; 1. We denote by i (α) the α-RDP budget (w.r.t. D i ) used to "hide" the individual contribution of the i-th user from the server. This contribution is the result of the composition of K adaptative s-subsampled mechanisms G:</p><p>• We first obtain an upper RDP bound for the s-subsampled mechanism with Lemma B.7. Suppose first α ∈ N and α ≥ 2, which is the case covered by Lemma B.7. Under Assumption 1-(i) and Assumption 1-(iii), the resulting mechanism is (α, O(s 2 α/σ 2 g ))-RDP. To extend this result to α &gt; 1, we use the result provided in (8): by factoring by s 2 /σ 2 g in the upper bound of (α, σ 2 g ), and bounding the rest of the inequality (a convex combination between α ( α -1)/(α -1) and α ( α -1)/(α -1)) by α + 1, we also obtain that this mechanism is (α, O(s 2 (α + 1)/σ 2 g ))-RDP.</p><p>• We then use the result of Lemma B.5 for the RDP composition rule over the K local iterations, which gives that i (α) ≤ O(Ks 2 (α + 1)/σ 2 g ).</p><p>We now consider the aggregation step. Taking into account all the contributions of the users from C t , we get a Gaussian noise of variance S 2 σ 2 a where σ 2 a = 1 lM σ 2 g . Note that the sensitivity of the aggregation (w.r.t. the joint dataset D) is lM times smaller than when considering an individual contribution. Therefore, with the previous approximation, the aggregated contributions satisfy (α, O(Ks 2 (α + 1)/lM σ 2 g ))-RDP w.r.t. D. After converting this result into a DP bound (Lemma B.4), we get that for any 0 &lt; δ &lt; 1, the aggregation at line 20 in Alg. 1 is ( a (α, δ ), δ )-DP w.r.t. D where a (α, δ ) = O Ks 2 (α+1)</p><formula xml:id="formula_48">lM σ 2 g + log(1/δ ) α-1 .</formula><p>Without approximation: we would obtain at this step an exact upper bound a (α, δ ) = K (α, lM σ 2 g ) + log(1/δ ) α-1 .</p><p>Step 2: user subsampling. In order to get explicit bounds, we then use classical DP tools to estimate an upper DP bound after T rounds taking into account the amplification by subsampling from the set of users.</p><p>Remark that these tools are however sub-optimal for practical implementations <ref type="bibr">(Abadi et al., 2016, Section 5.1.)</ref>.</p><p>• Using Lemma B.2, the subsampling of users enables a gain of privacy of the order of l, which gives O(l a (α, δ )), lδ -DP.</p><p>• Using Lemma B.1, we compose this mechanism over T iterations, which under Assumption 1-(ii) gives for any δ &gt; 0, O( T log(1/δ )l a (α, δ )), T lδ + δ -DP.</p><p>Without approximation: the mechanism is * (α, δ ) 2T log(1/δ ) + T * (α, δ )(e * (α,δ ) -1), T lδ + δ where * (α, δ ) = log(1 + l(e a (α,δ ) -1)).</p><p>Step 3: setting parameters. We denote</p><formula xml:id="formula_49">T (α, δ , δ ) = l T log(1/δ ) Ks 2 (α+1) lM σ 2 g + log(1/δ ) α-1</formula><p>. Given what is stated above, the final output of the algorithm is (O( T ), T lδ + δ )-DP.</p><p>Considering our final privacy budget δ, we arbitrarily fix δ := δ/2T l and δ := δ/2. We now aim to find an expression of σ g such that the privacy bound is minimized. By considering the approximated bound, this gives the following minimization problem:</p><formula xml:id="formula_50">min α&gt;1 T (α) := l T log(2/δ) Ks 2 (α + 1) lM σ 2 g + log(2T l/δ) α -1 .</formula><p>Using DP rather than RDP enables to solve this minimization problem pretty easily since only the second factor in T (α) depends on α, that is:</p><formula xml:id="formula_51">min α&gt;1 ˜ T (α) := Ks 2 (α + 1) lM σ 2 g + log(2T l/δ) α -1 .</formula><p>By omitting constants, we obtain the expression for the minimum value of T (α):</p><formula xml:id="formula_52">˜ = l T log(2/δ) s K log(2T l/δ) σ g √ lM + Ks 2 lM σ 2 g .</formula><p>Under Assumption 1-(iii), we can bound the second term by the first one, which gives:</p><formula xml:id="formula_53">˜ = O s lT K log(2/δ) log(2T l/δ) σ g √ M .</formula><p>We then invert the formula of this upper bound of ˜ to express σ g as a function of a given privacy budget :</p><formula xml:id="formula_54">σ g = Ω s lT K log(2T l/δ) log(2/δ)/ √ M ,</formula><p>which proves that the algorithm is (O( ), δ)-DP towards a third party observing its final output.</p><p>Without approximation: the minimization problem is much more complex and has to be solved numerically min</p><formula xml:id="formula_55">α&gt;1,δ &gt;0,δ &gt;0 * (α, δ ) 2T log(1/δ ) + T * (α, δ )(e * (α,δ ) -1) s.t. δ = T lδ + δ , or: min α&gt;1,x∈(0,1) * (α, xδ/T l) 2T log(1/(1 -x)δ) + T * (α, xδ/T l)(e * (α,xδ/T l) -1).</formula><p>Extension to privacy towards the server. The crucial difference with the third party case is that the server observes individual contributions and knows which users are subsampled at each step. Removing the privacy amplification effect of the l-subsampling of users and the aggregation step, the minimization problem becomes</p><formula xml:id="formula_56">min α&gt;1 T (α) := T log(2/δ) Ks 2 α σ 2 g + log(2T /δ) α -1 ,</formula><p>where the minimizing value can be approximated by:</p><formula xml:id="formula_57">˜ = T log(2/δ) s K log(2T /δ) σ g + Ks 2 σ 2 g .</formula><p>Under Assumption 1-(iii), we can bound the second term by the first one:</p><formula xml:id="formula_58">˜ = O s T K log(2/δ) log(2T /δ) σ g ,</formula><p>which proves that we obtain (O( s ), δ s )-DP towards the server where s = M l and δ s = δ 2 ( 1 l + 1).</p><p>Finer results for amplification by subsampling. To establish privacy towards a third party, it is actually possible to combine the subsampling ratios (user and data) to determine a bound upon the subsampling of data directly from D and thus to quantify a more precise gain in privacy <ref type="bibr">(Girgis et al., 2021b)</ref>. The difficulty in this setup is that this combined subsampling is not uniform overall, which requires extending the proof of Lemma B.2 as done by <ref type="bibr">Girgis et al. (2021b)</ref> in the case of classical differential privacy.</p><p>Implementation. In practice, we determine a RDP upper bound at Step 2, by using the theorem proved by <ref type="bibr" target="#b24">Wang et al. (2020)</ref> (which is not restricted to Gaussian mechanisms) with the exact RDP bound obtained at Step 1 and sampling parameter l. This result being accurate only for α ∈ N \ {0, 1}, we obtain an natural extension of the bound for any α &gt; 1 with Eq (7). Then, we invoke Lemma B.5 to obtain the final RDP bound after T communication rounds, for any α &gt; 1. Under fixed privacy parameter δ (chosen as 1/M R in our experiments), we finally obtain the minimal value for w.r.t. α ∈ (1, ∞), which is determined by Eq (5). The last step is done by using a fine grid search over parameter α.</p></div>
<div><head>C PROOF OF UTILITY</head><p>In this section, we provide the proof of our utility results. We first establish in Section C.1 some preliminary results about the impact of DP noise over stochastic gradients. In Section C.2, we provide the complete version of our utility result for DP-<software>SCAFFOLD</software>-warm (Theorem C.1), from which Theorem 4.2 is an immediate corollary. We prove this theorem for convex local loss functions in Section C.3 and non-convex loss functions in Section C.4. We finally state in Section C.5 our complete result for DP-<software ContextAttributes="used">FedAvg</software> (Theorem C.2).</p><p>For any C, σ g &gt; 0, we define Σ g (C) := 2C √ 2dσ g /sR. We recall that we assume that F is bounded from below by F * = F (x * ), for an x * ∈ R d .</p></div>
<div><head>C.1 Preliminaries</head><p>Properties of DP-noised stochastic gradients. Let i ∈ [M ], x ∈ R d , S i ⊂ D i and C, σ g &gt; 0. Suppose Assumptions 2 and 3.3 are verified (the last assumption ensures that the clipping on per-example local gradients with threshold C is not effective).</p><p>We recall below the expression of Hi (x) from Section 3.3, which is the noised version of the local gradient H i (x) of the i-th user over S i evaluated at x (omitting index k):</p><formula xml:id="formula_59">Hi (x) := H i (x) + 2C sR N (0, σ 2 g ), where H i (x) := 1 sR d i j ∈Si ∇f i (x, d i j ).</formula><p>We recall that the 2 -sensitivity of H i (x) w.r.t. S i is upper bounded by 2C/sR, which explains the scaling of the Gaussian noise in the expression of Hi (x). Since the variance of N (0, I d ) is 2d, the following statement holds directly:</p><formula xml:id="formula_60">E Hi (x) = H i (x) and E || Hi (x) -H i (x)|| 2 ≤ 8C 2 dσ 2 g s 2 R 2 = Σ g (C) 2 .</formula><p>By combining our utility assumptions with the result stated above, we can deduce the following lemma.</p><p>Lemma C.1 (Regularity of DP-noised stochastic gradients). Under Assumptions 2 and 3, for any iteration</p><formula xml:id="formula_61">t ∈ [T ], k ∈ [K],</formula><p>1.</p><formula xml:id="formula_62">E Hk i (y k-1 i )|y k-1 i = ∇F i (y k-1 i ), 2. E || Hk i (y k-1 i ) -∇F i (y k-1 i )|| 2 |y k-1 i ≤ ς 2 sR + Σ 2 g (C).</formula><p>The proof of Lemma C.1 is easily obtained by conditioning on the two sources of randomness (i.e., mini-batch sampling and Gaussian noise) which are independent, thus the variance is additive. This result can be seen as a degraded version of Assumption 3 due to the local injection DP noise, a fact that we will strongly leverage to derive convergence rates.</p><p>We now enumerate several statements that will be used in the utility proof. First, Lemma C.2 enables to control ||∇F || 2 using the assumption of smoothness over the local loss functions. Second, Lemma C.3 provides separation inequalities of mean and variance (Karimireddy et al., 2020b, Lemma 4), which enables to state a result on quantities of interest in Corollary C.1.</p><p>Lemma C.2 (Nesterov inequality). Suppose Assumption 2 is verified and assume that for all i ∈ <ref type="bibr">et al., 2004, Theorem 2.1.5)</ref> Lemma C.3 (Separating mean and variance). Let (A 1 , ..., A n ) be n random variables in R d not necessarily independent.</p><formula xml:id="formula_63">[M ], F i is convex. Then, ∀x ∈ R d , ||∇F (x)|| 2 ≤ 2ν(F (x) -F * ). Proof. Let x ∈ R d . ||∇F (x)|| 2 = ||∇F (x) -∇F (x * )|| 2 = || 1 M M i=1 ∇F i (x) -∇F i (x * )|| 2 ≤ 1 M M i=1 ||∇F i (x) -∇F i (x * )|| 2 (Jensen inequality) ≤ 2ν(F (x) -F * ) (Nesterov</formula><p>1. Suppose that their mean is E[A i ] = a i and their variance is uniformly bounded, i.e. for all i ∈</p><formula xml:id="formula_64">[n], E[||A i -a i || 2 ] ≤ σ 2 A . Then, E n i=1 A i 2 ≤ n i=1 a i 2 + n 2 σ 2 A .</formula><p>2. Suppose that their conditional mean is E[A i |A i-1 , ...A 1 ] = a i and their variance is uniformly bounded, i.e. for all i ∈</p><formula xml:id="formula_65">[n], E[||A i -a i || 2 ] ≤ σ 2 A . Then, E n i=1 A i 2 ≤ 2 n i=1 a i 2 + 2nσ 2 A . Corollary C.1. Let t ∈ [T ].</formula><p>In the following statements, the expectation is taken w.r.t. the randomness from their local data sampling and from the Gaussian DP noise, conditionally to the users' sampling C t and initial value of variables y i , that is y 0 = x t-1 (same for all users). We have:</p><formula xml:id="formula_66">• E   1 KlM i∈C t k∈[K] ( Hk i (y k-1 i ) -∇F i (y k-1 i )) 2 C t , y 0   ≤ Σ 2 g (C) + ς 2 /sR KlM , • E 1 lM i∈C t c t i -E[c t i ] 2 |C t , y 0 ≤ Σ 2 g (C) + ς 2 /sR KlM , • E c t -E[c t ] 2 |y 0 ≤ Σ 2 g (C) + ς 2 /sR KlM .</formula><p>Proof. First inequality. We define a random variable A such as</p><formula xml:id="formula_67">A := 1 KlM i∈C t k∈[K] A i,k , with A i,k := Hk i (y k-1 i ).</formula><p>From Lemma C.1, we have that for all i</p><formula xml:id="formula_68">∈ C t , k ∈ [K]: E[A i,k |y k-1 i ] = E[ Hk i (y k-1 i )|y k-1 i ] = ∇F i (y k-1 i ). Furthermore, by Lemma C.1, E || Hk i (y k-1 i ) -∇F i (y k-1 i )|| 2 |y k-1 i ≤ Σ 2 g (C) + ς 2 /sR.</formula><p>Furthermore, (a) for i, j</p><formula xml:id="formula_69">∈ C t , k∈[K] A i,k -∇F i (y k-1 i ) and k∈[K] A j,k -∇F j (y k-1 j</formula><p>) are independent conditionally to y 0 ; (b) for any</p><formula xml:id="formula_70">i ∈ C t , (A i,k -∇F i (y k-1 i )) k∈[K] is a martingale increment, i.e., E[A i,k - ∇F i (y k-1 i )|σ({y k i } k ∈[k-1] )] = 0. Consequently: E   1 KlM i∈C t k∈[K] A i,k -∇Fi(y k-1 i ) 2 C t , y 0   (a) = 1 (KlM ) 2 i∈C t E   k∈[K] A i,k -∇Fi(y k-1 i ) 2 y 0   (b) = 1 (KlM ) 2 i∈C t k∈[K] E       E A i,k -∇Fi(y k-1 i ) 2 σ {y k i } k ∈[k] ≤Σ 2 g (C)+ς 2 /sR y 0       ≤ Σ 2 g (C) + ς 2 /sR KlM .</formula><p>To prove the second equality, we need to "iteratively" expand the squared norm and take the conditional expectation w.r.t. σ {y k i } k ∈[k] for k = K, K -1, . . . , 1 and use the martingale property to obtain that the scalar products are equal to 0.</p><p>Second inequality. We recall that for any</p><formula xml:id="formula_71">i ∈ C t , c t i = 1 K K k=1 Hk i (y k-1 i</formula><p>). Thus 1 lM i∈C t c t i = A and we can directly use the results from the first inequality.</p><p>Third inequality. We recall that c t = 1 M M i=1 c t i (even if local control variates are not updated). Therefore, we can use the previous results and take the expectation over C t , which gives:</p><formula xml:id="formula_72">E c t -E[c t ] 2 |y 0 ≤ ς 2 /sR + Σ 2 g (C) KM ≤ ς 2 /sR + Σ 2 g (C) KlM .</formula><p>We combine Lemma C.3-1 on A 1 , A 2 , A 3 with Corollary C.1 which controls their individual variance (conditionally to the users' sampling and the local parameters) by</p><formula xml:id="formula_73">ς 2 /sR+Σ 2 g (C) KlM</formula><p>. We first get rid of the terms related to the variance of the data sampling and the DP noise, before bounding the quantities of interest. It leads to:</p><formula xml:id="formula_74">E||∆x t || 2 = η2 E   E   1 KlM k∈[K],i∈C t Hi (y k-1 i ) + c t-1 - 1 lM i∈C t c t-1 i 2 C t , y 0     ≤ η2 E   1 KlM k∈[K],i∈C t E[ Hi (y k-1 i )|y 0 ] + E[c t-1 |y 0 ] -E[c t-1 i |y 0 ] 2   + 9η 2 KlM (ς 2 /sR + Σ 2 g (C)),</formula><p>where the inequality is given by Lemma C.3-1.</p><formula xml:id="formula_75">For any i ∈ C t , k ∈ [K], we have E[ Hi (y k-1 i )|y 0 ] = E E[ Hi (y k-1 i )|y k-1 i ] y 0 = E ∇F i (y k-1 i )|y 0 = ∇F i (y k-1 i ). Then, E||∆x t || 2 ≤ η2 E   1 KlM k∈[K],i∈C t ∇F i (y k-1 i ) + E[c t-1 |y 0 ] -E[c t-1 i |y 0 ] 2   + 9η 2 KlM (ς 2 /sR + Σ 2 g (C)) (convexity of ||.|| 2 ) = η2 1 KM k∈[K],i∈[M ] E ∇F i (y k-1 i ) + E[c t-1 |y 0 ] -E[c t-1 i |y 0 ] ∇Fi(y k-1 i )-∇Fi(x t-1 ) +E[c t-1 |y 0 ]-∇F (x t-1 ) -E[c t-1 i |y 0 ]+∇Fi(x t-1 ) +∇F (x t-1 ) 2 + 9η 2 KlM (ς 2 /sR + Σ 2 g (C)) (definition of C t ) = η2 1 KM k∈[K],i∈[M ] E E ∇F i (y k-1 i ) -∇F i (x t-1 ) + c t-1 -∇F (x t-1 ) -c t-1 i + ∇F i (x t-1 ) + ∇F (x t-1 ) y 0 2 + 9η 2 KlM (ς 2 /sR + Σ 2 g (C)</formula><p>) (all variables are measurable wrt y 0 )</p><formula xml:id="formula_76">≤ η2 1 KM k∈[K],i∈[M ] E E ∇F i (y k-1 i ) -∇F i (x t-1 ) + c t-1 -∇F (x t-1 ) -c t-1 i + ∇F i (x t-1 ) + ∇F (x t-1 ) 2 y 0 + 9η 2 KlM (ς 2 /sR + Σ 2 g (C)) (Jensen inequality) ≤ 4η 2 KM k∈[K],i∈[M ] E ∇F i (y k-1 i ) -∇F i (x t-1 ) 2 + 8η 2 KM k∈[K],i∈[M ] E ∇F i (α t-1 i,k-1 ) -∇F i (x t-1 ) 2 + 4η 2 E ∇F (x t-1 ) 2 + 9η 2 KlM (ς 2 /sR + Σ 2 g (C)).</formula><p>The last inequality is obtained by definition of c and c i and by applying Jensen inequality. With Lemma C.2, this leads to the result.</p><formula xml:id="formula_77">Lemma C.5 (Lag in the control variate). ∀α ∈ [1/2, 1], ∀η ≤ 1 24ν l α , F t ≤ 1 - 17 36 l F t-1 + 1 24ν l 2α-1 E F (x t-1 ) -F (x * ) + 97 48 l 2α-1 E t + l ν 2 ς 2 /sR + Σ 2 g (C) 32KlM .</formula><p>Proof. We adapt the original proof made in the non-convex case <ref type="bibr">(Karimireddy et al., 2020b, Lemma 16)</ref>  Lemma C.7 (Progress made at each round). ∀η g ≥ 1, ∀η l ≤ min</p><formula xml:id="formula_78">1 24Kηgν l 2/3 , l 54µKηg , E||x t -x * || 2 + 27ν 2 η2 1 l F t ≤ 1 - µη 2 E||x t-1 -x * || 2 + 27ν 2 η2 1 l F t-1 - η 2 E F (x t-1 ) -F (x * ) + 10η 2 KlM 1 + lM η 2 g (ς 2 /sR + Σ 2 g (C)).</formula><p>Proof. We recall that ∆x t = -η KlM</p><formula xml:id="formula_79">k∈[K],i∈C t Hk i (y k-1 i ) + c t-1 -c t-1 i . Then, E[∆x t |y 0 ] = E[∆x t |x t-1 ] = -ηE[c t-1 |y 0 ] = - η KM k∈[K],i∈[M ] E[∇F i (y k-1 i )|y 0 ].<label>(9)</label></formula><p>We denote E t-1 <ref type="bibr">[.]</ref> as the expectation conditioned on randomness generated (strictly) prior to round t, i.e. conditionally to σ(x τ , τ ≤ t -1). We first bound the quantity</p><formula xml:id="formula_80">E t-1 ||x t -x * || 2 = E t-1 ||x t-1 + ∆x t -x * || 2 , Et-1||x t -x * || 2 = Et-1||x t-1 -x * || 2 + Et-1||∆x t || 2 + 2 Et-1[∆x t |y0], x t-1 -x * = ||x t-1 -x * || 2 + Et-1||∆x t || 2 + 2 - η KM k∈[K],i∈[M ] E[∇Fi(y k-1 i )|y0]</formula><p>by ( <ref type="formula" target="#formula_79">9</ref>)</p><formula xml:id="formula_81">, x t-1 -x * ≤ Et-1||x t-1 -x * || 2 + 4η 2 ν 2 Et + 8ν 2 η2 Ft-1 + 8ν η2 F (x t-1 ) -F (x * ) + 9η 2 KlM (ς 2 /sR + Σ 2 g (C)) + 2η KM Et-1   k∈[K],i∈[M ] ∇Fi(y k-1 i ), x * -x t-1   A , (Lemma C.4)</formula><p>where</p><formula xml:id="formula_82">E[A] ≤ 2η KM E   k∈[K],i∈[M ] F i (x * ) -F i (x t-1 ) + ν||y k-1 i -x t-1 || 2 - µ 4 ||x t-1 -x * || 2   = -2η E(F (x t-1 )) -F (x * ) + µ 4 E||x t-1 -x * || + 2ν ηE t ,</formula><p>where the inequality comes from the convexity and ν-smoothness property.</p><p>Hence, by taking the expectation:</p><formula xml:id="formula_83">E||x t -x * || 2 ≤ E||x t-1 -x * || 2 -2η E(F (x t-1 )) -F (x * ) + µ 4 E||x t-1 -x * || 2 + 2ν ηE t + 4η 2 ν 2 E t + 8ν 2 η2 F t-1 + 8ν η2 E F (x t-1 ) -F (x * ) + 9η 2 KlM (ς 2 /sR + Σ 2 g (C)).</formula><p>By combining all terms and multiplying by ν on each side of the inequality, it comes:</p><formula xml:id="formula_84">νE||x t -x * || 2 ≤ 1 - µη 2 νE||x t-1 -x * || 2 + (8ν 2 η2 -2ην) E(F (x t-1 )) -F (x * ) (10) + 9η 2 ν KlM (ς 2 /sR + Σ 2 g (C)) + (2ν 2 η + 4ν 3 η2 )E t + 8ν 3 η2 F t-1 .</formula><p>We now consider α ∈ [1/2, 1], η l ≤ 1 24Kνηg l α and η g ≥ 1. We use the result of Lemma C.5 where each side is multiplied by 27ν 3 η2 1 l to obtain:</p><formula xml:id="formula_85">27ν 3 η2 1 l F t ≤ 1 - µη 2 27ν 3 η2 1 l F t-1 + 27 µη 2l - 17 36 ν 3 η2 F t-1 (11) + 9 8 l 2α-2 ν 2 η2 E(F (x t-1 )) -F (x * ) + 873 16 l 2α-2 ν 3 η2 E t + 27 32 ν η2 ς 2 /sR + Σ 2 g (C) KlM .</formula><p>Since we have η l ≤ 1 24Kνηg , we recall the result from Lemma C.6:</p><formula xml:id="formula_86">9 2 ν 2 ηE t ≤ 9 2 ν 3 η2 F t-1 + 9 40 ην η 2 g E F (x t-1 ) -F (x * ) + 27 40 η2 ν Kη 2 g ς 2 /sR + Σ 2 g (C) .<label>(12)</label></formula><p>By summing inequalities ( <ref type="formula">10</ref>), ( <ref type="formula">11</ref>), ( <ref type="formula" target="#formula_86">12</ref>), we obtain:</p><formula xml:id="formula_87">νE||x t -x * || 2 + 27ν 3 η2 1 l F t ≤ 1 - µη 2 νE||x -x * || 2 + 27ν 3 η2 1 l F t-1 + 9 8 l 2α-2 ν 2 η2 + 9 40 ην η 2 g + 8ν 2 η2 -2ην E(F (x t-1 )) -F (x * ) (13) + 315 32 + 27 40 lM η 2 g η2 ν KlM ς 2 /sR + Σ 2 g (C) (14) + - 5 2 ν η + 4ν 2 η2 + 873 16 l 2α-2 ν 2 η2 νE t (15) + 27 µη 2l - 17 36 ν 2 η2 + 25 2 ν 2 η2 νF t-1 .<label>(16)</label></formula><p>We now consider η l ≤ l/54µKη g . Then η ≤ l/54µ and we recall that ν η ≤ 1/24. We fix α = 2/3 (then 2-2α = α).</p><p>In this part, we aim at simplifying the terms on the right side of the last inequality.</p><p>Simplifying (13):</p><formula xml:id="formula_88">9 8 l 2α-2 ν 2 η2 + 9 40 ην η 2 g + 8ν 2 η2 -2ην ≤ 9 8 × 24 + 9 40 + 8 24 -2 ν η = - 1339 960 ∼1.39 ν η ≤ - ν η 2 .</formula><p>Simplifying ( <ref type="formula">14</ref>):</p><formula xml:id="formula_89">315 32 + 27 40 lM η 2 g ≤ 10 1 + lM η 2 g .</formula><p>Simplifying (15):</p><formula xml:id="formula_90">Since l 2α-2 ν η = ν η 1 l 2/3 ≤ 1/24, - 5 2 ν η + 4ν 2 η2 + 873 16 l 2α-2 ν 2 η2 ≤ - 5 2 + 4 24 + 873 16 1 24 ν η = - 23 384 ν η ≤ 0.</formula><p>Deep neural network. To prove the advantage of DP-<software ContextAttributes="used">SCAFFOLD</software> with non-convex objectives, we perform experiments on MNIST data with a deep neural network. Its architecture is inspired by the network used by <ref type="bibr" target="#b0">Abadi et al. (2016)</ref> for DP-SGD. We use a feedforward neural network with ReLU units and softmax of 10 classes (corresponding to the 10 digits of MNIST) with cross-entropy loss. Our network combines a 60-dimensional Principal Component Analysis (PCA) projection layer and a hidden layer with 200 hidden units. Since the error bound for DP-FL algorithms grows linearly with the dimension of the parameters for non-convex objectives (see Theorems C.1,C.2), the PCA layer is actually necessary to prevent the curse of dimensionality due to the addition of noise for privacy. Note that neural networks with more layers would also suffer from the curse of dimensionality in the DP-FL context. Using a batch size of 500, we can reach a test accuracy higher than 98% with this architecture in 100 epochs under the centralized setting. This result is consistent with what can be achieved with a vanilla neural network <ref type="bibr">(LeCun et al., 1998)</ref>. In our framework, the PCA procedure is applied as preprocessing to all the samples without differential privacy. To avoid privacy leakage at this step, it would need to include a private mechanism, whose privacy loss should be added to that of the training phase (see the discussion in <ref type="bibr">Abadi et al., 2016, Section 4)</ref>.</p></div>
<div><head>D.2 Synthetic Data Generation</head><p>Each ground-truth model for user i consists in weights W i ∈ R d ×10 and bias b i ∈ R 10 , which are sampled from the following distributions: <ref type="bibr">Id)</ref> where u i ∼ N d ×10 (0, αId) and u i ∼ N 10 (0, αId). The data matrix X i of user i is sampled according to X i |v i ∼ N d (v i , Σ) where Σ is the covariance matrix defined by its diagonal Σ j,j = j -1.2 and v i |B i ∼ N d (B i , Id) where B i ∼ N d (0, νId). The labels are obtained by independently changing the labels given by the ground truth model with probability 0.05.</p><formula xml:id="formula_91">W i |u i ∼ N d ×10 (u i , Id) and b i |u i ∼ N 10 (u i ,</formula></div>
<div><head>D.3 Additional Experimental Results</head><p>We provide below more results on the experiments described in Section 5, including additional metrics and more extensive choices of heterogeneity levels. We also present additional experiments with higher privacy, including a study on the effect of sampling parameters l and s (and the trade-off with T ) on privacy and convergence.</p><p>Metrics. To measure the convergence and performance of the algorithms at any communication round t ∈ [T ], we consider the following metrics:</p><p>• Accuracy(t): the average test accuracy of the model over all users,</p><p>• Train Loss(t)= log 10 (F (x t ) -F (x * )): the log-gap between the objective function evaluated at parameter x t and its minimum,</p><formula xml:id="formula_92">• Train Gradient Dissimilarity(t)= 1 M M i=1 ||∇F i (x t )|| 2 -||∇F (x t )|| 2</formula><p>, and similarly the Train Gradient Log-</p><formula xml:id="formula_93">Dissimilarity(t)= log 1 M M i=1 ||∇F i (x t )|| 2 -log ||∇F (x t )|| 2 ,</formula><p>which measure how the local gradients differ from the global gradient (i.e., the average across users) when evaluated at x t , and hence quantify the user-drift over the rounds of communication.</p></div>
<div><head>D.3.1 Results with other metrics and different heterogeneity levels</head><p>We provide below some additional results which complement those provided in Section 5.</p><p>• Synthetic data. We plot in Fig. <ref type="figure" target="#fig_6">3</ref> the evolution of the accuracy over the rounds, which is consistent with the evolution of the train loss in Fig. <ref type="figure">1</ref>. While the variance of the accuracy for DP-<software ContextAttributes="used">FedAvg</software> grows with the heterogeneity, the results of <software ContextAttributes="used">DP-SCAFFOLD-warm</software> are not affected. We can observe an average difference of 10% in the accuracy for these two algorithms over the various heterogeneity settings. We provide in Fig. <ref type="figure" target="#fig_7">4</ref> the evolution of the gradient dissimilarity for the same settings as in Fig. <ref type="figure">1</ref> and Fig. <ref type="figure" target="#fig_6">3</ref>, which once again shows a better convergence of <software ContextAttributes="used">DP-SCAFFOLD-warm</software> compared to DP<software ContextAttributes="used">-FedAvg</software> for the same privacy level. We also provide the evolution of the train loss when varying a single heterogeneity parameter: either α (which controls model heterogeneity across users) in Fig. <ref type="figure" target="#fig_8">5</ref> or β (which controls data heterogeneity across users) in Fig. <ref type="figure" target="#fig_9">6</ref>. In both of these settings, <software ContextAttributes="used">DP-SCAFFOLD-warm</software> performs consistently better.</p><p>• <software ContextAttributes="used">FEMNIST</software> data. In Fig. <ref type="figure" target="#fig_10">7</ref>, we put in perspective the accuracy observed with K = 50 (see Fig. <ref type="figure">2</ref>, first row) with the one observed with K = 100. We also show the evolution of the gradient dissimilarity in Fig. <ref type="figure" target="#fig_11">8</ref>. These results on real data again show the superior performance of <software ContextAttributes="used">DP-SCAFFOLD-warm</software>, consistently with our observations on synthetic data.      Table <ref type="table">4</ref>: Test Accuracy (%) For <software ContextAttributes="used">DP-SCAFFOLD</software> On Synthetic Data ( = 3, l = 0.05, s = 0.2, (α, β) = (0, 0)). In Section 5 of the main text, we presented these trade-offs for <software ContextAttributes="used">DP-SCAFFOLD</software> under a high level of heterogeneity (see Table <ref type="table">1</ref>). We provide below the results of these trade-offs for <software ContextAttributes="used">DP-SCAFFOLD</software> with a lower level of heterogeneity (α, β) = (0, 0) (see Table <ref type="table">4</ref>). We consider again synthetic data with = 3 towards any third party. To report the test accuracy which is obtained at the end of the iterations, we proceed in two steps: (i) we compute the average test accuracy over the last 10% of the iterations for each random run, (ii) we calculate the mean and the standard deviation over the 3 runs and report them in the tables. We highlight in bold for each row (i.e., for each value of σ g ) in Tables 1,4 the best accuracy score obtained over all values of K.</p><formula xml:id="formula_94">σ g K = 1 K = 5 K = 10 K = 20 K =</formula><p>We observe the same trends as the ones described for Table <ref type="table">1</ref> in the main text. Indeed, our results clearly show a trade-off between T and K for DP-<software ContextAttributes="used">SCAFFOLD</software> under a fixed privacy budget. If K is set too low or too large, the performance of the algorithm is sub-optimal either because T has to be chosen too low or because control variates are inefficient under few local updates.</p><p>Moreover, we observe that setting σ g to a high value does not necessarily improve the gain in the number of communication rounds. In particular, for high values of σ g , the calculation of the privacy bound does not allow to obtain a large increase in T (T does not change between σ g = 40 and σ g = 160 for any value of K), which thus leads to poor performance. This can be explained by the fact that the upper bound for the subsampled Gaussian mechanism given in Lemma B.7 does not converge to 0 when σ g is very large. Indeed, given a subsampling ratio q &lt; 1, we can observe that this bound in the asymptotic regime becomes 1 α-1 log(1 + 2 α j=3 q j α j ), which is positive. Hence, by increasing the value of σ g , we cannot hope to inconditionally increase the number of compositions of the mechanism under a fixed privacy budget, if q is taken too large. This artefact thus proves how small subsampling ratios have to be chosen to compute differential privacy in practice. In our experiments, we can clearly notice that this asymptotic regime is reached as soon as σ g is greater than 80. One would have to consider lower subsampling ratios (for instance l = 10 -4 , s = 10 -4 ), to obtain different values for T when σ g = 80 and σ g = 160. We investigate such trade-offs in the next section.</p></div>
<div><head>D.3.3 Experiments under higher privacy regime and role of sampling parameters</head><p>In Section 5 of the main text, given private parameter σ g , we conducted experiments for (i) convex objective on <software>FEMNIST</software> (σ g = 30) and synthetic data (σ g = 60) and (ii) non-convex objective on MNIST data (σ g = 30). Considering the sampling parameters we used (l = 0.2, s = 0.2), this setting allows to reach, towards any third party, (11.4, δ)-DP for <software ContextAttributes="used">FEMNIST</software> data and (13, δ)-DP for synthetic data. In the case of MNIST data, we obtain (7.2, δ)-DP towards any third party for (K, T ) = (50, 100). Although these experiments only allow "low privacy", stronger DP guarantees can be obtained by simply decreasing the subsampling ratios (thus amplifying the privacy). For instance, setting l = l/4 = 0.05 in case of synthetic data would provide (4.2, δ)-DP for (K, T ) = (50, 400). We present below some results on numerical trade-offs under a higher privacy regime.</p><p>As observed in the experiments of the main text and consistently with previous work <ref type="bibr">(Karimireddy et al., 2020b)</ref>, we observe the superiority of <software ContextAttributes="used">SCAFFOLD</software> over <software ContextAttributes="used">FedAvg</software> and <software ContextAttributes="used">FedSGD</software> under heterogeneous data, but most importantly our results show that this hierarchy is preserved in our DP-FL framework with privacy constraints: this is especially clear with growing heterogeneity and with growing number K of local updates. Besides this, the results provided for logistic regression in the privacy regime numerically demonstrate that <software ContextAttributes="used">DP-SCAFFOLD</software> sometimes even outperforms (non-private) <software ContextAttributes="used">FedAvg</software> despite the local injection of Gaussian noise, see for instance Fig. <ref type="figure">1</ref>-10 and Fig. <ref type="figure">2</ref> (bottom row), and to a lesser extent Fig. <ref type="figure" target="#fig_13">9</ref>. Therefore, our results are quite promising with respect to obtaining efficient DP-FL algorithms under heterogeneous data for higher privacy regimes.</p><p>Trade-offs between l and T . In this section, we compare the robustness of DP-<software ContextAttributes="used">FedAvg</software> and <software ContextAttributes="used">DP-SCAFFOLD</software> w.r.t. user sampling ratio l, under a fixed privacy bound towards a third party. We fix parameters s = 0.2, K = 10 and report in Figure <ref type="figure" target="#fig_13">9</ref> the evolution of the test accuracy of these algorithms for l ∈ {0.08, 0.1, 0.12} over the communication rounds. For each value of l, the number of communication rounds T l is determined to be maximal w.r.t. to the privacy bound, so that the desired privacy level is achieved for the output after T l rounds. These values of T l are represented on Figure <ref type="figure" target="#fig_13">9</ref> with red vertical lines (note that the higher l, the lower T l ). We conduct this experiment on the following datasets: synthetic data with = 5 ( Our results first show that <software ContextAttributes="used">DP-SCAFFOLD</software> achieves much better performance than DP-<software ContextAttributes="used">FedAvg</software> in these high privacy regimes. The superiority of <software ContextAttributes="used">DP-SCAFFOLD</software> towards DP-<software ContextAttributes="used">FedAvg</software> is especially strong under high heterogeneity: we notice a gap of 20% in the accuracy score with synthetic data for (α, β) = (5, 5) and <software ContextAttributes="used">FEMNIST</software> data for γ = 0% with logistic regression model, a gap of 10% in the accuracy score for MNIST data with γ = 0% and DNN. Furthermore, <software ContextAttributes="used">DP-SCAFFOLD</software> is robust to a low value of user sampling parameter l. We observe that we obtain the best performance by choosing l = 0.08 (the lowest value considered), which allows to set T l to a high value. Note that the evolution of the accuracy is similar for all values of l. Therefore, setting a low l provides an effective way to achieve good accuracy with higher privacy. Trade-offs between s and T . In this section, we compare the behavior of DP-<software ContextAttributes="used">FedAvg</software> and <software ContextAttributes="used">DP-SCAFFOLD</software> w.r.t. data sampling ratio s under a fixed privacy bound towards a third party. We fix parameters l = 0.1, K = 10 and report in Figure <ref type="figure" target="#fig_15">10</ref> the evolution of the test accuracy of these algorithms for s ∈ {0.05, 0.1, 0.2} over the communication rounds. For each value of s, the number of communication rounds T s is determined to be maximal w.r.t. to the privacy bound, so that the desired privacy level is achieved for the output after T s rounds. These values of T s are represented on Figure <ref type="figure" target="#fig_15">10</ref> with red vertical lines (note that the higher s, the lower T ). We conduct this experiment for the following datasets: Our results confirm that DP-<software ContextAttributes="used">SCAFFOLD</software> leads to better performance than DP-<software ContextAttributes="used">FedAvg</software> with any value of s ∈ {0.2, 0.1, 0.05} under heterogeneity (as expected, we obtain very similar accuracy scores for these two algorithms with γ = 100% for <software ContextAttributes="used">FEMNIST</software> and MNIST data). However, this superiority decreases as s decreases. Consider for instance <software ContextAttributes="used">FEMNIST</software> data with 10% similarity: the gap in accuracy drops from 30% with s = 0.2, to less than 20% with s = 0.1. Our results seem to show that we obtain better performance with a high value of s, although it implies to set T s to a lower value. This is contrast to the effect of l shown in Fig <ref type="figure" target="#fig_13">9</ref>. </p></div><figure xml:id="fig_0"><head /><label /><figDesc>FL) enables a set of users with local datasets to collaboratively train a machine learning model without centralizing data (Kairouz et al., Proceedings of the 25 th International Conference on Artificial Intelligence and Statistics (AISTATS) 2022, Valencia, Spain. PMLR: Volume 151. Copyright 2022 by the author(s).</figDesc></figure>
<figure xml:id="fig_1"><head /><label /><figDesc>2021</figDesc></figure>
<figure xml:id="fig_2"><head /><label /><figDesc>i (Karimireddy et al., 2020b, Appendix E). Therefore, adding (c -c i ) in the update balances the local stochastic gradient and limits user-drift. Warm-start version of DP-SCAFFOLD. We adapt the warm-start strategy from Karimireddy et al. (2020b, Appendix E) to accommodate DP constraints, leading to DP-SCAFFOLD-warm. The first few rounds of communication are saved to set 2 the initial values of the control variates to c 0</figDesc></figure>
<figure xml:id="fig_4"><head /><label /><figDesc>algorithms. DP allows to keep track of the privacy guarantees when such a sequence of private mechanisms is run on the same dataset D. Simple composition(Dwork et al., 2010, Theorem III.1.)  states that the privacy parameters grow linearly with T .<ref type="bibr" target="#b11">Dwork et al. (2010)</ref> provide a strong composition result where the parameter grows sublinearly with T . This result is recalled in Lemma B.1. Lemma B.1 (Strong adaptive composition,<ref type="bibr" target="#b11">Dwork et al., 2010)</ref>. Let M 1 , ..., M T be T adaptive ( , δ)-DP mechanisms. Then, for any δ &gt; 0, the mechanism M = (M 1 , ..., M T ) is ( , δ)-DP where:=2T log(1/δ ) + T (e -1) and δ = T δ + δ .</figDesc></figure>
<figure xml:id="fig_5"><head /><label /><figDesc>Lemma B.2 quantifies the associated privacy amplification effect. Lemma B.2 (Amplification by subsampling,Kasiviswanathan et al., 2011). Let M : X m → Y be a ( , δ)-DP mechanism w.r.t. a given dataset D ∈ X m . Then, mechanism M : X n → Y defined as M := M • Samp n,m is ( , δ )-DP w.r.t. to any dataset D ∈ X n such that D = Samp n,m (D), where:</figDesc></figure>
<figure xml:id="fig_6"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Test Accuracy On Synthetic Data ( = 13). First Row: K = 50; Second Row: K = 100.</figDesc></figure>
<figure xml:id="fig_7"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Train Gradient Dissimilarity On Synthetic Data ( = 13). First Row: K = 50; Second Row: K = 100.</figDesc></figure>
<figure xml:id="fig_8"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Model Heterogeneity (Varying α): Train Loss On Synthetic Data ( = 13). First Row: K = 50; Second Row: K = 100.</figDesc></figure>
<figure xml:id="fig_9"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Data Heterogeneity Varying β): Train Loss On Synthetic Data ( = 13). First Row: K = 50; Second Row: K = 100.</figDesc></figure>
<figure xml:id="fig_10"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Test Accuracy On FEMNIST Data (LogReg) with = 11.5. First Row: K = 50; Second Row: K = 100.</figDesc></figure>
<figure xml:id="fig_11"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Train Gradient Dissimilarity On FEMNIST Data (LogReg) with = 11.5. First Row: K = 50; Second Row: K = 100.</figDesc></figure>
<figure xml:id="fig_12"><head /><label /><figDesc>Fig 9, first row), FEMNIST data with LogReg model, = 5 (Fig 9, second row) and MNIST data with DNN model, = 3 (Fig 9, third row).</figDesc></figure>
<figure xml:id="fig_13"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Test Accuracy With Various Values For l Under Fixed Privacy Budget. First Row: Synthetic data ( = 5); Second Row: FEMNIST data (LogReg, = 5); Third Row: MNIST (DNN, = 3).</figDesc></figure>
<figure xml:id="fig_14"><head /><label /><figDesc>synthetic data with = 5 (Fig 10, first row), FEMNIST data with LogReg model, = 5 (Fig 10, second row) and MNIST data with DNN model, = 3 (Fig 10, third row).</figDesc></figure>
<figure xml:id="fig_15"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Test Accuracy With Various Values For s Under Fixed Privacy Budget. First Row: Synthetic Data ( = 5); Second Row: FEMNIST Data (LogReg, = 5); Third Row: MNIST Data (DNN, = 3).</figDesc></figure>
<figure type="table" xml:id="tab_1"><head /><label /><figDesc>40 10 27.41 ±0.71 T = 542 45.53 ±0.99 T = 488 43.52 ±1.52 T = 428 42.51 ±0.80 T = 324 21.80 ±3.28 T = 20 27.34 ±1.31 T = 545 44.39 ±0.46 T = 502 43.47 ±1.74 T = 451 42.33 ±0.77 T = 352 20.14 ±2.67 T = 40 21.05 ±2.27 T = 546 34.50 ±0.65 T = 505 36.85 ±0.85 T = 457 33.24 ±0.41 T = 360 14.85 ±0.95 T = 80 17.61 ±2.62 T = 546 24.41 ±0.81 T = 506 27.33 ±0.37 T = 458 19.42 ±0.51 T = 362 14.08 ±0.14 T = 160 13.97 ±1.70 T = 546 15.99 ±0.30 T = 506 19.27 ±1.65 T = 458 14.86 ±0.75 T = 362 14.17 ±0.06 T =</figDesc><table><row><cell>and communication trade-offs. IEEE Journal on Se-</cell><cell cols="2">Shiva Prasad Kasiviswanathan, Homin K Lee, Kobbi</cell></row><row><cell>lected Areas in Information Theory, 2(1):464-478,</cell><cell cols="2">Nissim, Sofya Raskhodnikova, and Adam Smith.</cell></row><row><cell cols="3">2021b. Robert Mansel Gower, Nicolas Loizou, Xun Qian, Al-Supplementary Material: What can we learn privately? SIAM Journal on Computing, 40(3):793-826, 2011. ibek Sailanbayev, Egor Shulgin, and Peter Richtárik. Ahmed Khaled, Konstantin Mishchenko, and Peter Differentially Private Federated Learning on Heterogeneous Data</cell></row><row><cell>Sgd: General analysis and improved rates. In In-</cell><cell cols="2">Richtárik. Tighter theory for local sgd on identical</cell></row><row><cell>ternational Conference on Machine Learning, pages</cell><cell cols="2">and heterogeneous data. In International Confer-</cell></row><row><cell>5200-5209. PMLR, 2019.</cell><cell cols="2">ence on Artificial Intelligence and Statistics, pages</cell></row><row><cell>Tzu-Ming Harry Hsu, Hang Qi, and Matthew Brown.</cell><cell cols="2">4519-4529. PMLR, 2020.</cell></row><row><cell>Measuring the effects of non-identical data distri-</cell><cell cols="2">Diederik P. Kingma and Jimmy Ba. Adam: A</cell></row><row><cell>bution for federated visual classification. arXiv</cell><cell cols="2">method for stochastic optimization. arXiv preprint</cell></row><row><cell>preprint arXiv:1909.06335, 2019.</cell><cell cols="2">arXiv:1412.6980, 2014.</cell></row><row><cell>Rui Hu, Yuanxiong Guo, Hongning Li, Qingqi Pei, and</cell><cell cols="2">Yann LeCun, Léon Bottou, Yoshua Bengio, and</cell></row><row><cell>Yanmin Gong. Personalized federated learning with</cell><cell cols="2">Patrick Haffner. Gradient-based learning applied to</cell></row><row><cell>differential privacy. IEEE Internet of Things Jour-</cell><cell cols="2">document recognition. Proceedings of the IEEE, 86</cell></row><row><cell>nal, 7(10):9530-9539, 2020.</cell><cell cols="2">(11):2278-2324, 1998.</cell></row><row><cell>Peter Kairouz, Sewoong Oh, and Pramod Viswanath. The composition theorem for differential privacy. In International conference on machine learning, pages 1376-1385. PMLR, 2015.</cell><cell cols="2">Tian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, and Virginia Smith. Fed-erated optimization in heterogeneous networks. Pro-ceedings of Machine Learning and Systems, 2:429-450, 2020a.</cell></row><row><cell>Peter Kairouz, H. Brendan McMahan, Brendan Avent, Aurélien Bellet, Mehdi Bennis, Arjun Nitin Bhagoji, Kallista Bonawitz, Zachary Charles, Graham Cor-mode, Rachel Cummings, Rafael G. L. D'Oliveira, Hubert Eichner, Salim El Rouayheb, David Evans, Josh Gardner, Zachary Garrett, Adrià Gascón,</cell><cell cols="2">Yiwei Li, Tsung-Hui Chang, and Chong-Yung Chi. Se-cure federated averaging algorithm with differential privacy. In 2020 IEEE 30th International Workshop on Machine Learning for Signal Processing (MLSP), pages 1-6. IEEE, 2020b.</cell></row><row><cell>Badih Ghazi, Phillip B. Gibbons, Marco Gruteser,</cell><cell cols="2">Othmane Marfoq, Giovanni Neglia, Aurélien Bellet,</cell></row><row><cell>Zaid Harchaoui, Chaoyang He, Lie He, Zhouyuan</cell><cell cols="2">Laetitia Kameni, and Richard Vidal. Federated</cell></row><row><cell>Huo, Ben Hutchinson, Justin Hsu, Martin Jaggi,</cell><cell cols="2">multi-task learning under a mixture of distribu-</cell></row><row><cell>Tara Javidi, Gauri Joshi, Mikhail Khodak, Jakub</cell><cell cols="2">tions. Advances in Neural Information Processing</cell></row><row><cell>Konečný, Aleksandra Korolova, Farinaz Koushan-</cell><cell cols="2">Systems, 34, 2021.</cell></row><row><cell>far, Sanmi Koyejo, Tancrède Lepoint, Yang Liu,</cell><cell cols="2">H. Brendan McMahan, Eider Moore, Daniel Ram-</cell></row><row><cell>Prateek Mittal, Mehryar Mohri, Richard Nock, Ayfer Özgür, Rasmus Pagh, Mariana Raykova,</cell><cell cols="2">age, Seth Hampson, and Blaise Aguera y Arcas. Communication-efficient learning of deep networks</cell></row><row><cell>Hang Qi, Daniel Ramage, Ramesh Raskar, Dawn</cell><cell cols="2">from decentralized data. In Artificial intelligence</cell></row><row><cell>Song, Weikang Song, Sebastian U. Stich, Ziteng</cell><cell cols="2">and statistics, pages 1273-1282. PMLR, 2017.</cell></row><row><cell>Sun, Ananda Theertha Suresh, Florian Tramèr, Pra-neeth Vepakomma, Jianyu Wang, Li Xiong, Zheng Xu, Qiang Yang, Felix X. Yu, Han Yu, and Sen Zhao. Advances and open problems in federated learning. Foundations and Trends® in Machine</cell><cell cols="2">H. Brendan McMahan, Daniel Ramage, Kunal Talwar, and Li Zhang. Learning differentially private recur-rent language models. In International Conference on Learning Representations, 2018.</cell></row><row><cell>Learning, 14(1-2):1-210, 2021.</cell><cell cols="2">Ilya Mironov. Rényi differential privacy. In 2017</cell></row><row><cell>Sai Praneeth Karimireddy, Martin Jaggi, Satyen Kale, Mehryar Mohri, Sashank J. Reddi, Sebastian U.</cell><cell cols="2">IEEE 30th computer security foundations sympo-sium (CSF), pages 263-275. IEEE, 2017.</cell></row><row><cell>Stich, and Ananda Theertha Suresh. Mime: Mim-</cell><cell cols="2">Yurii Nesterov et al. Lectures on convex optimization,</cell></row><row><cell>icking centralized stochastic algorithms in federated</cell><cell cols="2">volume 137. Springer, 2004.</cell></row><row><cell>learning. arXiv preprint arXiv:2008.03606, 2020a.</cell><cell cols="2">Sashank J. Reddi, Zachary Charles, Manzil Zaheer,</cell></row><row><cell /><cell cols="2">Zachary Garrett, Keith Rush, Jakub Konečnỳ, San-</cell></row><row><cell /><cell cols="2">jiv Kumar, and H. Brendan McMahan. Adaptive</cell></row><row><cell /><cell cols="2">federated optimization. In International Conference</cell></row><row><cell /><cell cols="2">on Learning Representations, 2020.</cell></row><row><cell /><cell cols="2">Felix Sattler, Klaus-Robert Müller, and Wojciech</cell></row><row><cell /><cell>Samek.</cell><cell>Clustered federated learning: Model-</cell></row></table><note><p><p>Figure 1: Train Loss On Synthetic Data ( = 13). First Row: K = 50; Second Row: K = 100.</p>Figure 2: Test Accuracy With K = 50. First Row: <software ContextAttributes="used">FEMNIST</software> (LogReg), = 11.4; Second Row: MNIST (DNN), = 7.2. Sai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank Reddi, Sebastian Stich, and Ananda Theertha Suresh. Scaffold: Stochastic controlled averaging for federated learning. In International Conference on Machine Learning, pages 5132-5143. PMLR, 2020b.</p></note></figure>
<figure type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Table2summarizes the main notations used throughout the paper. Summary of the main notations.</figDesc><table><row><cell>Symbol</cell><cell>Description</cell></row><row><cell>[n]</cell><cell>set {1, 2, ..., n} for any n ∈ N</cell></row><row><cell cols="2">M , i ∈ [M ] number and index of users</cell></row><row><cell>T , t ∈ [T ]</cell><cell>number and index of communication rounds</cell></row><row><cell cols="2">K, k ∈ [K] number and index of local updates (for each user)</cell></row><row><cell>D i</cell><cell>local dataset held by the i-th user, composed of points d i 1 , . . . , d i R</cell></row><row><cell>R</cell><cell>size of any local dataset D i</cell></row><row><cell>D</cell><cell>joint dataset (</cell></row></table><note><p><p>M i=1 D i ) f i (x, d)</p>loss of the i-th user for model x on data record d F i local empirical risk function of the i-th user (</p></note></figure>
<figure type="table" xml:id="tab_3"><head /><label /><figDesc>User subsampling by the server: C t ⊂ [M ] of size lM</figDesc><table><row><cell cols="3">Algorithm 2: DP-FedAvg(T, K, l, s, σ g , C)</cell></row><row><cell /><cell>Server Input: initial x 0</cell></row><row><cell /><cell>Output: x T</cell></row><row><cell cols="2">1 for t = 1, ..., T do</cell></row><row><cell>2</cell><cell /></row><row><cell>3</cell><cell cols="2">Server communicates x t-1 to users i ∈ C t</cell></row><row><cell>4</cell><cell>for user i ∈ C t do</cell></row><row><cell>5</cell><cell cols="2">Initialize model: y 0 i ← x t</cell></row><row><cell>6</cell><cell>for k = 1, ..., K do</cell></row><row><cell>7</cell><cell cols="2">Data subsampling by user: S k i ⊂ D i of size sR</cell></row><row><cell>8</cell><cell cols="2">for sample j ∈ S k i do</cell></row><row><cell>9</cell><cell /></row><row><cell /><cell>R d</cell><cell>server model after round t</cell></row><row><cell /><cell>y k i ∈ R d c t ∈ R d</cell><cell>model of i-th user after local update k server control variate after round t</cell></row><row><cell /><cell>c t i ∈ R d</cell><cell>control variate of the i-th user after round t</cell></row><row><cell /><cell>l ∈ (0, 1)</cell><cell>user sampling ratio</cell></row><row><cell /><cell>s ∈ (0, 1)</cell><cell>data sampling ratio</cell></row><row><cell /><cell>, δ</cell><cell>differential privacy parameters</cell></row><row><cell /><cell>σ g</cell><cell>standard deviation of Gaussian noise added for privacy</cell></row><row><cell /><cell>C</cell><cell>gradient clipping threshold</cell></row><row><cell /><cell>ν</cell><cell>Lipschitz-smoothness constant</cell></row><row><cell /><cell>µ</cell><cell>strong convexity parameter</cell></row><row><cell /><cell>ς 2</cell><cell>variance of stochastic gradients</cell></row></table><note><p><p>A.2 DP-<software>FedAvg</software> Algorithm</p>The code of DP-<software ContextAttributes="used">FedAvg</software> is given in Algorithm 2.</p></note></figure>
<figure type="table" xml:id="tab_5"><head /><label /><figDesc>(Bounding the user drift). ∀η g ≥ 1, ∀η l ≤ 1/24νKη g , Proof. We once again adapt the original proof made in the non-convex case(Karimireddy et al., 2020b, Lemma  17), use Lemma C.2 and multiply on each side of the inequality by 9 2 ν 2 η.</figDesc><table><row><cell /><cell /><cell /><cell /><cell /><cell /><cell /><cell /><cell /><cell>and use</cell></row><row><cell cols="4">Lemma C.1 and Lemma C.2.</cell><cell /><cell /><cell /><cell /><cell /><cell /></row><row><cell>Lemma C.6 9 2</cell><cell>ν 2 ηE t ≤</cell><cell>9 2</cell><cell>ν 3 η2 F t-1 +</cell><cell>9 40</cell><cell>ην η 2 g</cell><cell>E F (x t-1 ) -F (x  *  ) +</cell><cell>27 40</cell><cell>η2 ν g Kη 2</cell><cell>ς 2 /sR + Σ 2 g (C) .</cell></row></table></figure>
<figure type="table" xml:id="tab_6"><head /><label /><figDesc>40 10 31.29 ±0.50 T = 542 44.37 ±0.15 T = 488 44.17 ±0.56 T = 428 41.71 ±0.36 T = 324 25.92 ±0.90 T = 72 20 28.31 ±1.15 T = 545 41.97 ±0.77 T = 502 43.27 ±0.90 T = 451 41.13 ±0.55 T = 352 28.20 ±2.23 T = 83 40 21.07 ±0.41 T = 546 33.01 ±1.20 T = 505 35.96 ±0.84 T = 457 32.82 ±1.11 T = 360 23.49 ±1.70 T = 86 80 17.09 ±1.31 T = 546 21.84 ±0.96 T = 506 25.92 ±0.59 T = 458 24.02 ±1.27 T = 362 17.95 ±1.14 T = 87 160 15.24 ±1.63 T = 546 15.37 ±0.28 T = 506 20.09 ±1.51 T = 458 18.09 ±1.86 T = 362 15.38 ±0.87 T = 87 D.3.2 Additional results on the trade-offs between K, T and σ g</figDesc><table /></figure>
			<note place="foot" n="1" xml:id="foot_0"><p>The use of composition for analyzing the privacy guarantee for the final model implies that the same guarantee holds even if every intermediate global model is observed.</p></note>
			<note place="foot" n="2" xml:id="foot_1"><p>This happens with high probability: typically, after 4/l where l = o(1), all users have been selected at least once with probability 1 -e -4 ≈ 0.98.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>We thank <rs type="person">Baptiste Goujaud</rs> and <rs type="person">Constantin Philippenko</rs> for interesting discussions. We thank anonymous reviewers for their constructive feedback. The work of A. Dieuleveut is partially supported by <rs type="grantNumber">ANR-19-CHIA-0002-01 /</rs>chaire SCAI, and <rs type="person">Hi! Paris</rs>. The work of <rs type="person">A. Bellet</rs> is supported by grants <rs type="grantNumber">ANR-16-CE23-0016</rs> (Project <rs type="projectName">PAMELA</rs>) and <rs type="grantNumber">ANR-20-CE23-0015</rs> (Project <rs type="projectName">PRIDE</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_SmbFken">
					<idno type="grant-number">ANR-19-CHIA-0002-01 /</idno>
				</org>
				<org type="funded-project" xml:id="_UEb5sw9">
					<idno type="grant-number">ANR-16-CE23-0016</idno>
					<orgName type="project" subtype="full">PAMELA</orgName>
				</org>
				<org type="funded-project" xml:id="_kuGZGs5">
					<idno type="grant-number">ANR-20-CE23-0015</idno>
					<orgName type="project" subtype="full">PRIDE</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div><head>C.2 Theorem of Convergence for DP-SCAFFOLD-warm</head><p>Theorem C.1 (Utility rates for DP-<software>SCAFFOLD</software>-warm, σ g chosen arbitrarily). Let σ g , C &gt; 0, x 0 ∈ R d . Suppose we run DP-<software ContextAttributes="used">SCAFFOLD</software>-warm(T, K, l, s, σ g , C) with initial local controls such that c 0 i = 1 K K k=1</p><p>Hk i (x 0 ) for any i ∈ [M ]. Under Assumptions 2 and 3, we consider the sequence of iterates (x t ) t≥0 of the algorithm, starting from x 0 .</p><p>1. If F i are µ-strongly convex (µ &gt; 0), η g = √ lM , η l = min ), then, there exist weights {w t } t∈ <ref type="bibr">[T ]</ref> such that the averaged output of DP-<software ContextAttributes="used">SCAFFOLD</software>-warm(T, K, l, s, σ g , C), defined by x T = t∈[T ] w t x t , has expected excess of loss such that:</p><p>, µl</p><p>24νKηg and T ≥ 1,then, there exist weights {w t } t∈ <ref type="bibr">[T ]</ref> such that the averaged output of DP-SCAFFOLD-warm(T, K, l, s, σ g , C), defined by x T = t∈[T ] w t x t , has expected excess of loss such that:</p><p>24νKηg and T ≥ 1, then there exist weights {w t } t∈[T ] such that the randomized output of DP-<software>SCAFFOLD</software>-warm(T, K, l, s, σ g , C), defined by {x T = x t with probability w t for all t}, has expected squared gradient of the loss such that:</p><p>where</p><p>We recover the result of Theorem 4.2 for DP-<software>SCAFFOLD</software>-warm where F i are convex by setting σ g = σ * g where σ</p><p>with numerical constants omitted for the asymptotic bound).</p></div>
<div><head>C.3 Proof of Theorem C.1 (Convex case)</head><p>In this section, we give a detailed proof of convergence of DP-<software ContextAttributes="used">SCAFFOLD</software>-warm with convex local loss functions. Our analysis is adapted from the proof given by <ref type="bibr">Karimireddy et al. (2020b)</ref> without DP noise, but requires original modifications (see below). Throughout this part, we re-use the notations from Section 3.3. Summary of the main steps. Let t ∈ [T ] be an arbitrary communication round of the algorithm. We detail below the updates that occur at this round.</p><p>• Then we define the local control variate ct i for this user by:</p><p>• For any i ∈ [M ], we update the control variate c t i such that:</p><p>• Finally, the global update is computed as:</p><p>To keep track of the lag in the update of c t i , we introduce α t i,k-1 defined for any i ∈ [M ], any t ∈ [T ] and any k ∈ [K] by:</p><p>We hence have the following property for any i ∈ [M ] and any t ∈ [T ]:</p><p>• User-drift:</p><p>• Control lag:</p><p>Originality of the proof. The proof substantially differs form the proof by <ref type="bibr">Karimireddy et al. (2020b)</ref> in the convex case. Indeed, Karimireddy et al. (2020b) control a combination of the quadratic distance to the optimum and a control of the deviation between the controls and the gradients at the optimal point c t i -∇F i (x * ) . Leveraging such a quantity in our proof would result in a worse upper bound on the utility than the one we get, as either the noise added to ensure DP (if c 0 i is defined w.r.t. a noised gradient) or the heterogeneity (if c 0 i = 0) would also appear in the initial condition c t i -∇F i (x * ) . On the other hand, in our approach, we combine the quadratic distance to the optimum to a control of the lag and user-drift. In some sense this resembles some aspects of the proof in the non-convex regime in <ref type="bibr">(Karimireddy et al., 2020b)</ref>, in which the excess risk (F (x t )-F * ) is combined with the lag. Nevertheless, our result (in the convex case), strongly leverages the convexity of the function in the proof.</p><p>Details of the proof. The idea of the proof is to find a contraction inequality involving</p><p>To do so, we will first bound the variance of the server's update. Then we will see how the control lag evolves through the communication rounds. We will also bound the user drift. To make the proof more readable, the index t may be omitted on random variables when the only communication round that is considered is the t-th one.</p><p>Lemma C.4 (Variance of the server's update). ∀η ∈ [0, 1/ν]</p><p>Proof. We consider the model gap ∆x t = x t -x t-1 .</p><p>Simplifying ( <ref type="formula">16</ref>):</p><p>We then obtain the final result by dividing by ν on each side of the inequality.</p><p>Lemma C.8 (Convergence of DP-<software>SCAFFOLD</software>-warm with convex loss functions). There exist weights {w t } such that x T = t∈[T ] w t x t and: ),</p><p>, µl</p><p>, where</p><p>Proof. The result of Lemma C.8 is obtained by combining the contraction inequality from Lemma C.7 and the results from technical contraction results (Karimireddy et al., 2020b, Lemmas 1 and 2).</p><p>We then obtain the result of Theorem C.1 by setting η g := √ lM ≥ 1 and η l as low as possible.</p></div>
<div><head>C.4 Proof of Theorem C.1 (Non-Convex case)</head><p>To state this result, we adapt the original proof in the case with a larger variance for DP-noised stochastic gradients (see Lemma C.1), which gives the following result.</p><p>Lemma C.9 (Convergence of DP-<software>SCAFFOLD</software>-warm with non-convex loss functions). There exist weights {w t } such that x T = t∈[T ] w t x t and:</p><p>, where</p><p>We obtain the result of Theorem C.1 by setting η g := √ lM ≥ 1 and η l as low as possible.</p></div>
<div><head>C.5 Theorem of Convergence for DP-FedAvg</head><p>Theorem C.2 (Utility rates of DP-<software>FedAvg</software>(T, K, l, s, σ g , C), σ g chosen arbitrarily). Let σ g , C &gt; 0, x 0 ∈ R d . Suppose we run DP-<software ContextAttributes="used">FedAvg</software>(T, K, l, s, σ g , C) (see Algorithm 2). Under Assumptions 2 and 3, we consider the sequence of iterates (x t ) t≥0 of the algorithm, starting from x 0 .</p><p>, then there exist weights {w t } t∈[T ] such that the averaged output of DP-<software>FedAvg</software>(T, K, l, s, σ g , C), defined by x T = t∈[T ] w t x t , has expected excess of loss such that:</p><p>)νKηg and T ≥ 1, then there exist weights {w t } t∈[T ] such that the averaged output of DP-<software>FedAvg</software>(T, K, l, s, σ g , C), defined by x T = t∈[T ] w t x t , has expected excess of loss such that:</p><p>)νKηg and T ≥ 1, then there exist weights {w t } t∈[T ] such that the randomized output of DP-<software>SCAFFOLD</software>-warm(T, K, l, s, σ g , C), defined by {x T = x t with probability w t for all t}, has expected squared gradient of the loss such that:</p><p>where</p><p>Proof. To state the result of Theorem C.2, we combine the original result (Karimireddy et al., 2020b, Theorem V) provided for any type of loss functions with the result of Lemma C.1.</p></div>
<div><head>D ADDITIONAL EXPERIMENTS DETAILS AND RESULTS</head><p>In this section, we give additional details on our experimental setup (Section D.1) and synthetic data generation process (Section D.2), and provide additional results (Section D.3). All results are summarized in Table <ref type="table">3</ref>.  Hyperparameter tuning. We tuned the step-size hyperparameter η 0 for each dataset, each algorithm and each version (with or without DP) over a grid of 10 values with the lowest level of heterogeneity (5-fold cross validation conducted on the training set). We then kept the same η 0 for experiments with higher heterogeneity.</p><p>Clipping heuristic. Setting a good clipping threshold C while preserving accuracy can be difficult <ref type="bibr">(McMahan et al., 2018)</ref>. Indeed, if C is too small, the clipped gradients may become biased, thereby affecting the convergence rate. On the other hand, if C is too large, we have to add more noise to stochastic gradients to ensure differential privacy (since the variance of the Gaussian noise is proportional to C 2 ). In practice, we follow the strategy proposed by <ref type="bibr" target="#b0">Abadi et al. (2016)</ref>, which consists in setting C as the median of the norms of the unclipped gradients over each stage of local training. Throughout the iterations, C will then decrease. However, we are aware that locally setting C may leak information to the server about the magnitude of stochastic gradients. We here consider this leak as minor and neglect its impact on privacy guarantees. Adaptive clipping <ref type="bibr" target="#b2">(Andrew et al., 2021)</ref> could be used to mitigate these concerns.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep learning with differential privacy</title>
		<author>
			<persName><forename type="first">Martin</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Brendan Mcmahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Mironov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kunal</forename><surname>Talwar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 ACM SIGSAC conference on computer and communications security</title>
		<meeting>the 2016 ACM SIGSAC conference on computer and communications security</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="308" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Federated learning based on dynamic regularization</title>
		<author>
			<persName><forename type="first">Durmus</forename><surname>Alp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emre</forename><surname>Acar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ramon</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Mattina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Whatmough</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Venkatesh</forename><surname>Saligrama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Differentially private learning with adaptive clipping</title>
		<author>
			<persName><forename type="first">Galen</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Om</forename><surname>Thakkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">Brendan</forename><surname>Mcmahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Swaroop</forename><surname>Ramaswamy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The privacy blanket of the shuffle model</title>
		<author>
			<persName><forename type="first">Borja</forename><surname>Balle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrià</forename><surname>Gascón</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kobbi</forename><surname>Nissim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual International Cryptology Conference</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="638" to="667" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Personalized and private peer-topeer machine learning</title>
		<author>
			<persName><forename type="first">Aurélien</forename><surname>Bellet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rachid</forename><surname>Guerraoui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mahsa</forename><surname>Taziki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Tommasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="473" to="481" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Distributed differential privacy via shuffling</title>
		<author>
			<persName><forename type="first">Albert</forename><surname>Cheu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Ullman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Zeber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxim</forename><surname>Zhilyaev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual International Conference on the Theory and Applications of Cryptographic Techniques</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="375" to="403" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Emnist: Extending mnist to handwritten letters</title>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saeed</forename><surname>Afshar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Tapson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andre</forename><surname>Van Schaik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 international joint conference on neural networks (IJCNN)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2921" to="2926" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">C</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Local privacy and statistical minimax rates</title>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">C</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><forename type="middle">J</forename><surname>Wainwright</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 54th Annual Symposium on Foundations of Computer Science</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013. 2013</date>
			<biblScope unit="page" from="429" to="438" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Minimax optimal procedures for locally private estimation</title>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">C</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><forename type="middle">J</forename><surname>Wainwright</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">113</biblScope>
			<biblScope unit="issue">521</biblScope>
			<biblScope unit="page" from="182" to="201" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The algorithmic foundations of differential privacy</title>
		<author>
			<persName><forename type="first">Cynthia</forename><surname>Dwork</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends® in Theoretical Computer Science</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="211" to="407" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Boosting and differential privacy</title>
		<author>
			<persName><forename type="first">Cynthia</forename><surname>Dwork</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guy</forename><forename type="middle">N</forename><surname>Rothblum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Salil</forename><surname>Vadhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 51st Annual Symposium on Foundations of Computer Science</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010">2010. 2010</date>
			<biblScope unit="page" from="51" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Amplification by shuffling: From local to central differential privacy via anonymity</title>
		<author>
			<persName><forename type="first">Úlfar</forename><surname>Erlingsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vitaly</forename><surname>Feldman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Mironov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kunal</forename><surname>Ananth Raghunathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhradeep</forename><surname>Talwar</surname></persName>
		</author>
		<author>
			<persName><surname>Thakurta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirtieth Annual ACM-SIAM Symposium on Discrete Algorithms</title>
		<meeting>the Thirtieth Annual ACM-SIAM Symposium on Discrete Algorithms</meeting>
		<imprint>
			<publisher>SIAM</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2468" to="2479" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Personalized federated learning with theoretical guarantees: A model-agnostic metalearning approach</title>
		<author>
			<persName><forename type="first">Alireza</forename><surname>Fallah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aryan</forename><surname>Mokhtari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Asuman</forename><surname>Ozdaglar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="3557" to="3568" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Model inversion attacks that exploit confidence information and basic countermeasures</title>
		<author>
			<persName><forename type="first">Matt</forename><surname>Fredrikson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Somesh</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Ristenpart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGSAC conference on computer and communications security</title>
		<meeting>the 22nd ACM SIGSAC conference on computer and communications security</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1322" to="1333" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Inverting gradients-how easy is it to break privacy in federated learning? Advances in</title>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Geiping</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hartmut</forename><surname>Bauermeister</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannah</forename><surname>Dröge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Moeller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="16937" to="16947" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">Robin</forename><forename type="middle">C</forename><surname>Geyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tassilo</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moin</forename><surname>Nabi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.07557</idno>
		<title level="m">Differentially private federated learning: A client level perspective</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">Badih</forename><surname>Ghazi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rasmus</forename><surname>Pagh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ameya</forename><surname>Velingker</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.08320</idno>
		<title level="m">Scalable and differentially private distributed aggregation in the shuffled model</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Renyi differential privacy of the subsampled shuffle model in distributed learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Antonious</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deepesh</forename><surname>Girgis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suhas</forename><surname>Data</surname></persName>
		</author>
		<author>
			<persName><surname>Diggavi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Shuffled model of federated learning: Privacy, accuracy agnostic distributed multitask optimization under privacy constraints</title>
		<author>
			<persName><forename type="first">M</forename><surname>Antonious</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deepesh</forename><surname>Girgis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suhas</forename><surname>Data</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Diggavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ananda</forename><surname>Kairouz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suresh</forename><surname>Theertha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Membership inference attacks against machine learning models</title>
		<author>
			<persName><forename type="first">Reza</forename><surname>Shokri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Stronati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Congzheng</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vitaly</forename><surname>Shmatikov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Symposium on Security and Privacy (SP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Federated learning with bayesian differential privacy</title>
		<author>
			<persName><forename type="first">Aleksei</forename><surname>Triastcyn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boi</forename><surname>Faltings</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Conference on Big Data (Big Data)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2587" to="2596" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Rényi divergence and kullback-leibler divergence</title>
		<author>
			<persName><forename type="first">Tim</forename><surname>Van Erven</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Harremos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3797" to="3820" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">Jianyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gauri</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">Brendan</forename><surname>Mcmahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maruan</forename><surname>Al-Shedivat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Galen</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Salman</forename><surname>Avestimehr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katharine</forename><surname>Daly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deepesh</forename><surname>Data</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.06917</idno>
		<title level="m">A field guide to federated optimization</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Subsampled Rényi Differential Privacy and Analytical Moments Accountant</title>
		<author>
			<persName><forename type="first">Yu-Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Borja</forename><surname>Balle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiva</forename><surname>Kasiviswanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Privacy and Confidentiality</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Federated learning with differential privacy: Algorithms and performance analysis</title>
		<author>
			<persName><forename type="first">Kang</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Howard</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Farhad</forename><surname>Farokhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shi</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">S</forename><surname>Tony</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Quek</surname></persName>
		</author>
		<author>
			<persName><surname>Vincent Poor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Forensics and Security</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="3454" to="3469" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Local Differential Privacy based Federated Learning for Internet of Things</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengmeng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ning</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingjuan</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dusit</forename><surname>Niyato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kwok-Yan</forename><surname>Lam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Internet of Things Journal</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="8836" to="8853" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</teiCorpus></text>
</TEI>