<?xml version='1.0' encoding='utf-8'?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" version="1.1" xsi:schemaLocation="http://www.tei-c.org/ns/1.0 http://api.archives-ouvertes.fr/documents/aofr-sword.xsd">
  <teiHeader>
    <fileDesc>
      <titleStmt>
        <title>HAL TEI export of hal-03381732</title>
      </titleStmt>
      <publicationStmt>
        <distributor>CCSD</distributor>
        <availability status="restricted">
          <licence target="http://creativecommons.org/licenses/by/4.0/">Distributed under a Creative Commons Attribution 4.0 International License</licence>
        </availability>
        <date when="2024-04-25T22:54:42+02:00" />
      </publicationStmt>
      <sourceDesc>
        <p part="N">HAL API platform</p>
      </sourceDesc>
    </fileDesc>
  </teiHeader>
  <text>
    <body>
      <listBibl>
        <biblFull>
          <titleStmt>
            <title xml:lang="en">Exathlon: A Benchmark for Explainable Anomaly Detection over Time Series</title>
            <author role="aut">
              <persName>
                <forename type="first">Vincent</forename>
                <surname>Jacob</surname>
              </persName>
              <email type="md5">a3c25903698d699f5a483e2e518636d9</email>
              <email type="domain">polytechnique.edu</email>
              <idno type="idhal" notation="numeric">1122859</idno>
              <idno type="halauthorid" notation="string">129644-1122859</idno>
              <idno type="ORCID">https://orcid.org/0000-0002-7055-213X</idno>
              <idno type="IDREF">https://www.idref.fr/119707748</idno>
              <affiliation ref="#struct-451441" />
            </author>
            <author role="aut">
              <persName>
                <forename type="first">Fei</forename>
                <surname>Song</surname>
              </persName>
              <idno type="halauthorid">1492132-0</idno>
              <affiliation ref="#struct-451441" />
            </author>
            <author role="aut">
              <persName>
                <forename type="first">Arnaud</forename>
                <surname>Stiegler</surname>
              </persName>
              <idno type="halauthorid">1635067-0</idno>
              <affiliation ref="#struct-451441" />
            </author>
            <author role="aut">
              <persName>
                <forename type="first">Bijan</forename>
                <surname>Rad</surname>
              </persName>
              <idno type="halauthorid">2311919-0</idno>
              <affiliation ref="#struct-451441" />
            </author>
            <author role="aut">
              <persName>
                <forename type="first">Yanlei</forename>
                <surname>Diao</surname>
              </persName>
              <email type="md5">8e5957ff0be464645ecda731b674af7c</email>
              <email type="domain">polytechnique.edu</email>
              <idno type="idhal" notation="numeric">1052430</idno>
              <idno type="halauthorid" notation="string">1213328-1052430</idno>
              <affiliation ref="#struct-451441" />
            </author>
            <author role="aut">
              <persName>
                <forename type="first">Nesime</forename>
                <surname>Tatbul</surname>
              </persName>
              <email type="md5">6e0e500d31095ebf69a3be0715bd941a</email>
              <email type="domain">csail.mit.edu</email>
              <idno type="idhal" notation="numeric">1113814</idno>
              <idno type="halauthorid" notation="string">1231479-1113814</idno>
              <affiliation ref="#struct-118323" />
              <affiliation ref="#struct-1050757" />
            </author>
            <editor role="depositor">
              <persName>
                <forename>Vincent</forename>
                <surname>Jacob</surname>
              </persName>
              <email type="md5">c8e9e141d02252e3023fbdd4ed6454e2</email>
              <email type="domain">inria.fr</email>
            </editor>
          </titleStmt>
          <editionStmt>
            <edition n="v1" type="current">
              <date type="whenSubmitted">2021-10-17 20:16:15</date>
              <date type="whenModified">2024-01-29 14:51:16</date>
              <date type="whenReleased">2021-10-18 08:58:38</date>
              <date type="whenProduced">2021-07</date>
              <date type="whenEndEmbargoed">2021-10-17</date>
              <ref type="file" target="https://hal.science/hal-03381732/document">
                <date notBefore="2021-10-17" />
              </ref>
              <ref type="file" subtype="author" n="1" target="https://hal.science/hal-03381732/file/exathlon.pdf">
                <date notBefore="2021-10-17" />
              </ref>
            </edition>
            <respStmt>
              <resp>contributor</resp>
              <name key="997283">
                <persName>
                  <forename>Vincent</forename>
                  <surname>Jacob</surname>
                </persName>
                <email type="md5">c8e9e141d02252e3023fbdd4ed6454e2</email>
                <email type="domain">inria.fr</email>
              </name>
            </respStmt>
          </editionStmt>
          <publicationStmt>
            <distributor>CCSD</distributor>
            <idno type="halId">hal-03381732</idno>
            <idno type="halUri">https://hal.science/hal-03381732</idno>
            <idno type="halBibtex">jacob:hal-03381732</idno>
            <idno type="halRefHtml">&lt;i&gt;Proceedings of the VLDB Endowment (PVLDB)&lt;/i&gt;, 2021</idno>
            <idno type="halRef">Proceedings of the VLDB Endowment (PVLDB), 2021</idno>
          </publicationStmt>
          <seriesStmt>
            <idno type="stamp" n="X">Ecole Polytechnique</idno>
            <idno type="stamp" n="CNRS">CNRS - Centre national de la recherche scientifique</idno>
            <idno type="stamp" n="INRIA">INRIA - Institut National de Recherche en Informatique et en Automatique</idno>
            <idno type="stamp" n="LIX">Laboratoire d'informatique de l'école polytechnique</idno>
            <idno type="stamp" n="INRIA-SACLAY" corresp="INRIA">INRIA Saclay - Ile de France</idno>
            <idno type="stamp" n="X-LIX" corresp="X">Laboratoire d'informatique de l'X (LIX)</idno>
            <idno type="stamp" n="X-DEP" corresp="X">Polytechnique</idno>
            <idno type="stamp" n="X-DEP-INFO" corresp="X-DEP">Département d'informatique</idno>
            <idno type="stamp" n="INRIA_TEST">INRIA - Institut National de Recherche en Informatique et en Automatique</idno>
            <idno type="stamp" n="TESTALAIN1">TESTALAIN1</idno>
            <idno type="stamp" n="INRIA2">INRIA 2</idno>
            <idno type="stamp" n="IP_PARIS">Institut Polytechnique de Paris</idno>
            <idno type="stamp" n="GS-COMPUTER-SCIENCE">Graduate School Computer Science</idno>
            <idno type="stamp" n="INRIA-ETATSUNIS">Copublications Inria-Etats-Unis</idno>
          </seriesStmt>
          <notesStmt>
            <note type="audience" n="2">International</note>
            <note type="popular" n="0">No</note>
            <note type="peer" n="1">Yes</note>
          </notesStmt>
          <sourceDesc>
            <biblStruct>
              <analytic>
                <title xml:lang="en">Exathlon: A Benchmark for Explainable Anomaly Detection over Time Series</title>
                <author role="aut">
                  <persName>
                    <forename type="first">Vincent</forename>
                    <surname>Jacob</surname>
                  </persName>
                  <email type="md5">a3c25903698d699f5a483e2e518636d9</email>
                  <email type="domain">polytechnique.edu</email>
                  <idno type="idhal" notation="numeric">1122859</idno>
                  <idno type="halauthorid" notation="string">129644-1122859</idno>
                  <idno type="ORCID">https://orcid.org/0000-0002-7055-213X</idno>
                  <idno type="IDREF">https://www.idref.fr/119707748</idno>
                  <affiliation ref="#struct-451441" />
                </author>
                <author role="aut">
                  <persName>
                    <forename type="first">Fei</forename>
                    <surname>Song</surname>
                  </persName>
                  <idno type="halauthorid">1492132-0</idno>
                  <affiliation ref="#struct-451441" />
                </author>
                <author role="aut">
                  <persName>
                    <forename type="first">Arnaud</forename>
                    <surname>Stiegler</surname>
                  </persName>
                  <idno type="halauthorid">1635067-0</idno>
                  <affiliation ref="#struct-451441" />
                </author>
                <author role="aut">
                  <persName>
                    <forename type="first">Bijan</forename>
                    <surname>Rad</surname>
                  </persName>
                  <idno type="halauthorid">2311919-0</idno>
                  <affiliation ref="#struct-451441" />
                </author>
                <author role="aut">
                  <persName>
                    <forename type="first">Yanlei</forename>
                    <surname>Diao</surname>
                  </persName>
                  <email type="md5">8e5957ff0be464645ecda731b674af7c</email>
                  <email type="domain">polytechnique.edu</email>
                  <idno type="idhal" notation="numeric">1052430</idno>
                  <idno type="halauthorid" notation="string">1213328-1052430</idno>
                  <affiliation ref="#struct-451441" />
                </author>
                <author role="aut">
                  <persName>
                    <forename type="first">Nesime</forename>
                    <surname>Tatbul</surname>
                  </persName>
                  <email type="md5">6e0e500d31095ebf69a3be0715bd941a</email>
                  <email type="domain">csail.mit.edu</email>
                  <idno type="idhal" notation="numeric">1113814</idno>
                  <idno type="halauthorid" notation="string">1231479-1113814</idno>
                  <affiliation ref="#struct-118323" />
                  <affiliation ref="#struct-1050757" />
                </author>
              </analytic>
              <monogr>
                <idno type="halJournalId" status="VALID">67527</idno>
                <idno type="issn">2150-8097</idno>
                <idno type="eissn">2150-8097</idno>
                <title level="j">Proceedings of the VLDB Endowment (PVLDB)</title>
                <imprint>
                  <publisher>VLDB Endowment</publisher>
                  <date type="datePub">2021-07</date>
                </imprint>
              </monogr>
            </biblStruct>
          </sourceDesc>
          <profileDesc>
            <langUsage>
              <language ident="en">English</language>
            </langUsage>
            <textClass>
              <classCode scheme="halDomain" n="info.info-ai">Computer Science [cs]/Artificial Intelligence [cs.AI]</classCode>
              <classCode scheme="halDomain" n="info.info-lg">Computer Science [cs]/Machine Learning [cs.LG]</classCode>
              <classCode scheme="halDomain" n="info.info-ne">Computer Science [cs]/Neural and Evolutionary Computing [cs.NE]</classCode>
              <classCode scheme="halDomain" n="info.info-db">Computer Science [cs]/Databases [cs.DB]</classCode>
              <classCode scheme="halDomain" n="info.info-it">Computer Science [cs]/Information Theory [cs.IT]</classCode>
              <classCode scheme="halTypology" n="ART">Journal articles</classCode>
              <classCode scheme="halOldTypology" n="ART">Journal articles</classCode>
              <classCode scheme="halTreeTypology" n="ART">Journal articles</classCode>
            </textClass>
            <abstract xml:lang="en">
              <p>Access to high-quality data repositories and benchmarks have been instrumental in advancing the state of the art in many experimental research domains. While advanced analytics tasks over time series data have been gaining lots of attention, lack of such community resources severely limits scientific progress. In this paper, we present Exathlon, the first comprehensive public benchmark for explainable anomaly detection over high-dimensional time series data. Exathlon has been systematically constructed based on real data traces from repeated executions of large-scale stream processing jobs on an Apache Spark cluster. Some of these executions were intentionally disturbed by introducing instances of six different types of anomalous events (e.g., misbehaving inputs, resource contention, process failures). For each of the anomaly instances, ground truth labels for the root cause interval as well as those for the extended effect interval are provided, supporting the development and evaluation of a wide range of anomaly detection (AD) and explanation discovery (ED) tasks. We demonstrate the practical utility of Exathlon's dataset, evaluation methodology, and end-toend data science pipeline design through an experimental study with three state-of-the-art AD and ED techniques.</p>
            </abstract>
          </profileDesc>
        </biblFull>
      </listBibl>
    </body>
    <back>
      <listOrg type="structures">
        <org type="researchteam" xml:id="struct-451441" status="VALID">
          <idno type="RNSR">201622056J</idno>
          <orgName>Rich Data Analytics at Cloud Scale</orgName>
          <orgName type="acronym">CEDAR</orgName>
          <date type="start">2016-01-01</date>
          <desc>
            <address>
              <addrLine>1 rue Honoré d'Estienne d'OrvesBâtiment Alan TuringCampus de l'École Polytechnique91120 Palaiseau</addrLine>
              <country key="FR" />
            </address>
          </desc>
          <listRelation>
            <relation active="#struct-2071" type="direct" />
            <relation active="#struct-300340" type="indirect" />
            <relation name="UMR7161" active="#struct-441569" type="indirect" />
            <relation active="#struct-118511" type="direct" />
            <relation active="#struct-300009" type="indirect" />
          </listRelation>
        </org>
        <org type="institution" xml:id="struct-118323" status="VALID">
          <orgName>Intel Corporation [USA]</orgName>
          <desc>
            <address>
              <country key="US" />
            </address>
          </desc>
        </org>
        <org type="laboratory" xml:id="struct-1050757" status="VALID">
          <orgName>MIT Computer Science &amp; Artificial Intelligence Lab</orgName>
          <orgName type="acronym">MIT CSAIL</orgName>
          <desc>
            <address>
              <country key="US" />
            </address>
            <ref type="url">https://www.csail.mit.edu</ref>
          </desc>
          <listRelation>
            <relation active="#struct-301950" type="direct" />
          </listRelation>
        </org>
        <org type="laboratory" xml:id="struct-2071" status="VALID">
          <idno type="IdRef">196509955</idno>
          <idno type="RNSR">200519331V</idno>
          <orgName>Laboratoire d'informatique de l'École polytechnique [Palaiseau]</orgName>
          <orgName type="acronym">LIX</orgName>
          <desc>
            <address>
              <addrLine>Route de Saclay 91128 PALAISEAU CEDEX</addrLine>
              <country key="FR" />
            </address>
            <ref type="url">https://www.lix.polytechnique.fr/</ref>
          </desc>
          <listRelation>
            <relation active="#struct-300340" type="direct" />
            <relation name="UMR7161" active="#struct-441569" type="direct" />
          </listRelation>
        </org>
        <org type="institution" xml:id="struct-300340" status="VALID">
          <idno type="IdRef">027309320</idno>
          <idno type="ROR">https://ror.org/05hy3tk52</idno>
          <orgName>École polytechnique</orgName>
          <orgName type="acronym">X</orgName>
          <date type="start">1794-03-11</date>
          <desc>
            <address>
              <addrLine>École polytechnique, 91128 Palaiseau Cedex</addrLine>
              <country key="FR" />
            </address>
            <ref type="url">https://www.polytechnique.edu/</ref>
          </desc>
        </org>
        <org type="regroupinstitution" xml:id="struct-441569" status="VALID">
          <idno type="IdRef">02636817X</idno>
          <idno type="ISNI">0000000122597504</idno>
          <idno type="ROR">https://ror.org/02feahw73</idno>
          <orgName>Centre National de la Recherche Scientifique</orgName>
          <orgName type="acronym">CNRS</orgName>
          <date type="start">1939-10-19</date>
          <desc>
            <address>
              <country key="FR" />
            </address>
            <ref type="url">https://www.cnrs.fr/</ref>
          </desc>
        </org>
        <org type="laboratory" xml:id="struct-118511" status="VALID">
          <idno type="RNSR">200818248E</idno>
          <idno type="ROR">https://ror.org/0315e5x55</idno>
          <orgName>Inria Saclay - Ile de France</orgName>
          <desc>
            <address>
              <addrLine>1 rue Honoré d'Estienne d'OrvesBâtiment Alan TuringCampus de l'École Polytechnique91120 Palaiseau</addrLine>
              <country key="FR" />
            </address>
            <ref type="url">http://www.inria.fr/centre/saclay</ref>
          </desc>
          <listRelation>
            <relation active="#struct-300009" type="direct" />
          </listRelation>
        </org>
        <org type="institution" xml:id="struct-300009" status="VALID">
          <idno type="ROR">https://ror.org/02kvxyf05</idno>
          <orgName>Institut National de Recherche en Informatique et en Automatique</orgName>
          <orgName type="acronym">Inria</orgName>
          <desc>
            <address>
              <addrLine>Domaine de VoluceauRocquencourt - BP 10578153 Le Chesnay Cedex</addrLine>
              <country key="FR" />
            </address>
            <ref type="url">http://www.inria.fr/en/</ref>
          </desc>
        </org>
        <org type="institution" xml:id="struct-301950" status="VALID">
          <idno type="ROR">https://ror.org/042nb2s44</idno>
          <orgName>Massachusetts Institute of Technology</orgName>
          <orgName type="acronym">MIT</orgName>
          <desc>
            <address>
              <addrLine>77 Massachusetts Ave, Cambridge, MA 02139</addrLine>
              <country key="US" />
            </address>
            <ref type="url">http://web.mit.edu</ref>
          </desc>
        </org>
      </listOrg>
    </back>
  <teiCorpus>
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Exathlon: A Benchmark for Explainable Anomaly Detection over Time Series</title>
				<funder ref="#_7GU9aWj">
					<orgName type="full">European Research Council</orgName>
					<orgName type="abbreviated">ERC</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher />
				<availability status="unknown"><licence /></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Vincent</forename><surname>Jacob</surname></persName>
							<email>vincent.jacob@polytechnique.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Ecole Polytechnique</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Fei</forename><surname>Song</surname></persName>
							<email>fei.song@polytechnique.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Ecole Polytechnique</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Arnaud</forename><surname>Stiegler</surname></persName>
							<email>arnaud.stiegler@polytechnique.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Ecole Polytechnique</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Bijan</forename><surname>Rad</surname></persName>
							<email>bijan.rad@polytechnique.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Ecole Polytechnique</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yanlei</forename><surname>Diao</surname></persName>
							<email>yanlei.diao@polytechnique.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Ecole Polytechnique</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Nesime</forename><surname>Tatbul</surname></persName>
							<email>tatbul@csail.mit.edu</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Intel Labs and MIT</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Exathlon: A Benchmark for Explainable Anomaly Detection over Time Series</title>
					</analytic>
					<monogr>
						<imprint>
							<date />
						</imprint>
					</monogr>
					<idno type="MD5">4D04200E718AFF0AE9DA01E584273E15</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-04-12T14:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid" />
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div><p>Access to high-quality data repositories and benchmarks have been instrumental in advancing the state of the art in many experimental research domains. While advanced analytics tasks over time series data have been gaining lots of attention, lack of such community resources severely limits scientific progress. In this paper, we present <software>Exathlon</software>, the first comprehensive public benchmark for explainable anomaly detection over high-dimensional time series data. <software ContextAttributes="created">Exathlon</software> has been systematically constructed based on real data traces from repeated executions of large-scale stream processing jobs on an <software ContextAttributes="created">Apache</software> <software ContextAttributes="created">Spark</software> cluster. Some of these executions were intentionally disturbed by introducing instances of six different types of anomalous events (e.g., misbehaving inputs, resource contention, process failures). For each of the anomaly instances, ground truth labels for the root cause interval as well as those for the extended effect interval are provided, supporting the development and evaluation of a wide range of anomaly detection (AD) and explanation discovery (ED) tasks. We demonstrate the practical utility of <software ContextAttributes="created">Exathlon</software>'s dataset, evaluation methodology, and end-toend data science pipeline design through an experimental study with three state-of-the-art AD and ED techniques.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div><head n="1">INTRODUCTION</head><p>Time series is one of the most ubiquitous types of data in our increasingly digital and connected society. Advanced analytics capabilities such as detecting anomalies and explaining them are crucial in understanding and reacting to temporal phenomena captured by this rich data type. Anomaly detection (AD) refers to the task of identifying patterns in data that deviate from a given notion of normal behavior <ref type="bibr" target="#b12">[13]</ref>. It finds use in almost every domain where data is plenty, but unusual patterns are the most critical to respond (e.g., cloud telemetry, autonomous driving, financial fraud management). AD over time series data has been of particular interest, not only because time-oriented data is highly prevalent and voluminous, but also more challenging to analyze due to its complex and diverse nature: multivariate time series can consist of 1000s of dimensions; anomalous patterns may be of arbitrary length and shape; there may be intricate cause and effect relationships among these patterns; data is rarely clean. Furthermore, by helping uncover how or why a detected anomaly may have happened, explanation discovery (ED) forms a crucial capability for any time series AD system.</p><p>Recent advances in data science and machine learning (ML) significantly reinforced the need for developing robust anomaly detection and explanation solutions that can be reliably deployed in production environments <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b37">38]</ref>. However, progress has been rather slow and limited. While there is extensive research activity going on <ref type="bibr" target="#b11">[12]</ref>, proposed solutions have been mostly adhoc and far from being generalizable to realistic settings. We believe that one of the critical roadblocks to progress has been the lack of open data repositories and benchmarks to serve as a common ground for reproducible research and experimentation. Indeed, access to such community resources has been instrumental in advancing the state of the art in many other domains (e.g., <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b18">19]</ref>). Inspired by those efforts, in this paper, we propose <software ContextAttributes="created">Exathlon</software>, the first comprehensive public benchmark for explainable anomaly detection over high-dimensional time series data.</p><p><software ContextAttributes="created">Exathlon</software> focuses on the familiar domain of metric monitoring in large-scale computing systems, and provides a benchmarking platform that consists of: (i) a curated anomaly dataset, (ii) a novel benchmarking methodology for AD and ED, and (iii) an end-to-end data science pipeline for implementing and evaluating AD and ED algorithms based on the provided dataset and methodology. More specifically, we make the following contributions in this work: Dataset. We constructed <software ContextAttributes="created">Exathlon</software> systematically based on real data traces collected from around 100 repeated executions of 10 distributed streaming jobs on a <software ContextAttributes="created">Spark</software> cluster over 2.5 months. Inspired by chaos engineering in industry <ref type="bibr" target="#b4">[5]</ref>, our traces were obtained by disturbing more than 30 job executions with nearly 100 instances of 6 different classes of anomalous events (e.g., misbehaving inputs, resource contention, process failures). For each of these anomalies, we provide ground truth labels for both the root cause interval and the corresponding effect interval, enabling the use of our dataset in a wide range of AD and ED tasks. Overall, both the normal (undisturbed) and anomalous (disturbed) traces contain enough variety (including some noise due to <software ContextAttributes="created">Spark</software>'s inherent behavior) to capture real-world data characteristics in this domain (Table <ref type="table" target="#tab_0">1</ref>). Evaluation Methodology. <software ContextAttributes="created">Exathlon</software> evaluates AD and ED algorithms in terms of two orthogonal aspects: functionality and computational performance. For AD, we primarily target semi-supervised techniques (i.e., with a model developed/trained using only normal data, possibly with occasional noise, and then tested against anomalous data) for range-based anomalies (i.e., contextual and collective anomalies occurring over a time interval instead of only at a single time point <ref type="bibr" target="#b12">[13]</ref>) over high-dimensional time series (i.e., multivariate with 1000s of dimensions). This decision is informed by our observation of this being the most common and inclusive usage scenario in practice. For ED, we broadly consider both modelfree (e.g., <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b57">58]</ref>) and model-dependent (e.g., <ref type="bibr" target="#b43">[44]</ref>) techniques. AD functionality is evaluated under four well-defined model learning settings, based on four key evaluation criteria -anomaly existence, range detection, early detection, and exactly-once detection -using a novel range-based accuracy framework <ref type="bibr" target="#b50">[51]</ref>. Similarly, ED functionality is tested for two capabilities -local explanation and global explanation -each measured in terms of conciseness, consistency, and accuracy. Computational efficiency and scalability for both AD model training/inference as well as for ED execution can also be evaluated at varying data dimensions and sampling rates. Overall, <software ContextAttributes="created">Exathlon</software> provides a rich and challenging testbed with a well-organized evaluation methodology (Table <ref type="table" target="#tab_1">2</ref>). Data Science Pipeline. We designed an end-to-end pipeline for explainable time series anomaly detection. This pipeline includes all the data processing steps necessary to turn our raw datasets into AD and ED results together with their benchmark scores. Our design is modular and extensible. This not only makes it easy to implement new AD and ED techniques to benchmark, but also allows creating multiple variants of pipeline steps to experiment with and compare. For example, training data preparation for different AD learning settings or scoring AD results for different criteria levels can be easily configured, run, and compared in our pipeline (Figure <ref type="figure" target="#fig_4">3</ref>). Experimental Study. We provide the first experimental study evaluating a representative set of state-of-the-art AD and ED techniques to illustrate the usage and benefits of <software ContextAttributes="created">Exathlon</software>. Results suggest that our dataset carries useful signals that can be picked up by the tested AD and ED algorithms in a way that can be effectively quantified by our evaluation criteria and metrics. Furthermore, we observe that our benchmarking framework exposes increasing levels of challenges to stress-test these algorithms in a systematic way ( §6).</p><p>Compared to current public resources for time series AD research <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b42">43]</ref>, a key contribution of <software ContextAttributes="created">Exathlon</software> is that it comprehensively covers one challenging application domain end to end, as opposed to providing multiple smaller and simpler datasets from several independent domains. Furthermore, a public benchmark for time series ED research with ground truth labels is largely lacking today, making it hard to evaluate and compare an increasing number of published papers on this important topic. Thus, we believe <software ContextAttributes="created">Exathlon</software> provides an opportunity for a more in-depth investigation and evaluation of models and algorithms in both time series AD and ED, potentially revealing new insights for accelerating research progress in explainable anomaly detection.</p><p>In the rest of the paper, we first briefly summarize related work. After presenting our dataset, evaluation methodology, and pipeline design in more detail, we show the practical utility of <software>Exathlon</software> through an experimental analysis of three state-of-the-art AD and ED algorithms using a selected set of evaluation criteria and settings from our benchmark. Finally, we conclude with an outline of future directions. The dataset, code, and documentation for <software ContextAttributes="created">Exathlon</software> are publicly available at https://github.com/exathlonbenchmark/ exathlon.</p></div>
<div><head n="2">RELATED WORK</head><p>Datasets and Benchmarks. Benchmarks to evaluate database (DB) system performance have been around for more than 30 years <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b25">26]</ref>. In addition to industry-standard benchmarks for relational DB workloads such as TPC-C and TPC-H, new domainspecific benchmarks for emerging workloads have been proposed (e.g., Linear Road Benchmark for stream processing <ref type="bibr" target="#b0">[1]</ref>, YCSB for scalable key-value stores <ref type="bibr" target="#b14">[15]</ref>, <software ContextAttributes="created">BigBench</software> for big data analytics <ref type="bibr" target="#b24">[25]</ref>). The main focus of these benchmarks has been on computational performance. With recent benchmarks for ML/DL-based advanced data analytics such as ADABench and DAWNBench, there has been a focus shift toward end-to-end ML pipelines and new evaluation metrics such as time to accuracy <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b41">42]</ref>. Like in DB and systems communities, the ML community has also been publishing datasets and benchmarks to support research in many problem domains from object recognition to natural language processing <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b52">53]</ref>. Well-known data archives for time series research include: UCI <ref type="bibr" target="#b19">[20]</ref>, UCR <ref type="bibr" target="#b17">[18]</ref>, and UEA <ref type="bibr" target="#b1">[2]</ref>. These archives provide real-world data collections created for general ML tasks, such as classification and clustering. While the need for systematically constructing AD benchmarks from real data has also been recognized by others <ref type="bibr" target="#b21">[22]</ref>, public availability of anomaly datasets is still limited <ref type="bibr" target="#b42">[43]</ref>. To our knowledge, Numenta Anomaly Benchmark (NAB) is the only public benchmark designed for time series AD <ref type="bibr" target="#b33">[34]</ref>. NAB provides 50+ real and artificial datasets, primarily focusing on real-time AD for streaming data. Compared to ours, each of these datasets is much smaller in scale and dimensionality, and does not capture any information to enable ED. NAB also has several technical weaknesses that hinder its use in practice (e.g., ambiguities in its scoring function, missing values in its datasets) <ref type="bibr" target="#b47">[48]</ref>.</p></div>
<div><head>Anomaly Detection (AD).</head><p>There is a long history of research in AD <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b26">27]</ref>. The high degree of diversity in data characteristics, anomaly types, and application domains has led to a plethora of AD approaches from simple statistical methods <ref type="bibr" target="#b6">[7]</ref> to distancebased <ref type="bibr" target="#b51">[52]</ref>, density-based <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b12">13]</ref>, and isolation forests <ref type="bibr" target="#b35">[36]</ref> to deep learning (DL) methods <ref type="bibr" target="#b11">[12]</ref>. It is beyond the scope of this paper to provide a complete survey; we refer the reader to recent survey papers for a full discussion of such methods <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b26">27]</ref>. In our experimental study, we particularly focus on three DL methods that represent the recent state of the art <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b55">56]</ref> (detailed in §6). Such DL methods have the potential to handle a variety of anomaly patterns, such as contextual and collective patterns <ref type="bibr" target="#b12">[13]</ref>, and overcome known limitations of previous density-and distancebased methods that are very sensitive to data dimensions. Interpretable Machine Learning. Interpretable ML has recently attracted a lot of attention <ref type="bibr" target="#b40">[41]</ref>. Relevant techniques generally belong to two broad families: interpretable models and model-agnostic methods. Interpretable models directly build a human-readable model from the data (e.g., linear or logistic regression, decision trees or rules) <ref type="bibr" target="#b40">[41]</ref>. In contrast, model-agnostic methods separate explanations from the ML model, hence offering the flexibility to mix and match ML models with interpretation methods. In the modelagnostic family, several methods obtain interpretable classifiers by perturbing the inputs and observing the response <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b49">50]</ref>. LIME <ref type="bibr" target="#b43">[44]</ref> explains a prediction of any classifier by approximating it locally with an interpretable sparse linear model, and explains the overall model by selecting a set of representative instances with explanations. As such, it is generally considered a method for local explanations. We evaluate LIME in our experimental study. Anchors <ref type="bibr" target="#b44">[45]</ref> improved upon LIME by replacing its linear model with a logical rule for explaining a data instance. It offers better coverage of data points in a local neighborhood, but does not support time series data. SHAP scores <ref type="bibr" target="#b36">[37]</ref>, RESP scores <ref type="bibr" target="#b5">[6]</ref>, and axiomatic attribution <ref type="bibr" target="#b49">[50]</ref> are also instance-level explanations that assign a numerical score to each feature, representing their importance in the outcome. In contrast to local explanations, other work aims to explain a model via global explanations. Some of them approximate a DL model using a decision tree <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b54">55]</ref>, or by learning a decision set <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b32">33]</ref> directly as explainable models. All of these methods suffer from lacking a benchmark dataset and evaluation methodology. The FICO challenge was designed to evaluate such methods using a home loan application dataset, with a known label (high or low risk) for each application, but it relies on manual evaluation of returned explanations by real-world data scientists <ref type="bibr" target="#b22">[23]</ref>. As a result, it remains hard to compare different ED methods due to the lack of ground truth explanations and automated evaluation procedures. Explaining Outliers in Data Streams. There is a handful of work in explaining outliers in data streams. Given normal and abnormal time periods by the user, EXstream finds explanations to best distinguish the abnormal periods from the normal ones <ref type="bibr" target="#b57">[58]</ref>. MacroBase helps the user prioritize attention over data streams, with modules for both AD and ED tasks <ref type="bibr" target="#b2">[3]</ref>. Its AD module uses simple statistical methods like MAD, which is known to be suitable only for detecting simple point outliers <ref type="bibr" target="#b12">[13]</ref>. For a detected anomaly, MacroBase's ED module discovers an explanation in the form of conjunctive predicates, by using a frequent itemset mining framework that takes minimum support and risk ratio as input parameters. We evaluate both of these techniques in our experimental study. Explaining Outliers in SQL Query Results. Scorpion explains outliers in group-by aggregate queries by searching through various subsets of the tuples that were used to compute the query answers <ref type="bibr" target="#b53">[54]</ref>. Given a set of explanation templates by the user, Roy et al. 's approach performs precomputation in a given DB to enable interactive ED <ref type="bibr" target="#b45">[46]</ref>. Similarly, given a table, El Gebaly et al. 's work constructs an explanation table and finds patterns affecting a binary value of each tuple <ref type="bibr" target="#b20">[21]</ref>. These approaches target traditional DB workloads and are not applicable to our problem. There have been recent industrial efforts on time series anomaly explanation and root cause analysis in DB systems <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b37">38]</ref>. These approaches require a variety of inputs from the user, e.g., causal hypotheses <ref type="bibr" target="#b29">[30]</ref> or labels of root causes <ref type="bibr" target="#b37">[38]</ref>, whereas our work focuses on semi-supervised learning for explainable AD. Moreover, these systems are largely based on proprietary code and datasets that are not accessible to the research community.</p></div>
<div><head n="3">DATASET</head><p>The <software>Exathlon</software> dataset has been systematically constructed based on real data traces collected from a use case scenario that we implemented on <software ContextAttributes="created">Apache Spark</software>. In this section, we first describe this scenario, followed by the details of how we created the normal and anomalous data traces themselves.</p></div>
<div><head n="3.1">Use Case: Spark Application Monitoring</head><p>Large-scale data analytics applications are deployed on <software ContextAttributes="created">Apache</software> <software ContextAttributes="created">Spark</software> clusters everyday. Monitoring the execution of these jobs to ensure their correct and timely completion via AD can be businesscritical. For example, some of the largest e-commerce platforms run <software ContextAttributes="created">Spark</software> jobs on petabytes of data each day to analyze purchase patterns, target offers, and enhance customer experiences <ref type="bibr" target="#b48">[49]</ref>. Since results of these jobs affect immediate business decisions such as inventory management and sales strategies, they are often specified with deadlines. Anomalies that occur in job execution prevent analytical jobs from meeting their deadlines and hence cause disruption to critical operations on those e-commerce platforms. We model this widespread and challenging AD use case in our benchmark. System Setup. Our <software ContextAttributes="created">Spark</software> workload consists of 10 stream processing applications (detailed in our technical report <ref type="bibr" target="#b28">[29]</ref>), analyzing user click streams from the WorldCup 1998 website <ref type="bibr" target="#b34">[35]</ref>. As in Figure <ref type="figure" target="#fig_1">1</ref>(a), Data Sender servers send streams at a controlled input rate to a <software ContextAttributes="created">Spark</software> cluster of 4 nodes, each with 2 Intel ® Xeon ® Gold 6130 16core processors, 768GB of memory, and 64TB disk. Each application has certain workload characteristics (e.g., CPU or I/O intensive) and is executed by <software ContextAttributes="created">Spark</software> in a distributed manner, as in Figure <ref type="figure" target="#fig_1">1</ref>(b). Submitted an application, <software ContextAttributes="created">Spark</software> launches a Driver process to coordinate the execution. The driver connects to a resource manager (<software ContextAttributes="created">Apache Hadoop YARN</software>), which launches Executor processes on a subset of nodes where tasks (units of work on a data partition, e.g., map or reduce) will be executed in parallel. Given 32 cores, each node can also run tasks from multiple applications concurrently. As common real-world practice, we run 5/10 randomly selected applications at a time. The placement of Driver and Executor processes to cluster nodes is decided by YARN based on data locality, load on nodes, etc. Except for I/O activities, YARN offers container isolation for resource usage of all parallel processes. Trace Collection. We ran the 10 <software ContextAttributes="created">Spark</software> streaming applications in our 4-node cluster over a 2.5-month period. The data collected from each run of a <software ContextAttributes="created">Spark</software> streaming application is called a Trace. Some of the traces were manually pruned, because they were affected by cluster downtimes or the injected anomalies were not well reflected in the data due to failed attempts. After this manual pruning, we kept 93 traces to constitute the <software ContextAttributes="created">Exathlon</software> dataset. Metrics Collected. During each application execution, we collected metrics from both the <software ContextAttributes="created">Spark</software> Monitoring and Instrumentation Interface (UI) and underlying operating system (OS). Table <ref type="table" target="#tab_0">1</ref>(a) gives a summary of the metrics collected per trace. The Driver offers 243 <software ContextAttributes="created">Spark</software> UI metrics covering scheduling delay, statistics on the streaming data received and processed, etc. Each executor provides 140 metrics on various time measurements, data sizes, network traffic, as well as memory and I/O activities. As we wanted to keep the number of metrics the same for all traces, we set a fixed limit of 5 for the number of <software ContextAttributes="created">Spark</software> executors (3 active + 2 backup). This way, even if an active executor fails during a run and a backup takes over, the number of metrics collected stays the same, 5 × 140 = 700, with null values set for inactive executors. 335 OS metrics for each of the 4 cluster nodes were collected using the Nmon command, capturing CPU time, network traffic, memory usage, etc. All in all, each trace consists of a total of 2,283 metrics recorded each second for 7 hours on average, constituting a multi-dimensional time series.</p></div>
<div><head n="3.2">Undisturbed vs. Disturbed Traces</head><p>In generating our traces, we followed an approach similar to chaos engineering (i.e., an approach devised by high-tech companies like Netflix for injecting failures and workload surges into a production system to verify/improve its reliability) <ref type="bibr" target="#b4">[5]</ref> and general systems monitoring (e.g., Microsoft's <software ContextAttributes="created">NetMedic</software> <ref type="bibr" target="#b30">[31]</ref>). Thus, we first generated undisturbed traces to characterize the normal execution behavior of our <software ContextAttributes="created">Spark</software> cluster; we then introduced various anomalous events to generate disturbed traces. Table <ref type="table" target="#tab_0">1</ref>(b) provides an overview. Undisturbed Traces. Uninterrupted executions of 5/10 randomly selected applications at a time, at parameter settings within the capacity limits of our <software ContextAttributes="created">Spark</software> cluster, over a period of 1 month, gave us 59 normal traces of 15.3GB in size. Any instances of occasional   cluster downtime were manually removed from these traces. It is important to note that, although undisturbed, these traces still exhibit occasional variations in metrics due to <software ContextAttributes="created">Spark</software>'s inherent system mechanisms (e.g., checkpointing, CPU usage by a DataNode in the distributed file system). Since such variations do appear in almost every trace, we consider them as part of the normal system behavior. In other words, our normal data traces include some "noise", as most real-world datasets typically do. Disturbed Traces. Disturbed traces were obtained by introducing anomalous events during an execution. Based on discussions with industry contacts from the <software ContextAttributes="created">Spark</software> ecosystem, we came up with 6 types of anomalous events. When designing these, we considered that: (i) they lead to a visible effect in the trace, (ii) they do not lead to an instant crash of the application (since AD would be of little help in this case), (iii) they can be tracked back to their root causes. We briefly describe these anomalies below; please refer to our report for further details. Bursty Input (Type 1): To mimic input rate spikes, we ran a disruptive event generator (DEG) on the Data Senders to temporarily increase the input rate by a given factor for a duration of 15-30 minutes. We repeated this pattern multiple times during a given trace, creating a total of 29 instances of this anomaly type over 6 different traces. Please see Figure <ref type="figure" target="#fig_1">1</ref>(c) for an example. Bursty Input Until Crash (Type 2): This is a longer version of Type 1 anomalies, where we let the DEG period last forever, crashing the executors due to lack of memory. When an executor crashes, <software ContextAttributes="created">Spark</software> launches a replacement, but the sustained high rates keeps crashing the executors, until <software ContextAttributes="created">Spark</software> eventually decides to kill the whole application. We injected this anomaly into 7 different traces. Stalled Input (Type 3): This type of anomaly mimics failures of <software ContextAttributes="created">Spark</software> data sources (e.g., Kafka or HDFS). To create it, we ran a DEG that set the input rates to 0 for about 15 minutes, and then periodically repeated this pattern every few hours, giving us a total of 16 anomaly instances across 4 different traces. CPU Contention (Type 4): The YARN resource manager cannot prevent external programs from using the CPU cores that it has allocated to <software ContextAttributes="created">Spark</software> processes, causing scheduling delays to build up due to CPU contention. We reproduced this anomaly using a DEG that ran Python <software ContextAttributes="created">programs</software> to consume all CPU cores on a given <software ContextAttributes="created">Spark</software> node. We created 26 such anomaly instances over 6 different traces. Driver Failure (Type 5) and Executor Failure (Type 6): Hardware faults or maintenance operations may cause a node to fail all of a sudden, making all processes (drivers and/or executors) located on that node unreachable. Such processes must be restarted on another node, which causes delays. We created such anomalies by failing driver processes, where the number of processed records drops to 0 until the driver comes back up again in about 20 seconds. We also created anomalies by failing executor processes, which get restarted 10 seconds after the failure, but whose effects on metrics such as processing delay may continue longer. We created 9 driver failures and 10 executor failures over 11 different traces. Ground Truth Table . For all of these 97 anomaly instances over 34 anomalous traces, we provide ground truth labels with the information shown in Table <ref type="table" target="#tab_0">1</ref>(b). Such labels include both root cause intervals (RCIs) and their respective extended effect intervals (EEIs). RCIs typically correspond to the time period during which DEG programs are running, whereas the EEIs are the time periods that start immediately after an RCI and end when important system metrics return to normal values or the application is eventually pushed to crash. The EEIs are manually determined using domain knowledge. Additional details can be found in our report.</p></div>
<div><head n="4">BENCHMARK DESIGN</head><p>In this section, we present the evaluation methodology we designed to benchmark anomaly detection (AD) and explanation discovery  (ED) algorithms based on the curated, high-dimensional time series dataset described in the previous section. As summarized in Table <ref type="table" target="#tab_1">2</ref>, <software ContextAttributes="used">Exathlon</software> is designed to evaluate AD and ED algorithms in two orthogonal aspects, functionality and computational performance, using well-defined metrics. In terms of functionality, the evaluation criteria capture that an AD/ED algorithm is exposed to increasingly more challenging requirements as the functionality level is raised from one to the next. In terms of computational performance, <software ContextAttributes="used">Exathlon</software> provides three complementary criteria that can be evaluated by varying dimensionality and size of the dataset.</p></div>
<div><head n="4.1">Anomaly Detection (AD) Functionality</head><p>First and foremost, we designed <software ContextAttributes="used">Exathlon</software> targeting semi-supervised AD techniques (i.e., trained only with normal data, possibly with occasional noise, and then tested against anomalous data) for rangebased anomalies (i.e., contextual and collective anomalies occurring over a time interval instead of only at a single time point) over high-dimensional (i.e., multivariate with 1000s of dimensions) time series. This decision is informed by our observation of this being the most common and inclusive usage scenario in practice. Evaluation Criteria. We identified four key criteria for evaluating AD functionality, listed below from basic towards advanced, where a higher AD level includes the requirements of all preceding levels: AD1 (Anomaly Existence): The first expectation is to flag the existence of an anomaly somewhere within the anomaly interval (i.e., the root cause interval (RCI) + the extended effect interval (EEI)). AD2 (Range Detection): The next expectation is to report not only the existence, but also the precise time range of an anomaly. The wider a range of an anomaly that an AD method can detect, the better its understanding of the underlying real-world phenomena. AD3 (Early Detection): The third expectation is to minimize the detection latency, i.e., the difference between the time an anomaly is first flagged and the start time of the corresponding RCI. AD4 (Exactly-Once Detection): The last expectation is to report each anomaly instance exactly once. Duplicate detections are undesirable, because they may not only redundantly cause repeated alerts for a single anomalous event, but also confusion if those alerts are for the same anomaly event or not. Evaluation Metrics. To assess how well an AD algorithm can meet these four functionality levels, we use the customizable accuracy evaluation framework for time series <ref type="bibr" target="#b50">[51]</ref>. This framework extends the classical precision/recall from point-based data to range-based data, by introducing a set of tunable parameters. By setting the values of these parameters in a particular way and applying the resulting precision/recall formulas to the output of an AD algorithm, one can assess how well that output measures up to the quality expectations represented by those parameter settings. We leverage this as a mathematical tool to quantify how well an AD algorithm meets AD1-AD4. Furthermore, we chose to do this in a way that every level AD 𝑖 builds on and adds to the requirements of the previous level AD 𝑖-1 . This monotonic design ensures that the AD functionality score that an algorithm gets (Precision, Recall, or other metrics obtained by combining them, e.g., F-Score or Area Under the Precision/Recall Curve (AUPRC)) is always ordered as: 𝑠𝑐𝑜𝑟𝑒 (𝐴𝐷1) ≥ 𝑠𝑐𝑜𝑟𝑒 (𝐴𝐷2) ≥ 𝑠𝑐𝑜𝑟𝑒 (𝐴𝐷3) ≥ 𝑠𝑐𝑜𝑟𝑒 (𝐴𝐷4), which facilitates evaluating and interpreting results in a systematic way.</p><p>In <software>Exathlon</software>, we preferred this design over the alternative of treating each AD level as orthogonal to enable users to develop/perfect their models for tasks that become increasingly more challenging.</p><p>Figure <ref type="figure" target="#fig_3">2</ref> provides a simple example to illustrate how range-based precision and recall are computed for different AD levels. Given real anomaly ranges R1..R4 and predicted anomaly ranges P1..P4 produced by an AD algorithm, we first compute precision/recall for each range and then average them for overall precision/recall. Intuitively, precision focuses on the size of TP ranges (colored green) relative to TP+FP ranges (colored yellow), and recall focuses on the size of TP ranges relative to TP+FN ranges (colored blue). For AD1, Recall(Ri) is 1 if Ri is flagged, 0 otherwise. For AD2, Recall(Ri) is proportional to the relative size of the TP range. For AD3, Recall(Ri) is further weighted by position of the TP range relative to the start of Ri. Finally, at AD4, Recall(Ri) degrades to 0 for any Ri that is not flagged exactly once. Precision(Pi) is computed in an analogous way, except that AD levels about anomaly coverage quality (AD1 and AD3) are not relevant to it; rather, the main focus is on the size and number of the real anomaly ranges that are successfully predicted. In our simple example, it turns out that all AD levels for Precision consider the same Pi subranges.</p><p>To achieve precision and recall at different AD levels, we set the tunable parameters of the range-based precision/recall framework with necessary modifications. These details are deferred to our technical report due to space constraints <ref type="bibr" target="#b28">[29]</ref>. Further note that both the semi-supervised AD algorithms we investigate and the precision/recall for time series model we use to assess them focus on binary classification (normal vs. anomalous ranges). On the other hand, our dataset is inherently a multi-class one (normal ranges vs. six types of anomalous ranges). This raises a question about how to evaluate binary predictions under multi-class labels. We take a holistic approach, and evaluate the AD prediction results both globally and grouped by type, whenever this is reasonable and provides useful insights. For example, even though a binary predictor is not able to detect different anomaly types, we can still measure its resulting coverage (i.e., recall) for each type. However, type-wise measurement is not entirely meaningful for precision, since false positives (FPs) are essentially typeless.</p><p>Our benchmark also includes a set of four learning settings LS1-LS4, ranging from simple to more complex yet realistic ones. These are detailed in our technical report <ref type="bibr" target="#b28">[29]</ref>.</p></div>
<div><head n="4.2">Explanation Discovery (ED) Functionality</head><p>Once an anomalous instance is flagged by an AD method, the next desirable functionality is to find the best explanation for the anomaly detected, or more precisely, a human-readable formula offering useful information about what has led to the anomaly.</p><p>There have been many ED methods in recent work (see §2). These differ in the form of "explanation" provided: some return a logical formula as an explanation <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b57">58]</ref>, others return a decision tree <ref type="bibr" target="#b54">[55]</ref>, and some others return a numerical score for each feature such as the coefficient in linear regression <ref type="bibr" target="#b43">[44]</ref> or the SHAP score <ref type="bibr" target="#b36">[37]</ref>. <software ContextAttributes="used">Exathlon</software> does not pose any restrictions on the form of explanation used. Instead, it takes an abstract view of explanations. Formally, we model each trace in the test dataset as a multi-dimensional time series, [ 𝒙 1 . . . 𝒙 𝑡 . . . 𝒙 𝑛 ] 𝑇 , where each data item includes 𝑚 features, 𝒙 𝑡 = (𝑥 𝑡 1 , . . . , 𝑥 𝑡𝑚 ). A detected anomaly is a subsequence of the time series that starts at timestamp 𝑡 and has duration 𝑤, 𝑋 𝑡,𝑤 = [ 𝒙 𝑡 . . . 𝒙 𝑡 +𝑤 ] 𝑇 . If an AD method can provide only point-based detection, then 𝑤 is set to 0. We denote the explanation generated for the anomaly 𝑋 𝑡,𝑤 as 𝐹 𝑡,𝑤 and treat it as a function of the features, A = (𝑎 1 , . . . , 𝑎 𝑚 ), from the data:</p><formula xml:id="formula_0">𝐹 𝑡,𝑤 (𝑎 1 , . . . , 𝑎 𝑚 ) |= 𝑋 𝑡,𝑤</formula><p>where |= means that 𝐹 𝑡,𝑤 "explains" the anomaly 𝑋 𝑡,𝑤 . In addition, we define an extraction function, 𝐺 A over 𝐹 𝑡,𝑤 , that returns the set of features used in the explanation (e.g., appearing in a logical formula or having non-zero coefficients in a regression model):</p><formula xml:id="formula_1">𝐺 A 𝐹 𝑡,𝑤 (𝑎 1 , . . . , 𝑎 𝑚 ) = A 𝑡,𝑤 ⊆ A</formula><p>Finally, we define the size of 𝐹 𝑡,𝑤 as the size of its feature set A 𝑡,𝑤 :</p><formula xml:id="formula_2">|𝐹 𝑡,𝑤 (𝑎 1 , . . . , 𝑎 𝑚 )| = |A 𝑡,𝑤 |</formula><p>Evaluation Criteria: Subject of Explanation. The key distinction that <software ContextAttributes="used">Exathlon</software> makes is whether an ED method is attempting to explain a single anomaly (local) or a broad set of anomalies (global). ED1: Local Explanation: This corresponds to explaining one anomaly instance, offering a compact yet meaningful piece of information to help the user understand this particular instance. As mentioned by LIME <ref type="bibr" target="#b43">[44]</ref>, the explanation should be locally faithful. In our context, it means that the same explanation can hold over immediate "neighbors", which are anomaly instances of the same application and same anomalous type, and around the same time period. ED2: Global Explanation: Alternatively, an ED method may attempt to explain a (potentially large) set of anomalies, called a global explanation. In general, it is not possible to find an identical succinct explanation for many different instances. Hence, a global explanation is usually composed of a set of explanations; e.g., LIME <ref type="bibr" target="#b43">[44]</ref> chooses the most representative 𝑘 instances to explain a model. In our benchmark, it makes most sense to construct a global model for a set of anomalies of the same type, but potentially from different applications or different runs of the same application. This helps us understand for "semantically similar" anomalies, whether an ED method can return explanations that are consistent, or even of predictive power of similar anomalies that arise in the future. Evaluation Metrics. <software ContextAttributes="used">Exathlon</software> evaluates both local and global explanations for three desired properties: 1. Conciseness: This corresponds to the number of features used in the explanation. Following the Occam's razor principle, humans favor smaller, and thus simpler explanations. As different ED methods return explanations of different forms, our benchmark counts the number of features used in the explanation as its conciseness measure. In the ED1 case, that is |𝐹 𝑡,𝑤 | defined above. In the ED2 case, a global explanation includes a set of explanations, and its conciseness measure is the average of the size of each explanation. 2. Consistency: Anomalies of the same type occurring in a similar context should have consistent explanations. We customize this notion for ED1 and ED2, respectively. In both cases, we care only about the set of features employed in the explanation, without considering the numerical or categorical values used.</p><p>Stability (ED1) is the customized consistency measure for ED1. It means that the anomalies occurring in a similar context (e.g., for the same application, same run, and same time period) should have similar explanations, subject to a small perturbation of the data. Formally, we introduce a subsampling procedure over an anomaly 𝑋 𝑡,𝑤 , which generates a set of samples, {𝑋 (𝑖) 𝑡,𝑤 }. We denote the corresponding explanations generated for them as {𝐹 (𝑖) 𝑡,𝑤 }. The extraction function for a set of explanations is defined to be the duplicatepreserving union (like Union All in SQL) of the extraction function of each respective explanation:</p><formula xml:id="formula_3">𝐺 A 𝐹 (𝑖) 𝑡,𝑤 (𝑎 1 , . . . , 𝑎 𝑚 ) = 𝑖 𝐺 A 𝐹 (𝑖) 𝑡,𝑤 = 𝑖 A (𝑖) 𝑡,𝑤 = A ∪ 𝑡,𝑤</formula><p>Finally, for each feature 𝑎 𝑗 ∈ A ∪ 𝑡,𝑤 , we count its frequency in this feature set and normalize it by the total size of the feature set. The consistency measure is then defined as the entropy of the set of normalized frequencies of such features, 𝑎 𝑗 , 𝑗 = 1, 2, . . .:</p><formula xml:id="formula_4">𝐻 𝐴 ∪ 𝑡,𝑤 = - 𝑗 𝑝 (𝑎 𝑗 ) log 2 𝑝 (𝑎 𝑗 ), 𝑎 𝑗 ∈ A ∪ 𝑡,𝑤 𝑝 (𝑎 𝑗 ) = 1 A ∪ 𝑡,𝑤 𝑎 𝑗 / A ∪ 𝑡,𝑤 Where 1 A ∪ 𝑡,𝑤</formula><p>𝑎 𝑗 is here an indicator function that counts the occurrences of a feature in a multiset. For capturing consistency, our choice of entropy is motivated by information theory that a set of explanations that lack consistency will require using more bits to encode, hence a larger entropy value. In the ideal case, all explanations, {𝐹 (𝑖) 𝑡,𝑤 }, are identical, and its entropy takes the minimum value 0 if the size of the explanation is 1 (denoted as 𝐻 1 ), the value 1 if the size is 2 (𝐻 2 ), or the value 1.58 if the size is 3 (𝐻 3 ).</p><p>Concordance (ED2) is the customized consistency measure for ED2. Here, it means that the anomalies of the same type are expected to have consistent explanations, subject to larger amounts of deviation in data due to different time periods in the same run of a <software>Spark</software> application, different runs of the application, or even different <software ContextAttributes="used">Spark</software> applications. Formally, we are given a set of anomalies, {𝑋 𝑡 𝑖 ,𝑤 𝑖 }. Denote their corresponding explanations as {𝐹 𝑡 𝑖 ,𝑤 𝑖 }. The consistency measure of this set of explanations is computed similarly to ED1, except that we are replacing the subsampled anomalies, {𝑋 (𝑖) 𝑡,𝑤 }, with the given set of anomalies, {𝑋 𝑡 𝑖 ,𝑤 𝑖 }. However, one may notice that the conciseness measure also has an impact on consistency. To factor out this impact, we further define Normalized Consistency as 2 𝐶𝑜𝑛𝑠𝑖𝑠𝑡𝑒𝑛𝑐𝑦 𝐶𝑜𝑛𝑐𝑖𝑠𝑒𝑛𝑒𝑠𝑠 , which captures the variability of the explanations conditioned on their average size. 3. Accuracy: The last property, which is also the hardest to achieve, is to view an explanation of an anomaly as a predictive model, apply it to other similar instances (defined above for ED1 and ED2, respectively), and then evaluate accuracy of such predictions.</p><p>Note that not all explanations can serve as a predictive model. Only those that are a function mapping a given data item to 0/1, 𝐹 𝑡,𝑤 : 𝒙 𝑡 ∈ R 𝑚 → {0, 1}, can offer predictive power over test data. For example, a logical formula <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b57">58]</ref> or a decision tree <ref type="bibr" target="#b54">[55]</ref> can be used to run prediction on new data items, but feature importance scores or SHAP scores <ref type="bibr" target="#b36">[37]</ref> cannot. Even with those ED methods that return a predictive explanation, it is only a point-based predictive model. The literature largely lacks ED methods that can return explanations that characterize a temporal pattern. For this reason, <software ContextAttributes="created">Exathlon</software> evaluates the accuracy of such explanations using point-based precision and recall.</p><p>In the case of ED1, we are given a particular anomaly 𝑋 𝑡,𝑤 . To measure the accuracy of an ED method, we subsample from 𝑋 𝑡,𝑤 , yielding a sample, 𝑋 (𝑖) 𝑡,𝑤 . We run the ED method to generate an explanation, 𝐹 (𝑖) 𝑡,𝑤 . Then we run 𝐹 (𝑖) 𝑡,𝑤 as a predictive model over a test dataset that includes the remainder of the anomalous data, 𝑋 𝑡,𝑤 -𝑋 (𝑖) 𝑡,𝑤 , as well as some normal data that immediately proceeds or follows 𝑋 𝑡,𝑤 . For each test point, we obtain a 0/1 prediction and compare it to the ground truth. We repeat this procedure for all test points to compute the final precision, recall, and F-score.</p><p>In ED2, we are given a set of anomalies 𝑋 𝑡 𝑖 ,𝑤 𝑖 . We randomly split this set into a training set and a test set. We can run a suitable ED method to generate a global explanation from the training set, and then use it as a predictive model over the test set. For each anomaly in the test set, we compare the point-wise prediction against the ground truth and compute precision, recall, F-score, similar to ED1.</p></div>
<div><head n="4.3">Computational Performance</head><p><software ContextAttributes="created">Exathlon</software> can also be used to evaluate computational performance. Evaluation Criteria and Metrics. ML algorithm performance is typically measured in terms of the total time it takes for model training as well as for using that model for making predictions. For AD, we define P1 and P2 to evaluate training and inference performance, respectively. For ED, the time to discover each explanation, P3, is our third performance metric. Experimental Parameters. <software ContextAttributes="created">Exathlon</software> offers scalability tests by varying the following two data-related parameters: Dimensionality 𝑀: Our dataset consists of high-dimensional time series data. The 2,283 metrics (features) may be correlated and contain a lot of null values, which are representative of real-world datasets. The benchmark leaves it to each user algorithm as how it copes with the high dimensionality. The relevant techniques may include dimensionality reduction using linear transformation (e.g., PCA), or feature selection by leveraging the correlation structure in the data. Such choices are left to the discretion of each user algorithm, and <software ContextAttributes="created">Exathlon</software> reports on the resulting dimensionality 𝑀 used in AD and ED tasks. Cardinality Factor 𝛼: Besides high dimensionality, our dataset also has high cardinality, 𝑁 = 2,335,781 data items, which is significantly higher than the existing Numenta Anomaly Benchmark <ref type="bibr" target="#b33">[34]</ref>. If the training time of an algorithm is too long, a user algorithm can choose to reduce the cardinality via resampling, i.e., by taking average of the data items in each 𝑙-second interval, which amounts to a cardinality factor 𝛼 = 1/𝑙 and reduced data size of 𝛼𝑁 .</p></div>
<div><head n="4.4">Broader Applicability</head><p>The <software ContextAttributes="created">Exathlon</software> benchmark is more broadly applicable beyond our particular dataset <ref type="bibr" target="#b28">[29]</ref>. AD Benchmark. The <software ContextAttributes="created">Exathlon</software> benchmark considers all techniques that 1) train a model for data normality via learning from undisturbed traces, 2) assign outlier scores to new test records, and 3) derive binary predictions from these scores using a threshold. When meeting the above conditions, our AD metrics can be used with any labeled time series anomaly test datasets similar to ours. The four AD levels can be directly usable if labels are available as ranges (like for real or synthetic datasets from the discord discovery literature, also used in <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b7">[8]</ref>), while one would need to set our evaluation parameters to classical precision and recall if labels are available only as points (like for classification-oriented datasets tuned to data points of imbalanced classes, some of which are used in <ref type="bibr" target="#b51">[52]</ref>). Applying such metrics also helps assess the performance and efficiency of any time series AD technique capable of assigning outlier scores and binary predictions to each record of a test sequence. ED Benchmark. While a user study may be the best way to evaluate the usefulness of explanations, it is not always available and may come at a high cost. Therefore, our benchmark aims to provide automated evaluation of ED methods based on intuitive metrics, namely, conciseness, consistency, and accuracy, as well as their various variants in the ED1 and ED2 settings. As further detailed in our technical report <ref type="bibr" target="#b28">[29]</ref>, our metrics cover many of the metrics used in prior ED works, except for specific metrics that depend on a particular model or algorithm, which <software ContextAttributes="created">Exathlon</software> deliberately avoids as a general benchmark, or require ground truth features or visual inspection by domain experts, which are not always available in complex domains. As the result of sharing metrics with existing works, the ED metrics of <software ContextAttributes="created">Exathlon</software> can be applied to the datasets used in <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b57">58]</ref>, as well as those in <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b54">55]</ref> for the sake of evaluating the explanation for a classification result, and those in <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b43">44]</ref> with sufficient preprocessing on the text or image data used. To reduce dimensionality, our pipeline offers a PCA-based method (with a parameter that controls different coverage of the data variance and the resulting feature set size), as well as a manually curated feature set with 19 features selected using domain knowledge. (iii) Rescaling: Most ML algorithms require the features to be scaled into a range, e.g., between [0, 1], to better align features whose raw values may differ by orders of magnitude. A unique issue in our problem is that each test trace may represent a new context, e.g., a combination of input rate and concurrency not seen in training data. As a result, rescaling has to take into account this new context. To simplify this setting, we provide the option of performing rescaling per trace, as well as a customized scaling method that rescales test data dynamically as we run an AD model over the data.</p><p>3. AD Modeling. The next phase takes the transformed training data and builds an AD model. Most AD methods build a model that describes the normal behavior in the data, called a "normality model", such that any future (test) data that deviates significantly from it will be flagged as an anomaly. Our pipeline offers an open architecture to embrace any AD method that builds such a normality model to detect anomalies. In this paper, we focus on recent DL-based AD methods <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b55">56]</ref>, to explore their potential for handling the complexity of our dataset (high-dimensional, with noise), anomaly patterns (a variety of contextual and collective anomalies), and learning settings (noisy semi-supervised AD modeling). (i) Normality modeling: The first step is to train a normality model based on a DL method of choice. Most DL methods take input data of fixed window size 𝑠. Given each of our traces, we create sliding windows of size 𝑠 and slide 1, and feed them as input to the model. Different DL methods model the data in the window by either trying to forecast the data point following the window (forecasting-based, e.g., LSTM <ref type="bibr" target="#b9">[10]</ref>) or reconstructing the window via a succinct internal representation (reconstruction based, e.g., Autoencoder <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b55">56]</ref> or GANs <ref type="bibr" target="#b46">[47]</ref>). To train each specific model, we divide the transformed 𝐷 train set into internal training (𝐷 0 train ), validation (𝐷 1 train ), and test (𝐷 2 train ) sets. The DL model is trained on 𝐷 0 train , with early stopping applied based on the model performance on 𝐷 1 train . Hyperparameter tuning is performed by choosing a configuration that maximizes model performance on 𝐷 2 train . (ii) Outlier score derivation: We next build an initial AD model, 𝑔 : 𝒙 ∈ R 𝑚 → R, which maps each data point to an outlier score. For forecasting models, we compute the difference, 𝑑, between the forecast and true values of each data point, and derive the outlier score 𝑣 based on 𝑑; the higher the 𝑑 value, the higher the 𝑣 score. For reconstruction-based models, we treat the reconstruction error of each window as the 𝑣 score for that window, and then derive the 𝑣 score of each data point by averaging the scores of its enclosed sliding windows. Our pipeline implements the LSTM <ref type="bibr" target="#b9">[10]</ref>; Autoencoder (AE) <ref type="bibr" target="#b27">[28]</ref>; and BiGAN <ref type="bibr" target="#b46">[47]</ref> for AD. Details of these models and the necessary modifications we made to suit our dataset are deferred to our technical report due to space constraints. (iii) Threshold selection: The last step aims to find a threshold on the outlier score to return a 0/1 prediction. It returns a final AD model, 𝑓 : 𝒙 ∈ R 𝑚 → {0, 1}, mapping each data point to 0/1. <software ContextAttributes="used">Exathlon</software> does not offer labeled data for threshold selection. Hence, we provide unsupervised threshold selection fit on 𝐷 2 train . Among the methods listed in a recent survey <ref type="bibr" target="#b56">[57]</ref>, we choose three most used automatic techniques: SD, MAD, and IQR, with the possibility of repeating them multiple times to filter large outlier scores. 4. AD Inference. Once the AD model is built, the next phase of the pipeline runs the AD model over each test trace to detect anomalies. In the context of range-based AD, predicted anomalies for a test trace are defined as sequences of positive predictions within that trace, denoted as 𝑋 𝑡,𝑤 , which starts at 𝑡 and has duration 𝑤. 5. AD Evaluation. The last AD phase evaluates the AD model for a given set of requirements. We evaluate both a model's ability to separate normal from anomalous data in the outlier score space and its final AD ability based on threshold selection. The separation ability (𝑔) is assessed at the trace, application, and global levels. Global separation is reported as the AUPRC computed on all test data, while the application/trace-level separation is reported by computing an AUPRC for each application/trace, and averaging the results. The detection ability (𝑓 ) is assessed by reporting its rangebased precision, recall, and F-score, with parameters specified by the AD functionality. Recall is also reported by anomaly type. 6. ED Execution. For each test trace, AD inference reports a set of anomalies, and for each reported anomaly, the ED module returns an explanation for it. Our pipeline supports two families of ED methods. (i) Model-free ED methods do not require the access to an ML model. Instead, they only require the anomalous instance, 𝑋 𝑡,𝑤 , and a reference dataset, to generate an explanation. Examples include EXstream <ref type="bibr" target="#b57">[58]</ref> and MacroBase <ref type="bibr" target="#b2">[3]</ref> ( §2). Our implementation sets the reference dataset as the subset of data that immediately proceeds the detected anomaly, denoted by 𝑋 𝑡,-𝑤 ′ , and was classified as normal. Then the pair of datasets, (𝑋 𝑡,𝑤 , 𝑋 𝑡,-𝑤 ′ ), are provided to the ED method to generate an explanation, 𝐹 𝑡,𝑤 . (ii) Model-dependent ED methods take not only the anomalous instance, 𝑋 𝑡,𝑤 , but also an AD model, 𝑓 : 𝒙 ∈ R 𝑚 → {0, 1}. Examples include LIME <ref type="bibr" target="#b43">[44]</ref>, Anchors <ref type="bibr" target="#b44">[45]</ref>, and SHAP <ref type="bibr" target="#b36">[37]</ref> ( §2). In our implementation, we provide the AD model used in inference to the ED method. 7. ED Evaluation. After processing each test trace, we obtain a set of anomalies with their corresponding explanations. We then collect the explanations from all the test traces to run the final ED evaluation and compute conciseness, consistency, accuracy, and time metrics. Further details are given in our technical report <ref type="bibr" target="#b28">[29]</ref>.</p></div>
<div><head n="6">EXPERIMENTAL STUDY</head><p>In this section, we apply our benchmark to a select set of AD and ED methods. While a comprehensive comparison of related AD and ED methods is beyond the scope of this paper, analyzing the select methods allows us to demonstrate the value of our dataset and benchmark. Our analyses include the strengths and limitations of these AD and ED methods, challenges posed by our dataset and evaluation criteria, and some potential directions of future research.</p></div>
<div><head n="6.1">Experimental Setup</head><p>In our experimental setup, we integrated into our pipeline three DL-based AD methods: LSTM <ref type="bibr" target="#b9">[10]</ref>, AE <ref type="bibr" target="#b27">[28]</ref>, and BiGAN <ref type="bibr" target="#b46">[47]</ref>. We also integrated three recent ED methods: EXstream <ref type="bibr" target="#b57">[58]</ref> and Mac-roBase <ref type="bibr" target="#b2">[3]</ref>, from the DB community for outlier explanation in data streams, and LIME <ref type="bibr" target="#b43">[44]</ref>, an influential method from the ML community. Details of these methods can be found in our report <ref type="bibr" target="#b28">[29]</ref>.</p><p>Besides the methods, our pipeline also needs to be configured with the following options: (a) Data Size and Feature Set (FS): Since some of the DL models (e.g., GANs) could not complete training on our cluster using the full dataset, we reduced the data size by setting the cardinality factor, 𝛼 = 1/15. We also used a reduced feature set of 19 features, 𝑚 = 19, produced by either manual selection based on domain knowledge, denoted as FS custom , or by PCA with the same number <ref type="bibr" target="#b18">(19)</ref> of features, denoted as FS pca . For each given training set, we allowed each DL algorithm to train for 1.5 days (including hyperparameter tuning) to obtain an AD model. (b) Level of AD Evaluation (AD1-4), as described in Table <ref type="table" target="#tab_1">2</ref>, with a default setting of AD2 (range detection).</p></div>
<div><head n="6.2">AD Evaluation Results and Discussion</head><p>We begin by applying the LSTM <ref type="bibr" target="#b9">[10]</ref>, AE <ref type="bibr" target="#b27">[28]</ref>, and BiGAN <ref type="bibr" target="#b46">[47]</ref> methods on our benchmark dataset and report on the AD metrics.  Experiment 1 (FS custom , AD2). The first experiment compares the three AD methods under a default setting, (FS custom , AD2). Here, we focus on the model's ability to separate anomalous data from normal data, via the analysis of trace-level, application-level, and global AUPRC results summarized in Table <ref type="table" target="#tab_3">3</ref>.</p><p>(1) Trace-level Separation: We first consider trace-level separation. All three methods achieved decent AUPRC scores for most (or a subset) of anomaly types, with AE achieving the highest score of 0.73. Figure <ref type="figure" target="#fig_6">4(a)</ref> shows the distribution of outlier scores assigned by the AE method to the records in the T2 trace of Application 2. In this example, the normal records are separated from anomalous ones for most the data. This shows that our data indeed carries useful signals that can be picked up by the AD method, which allows the AD method to perform better than naive classifiers that randomly assign a normal or abnormal label, or assign each instance to the majority (normal) class -we refer to this remark as R1.</p><p>(2) Application-level and Global Separation: Moving from the trace-to application-to global level separation, the AUPRC scores gradually decrease. This is because the separation of normal from anomalous instances in outlier score becomes increasingly harder as we broaden the contexts in which data is generated. Figure <ref type="figure" target="#fig_6">4(b)</ref> shows the distributions of outlier scores assigned by the AE method to all disturbed traces of Application 2. <ref type="foot" target="#foot_0">1</ref> At the application level, the outlier scores assigned to normal instances spread further, and start to mix with the outlier scores assigned to T2 anomaly instances, hence decreasing the model's separation ability. At the global level, this trend is aggravated, as we can see in Figure <ref type="figure" target="#fig_6">4(c)</ref>.</p><p>To understand why, Figures <ref type="figure" target="#fig_6">4(e</ref>) and 4(f) show the outlier scores of the T1 and T2 traces of Application 2. The outlier scores assigned to some normal points in the T1 trace are in fact higher than the T2 anomalies, due to two reasons: (a) Different contexts: T1 and T2 traces were generated under different input rates, with the rate increase in T1 events around 2.5 times higher than in the T2 events. (b) Noisy training data: The normal data in T1 is "noisy". In fact, the normal records in the T1 trace that obtained higher outlier scores than the T2 anomaly exactly match the high processing delay due to <software ContextAttributes="used">Spark</software> checkpointing activities. This indicates that the model has failed to capture these activities as normal behavior.</p><p>The above analyses show that our dataset carries a great deal of variability across traces (e.g., different input rates, concurrency among programs), and a small amount of noise. Such variability and noise make our dataset challenging for the three DL-based AD methods tested in this study (R2).    (3) Anomaly Type Comparison: For different anomaly types, Table <ref type="table" target="#tab_3">3</ref> shows that at the global level, the best separated types are T1, T3 and T5 for LSTM and AE, and T1, T4 and T5 for BiGAN. The good performance for T1 (bursty input) and T5 (driver failure) across all methods are largely due to the fact these types have very visible impacts on many of the features output by FS custom , e.g., features relating to the input rate, application delays and memory usage for bursty input, and virtually all features for driver failure. However, most methods offer poor separation for T6 (executor failure) anomalies, due to the limited impact such anomalies have on the FS custom features, where the 6 executor features are averaged across active executor spots. As such, the impact of an executor going down is only visible during the (short) period of time for which it shuts down and is potentially replaced. The above discussion shows that the variety of our anomaly types present signals of different strength levels in the data. They offer challenges for designing, as well as opportunities for analyzing, different AD methods, and feature engineering in the AD method will play a key role in preserving the signals for each anomaly type. (R3).</p><p>(4) Method Comparison: Regarding the separation ability, the best performing method is AE, followed by BiGAN, then LSTM for all levels. AE (and BiGAN) typically produce smooth pointwise outlier scores, by taking averages over overlapping windows. The outlier scores produced by the LSTM, however, often exhibit discontinuous spikes. For the task of range detection (AD2), such frequent mixes of high and low values make it hard to produce continuous ranges of high outlier scores, penalizing recall when the outlier threshold is set high or precision when the threshold is set low. Hence, we observe differences among AD methods as follows: for range detection (AD2), AE works the best while LSTM is the worse, mostly because the non-smooth outlier scores of LSTM make it hard to handle range anomalies (R4). Experiment 2 (FS custom , AD2). We next examine how the separation abilities translate into actual AD performance via threshold selection. Detection metrics for AD2 (range detection) are reported in the second section of Table <ref type="table" target="#tab_5">4</ref>. To generate the results, we ran each of the STD, IQR and MAD thresholding techniques, leading to different AD performance results for each AD method, for which we report the median performance in Table <ref type="table" target="#tab_5">4</ref>.</p><p>Among the three methods, AE provided the best median F1score, due to its best separation ability reported in the previous experiment. However, this F1-score of AE is not very high (0.52). This is due to the difficulty in choosing a single threshold 𝑇 on the outlier score for all traces and anomaly types in an unsupervised setting, where we select 𝑇 by using part of the training data, 𝐷 2 train . Figure <ref type="figure" target="#fig_6">4(d)</ref> shows the distribution of the outlier scores assigned to the 𝐷 2 train samples (the 3% largest were cut for readability), along with the best threshold found on them. This threshold is then used to flag anomalies in the test (disturbed) traces, as shown in Figure <ref type="figure" target="#fig_6">4(c</ref>). We see that the anomalies whose scores lie left to 𝑇 will be missed, penalizing recall, and the normal records whose scores lie right to 𝑇 will lead to false positives, hurting precision. The above discussion shows that besides data characteristics, our benchmark poses another challenge on AD methods due to the requirement of unsupervised threshold selection (R5). Experiment 3 (FS custom , AD2). To better understand the reasons behind the low F1-scores, we study the effect of the amount of training data on each method in Figure <ref type="figure" target="#fig_7">5</ref>. The amount of training data was varied by starting from the largest undisturbed trace, and then randomly adding one undisturbed trace at a time until reaching the full set of undisturbed traces (except for the BiGAN method for which multiple traces could be added at once due to its longer training time). For each method, the above process was repeated 5 times. The average performance is reported in Figure <ref type="figure" target="#fig_7">5</ref> using solid lines, while the shaded areas correspond to the confidence region with width of one standard deviation.</p><p>We can see that the three methods behave quite differently as training data increases. First, the AE method benefits a lot from the first few traces that it obtains for training, but quickly reaches a performance plateau afterwards. This seems to indicate that what is holding back the AE performance is not simply the lack of training data, but rather the actual challenges posed by our benchmark (see remarks R2, R3, R5). On the other hand, the LSTM method and the BiGAN method to some extent seem to require more data to perform well. While LSTM exhibits roughly a linear trend, BiGAN appears less stable, which probably arises from the fact that GANs typically require more manual and calibrated tuning in order to converge to a good solution. For both these methods, adding more data could be beneficial, along with more tuning and experimentation to try to improve performance. Overall, this experiment suggests that the F1-score observed for AE could be primarily due its technical limitations for handling complex data, extracting most informative features for different anomaly types, and unsupervised threshold selection, while LSTM and BiGAN can further benefit from more training data and extensive hyperparameter tuning (R6). Experiment 4 (FS custom ). We next evaluate the AD methods under different AD levels, AD1-4, of our benchmark. Results are reported in Table <ref type="table" target="#tab_5">4</ref>. (AD1) Given our range-based anomalies, a good recall score is easier to reach under AD1. We observe a general increase in performance for all methods. LSTM becomes the best method, because its spikes inside a real anomaly range are now sufficient for getting a good recall score, while using a high threshold to ensure good precision. (AD3) As AD3 awards less recall scores for late detection, AE maintains its performance, indicating that its reported range anomaly is not concentrated at the end of the true range. For LSTM, the performance drops because the early detections it makes are more scattered and hence weigh less in recall. (AD4) In AD4, reporting the same anomaly multiple times reduces the recall score. AE and BiGAN can maintain their performance while LSTM degrades significantly. Again, the tendency of LSTM to produce outlier scores in discontinuous spikes makes it more likely to report multiple anomalies where only one is needed. Hence, we see that the different AD levels in our benchmark indeed pose varying levels of challenges to the AD methods (R7).</p></div>
<div><head n="6.3">ED Evaluation Results and Discussion</head><p>Next, we report results of running MacroBase <ref type="bibr" target="#b2">[3]</ref>, EXstream <ref type="bibr" target="#b57">[58]</ref>, LIME <ref type="bibr" target="#b43">[44]</ref> on our benchmark dataset to generate anomaly explanations. For each anomaly, MacroBase and EXstream tried to explain its separation from a reference dataset, while LIME tried to explain the reason behind the high record outlier scores assigned by an AD model (AE in our case). Since LIME only explained the predictions of window size 𝑠 of our AE model, if an anomaly was larger than 𝑠, we created multiple windows for LIME to explain. Table <ref type="table" target="#tab_6">5</ref> summarizes conciseness, consistency, normalized consistency, accuracy, and running time for local (ED1) or global (ED2) explanations using the three ED methods. We also show example explanations in Figure <ref type="figure" target="#fig_8">6</ref>, which are the explanations returned for two instances of stalled input anomaly (T3). Only feature indices are reported here (for feature names, see <ref type="bibr" target="#b28">[29]</ref>). Figure <ref type="figure" target="#fig_8">6</ref>(a) reports the complete explanations returned by EXstream, while showing the features appearing in the explanations for the others due to limited space. For each method, we also show the features returned when explaining 5 different samples of anomaly instance #12 (stability). Explanations. The explanations shown highlight the impact of feature correlation. Although MacroBase and EXstream output different features (Figure <ref type="figure" target="#fig_8">6</ref>(a) and 6(b)), they might both be correct. For example, features 4, 5 and 14 are related to processed records, received records, and CPU time <ref type="bibr" target="#b28">[29]</ref>. For a human user, it makes sense for these three features to be used for a stalled input anomaly. EXstream picks up only one most important feature among the correlated ones. MacroBase returns all important features no matter whether they are correlated or not. LIME is known to have inconsistency issues <ref type="bibr" target="#b40">[41]</ref>, which is also illustrated here. Algorithm Analyses. We discuss the results for each algorithm. MacroBase. (1) MacroBase generated explanations of 3.16 features on avg. across anomaly types. For some anomaly types, e.g., T3, it generated longer explanations (6-7 features). The algorithm does not consider compactness, outputting longer explanations in presence of correlated features. <ref type="bibr" target="#b1">(2)</ref> The explanations it provided were not very locally stable (ED1 consistency), with an entropy score outside the ideal range of 𝐻 1 = 0 and 𝐻 3 = 1.58. This relates to a correlation between conciseness and stability: as Table <ref type="table" target="#tab_6">5</ref> shows for different anomaly types, longer explanations tend to be less stable. For global consistency, its concordance value further degrades, using inconsistent features for explanations of the same anomaly type. This issue was alleviated by measuring normalized consistency. In the example of Fig. <ref type="figure" target="#fig_8">6</ref>(b), the stability of MacroBase for instance #12 is 3.09, while its normalized stability is 1.03, almost perfect. The latter value is in accordance with human intuition, since across the 5 runs, the reported features were almost the same. (3) Its ED1 accuracy is good for some anomaly types (e.g., T1-3), but poor for some others (e.g., T5). As expected, its accuracy degraded from ED1 to ED2. (4) Its execution time is around 1 sec, except for T3. EXstream. (1) EXstream provided more concise explanations, using multiple techniques to prune marginally related features. (2) It achieved good explanation stability, with an average entropy score of 𝐻 3 = 1.58, but worse explanation concordance, be it normalized or not. This is likely due to our exclusion of the false positive filtering step, requiring additional labels. Thus, some features standing out as different during anomalous periods might not be related to the anomaly but to normal changes between two contiguous periods. Such features are likely to be more distinct between instances of different contexts than for perturbations of the same instance, hence the greater effect on concordance. (3) Its ED1 accuracy is good for some anomaly types (e.g., T1-3), but not for others (e.g., T3-6), especially in recall. Its accuracy degraded for ED2, as expected. (4) Its execution time is very short, being a streaming algorithm. LIME. (1) LIME generated longer explanations, e.g., for anomaly types T1-T3. (2) Its consistency scores were better maintained from ED1 to ED2, with normalized metrics alleviating the observed correlations between size and consistency. Accuracy measures do not apply to LIME, since it could not be compared to the others for prediction. (4) Its execution time is very long, up to 245 sec on avg.</p><p>Comparison. We next compare the three methods, first in conciseness and stability. MacroBase lacks a mechanism for minimizing the size of an explanation, while LIME relies on sparse linear regression to select few features. Neither was as effective as EXstream, which eagerly prunes marginally related features through its heuristic to a non-monotone submodular optimization problem. Stability being positively correlated to conciseness, the longer explanations of Mac-roBase and LIME are also less stable. For global explanations, concordance was harder to achieve than stability for all methods, i.e., a user is likely to see explanations built on different features for anomalies of the same type, which is undesirable. This lack of concordance indicates a direction for future ED research. For local accuracy, the logical formulas derived by MacroBase and EXstream on a subset of each instance could be evaluated for AD on their remaining part and neighboring normal data. MacroBase was more accurate, paying the cost of evaluating a large number of feature combinations, while EXstream suffered in recall, "overfitting" each anomalous instance. LIME, returning only feature importance scores, could not be evaluated for accuracy. In the global setting, accuracy degraded for all methods. By hard-coding context-dependent constants in predicates, the explanations of MacroBase and EXstream did not generalize well to other contexts (e.g., different input rates). This phenomenon is intrinsic to point-based explanations. In order to free explanations from such context-dependent predicates, transitioning from point-based to temporal explanations, capturing causal and context-free relationships between events, could be a direction for future research. In efficiency, EXstream was the fastest, taking 0.01 sec to generate explanations, against 0.2-9 sec for MacroBase and &gt; 4 min for LIME. As such, LIME is unsuitable for stream processing use cases, with its high latency preventing timely corrective actions, e.g., avoiding an application crash or denial of service.</p></div>
<div><head n="7">CONCLUSIONS AND FUTURE DIRECTIONS</head><p>In this paper, we presented <software>Exathlon</software> -a novel public benchmark for explainable AD, and demonstrated its utility through an experimental analysis of selected AD and ED algorithms from recent literature.</p><p>Our AD results show that <software>Exathlon</software>'s dataset is valuable for evaluating AD algorithms due to rich signals and diverse anomaly types included in the data. Yet more importantly, our results reveal the limitations of these AD methods for semi-supervised learning under noisy training data and mixed anomaly types. On the ED front, the literature lacked comparative analysis tools and studies. Our benchmark fills this gap by providing a common framework for analyzing the strengths and limitations of diverse ED methods in their conciseness, consistency, accuracy, and efficiency. These results call for new research to advance the current state of the art of AD and ED, as well as integrated solutions to anomaly and explanation discovery. For a true integration, ED methods should first become capable of discovering range-based explanations, which is also a key step towards automated root cause analysis (a.k.a., "why explanations"). <software ContextAttributes="created">Exathlon</software>'s dataset and extensible design are well-positioned to support research progress towards these goals in the long term. Going forward, we envision <software ContextAttributes="created">Exathlon</software> to develop into a collaborative community platform for fostering reproducible research and experimentation in the area. We intend to actively maintain and extend this platform, as well as welcoming feedback and contributions from the AD and ED communities.</p></div><figure xml:id="fig_0"><head /><label /><figDesc>Spark execution environment (c) Trace with bursty input anomalies</figDesc></figure>
<figure xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Spark application monitoring, and metrics observed in anomaly instances (a pair of red vertical bars marks a root cause event)</figDesc><graphic coords="5,383.02,88.67,166.46,93.54" type="bitmap" /></figure>
<figure xml:id="fig_2"><head /><label /><figDesc>Efficiency AD4: Exactly-Once Detection Evaluation Accuracy: Range-based Precision, Conciseness Time, given Metrics Recall, F-Score, AUPRC Consistency: Stability (ED1), Concordance (ED2) different Dimensionality Accuracy: Point-based Precision, Recall, F-Score and Cardinality factors</figDesc></figure>
<figure xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Range-based precision and recall at AD levels 1-4. Precision evaluates prediction quality (green out of yellow for each Pi). Recall evaluates anomaly coverage (green out of blue for each Ri).</figDesc><graphic coords="6,317.96,195.05,240.25,104.48" type="bitmap" /></figure>
<figure xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: A pipeline for explainable AD on multivariate time series 5 A FULL PIPELINE FOR EXPLAINABLE AD Besides a curated dataset and an evaluation methodology, Exathlon also provides a full pipeline for explainable AD on high-dimensional time series data. Our pipeline is characterized as follows: (i) It consists of the typical steps in a deep ML pipeline, ranging from data partitioning, feature engineering, dimensionality reduction, to AD and ED. (ii) It implements a variety of AD and ED functionalities and evaluation modules that score them based on the metrics of the benchmark (see §4). (iii) It provides an open, modular architecture that allows different methods to be added and combined through the pipeline. Figure 3 provides an overview. 1. Data Partitioning. The first phase takes as input the 93 raw traces, described in §3. It performs simple data cleaning, e.g., replacing missing data with a default value. It then performs data partitioning of the 93 traces. In the default setting, we take all undisturbed traces as training data, 𝐷 train , and all disturbed traces as test data, 𝐷 test . Other implementation choices are left to our report [29]. 2. Data Transformation. As ML algorithms require data transformations to perform well, our pipeline offers the following steps: (i) Resampling (optional): For the multivariate time series in each trace, the user can choose to resample, by taking the average of data points in each 𝑙-second interval. This step reduces the cardinality factor, 𝛼 = 1/𝑙, of the time series data, if the training time turns out to be too long for some ML algorithms. (ii) Dimensionality reduction: Since our dataset includes 𝑀 = 2, 283 raw features, such high dimensionality may affect both model accuracy, known as the "curse of dimensionality", and training time.To reduce dimensionality, our pipeline offers a PCA-based method (with a parameter that controls different coverage of the data variance and the resulting feature set size), as well as a manually curated feature set with 19 features selected using domain knowledge. (iii) Rescaling: Most ML algorithms require the features to be scaled into a range, e.g., between [0, 1], to better align features whose raw values may differ by orders of magnitude. A unique issue in our problem is that each test trace may represent a new context, e.g., a combination of input rate and concurrency not seen in training data. As a result, rescaling has to take into account this new context. To simplify this setting, we provide the option of performing rescaling per trace, as well as a customized scaling method that rescales test data dynamically as we run an AD model over the data.3. AD Modeling. The next phase takes the transformed training data and builds an AD model. Most AD methods build a model that describes the normal behavior in the data, called a "normality model", such that any future (test) data that deviates significantly</figDesc><graphic coords="9,53.80,85.60,240.25,104.88" type="bitmap" /></figure>
<figure xml:id="fig_5"><head /><label /><figDesc>(a) Trace-wise separation: T2 trace of Application 2 (b) App-level separation: all disturbed traces of Application 2 (c) Global-level separation: all disturbed traces with "best" AD2 threshold (d) Modeling test samples with "best" AD2 threshold (e) Record-wise outlier scores on a T1 trace of Application 2 (f) Record-wise outlier scores on a T2 trace of Application 2</figDesc></figure>
<figure xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Outlier score distributions and record-wise outlier scores using the AE method (FS custom )</figDesc><graphic coords="11,58.91,248.49,242.10,51.02" type="bitmap" /></figure>
<figure xml:id="fig_7"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Median F1-score as training data increases for the three methods (FS custom , AD2)</figDesc><graphic coords="12,53.80,85.60,240.24,70.87" type="bitmap" /></figure>
<figure xml:id="fig_8"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Example explanations given by EXstream, MacroBase, and LIME for two instances of stalled input (T3) anomaly</figDesc><graphic coords="12,322.94,88.67,63.73,62.36" type="bitmap" /></figure>
<figure type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Undisturbed traces, disturbed traces, and ground truth labels of 97 anomalies The Exathlon dataset</figDesc><table><row><cell /><cell /><cell /><cell /><cell>Trace</cell><cell>Anomaly</cell><cell cols="4"># of Anomaly Anomaly Length (RCI + EEI) Data</cell></row><row><cell /><cell /><cell /><cell /><cell>Type</cell><cell>Type</cell><cell cols="2">Traces Instances</cell><cell>min, avg, max</cell><cell>Items</cell></row><row><cell cols="3">Metric Spark UI Spark UI</cell><cell>OS</cell><cell>Undisturbed</cell><cell>N/A</cell><cell>59</cell><cell>N/A</cell><cell>N/A</cell><cell>1.4M</cell></row><row><cell cols="4">Type Driver Executor (Nmon)</cell><cell cols="2">Disturbed T1: Bursty input</cell><cell>6</cell><cell>29</cell><cell>15m, 22m, 33m</cell><cell>360K</cell></row><row><cell># of</cell><cell /><cell cols="2">5 x 140 4 x 335</cell><cell cols="2">Disturbed T2: Bursty input until crash</cell><cell>7</cell><cell>7</cell><cell>8m, 35m, 1.5h</cell><cell>31K</cell></row><row><cell>Metrics</cell><cell>243</cell><cell>= 700</cell><cell>= 1340</cell><cell cols="2">Disturbed T3: Stalled input</cell><cell>4</cell><cell>16</cell><cell>14m, 16m, 16m</cell><cell>187K</cell></row><row><cell>Total</cell><cell /><cell>2,283</cell><cell /><cell cols="2">Disturbed T4: CPU contention</cell><cell>6</cell><cell>26</cell><cell>8m, 15m, 27m</cell><cell>181K</cell></row><row><cell>Frequency</cell><cell cols="3">1 data item per second</cell><cell cols="2">Disturbed T5: Driver failure</cell><cell>11</cell><cell>9</cell><cell>1m, 1m, 1m</cell><cell>128K</cell></row><row><cell>Data Items</cell><cell /><cell>2,335,781</cell><cell /><cell cols="2">Disturbed T6: Executor failure</cell><cell /><cell>10</cell><cell>2m, 23m, 2.8h</cell></row><row><cell>Duration</cell><cell /><cell>649 hours</cell><cell /><cell>Ground</cell><cell cols="4">(app_id, trace_id, anomaly_type, root_cause_start, root_cause_end,</cell></row><row><cell>Total Size</cell><cell /><cell>24.6 GB</cell><cell /><cell>Truth</cell><cell cols="3">extended_effect_start, extended_effect_end)</cell><cell /></row><row><cell cols="3">(a) Metrics and data size</cell><cell /><cell>(b)</cell><cell /><cell /><cell /><cell /></row></table></figure>
<figure type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>The Exathlon evaluation methodology and benchmark design</figDesc><table /></figure>
<figure type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Separation abilities of AD methods (FS custom , AD2)</figDesc><table /></figure>
<figure type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Median anomaly detection results (FS custom , AD1:4)</figDesc><table /></figure>
<figure type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Results of ED methods, MacroBase, EXstream, and LIME, in terms of conciseness, consistency, accuracy, and running time</figDesc><table><row><cell cols="2">MacroBase</cell><cell /><cell /><cell cols="2">EXstream</cell><cell /><cell>LIME</cell></row><row><cell>Concise Consistency Norm.Cons</cell><cell>Prec</cell><cell>Rel</cell><cell cols="2">Time (sec) Concise Consistency Norm.Cons</cell><cell>Prec</cell><cell>Rel</cell><cell>Time (sec) Concise Consistency Norm.Cons Time (sec)</cell></row><row><cell cols="3">ED1 ED2 ED1 ED2 ED1 ED2 ED1 ED2 ED1 ED2</cell><cell>ED1/2</cell><cell cols="3">ED1 ED2 ED1 ED2 ED1 ED2 ED1 ED2 ED1 ED2</cell><cell>ED1/2</cell><cell>ED1 ED2 ED1 ED2 ED1 ED2</cell><cell>ED1/2</cell></row><row><cell cols="3">T1 2.36 2.10 1.41 2.12 1.23 2.06 0.96 0.81 0.92 0.84</cell><cell>0.87</cell><cell cols="3">2.25 2.17 1.59 3.36 1.56 4.74 0.88 0.82 0.75 0.45</cell><cell>0.0162</cell><cell>3.58 5.86 3.04 3.82 2.33 2.41</cell><cell>259</cell></row><row><cell cols="3">T2 1.94 1.71 0.98 1.33 1.11 1.46 0.97 0.79 0.97 0.90</cell><cell>1.05</cell><cell cols="3">3.89 3.71 2.13 3.59 1.37 3.25 0.71 0.87 0.53 0.51</cell><cell>0.0087</cell><cell>3.74 9.29 3.42 4.16 2.88 1.93</cell><cell>237</cell></row><row><cell cols="3">T3 6.08 4.83 2.98 3.07 1.37 1.74 0.97 0.88 0.91 0.51</cell><cell>8.75</cell><cell cols="3">3.35 3.75 2.13 3.67 1.57 3.40 0.82 0.78 0.65 0.18</cell><cell>0.0156</cell><cell>3.83 6.58 3.28 3.93 2.59 2.32</cell><cell>254</cell></row><row><cell cols="3">T4 1.51 1.55 1.11 2.90 1.59 4.82 0.73 0.52 0.77 0.42</cell><cell>0.14</cell><cell cols="3">3.41 3.80 1.80 3.78 1.27 3.62 0.61 0.51 0.33 0.11</cell><cell>0.0106</cell><cell>4.03 6.10 3.34 3.97 2.56 2.57</cell><cell>241</cell></row><row><cell cols="3">T5 4.63 1.71 2.79 2.79 1.77 4.04 0.13 0.29 0.11 0.40</cell><cell>0.96</cell><cell cols="3">1.66 1.71 1.08 2.52 1.47 3.35 0.34 0.40 0.34 0.41</cell><cell>0.0067</cell><cell>4.57 4.33 3.53 3.72 2.55 3.04</cell><cell>242</cell></row><row><cell cols="3">T6 2.42 1.75 1.55 2.22 1.29 2.66 0.80 0.17 0.82 0.50</cell><cell>0.24</cell><cell cols="3">2.75 2.88 1.20 3.50 1.30 3.94 0.72 0.30 0.59 0.27</cell><cell>0.0088</cell><cell>4.20 4.43 3.44 3.65 2.66 2.84</cell><cell>239</cell></row><row><cell cols="3">Ave 3.16 2.28 1.80 2.40 1.39 2.80 0.76 0.58 0.75 0.59</cell><cell>2.00</cell><cell cols="3">2.88 3.00 1.66 3.41 1.42 3.72 0.68 0.61 0.53 0.32</cell><cell>0.0111</cell><cell>3.99 6.10 3.34 3.88 2.60 2.52</cell><cell>245</cell></row></table></figure>
			<note place="foot" n="1" xml:id="foot_0"><p>For readability, outlier scores greater than 3 times the IQR were grouped together and shown separately in the right plot, which shows the proportion of records with outlier scores beyond 3 * IQR for each anomaly type.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGMENTS</head><p>This work was supported by the <rs type="funder">European Research Council (ERC)</rs> <rs type="programName">Horizon 2020 research and innovation programme</rs> (grant <rs type="grantNumber">n725561</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_7GU9aWj">
					<idno type="grant-number">n725561</idno>
					<orgName type="program" subtype="full">Horizon 2020 research and innovation programme</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Linear Road: A Stream Data Management Benchmark</title>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Arasu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mitch</forename><surname>Cherniack</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eddie</forename><forename type="middle">F</forename><surname>Galvez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Maier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anurag</forename><surname>Maskey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Esther</forename><surname>Ryvkina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Stonebraker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Tibbetts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VLDB Conference</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="480" to="491" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">The UEA Multivariate Time Series Classification Archive</title>
		<author>
			<persName><forename type="first">Anthony</forename><forename type="middle">J</forename><surname>Bagnall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anh</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Dau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Lines</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Large</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Bostrom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eamonn</forename><forename type="middle">J</forename><surname>Southam</surname></persName>
		</author>
		<author>
			<persName><surname>Keogh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.00075</idno>
		<ptr target="http://arxiv.org/abs/1811.00075Accessed:2021-07-27" />
		<imprint>
			<date type="published" when="2018">2018. 2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">MacroBase: Prioritizing Attention in Fast Data</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Bailis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Madden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deepak</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kexin</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sahaana</forename><surname>Suri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Management of Data (SIGMOD)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="541" to="556" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">ObjectNet: A Large-Scale Bias-controlled Dataset for Pushing the Limits of Object Recognition Models</title>
		<author>
			<persName><forename type="first">Andrei</forename><surname>Barbu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Mayo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Alverio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Gutfreund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josh</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boris</forename><surname>Katz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Conference on Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="9453" to="9463" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Chaos Engineering</title>
		<author>
			<persName><forename type="first">Ali</forename><surname>Basiri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niosha</forename><surname>Behnam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruud</forename><surname>De Rooij</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lorin</forename><surname>Hochstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Kosewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Justin</forename><surname>Reynolds</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Casey</forename><surname>Rosenthal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Software</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="35" to="41" />
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Causality-based Explanation of Classification Outcomes</title>
		<author>
			<persName><forename type="first">Leopoldo</forename><forename type="middle">E</forename><surname>Bertossi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jordan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maximilian</forename><surname>Schleich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Suciu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zografoula</forename><surname>Vagena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fourth Workshop on Data Management for End-To-End Machine Learning (DEEM)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Outlier Detection in Regression Models with ARIMA Errors using Robust Estimates</title>
		<author>
			<persName><forename type="first">Ana</forename><surname>Maria Bianco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marta</forename><surname>Garcia Ben</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eunie</forename><surname>Martinez</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><forename type="middle">J</forename><surname>Yohai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Forecasting</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="565" to="579" />
			<date type="published" when="2001">2001. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Automated Anomaly Detection in Large Sequences</title>
		<author>
			<persName><forename type="first">Paul</forename><surname>Boniol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michele</forename><surname>Linardi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Federico</forename><surname>Roncallo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Themis</forename><surname>Palpanas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 36th International Conference on Data Engineering (ICDE)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1834" to="1837" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Series2Graph: Graph-Based Subsequence Anomaly Detection for Time Series</title>
		<author>
			<persName><forename type="first">Paul</forename><surname>Boniol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Themis</forename><surname>Palpanas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the VLDB Endowment (PVLDB)</title>
		<meeting>the VLDB Endowment (PVLDB)</meeting>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="1821" to="1834" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Collective Anomaly Detection Based on Long Short-Term Memory Recurrent Neural Networks</title>
		<author>
			<persName><forename type="first">Loïc</forename><surname>Bontemps</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Van Loi Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nhien-An</forename><surname>Mcdermott</surname></persName>
		</author>
		<author>
			<persName><surname>Le-Khac</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Future Data and Security Engineering (FDSE)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">10018</biblScope>
			<biblScope unit="page" from="141" to="152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">LOF: Identifying Density-based Local Outliers</title>
		<author>
			<persName><forename type="first">Markus</forename><forename type="middle">M</forename><surname>Breunig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hans-Peter</forename><surname>Kriegel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raymond</forename><forename type="middle">T</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jörg</forename><surname>Sander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Management of Data (SIGMOD)</title>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="93" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Deep Learning for Anomaly Detection: A Survey</title>
		<author>
			<persName><forename type="first">Raghavendra</forename><surname>Chalapathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjay</forename><surname>Chawla</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.03407</idno>
		<ptr target="http://arxiv.org/abs/1901.03407" />
		<imprint>
			<date type="published" when="2019">2019. 2019. 2021-07-27</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Anomaly Detection</title>
		<author>
			<persName><forename type="first">Varun</forename><surname>Chandola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arindam</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vipin</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">A ACM Computing Surveys</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Analysis of DAWNBench, a Time-to-Accuracy Machine Learning Performance Benchmark</title>
		<author>
			<persName><forename type="first">Cody</forename><surname>Coleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deepak</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luigi</forename><surname>Nardi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tian</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Bailis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kunle</forename><surname>Olukotun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Ré</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matei</forename><surname>Zaharia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGOPS Operating Systems Review</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="14" to="25" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Benchmarking Cloud Serving Systems with YCSB</title>
		<author>
			<persName><forename type="first">Brian</forename><forename type="middle">F</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Silberstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erwin</forename><surname>Tam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raghu</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Russell</forename><surname>Sears</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Symposium on Cloud Computing (SoCC)</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="143" to="154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<ptr target="https://www.spec.org/Accessed:2021-07-27" />
	</analytic>
	<monogr>
		<title level="m">SPEC Benchmarks</title>
		<title level="s">The Standard Performance Evaluation Corporation</title>
		<imprint />
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<ptr target="http://www.tpc.org/Accessed:2021-07-27" />
	</analytic>
	<monogr>
		<title level="m">TPC Benchmarks</title>
		<imprint />
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">The UCR Time Series Archive</title>
		<author>
			<persName><forename type="first">Anh</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><forename type="middle">J</forename><surname>Dau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaveh</forename><surname>Bagnall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chin-Chia</forename><surname>Kamgar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Michael Yeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaghayegh</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chotirat</forename><surname>Gharghabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ann</forename><surname>Ratanamahatana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eamonn</forename><forename type="middle">J</forename><surname>Keogh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.07758</idno>
		<ptr target="http://arxiv.org/abs/1810.07758" />
		<imprint>
			<date type="published" when="2018">2018. 2018. 2021-07-27</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Ima-geNet: A Large-Scale Hierarchical Image Database</title>
		<author>
			<persName><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei-Fei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">The UCI Machine Learning Repository</title>
		<author>
			<persName><forename type="first">Dheeru</forename><surname>Dua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Casey</forename><surname>Graff</surname></persName>
		</author>
		<ptr target="http://archive.ics.uci.edu/ml/" />
		<imprint>
			<date type="published" when="2021-07-27">2021-07-27</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Interpretable and Informative Explanations of Outcomes</title>
		<author>
			<persName><forename type="first">Kareem</forename><forename type="middle">El</forename><surname>Gebaly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Parag</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Golab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Flip</forename><surname>Korn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Divesh</forename><surname>Srivastava</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the VLDB Endowment (PVLDB)</title>
		<meeting>the VLDB Endowment (PVLDB)</meeting>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="61" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Systematic Construction of Anomaly Detection Benchmarks from Real Data</title>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">F</forename><surname>Emmott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shubhomoy</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Dietterich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Fern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weng-Keen</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGKDD Workshop on Outlier Detection and Description (ODD)</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="16" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><surname>Fico</surname></persName>
		</author>
		<ptr target="https://community.fico.com/s/explainable-machine-learning-challenge" />
		<title level="m">Explainable Machine Learning Challenge</title>
		<imprint>
			<date type="published" when="2018">2018. 2021-07-27</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Distilling a Neural Network Into a Soft Decision Tree</title>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Frosst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Comprehensibility and Explanation in AI and ML</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">BigBench: Towards an Industry Standard Benchmark for Big Data Analytics</title>
		<author>
			<persName><forename type="first">Ahmad</forename><surname>Ghazal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tilmann</forename><surname>Rabl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minqing</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francois</forename><surname>Raab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meikel</forename><surname>Poess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alain</forename><surname>Crolotte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hans-Arno</forename><surname>Jacobsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGMOD International Conference on Management of Data</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1197" to="1208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">The Benchmark Handbook for Database and Transaction Systems</title>
		<author>
			<persName><forename type="first">Jim</forename><surname>Gray</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993">1993</date>
			<publisher>Morgan Kaufmann</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Outlier Detection for Temporal Data: A Survey</title>
		<author>
			<persName><forename type="first">Manish</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Charu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2250" to="2267" />
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Reducing the Dimensionality of Data with Neural Networks</title>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">313</biblScope>
			<biblScope unit="page" from="504" to="507" />
			<date type="published" when="2006">2006. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Exathlon: A Benchmark for Explainable Anomaly Detection over Time Series</title>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Jacob</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arnaud</forename><surname>Stiegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bijan</forename><surname>Rad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanlei</forename><surname>Diao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nesime</forename><surname>Tatbul</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.05073</idno>
		<ptr target="http://arxiv.org/abs/2010.05073" />
		<imprint>
			<date type="published" when="2021-07-27">2021. 2021. 2021-07-27</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">ExplainIt! -A Declarative Root-cause Analysis Engine for Time Series Data</title>
		<author>
			<persName><forename type="first">Vimalkumar</forename><surname>Jeyakumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omid</forename><surname>Madani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Parandeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashutosh</forename><surname>Kulshreshtha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weifei</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Navindra</forename><surname>Yadav</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Management of Data (SIGMOD)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="333" to="348" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Detailed Diagnosis in Enterprise Networks</title>
		<author>
			<persName><forename type="first">Srikanth</forename><surname>Kandula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ratul</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Verkaik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharad</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jitu</forename><surname>Padhye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Bahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGCOMM Conference</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="243" to="254" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Anomaly Explanation with Random Forests</title>
		<author>
			<persName><forename type="first">Martin</forename><surname>Kopp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomás</forename><surname>Pevný</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Holena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Systems with Applications</title>
		<imprint>
			<biblScope unit="volume">149</biblScope>
			<biblScope unit="page">113187</biblScope>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Interpretable Decision Sets: A Joint Framework for Description and Prediction</title>
		<author>
			<persName><forename type="first">Himabindu</forename><surname>Lakkaraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><forename type="middle">H</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGKDD Conference</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1675" to="1684" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Evaluating Real-Time Anomaly Detection Algorithms -The Numenta Anomaly Benchmark</title>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Lavin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Subutai</forename><surname>Ahmad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Machine Learning and Applications (ICMLA)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="38" to="44" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">SCALLA: A Platform for Scalable One-Pass Analytics Using MapReduce</title>
		<author>
			<persName><forename type="first">Boduo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Mazur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanlei</forename><surname>Diao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Mcgregor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prashant</forename><forename type="middle">J</forename><surname>Shenoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Database Systems</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">27</biblScope>
			<date type="published" when="2012">2012. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Isolation-Based Anomaly Detection</title>
		<author>
			<persName><forename type="first">Tony</forename><surname>Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Ting</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhi-Hua</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Knowledge Discovery from Data</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="3" />
			<date type="published" when="2012">2012. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A Unified Approach to Interpreting Model Predictions</title>
		<author>
			<persName><forename type="first">M</forename><surname>Scott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Su-In</forename><surname>Lundberg</surname></persName>
		</author>
		<author>
			<persName><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Conference on Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4765" to="4774" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Diagnosing Root Causes of Intermittent Slow Queries in Large-Scale Cloud Databases</title>
		<author>
			<persName><forename type="first">Minghua</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shenglin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinhao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanwen</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yilin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nengjun</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feifei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changcheng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Pei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the VLDB Endowment (PVLDB)</title>
		<meeting>the VLDB Endowment (PVLDB)</meeting>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="1176" to="1189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Long Short Term Memory Networks for Anomaly Detection in Time Series</title>
		<author>
			<persName><forename type="first">Pankaj</forename><surname>Malhotra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lovekesh</forename><surname>Vig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gautam</forename><surname>Shroff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Puneet</forename><surname>Agarwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Symposium on Artificial Neural Networks (ESANN)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="89" to="94" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">WordNet: A Lexical Database for English</title>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="39" to="41" />
			<date type="published" when="1995">1995. 1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Interpretable Machine Learning: A Guide for Making Black Box Models Explainable</title>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Molnar</surname></persName>
		</author>
		<ptr target="https://christophm.github.io/interpretable-ml-book/" />
		<imprint>
			<date type="published" when="2021-07-27">2021. 2021-07-27</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">ADABench -Towards an Industry Standard Benchmark for Advanced Analytics</title>
		<author>
			<persName><forename type="first">Tilmann</forename><surname>Rabl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Brücke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Härtling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stella</forename><surname>Stars</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rodrigo</forename><forename type="middle">Escobar</forename><surname>Palacios</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hamesh</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Satyam</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Boden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jens</forename><surname>Meiners</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Schelter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TPC Technology Conference on Performance Evaluation and Benchmarking (TPCTC)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="47" to="63" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Outlier Detection DataSets (ODDS) Library</title>
		<author>
			<persName><forename type="first">Shebuti</forename><surname>Rayana</surname></persName>
		</author>
		<ptr target="http://odds.cs.stonybrook.edu/" />
		<imprint>
			<date type="published" when="2016">2016. 2021-07-27</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Explaining the Predictions of Any Classifier</title>
		<author>
			<persName><forename type="first">Marco</forename><surname>Túlio Ribeiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGKDD Conference</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1135" to="1144" />
		</imprint>
	</monogr>
	<note>Why Should I Trust You?</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Anchors: High-Precision Model-Agnostic Explanations</title>
		<author>
			<persName><forename type="first">Marco</forename><surname>Túlio Ribeiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Explaining Query Answers with Explanation-Ready Databases</title>
		<author>
			<persName><forename type="first">Sudeepa</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurel</forename><surname>Orr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Suciu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the VLDB Endowment (PVLDB)</title>
		<meeting>the VLDB Endowment (PVLDB)</meeting>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="348" to="359" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Unsupervised Anomaly Detection with Generative Adversarial Networks to Guide Marker Discovery</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Schlegl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Seeböck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><forename type="middle">M</forename><surname>Waldstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ursula</forename><surname>Schmidt-Erfurth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georg</forename><surname>Langs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Information Processing in Medical Imaging</title>
		<imprint>
			<publisher>IPMI</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="146" to="157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Demystifying Numenta Anomaly Benchmark</title>
		<author>
			<persName><forename type="first">Nidhi</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Craig</forename><surname>Olinsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Neural Networks (IJCNN)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1570" to="1577" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<ptr target="https://medium.com/@tao_66792/how-are-big-companies-using-apache-spark-413743dbbbae" />
		<title level="m">How are Big Companies using Apache Spark</title>
		<imprint>
			<date type="published" when="2021-07-27">2021-07-27</date>
		</imprint>
	</monogr>
	<note>Spark-uses</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Axiomatic Attribution for Deep Networks</title>
		<author>
			<persName><forename type="first">Mukund</forename><surname>Sundararajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankur</forename><surname>Taly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiqi</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<biblScope unit="page" from="3319" to="3328" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Precision and Recall for Time Series</title>
		<author>
			<persName><forename type="first">Nesime</forename><surname>Tatbul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Tae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mejbah</forename><surname>Zdonik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Justin</forename><surname>Alam</surname></persName>
		</author>
		<author>
			<persName><surname>Gottschlich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Conference on Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="1924">2018. 1924-1934</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Distance Based Outlier Detection for Data Streams</title>
		<author>
			<persName><forename type="first">Luan</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liyue</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cyrus</forename><surname>Shahabi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the VLDB Endowment (PVLDB)</title>
		<meeting>the VLDB Endowment (PVLDB)</meeting>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1089" to="1100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Scorpion: Explaining Away Outliers in Aggregate Queries</title>
		<author>
			<persName><forename type="first">Eugene</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Madden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the VLDB Endowment (PVLDB)</title>
		<meeting>the VLDB Endowment (PVLDB)</meeting>
		<imprint>
			<date type="published" when="2013">2013. 2013</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="553" to="564" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Beyond Sparsity: Tree Regularization of Deep Models for Interpretability</title>
		<author>
			<persName><forename type="first">Mike</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">C</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sonali</forename><surname>Parbhoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maurizio</forename><surname>Zazzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roth</forename><surname>Volker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Finale</forename><surname>Doshi-Velez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Unsupervised Anomaly Detection via Variational Auto-Encoder for Seasonal KPIs in Web Applications</title>
		<author>
			<persName><forename type="first">Haowen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenxiao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nengwen</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeyan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiahao</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhihan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youjian</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Honglin</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International World Wide Web Conference (WWW)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="187" to="196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Outlier Detection: How to Threshold Outlier Scores?</title>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Susanto</forename><surname>Rahardja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pasi</forename><surname>Fränti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence, Information Processing and Cloud Computing (AIIPCC)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="1" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">EXstream: Explaining Anomalies in Event Stream Monitoring</title>
		<author>
			<persName><forename type="first">Haopeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanlei</forename><surname>Diao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandra</forename><surname>Meliou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Extending Database Technology (EDBT)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="156" to="167" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</teiCorpus></text>
</TEI>