<?xml version='1.0' encoding='utf-8'?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" version="1.1" xsi:schemaLocation="http://www.tei-c.org/ns/1.0 http://api.archives-ouvertes.fr/documents/aofr-sword.xsd">
  <teiHeader>
    <fileDesc>
      <titleStmt>
        <title>HAL TEI export of hal-03769993v6</title>
      </titleStmt>
      <publicationStmt>
        <distributor>CCSD</distributor>
        <availability status="restricted">
          <licence target="http://creativecommons.org/licenses/by/">Attribution</licence>
        </availability>
        <date when="2024-04-29T11:49:22+02:00" />
      </publicationStmt>
      <sourceDesc>
        <p part="N">HAL API platform</p>
      </sourceDesc>
    </fileDesc>
  </teiHeader>
  <text>
    <body>
      <listBibl>
        <biblFull>
          <titleStmt>
            <title xml:lang="en">On the benefits of self-taught learning for brain decoding</title>
            <author role="aut">
              <persName>
                <forename type="first">Elodie</forename>
                <surname>Germani</surname>
              </persName>
              <email type="md5">7dfd091c0353d9d77350b8ab48a9f0e0</email>
              <email type="domain">irisa.fr</email>
              <idno type="idhal" notation="string">elodie-germani</idno>
              <idno type="idhal" notation="numeric">753996</idno>
              <idno type="halauthorid" notation="string">51689-753996</idno>
              <idno type="ORCID">https://orcid.org/0000-0002-5786-9538</idno>
              <affiliation ref="#struct-491653" />
              <affiliation ref="#struct-1092625" />
            </author>
            <author role="aut">
              <persName>
                <forename type="first">Elisa</forename>
                <surname>Fromont</surname>
              </persName>
              <email type="md5">285c20c8a13c3fb5d7d8664b4afde36a</email>
              <email type="domain">irisa.fr</email>
              <idno type="idhal" notation="string">efromont</idno>
              <idno type="idhal" notation="numeric">9985</idno>
              <idno type="halauthorid" notation="string">8420-9985</idno>
              <idno type="ORCID">https://orcid.org/0000-0003-0133-3491</idno>
              <idno type="GOOGLE SCHOLAR">https://scholar.google.com/citations?user=c_7y_6MAAAAJ&amp;hl=ru&amp;cstart=0&amp;pagesize=20</idno>
              <idno type="IDREF">https://www.idref.fr/095621601</idno>
              <affiliation ref="#struct-491653" />
              <affiliation ref="#struct-56663" />
            </author>
            <author role="aut">
              <persName>
                <forename type="first">Camille</forename>
                <surname>Maumet</surname>
              </persName>
              <email type="md5">5cb96be2aaa87d4cf2f460c053a579b2</email>
              <email type="domain">inria.fr</email>
              <idno type="idhal" notation="string">cmaumet</idno>
              <idno type="idhal" notation="numeric">2060</idno>
              <idno type="halauthorid" notation="string">10275-2060</idno>
              <idno type="IDREF">https://www.idref.fr/171543009</idno>
              <idno type="ORCID">https://orcid.org/0000-0002-6290-553X</idno>
              <affiliation ref="#struct-1092625" />
            </author>
            <editor role="depositor">
              <persName>
                <forename>Elodie</forename>
                <surname>Germani</surname>
              </persName>
              <email type="md5">7dfd091c0353d9d77350b8ab48a9f0e0</email>
              <email type="domain">irisa.fr</email>
            </editor>
            <funder ref="#projanr-51233" />
            <funder>Region Bretagne (ARED MAPIS)</funder>
          </titleStmt>
          <editionStmt>
            <edition n="v1">
              <date type="whenSubmitted">2022-09-16 15:09:13</date>
            </edition>
            <edition n="v2">
              <date type="whenSubmitted">2022-10-21 11:11:43</date>
            </edition>
            <edition n="v3">
              <date type="whenSubmitted">2023-01-25 15:20:47</date>
            </edition>
            <edition n="v4">
              <date type="whenSubmitted">2023-02-15 15:44:23</date>
            </edition>
            <edition n="v5">
              <date type="whenSubmitted">2023-04-17 11:06:05</date>
            </edition>
            <edition n="v6" type="current">
              <date type="whenSubmitted">2023-05-03 13:11:46</date>
              <date type="whenWritten">2022</date>
              <date type="whenModified">2024-04-15 11:25:23</date>
              <date type="whenReleased">2023-05-03 15:39:32</date>
              <date type="whenProduced">2023-05-03</date>
              <date type="whenEndEmbargoed">2023-05-03</date>
              <ref type="file" target="https://inria.hal.science/hal-03769993v6/document">
                <date notBefore="2023-05-03" />
              </ref>
              <ref type="file" subtype="publisherPaid" n="1" target="https://inria.hal.science/hal-03769993v6/file/on_the_benefits_of_self_taught_learning.pdf">
                <date notBefore="2023-05-03" />
              </ref>
              <ref type="externalLink" target="http://arxiv.org/pdf/2209.10099" />
            </edition>
            <respStmt>
              <resp>contributor</resp>
              <name key="1192050">
                <persName>
                  <forename>Elodie</forename>
                  <surname>Germani</surname>
                </persName>
                <email type="md5">7dfd091c0353d9d77350b8ab48a9f0e0</email>
                <email type="domain">irisa.fr</email>
              </name>
            </respStmt>
          </editionStmt>
          <publicationStmt>
            <distributor>CCSD</distributor>
            <idno type="halId">hal-03769993</idno>
            <idno type="halUri">https://inria.hal.science/hal-03769993</idno>
            <idno type="halBibtex">germani:hal-03769993</idno>
            <idno type="halRefHtml">&lt;i&gt;GigaScience&lt;/i&gt;, 2023, 12, pp.1-17. &lt;a target="_blank" href="https://dx.doi.org/10.1093/gigascience/giad029"&gt;&amp;#x27E8;10.1093/gigascience/giad029&amp;#x27E9;&lt;/a&gt;</idno>
            <idno type="halRef">GigaScience, 2023, 12, pp.1-17. &amp;#x27E8;10.1093/gigascience/giad029&amp;#x27E9;</idno>
            <availability status="restricted">
              <licence target="http://creativecommons.org/licenses/by/">Attribution</licence>
            </availability>
          </publicationStmt>
          <seriesStmt>
            <idno type="stamp" n="UNIV-RENNES1">Université de Rennes 1</idno>
            <idno type="stamp" n="CNRS">CNRS - Centre national de la recherche scientifique</idno>
            <idno type="stamp" n="INRIA">INRIA - Institut National de Recherche en Informatique et en Automatique</idno>
            <idno type="stamp" n="UNIV-UBS">Université de Bretagne Sud</idno>
            <idno type="stamp" n="INSA-RENNES">Institut National des Sciences Appliquées de Rennes</idno>
            <idno type="stamp" n="INRIA-RENNES">INRIA Rennes - Bretagne Atlantique</idno>
            <idno type="stamp" n="IRISA">Irisa</idno>
            <idno type="stamp" n="IRISA_SET">IRISA_SET</idno>
            <idno type="stamp" n="INRIA_TEST">INRIA - Institut National de Recherche en Informatique et en Automatique</idno>
            <idno type="stamp" n="TESTALAIN1">TESTALAIN1</idno>
            <idno type="stamp" n="CENTRALESUPELEC">Ecole CentraleSupélec</idno>
            <idno type="stamp" n="INRIA2">INRIA 2</idno>
            <idno type="stamp" n="UR1-HAL">Publications labos UR1 dans HAL-Rennes 1</idno>
            <idno type="stamp" n="UR1-MATH-STIC">UR1 - publications Maths-STIC</idno>
            <idno type="stamp" n="UR1-UFR-ISTIC">UFR ISTIC Informatique et électronique</idno>
            <idno type="stamp" n="TEST-UR-CSS">TEST Université de Rennes CSS</idno>
            <idno type="stamp" n="UNIV-RENNES">Université de Rennes</idno>
            <idno type="stamp" n="INRIA-RENGRE">INRIA-RENGRE</idno>
            <idno type="stamp" n="ANR">ANR</idno>
            <idno type="stamp" n="UR1-MATH-NUM">Pôle UnivRennes - Mathématiques - Numérique </idno>
            <idno type="stamp" n="CYBERSCHOOL">CyberSchool - Ecole universitaire de recherche en Cybersécurité</idno>
          </seriesStmt>
          <notesStmt>
            <note type="audience" n="2">International</note>
            <note type="popular" n="0">No</note>
            <note type="peer" n="1">Yes</note>
          </notesStmt>
          <sourceDesc>
            <biblStruct>
              <analytic>
                <title xml:lang="en">On the benefits of self-taught learning for brain decoding</title>
                <author role="aut">
                  <persName>
                    <forename type="first">Elodie</forename>
                    <surname>Germani</surname>
                  </persName>
                  <email type="md5">7dfd091c0353d9d77350b8ab48a9f0e0</email>
                  <email type="domain">irisa.fr</email>
                  <idno type="idhal" notation="string">elodie-germani</idno>
                  <idno type="idhal" notation="numeric">753996</idno>
                  <idno type="halauthorid" notation="string">51689-753996</idno>
                  <idno type="ORCID">https://orcid.org/0000-0002-5786-9538</idno>
                  <affiliation ref="#struct-491653" />
                  <affiliation ref="#struct-1092625" />
                </author>
                <author role="aut">
                  <persName>
                    <forename type="first">Elisa</forename>
                    <surname>Fromont</surname>
                  </persName>
                  <email type="md5">285c20c8a13c3fb5d7d8664b4afde36a</email>
                  <email type="domain">irisa.fr</email>
                  <idno type="idhal" notation="string">efromont</idno>
                  <idno type="idhal" notation="numeric">9985</idno>
                  <idno type="halauthorid" notation="string">8420-9985</idno>
                  <idno type="ORCID">https://orcid.org/0000-0003-0133-3491</idno>
                  <idno type="GOOGLE SCHOLAR">https://scholar.google.com/citations?user=c_7y_6MAAAAJ&amp;hl=ru&amp;cstart=0&amp;pagesize=20</idno>
                  <idno type="IDREF">https://www.idref.fr/095621601</idno>
                  <affiliation ref="#struct-491653" />
                  <affiliation ref="#struct-56663" />
                </author>
                <author role="aut">
                  <persName>
                    <forename type="first">Camille</forename>
                    <surname>Maumet</surname>
                  </persName>
                  <email type="md5">5cb96be2aaa87d4cf2f460c053a579b2</email>
                  <email type="domain">inria.fr</email>
                  <idno type="idhal" notation="string">cmaumet</idno>
                  <idno type="idhal" notation="numeric">2060</idno>
                  <idno type="halauthorid" notation="string">10275-2060</idno>
                  <idno type="IDREF">https://www.idref.fr/171543009</idno>
                  <idno type="ORCID">https://orcid.org/0000-0002-6290-553X</idno>
                  <affiliation ref="#struct-1092625" />
                </author>
              </analytic>
              <monogr>
                <idno type="halJournalId" status="VALID">69034</idno>
                <idno type="eissn">2047-217X</idno>
                <title level="j">GigaScience</title>
                <imprint>
                  <publisher>Oxford Univ Press</publisher>
                  <biblScope unit="volume">12</biblScope>
                  <biblScope unit="pp">1-17</biblScope>
                  <date type="datePub">2023-05-03</date>
                </imprint>
              </monogr>
              <idno type="arxiv">2209.10099</idno>
              <idno type="doi">10.1093/gigascience/giad029</idno>
            </biblStruct>
          </sourceDesc>
          <profileDesc>
            <langUsage>
              <language ident="en">English</language>
            </langUsage>
            <textClass>
              <keywords scheme="author">
                <term xml:lang="en">Self-taught Learning</term>
                <term xml:lang="en">Brain Decoding</term>
                <term xml:lang="en">Autoencoder</term>
                <term xml:lang="en">Convolutional Neural Network</term>
                <term xml:lang="en">Deep Learning</term>
              </keywords>
              <classCode scheme="halDomain" n="info.info-ai">Computer Science [cs]/Artificial Intelligence [cs.AI]</classCode>
              <classCode scheme="halDomain" n="scco.neur">Cognitive science/Neuroscience</classCode>
              <classCode scheme="halTypology" n="ART">Journal articles</classCode>
              <classCode scheme="halOldTypology" n="ART">Journal articles</classCode>
              <classCode scheme="halTreeTypology" n="ART">Journal articles</classCode>
            </textClass>
            <abstract xml:lang="en">
              <p>Context. We study the benefits of using a large public neuroimaging database composed of fMRI statistic maps, in a self-taught learning framework, for improving brain decoding on new tasks. First, we leverage the NeuroVault database to train, on a selection of relevant statistic maps, a convolutional autoencoder to reconstruct these maps. Then, we use this trained encoder to initialize a supervised convolutional neural network to classify tasks or cognitive processes of unseen statistic maps from large collections of the NeuroVault database. Results. We show that such a self-taught learning process always improves the performance of the classifiers but the magnitude of the benefits strongly depends on the number of samples available both for pre-training and finetuning the models and on the complexity of the targeted downstream task. Conclusion. The pre-trained model improves the classification performance and displays more generalizable features, less sensitive to individual differences.</p>
            </abstract>
          </profileDesc>
        </biblFull>
      </listBibl>
    </body>
    <back>
      <listOrg type="structures">
        <org type="researchteam" xml:id="struct-491653" status="VALID">
          <idno type="RNSR">201622044W</idno>
          <orgName>Large Scale Collaborative Data Mining</orgName>
          <orgName type="acronym">LACODAM</orgName>
          <desc>
            <address>
              <addrLine>Campus de Beaulieu 35042 Rennes cedex</addrLine>
              <country key="FR" />
            </address>
            <ref type="url">https://www.inria.fr/equipes/lacodam</ref>
          </desc>
          <listRelation>
            <relation active="#struct-419153" type="direct" />
            <relation active="#struct-300009" type="indirect" />
            <relation active="#struct-491231" type="direct" />
            <relation active="#struct-490899" type="indirect" />
            <relation active="#struct-105160" type="indirect" />
            <relation active="#struct-117606" type="indirect" />
            <relation active="#struct-301232" type="indirect" />
            <relation active="#struct-172265" type="indirect" />
            <relation active="#struct-247362" type="indirect" />
            <relation active="#struct-411575" type="indirect" />
            <relation name="UMR6074" active="#struct-441569" type="indirect" />
            <relation active="#struct-481355" type="indirect" />
            <relation active="#struct-302102" type="indirect" />
          </listRelation>
        </org>
        <org type="researchteam" xml:id="struct-1092625" status="VALID">
          <idno type="RNSR">200518339S</idno>
          <idno type="ROR">https://ror.org/02hp31992</idno>
          <orgName>Neuroimagerie: méthodes et applications</orgName>
          <orgName type="acronym">EMPENN</orgName>
          <date type="start">2022-01-01</date>
          <date type="end">2023-12-31</date>
          <desc>
            <address>
              <addrLine>Irisa, Campus de Beaulieu 35042 Rennes cedex</addrLine>
              <country key="FR" />
            </address>
            <ref type="url">https://www.inria.fr/equipes/empenn</ref>
          </desc>
          <listRelation>
            <relation active="#struct-303623" type="direct" />
            <relation active="#struct-419153" type="direct" />
            <relation active="#struct-300009" type="indirect" />
            <relation active="#struct-1092619" type="direct" />
            <relation active="#struct-490899" type="indirect" />
            <relation active="#struct-105160" type="indirect" />
            <relation active="#struct-117606" type="indirect" />
            <relation active="#struct-301232" type="indirect" />
            <relation active="#struct-172265" type="indirect" />
            <relation active="#struct-247362" type="indirect" />
            <relation active="#struct-411575" type="indirect" />
            <relation name="UMR6074" active="#struct-441569" type="indirect" />
            <relation active="#struct-481355" type="indirect" />
            <relation active="#struct-302102" type="indirect" />
          </listRelation>
        </org>
        <org type="laboratory" xml:id="struct-56663" status="VALID">
          <idno type="IdRef">03442945X</idno>
          <idno type="ISNI">0000000119314817</idno>
          <idno type="ROR">https://ror.org/055khg266</idno>
          <idno type="Wikidata">Q1665127</idno>
          <orgName>Institut universitaire de France</orgName>
          <orgName type="acronym">IUF</orgName>
          <desc>
            <address>
              <addrLine>Maison des Universités 103 Boulevard Saint-Michel 75005 Paris</addrLine>
              <country key="FR" />
            </address>
            <ref type="url">http://iuf.amue.fr/</ref>
          </desc>
          <listRelation>
            <relation active="#struct-301855" type="direct" />
          </listRelation>
        </org>
        <org type="laboratory" xml:id="struct-419153" status="VALID">
          <idno type="RNSR">198018249C</idno>
          <idno type="ROR">https://ror.org/04040yw90</idno>
          <orgName>Inria Rennes – Bretagne Atlantique</orgName>
          <desc>
            <address>
              <addrLine>Campus de beaulieu35042 Rennes cedex</addrLine>
              <country key="FR" />
            </address>
            <ref type="url">http://www.inria.fr/centre/rennes</ref>
          </desc>
          <listRelation>
            <relation active="#struct-300009" type="direct" />
          </listRelation>
        </org>
        <org type="institution" xml:id="struct-300009" status="VALID">
          <idno type="ROR">https://ror.org/02kvxyf05</idno>
          <orgName>Institut National de Recherche en Informatique et en Automatique</orgName>
          <orgName type="acronym">Inria</orgName>
          <desc>
            <address>
              <addrLine>Domaine de VoluceauRocquencourt - BP 10578153 Le Chesnay Cedex</addrLine>
              <country key="FR" />
            </address>
            <ref type="url">http://www.inria.fr/en/</ref>
          </desc>
        </org>
        <org type="department" xml:id="struct-491231" status="VALID">
          <orgName>GESTION DES DONNÉES ET DE LA CONNAISSANCE</orgName>
          <orgName type="acronym">IRISA-D7</orgName>
          <desc>
            <address>
              <country key="FR" />
            </address>
            <ref type="url">https://www.irisa.fr/fr/departements/d7-gestion-donnees-connaissance</ref>
          </desc>
          <listRelation>
            <relation active="#struct-490899" type="direct" />
            <relation active="#struct-105160" type="indirect" />
            <relation active="#struct-117606" type="indirect" />
            <relation active="#struct-301232" type="indirect" />
            <relation active="#struct-172265" type="indirect" />
            <relation active="#struct-247362" type="indirect" />
            <relation active="#struct-300009" type="indirect" />
            <relation active="#struct-411575" type="indirect" />
            <relation name="UMR6074" active="#struct-441569" type="indirect" />
            <relation active="#struct-481355" type="indirect" />
            <relation active="#struct-302102" type="indirect" />
          </listRelation>
        </org>
        <org type="laboratory" xml:id="struct-490899" status="VALID">
          <idno type="IdRef">026386909</idno>
          <idno type="ISNI">0000 0001 2298 7270</idno>
          <idno type="RNSR">200012163A</idno>
          <idno type="ROR">https://ror.org/00myn0z94</idno>
          <orgName>Institut de Recherche en Informatique et Systèmes Aléatoires</orgName>
          <orgName type="acronym">IRISA</orgName>
          <date type="start">2017-01-01</date>
          <desc>
            <address>
              <addrLine>Avenue du général LeclercCampus de Beaulieu 35042 RENNES CEDEX</addrLine>
              <country key="FR" />
            </address>
            <ref type="url">http://www.irisa.fr</ref>
          </desc>
          <listRelation>
            <relation active="#struct-105160" type="direct" />
            <relation active="#struct-117606" type="direct" />
            <relation active="#struct-301232" type="indirect" />
            <relation active="#struct-172265" type="direct" />
            <relation active="#struct-247362" type="direct" />
            <relation active="#struct-300009" type="direct" />
            <relation active="#struct-411575" type="direct" />
            <relation name="UMR6074" active="#struct-441569" type="direct" />
            <relation active="#struct-481355" type="direct" />
            <relation active="#struct-302102" type="indirect" />
          </listRelation>
        </org>
        <org type="institution" xml:id="struct-105160" status="VALID">
          <idno type="ROR">https://ror.org/015m7wh34</idno>
          <orgName>Université de Rennes</orgName>
          <orgName type="acronym">UR</orgName>
          <desc>
            <address>
              <addrLine>Campus de Beaulieu, 263 avenue Général Leclerc, CS 74205, 35042 RENNES CEDEX</addrLine>
              <country key="FR" />
            </address>
            <ref type="url">https://www.univ-rennes.fr/</ref>
          </desc>
        </org>
        <org type="institution" xml:id="struct-117606" status="VALID">
          <idno type="ROR">https://ror.org/04xaa4j22</idno>
          <orgName>Institut National des Sciences Appliquées - Rennes</orgName>
          <orgName type="acronym">INSA Rennes</orgName>
          <desc>
            <address>
              <addrLine>20, avenue des Buttes de Coësmes - CS 70839 - 35708 Rennes cedex 7</addrLine>
              <country key="FR" />
            </address>
            <ref type="url">http://www.insa-rennes.fr/</ref>
          </desc>
          <listRelation>
            <relation active="#struct-301232" type="direct" />
          </listRelation>
        </org>
        <org type="regroupinstitution" xml:id="struct-301232" status="VALID">
          <idno type="IdRef">162105150</idno>
          <orgName>Institut National des Sciences Appliquées</orgName>
          <orgName type="acronym">INSA</orgName>
          <desc>
            <address>
              <country key="FR" />
            </address>
          </desc>
        </org>
        <org type="institution" xml:id="struct-172265" status="VALID">
          <idno type="ROR">https://ror.org/04ed7fw48</idno>
          <orgName>Université de Bretagne Sud</orgName>
          <orgName type="acronym">UBS</orgName>
          <desc>
            <address>
              <addrLine>BP 92116 - 56321 Lorient cedex</addrLine>
              <country key="FR" />
            </address>
            <ref type="url">http://www.univ-ubs.fr/</ref>
          </desc>
        </org>
        <org type="institution" xml:id="struct-247362" status="VALID">
          <idno type="ROR">https://ror.org/03rxtdc22</idno>
          <orgName>École normale supérieure - Rennes</orgName>
          <orgName type="acronym">ENS Rennes</orgName>
          <desc>
            <address>
              <addrLine>Campus de Ker Lann - avenue Robert Schuman - 35170 Bruz</addrLine>
              <country key="FR" />
            </address>
            <ref type="url">http://www.ens-rennes.fr</ref>
          </desc>
        </org>
        <org type="institution" xml:id="struct-411575" status="VALID">
          <idno type="IdRef">184443237</idno>
          <idno type="ROR">https://ror.org/019tcpt25</idno>
          <orgName>CentraleSupélec</orgName>
          <desc>
            <address>
              <addrLine>3, rue Joliot Curie,Plateau de Moulon,91192 GIF-SUR-YVETTE Cedex</addrLine>
              <country key="FR" />
            </address>
            <ref type="url">http://www.centralesupelec.fr</ref>
          </desc>
        </org>
        <org type="regroupinstitution" xml:id="struct-441569" status="VALID">
          <idno type="IdRef">02636817X</idno>
          <idno type="ISNI">0000000122597504</idno>
          <idno type="ROR">https://ror.org/02feahw73</idno>
          <orgName>Centre National de la Recherche Scientifique</orgName>
          <orgName type="acronym">CNRS</orgName>
          <date type="start">1939-10-19</date>
          <desc>
            <address>
              <country key="FR" />
            </address>
            <ref type="url">https://www.cnrs.fr/</ref>
          </desc>
        </org>
        <org type="institution" xml:id="struct-481355" status="VALID">
          <idno type="IdRef">202743233</idno>
          <idno type="ROR">https://ror.org/030hj3061</idno>
          <orgName>IMT Atlantique</orgName>
          <orgName type="acronym">IMT Atlantique</orgName>
          <date type="start">2017-01-01</date>
          <desc>
            <address>
              <addrLine>Campus Brest : Technopôle Brest-Iroise CS 8381829238 BREST Cedex 3 -Campus Nantes : 4, rue Alfred Kastler- La chantrerie 44300 NANTES -Campus Rennes :  2 Rue de la Châtaigneraie, 35510 CESSON SEVIGNE</addrLine>
              <country key="FR" />
            </address>
            <ref type="url">https://www.imt-atlantique.fr</ref>
          </desc>
          <listRelation>
            <relation active="#struct-302102" type="direct" />
          </listRelation>
        </org>
        <org type="regroupinstitution" xml:id="struct-302102" status="VALID">
          <idno type="ROR">https://ror.org/025vp2923</idno>
          <orgName>Institut Mines-Télécom [Paris]</orgName>
          <orgName type="acronym">IMT</orgName>
          <date type="start">2012-03-01</date>
          <desc>
            <address>
              <addrLine>37-39 Rue Dareau, 75014 Paris</addrLine>
              <country key="FR" />
            </address>
            <ref type="url">http://www.mines-telecom.fr/</ref>
          </desc>
        </org>
        <org type="institution" xml:id="struct-303623" status="VALID">
          <idno type="IdRef">026388278</idno>
          <idno type="ROR">https://ror.org/02vjkv261</idno>
          <orgName>Institut National de la Santé et de la Recherche Médicale</orgName>
          <orgName type="acronym">INSERM</orgName>
          <desc>
            <address>
              <addrLine>101, rue de Tolbiac, 75013 Paris </addrLine>
              <country key="FR" />
            </address>
            <ref type="url">http://www.inserm.fr</ref>
          </desc>
        </org>
        <org type="department" xml:id="struct-1092619" status="VALID">
          <orgName>SIGNAL, IMAGE ET LANGAGE</orgName>
          <orgName type="acronym">IRISA-D6</orgName>
          <date type="start">2022-01-01</date>
          <desc>
            <address>
              <country key="FR" />
            </address>
            <ref type="url">https://www.irisa.fr/departements/media-interactions</ref>
          </desc>
          <listRelation>
            <relation active="#struct-490899" type="direct" />
            <relation active="#struct-105160" type="indirect" />
            <relation active="#struct-117606" type="indirect" />
            <relation active="#struct-301232" type="indirect" />
            <relation active="#struct-172265" type="indirect" />
            <relation active="#struct-247362" type="indirect" />
            <relation active="#struct-300009" type="indirect" />
            <relation active="#struct-411575" type="indirect" />
            <relation name="UMR6074" active="#struct-441569" type="indirect" />
            <relation active="#struct-481355" type="indirect" />
            <relation active="#struct-302102" type="indirect" />
          </listRelation>
        </org>
        <org type="institution" xml:id="struct-301855" status="VALID">
          <orgName>Ministère de l'Education nationale, de l’Enseignement supérieur et de la Recherche</orgName>
          <orgName type="acronym">M.E.N.E.S.R.</orgName>
          <desc>
            <address>
              <addrLine>1 rue Descartes - 75231 Paris cedex 05</addrLine>
              <country key="FR" />
            </address>
          </desc>
        </org>
      </listOrg>
      <listOrg type="projects">
        <org type="anrProject" xml:id="projanr-51233" status="VALID">
          <idno type="anr">ANR-20-THIA-0018</idno>
          <orgName>AI4SDA</orgName>
          <desc>IA pour l'analyse de données sémantiques</desc>
          <date type="start">2020</date>
        </org>
      </listOrg>
    </back>
  <teiCorpus>
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">On the benefits of self-taught learning for brain decoding</title>
				<funder>
					<orgName type="full">NIH Blueprint for Neuroscience Research</orgName>
				</funder>
				<funder ref="#_kxuW6vU">
					<orgName type="full">Agence Nationale de la Recherche</orgName>
					<orgName type="abbreviated">ANR</orgName>
				</funder>
				<funder>
					<orgName type="full">Region Bretagne (ARED MAPIS)</orgName>
				</funder>
				<funder ref="#_bdJGsqq">
					<orgName type="full">16 NIH Institutes and Centers</orgName>
				</funder>
				<funder>
					<orgName type="full">McDonnell Center for Systems Neuroscience at Washington University</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher />
				<availability status="unknown"><licence /></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Elodie</forename><surname>Germani</surname></persName>
							<email>elodie.germani@irisa.fr</email>
							<idno type="ORCID">0000-0002-5786-9538</idno>
						</author>
						<author>
							<persName><forename type="first">Elisa</forename><surname>Fromont</surname></persName>
							<idno type="ORCID">0000-0003-0133-3491</idno>
						</author>
						<author>
							<persName><forename type="first">Camille</forename><surname>Maumet</surname></persName>
							<idno type="ORCID">0000-0002-6290-553X</idno>
						</author>
						<author>
							<affiliation key="aff0">
								<address>
									<settlement>France</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
							</affiliation>
						</author>
						<title level="a" type="main">On the benefits of self-taught learning for brain decoding</title>
					</analytic>
					<monogr>
						<imprint>
							<date />
						</imprint>
					</monogr>
					<idno type="MD5">70B9592A7DAE196150D8A909B119968C</idno>
					<idno type="DOI">10.1093/gigascience/giad029Research</idno>
					<note type="submission">Received: October 10, 2022. Revised: January 24, 2023. Accepted: April 14, 2023</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-04-12T14:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid" />
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>self-taught learning</term>
					<term>brain decoding</term>
					<term>autoencoder</term>
					<term>convolutional neural network</term>
					<term>deep learning</term>
				</keywords>
			</textClass>
			<abstract>
<div><head>Context:</head><p>We study the benefits of using a large public neuroimaging database composed of functional magnetic resonance imaging (fMRI) statistic maps, in a self-taught learning framework, for improving brain decoding on new tasks. First, we leverage the NeuroVault database to train, on a selection of relevant statistic maps, a convolutional autoencoder to reconstruct these maps. Then, we use this trained encoder to initialize a supervised convolutional neural network to classify tasks or cognitive processes of unseen statistic maps from large collections of the NeuroVault database.</p></div>
<div><head>Results:</head><p>We show that such a self-taught learning process always improves the performance of the classifiers, but the magnitude of the benefits strongly depends on the number of samples available both for pretraining and fine-tuning the models and on the complexity of the targeted downstream task.</p></div>
<div><head>Conclusion:</head><p>The pretrained model improves the classification performance and displays more generalizable features, less sensitive to individual differences.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div><head>Introduction</head><p>In the past few years, deep learning (DL) approaches have achieved outstanding performance in the field of neuroimaging <ref type="bibr" target="#b0">[1]</ref> due to their ability to model complex nonlinear relationships in the data. Functional magnetic resonance imaging (fMRI) data, a noninvasive neuroimaging technique in which brain activity is recorded during specific experimental protocols probing different mental processes and giving a big picture on cognition, are often used as input data to these models. These can be used for different purpose, such as disease diagnosis <ref type="bibr" target="#b1">[2]</ref> or brain decoding (i.e., identifying stimuli and cognitive states from brain activities) <ref type="bibr" target="#b2">[3]</ref>, with a common goal: linking a target with highly variable patterns in the data and ignoring aspects of the data that are unrelated to the learning task. Researchers took advantage of the specific properties of fMRI data to build more and more sophisticated models <ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref>.</p><p>However, training effective DL models using MRI data comes with many challenges <ref type="bibr" target="#b11">[12]</ref>. Brain images differ from natural images that are typically used in the DL community on multiple aspects: they contain quantitative information (i.e., statistical values in our context of task fMRI that need to be considered differently during preprocessing steps such as standardization or during training with batch normalization), spatial localization is crucial information (i.e., the same activation in different regions of the brain leads to a completely different interpretation), and the dimensionality of medical images is much larger (i.e., an fMRI statistic map contains tens of thousands of dimensions). This means that a technique used for natural images may fail for medical images (see <ref type="bibr" target="#b12">[13]</ref>).</p><p>While DL models have helped resolve important problems in subfields of brain imaging (such as the segmentation of anatomical datasets), their use in task fMRI is still limited <ref type="bibr" target="#b13">[14]</ref>. Performance of DL models in task fMRI has been limited by the high dimensionality, lack of diversity, and low sample size of conventional datasets <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16]</ref>. In relation with the large number of trainable parameters in DL models <ref type="bibr" target="#b16">[17]</ref>, this makes it particularly difficult to build fair and generalizable DL models for fMRI. Indeed, fMRI datasets are typically composed of 3-dimensional (3D) volumes with hundred thousand dimensions (or voxels) for a rather small number of subjects (typically 10-100). The field also suffers from a large number of sources of variability in the data at the subject level (brain activity patterns differ across subjects), the acquisition level (fMRI scanners and protocols often vary between centers and studies), and the analysis level (different analysis pipelines lead to different brain patterns). In our case, brain decoding models should be robust to all these sources of variability <ref type="bibr" target="#b17">[18]</ref>.</p><p>This problem of small and uniform training sets is not limited to neuroimaging and is well known in the field of machine learning, where researchers extensively use deep transfer learning to improve classification and generalization performance of their models (see, e.g., <ref type="bibr" target="#b18">[19]</ref>). This method consists in using the knowledge obtained from a model trained for a source task on a source dataset and applying it to a target task on a target dataset. Transfer learning proved its worth on natural images by using large, publicly available datasets <ref type="bibr" target="#b19">[20]</ref> to pretrain DL models before fine-tuning them on smaller datasets of a related domain.</p><p>While the availability of large datasets has enabled solving difficult machine learning problems (such as classification), collecting similarly large datasets in brain imaging (including a diversity of participants and modalities) is especially challenging. In fact, in fMRI studies, the median sample size was still limited to N = 30 participants in 2015 <ref type="bibr" target="#b14">[15]</ref>. Efforts for collecting large-scale datasets have arisen in the field in the past 10 years with, for instance, the Human Connectome Project (HCP) <ref type="bibr" target="#b20">[21]</ref> or the UK Biobank <ref type="bibr" target="#b21">[22]</ref> and give hope for an improvement of model performance. But these datasets are still much smaller than those that brought breakthroughs in computer vision and are often much less diverse <ref type="bibr" target="#b22">[23]</ref>.</p><p>To prevent overfitting and allow for generalizable statistical inference, neuroimaging researchers proposed methods to tackle this lack of training data <ref type="bibr" target="#b23">[24]</ref><ref type="bibr" target="#b24">[25]</ref><ref type="bibr" target="#b25">[26]</ref>. For instance, Mensch &amp; al. <ref type="bibr" target="#b26">[27]</ref> built a decoding model using data gathered from 35 studies and thousands of individuals that cover various cognitive domains. Despite the good performance of the models, these can only be applied on restricted sets of studies, discriminating between few cognitive concepts. More annotated training data (e.g., using large public databases) would be required to map a wider set of cognitive processes.</p><p>Many studies were also made on inductive transfer learning with labeled source data as defined in <ref type="bibr" target="#b18">[19]</ref> (e.g., source task and target task are different, as well as source domain and target domain) <ref type="bibr" target="#b27">[28]</ref><ref type="bibr" target="#b28">[29]</ref><ref type="bibr" target="#b29">[30]</ref>. For instance, Thomas &amp; al. <ref type="bibr" target="#b27">[28]</ref> pretrained 2 DL classifiers on a large, public whole-brain fMRI dataset of the HCP, finetuned them, and evaluated their performance on another task on the same dataset and on a fully independent dataset. In another study, <ref type="bibr">Gao &amp; al. [29]</ref> used the ImageNet database <ref type="bibr" target="#b19">[20]</ref>, a large, public dataset containing naturalistic images from more than 1,000 classes, to pretrain a model and adapt it to classify tasks from 2dimensional (2D) fMRI data. This database was also used in <ref type="bibr" target="#b30">[31]</ref> for pretraining a 2D structural MRI classifier. In the same study, the Kinetics dataset <ref type="bibr" target="#b31">[32]</ref> was also used to evaluate the transfer learning process with 3D images. In a recent work, Thomas &amp; al. <ref type="bibr" target="#b32">[33]</ref> used self-supervised learning frameworks to pretrain DL decoding models across a broad fMRI dataset, comprising many individuals, experimental domains, and acquisition sites. These studies show improved classification accuracies as well as quicker learning and less training data required.</p><p>However, labeled databases are not always available in neuroimaging, despite the growing effort in data sharing to build public databases <ref type="bibr" target="#b33">[34]</ref>, such as OpenNeuro for raw data <ref type="bibr" target="#b34">[35]</ref> and Neu-roVault for fMRI statistic maps <ref type="bibr" target="#b35">[36]</ref>. The unconstrained annotations and the heterogeneity of tasks and studies make them difficult to use to pretrain a supervised DL model. To compensate this, weakly supervised learning techniques such as automatic labeling of data has proven its worth. For instance, Menuet &amp; al. <ref type="bibr" target="#b36">[37]</ref> enriched NeuroVault annotations using the Cognitive Atlas ontology <ref type="bibr" target="#b37">[38]</ref> and used these labeled data to train a multitask decoding model that successfully decoded more than 50 classes of mental processes on a large test set.</p><p>A specific type of inductive transfer learning named self-taught learning <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b39">40]</ref> showed strong empirical success in the field of machine learning. It does not require any labels as it consists in training models to autonomously learn latent representations of the data and using these to improve learning in a supervised setting. This approach is motivated by the observation that data from similar domains contain patterns that are similar to those of the target domain. By initializing the weights of a supervised classifier with the pretrained weights of an unsupervised model trained on many images, the aim is to improve the model performance by placing the parameters close to a local minimum of the loss function and by acting as a regularizer <ref type="bibr" target="#b40">[41]</ref>.</p><p>In the field of neuroimaging, latent representations have recently been used in a task-relevant autoencoding framework. In <ref type="bibr" target="#b41">[42]</ref>, an autoencoder was used with a classifier attached to the bottleneck layer on a small fMRI dataset. This model outperformed the classifier trained on raw input data by focusing on cleaner, task-relevant representations. This suggests that a low-level representation of fMRI data, learned for a reconstruction task, can be helpful in a classification task, as in a self-taught learning framework.</p><p>In this work, we propose to take advantage of NeuroVault-a large public neuroimaging database that was built collaboratively and therefore displays a good level of variability in terms of fMRI acquisition protocols, machines, sites, and analysis pipelinesin a self-taught learning framework. We pretrained an unsupervised DL model to learn a latent representation of fMRI statistic maps, and we fine-tuned this model to decode tasks or mental processes involved in several studies. In the first part, we leveraged the NeuroVault database to select the most relevant statistic maps and train a convolutional autoencoder (CAE) to reconstruct these maps. In the second part, we used the final weights of the encoder to initialize a supervised convolutional neural network (CNN) to classify the cognitive processes, tasks, or contrasts of unseen statistic maps from large collections of the NeuroVault database (an homogeneous collection of more than 18,000 statistic maps and an heterogeneous one with 6,500 maps). Our goal was to investigate how the use of a large and diverse database in a self-taught learning framework can be beneficial in the field of brain imaging for DL models.</p></div>
<div><head>Material and Methods</head><p>The code produced to run the experiments and to create the figures and tables of this article is available in the Software Heritage public archive <ref type="bibr" target="#b42">[43]</ref>. Derived data used by these notebooks are stored in Zenodo <ref type="bibr" target="#b43">[44]</ref>.</p><p>Fig. <ref type="figure" target="#fig_0">1</ref> illustrates the overall process used to implement our selftaught learning framework: a CAE was first trained to reconstruct the maps of a large dataset extracted from NeuroVault. Then, the encoder part of the CAE was fine-tuned to answer a classification problem on another dataset (with labels). After hyperparameter optimization, performance of the pretrained classifier was compared to those of a classifier initialized with a default algorithm. Details regarding the datasets (NeuroVault dataset and classification datasets) can be found in the next subsection. The models of the CAEs and the CNNs are presented in section "Model architectures". Further explanations on the workflow used to train the CAE and the CNN and to evaluate their performance are available in sections "CAE training" and "Classifier training", respectively.</p></div>
<div><head>Overview of the datasets</head><p>A summary of the different datasets can be found in Table <ref type="table" target="#tab_1">1</ref>. Details are given below.</p></div>
<div><head>NeuroVault dataset</head><p>NeuroVault <ref type="bibr" target="#b35">[36]</ref> (RRID:SCR_003806) is a web-based repository for statistic maps, parcellations, and atlases produced by MRI and positron emission tomography studies. This is currently the largest public database of fMRI statistic maps. NeuroVault has  NeuroVault dataset is used to train a convolutional autoencoder (CAE). The encoder of this CAE is used to initialize a convolutional neural network (CNN) and to train it to classify other datasets. These classification datasets are split in 2 disjoints datasets: a "validation" one used to optimize hyperparameters and a "test" one to evaluate performance. In each one, a 5-fold cross-validation is performed.  <ref type="bibr" target="#b35">(36)</ref> its own public <software ContextAttributes="used">Application Programming Interface</software> that provides a full access to all images (grouped by collections) and enables filtering of images or collections with associated metadata. At the time of the experiment (19 January 2022), a total of 461,461 images in 6,782 collections were available. Among the available metadata, some are mandatory and specified for all maps such as the modality (e.g., "fMRI-BOLD" for blood oxygen level dependent functional MRI; dMRI for diffusion MRI, etc.), the type of statistic (e.g., "T map" or "Z map"), or the cognitive paradigm (e.g., "working memory" or "motor fMRI task paradigm"), and others are optional and only available if additionally entered at the time of the upload.</p><p>From this large database, relevant maps were selected based on multiple criteria. First, we chose maps for which the modality was "fMRI-BOLD" to exclude other modalities such as structural or diffusion MRI. To get comparable maps, we set 3 additional inclusion criteria and selected maps (i) for which all required metadata were provided ("is_valid" to True), (ii) that were registered in MNI space ("not_mni" to False) to ensure that anatomical structures were located at the same coordinates in each map, and (iii) that were referenced as "T map" or "Z map" to exclude maps in which voxel values did not have the same meaning (e.g., P value maps, chi-squared maps). Among these, thresholded statistic maps were excluded.</p><p>We found that some maps in our initial dataset were wrongly referenced as T map or Z map. These misclassified maps were removed by filtering the "filename" column of the dataframe to exclude SetA_mean SetB_mean (AFNI contrast maps), con (SPM contrast maps), and cope (FSL contrast maps).</p><p>Using these criteria, a total of 28,532 statistic maps were selected from the NeuroVault database and constituted our "Neu-roVault dataset." Most of these maps were unlabeled (i.e., cognitive processes or tasks performed described as "None/Other") or not labeled in a standardized way (i.e., use of terms that are specific for a study instead of generic terms, such as those defined in <ref type="bibr" target="#b37">[38]</ref>; e.g., some maps were labeled as "word-picture matching task" for the cognitive paradigm, whereas others in which a similar task was performed were referenced as "working memory fMRI task paradigm," which is a label that includes other specific tasks).</p></div>
<div><head>HCP dataset (NeuroVault Collection 4337)</head><p>NeuroVault collection 4337 [45] includes 18,070 z-statistic maps, for base contrasts (task vs. baseline), corresponding to 787 subjects of the HCP 900 release <ref type="bibr" target="#b44">[46]</ref>. This collection was excluded from our pretraining dataset (see subsection "NeuroVault dataset") due to missing metadata (i.e., "is__valid" is false).</p><p>All maps in this collection were grouped together and referred to as the "HCP dataset" in the following. Multiple labels were entered for each map, including mental concepts ("cog-nitive_paradigm_cogatlas"), tasks ("task"), and contrasts ("con-trast_definition") (as defined in <ref type="bibr" target="#b37">[38]</ref>). For each subject, 23 contrasts distributed in 7 tasks were available: r Working memory: "0-back body," "0-back face," "0-back places," "0-back tools," "2-back body," "2-back face," "2-back places," "2-back tools" r Motors: "cue," "left foot," "left hand," "right foot," "right hand", "tongue" r Relational: "relational," "match" r Gambling: "punish," "reward" r Emotion: "faces," "shapes" r Language: "math," "story" r Social: "tom" For more details on contrasts, tasks, and mental concepts of this study, see <ref type="bibr" target="#b44">[46]</ref>.</p></div>
<div><head>BrainPedia dataset (NeuroVault collection 1952)</head><p>NeuroVault collection 1952 <ref type="bibr" target="#b45">[47]</ref>, known as <software ContextAttributes="used">BrainPedia</software> <ref type="bibr" target="#b46">[48]</ref>, contains fMRI statistic maps of about 30 fMRI studies from Open-Neuro <ref type="bibr" target="#b34">[35]</ref>, the HCP <ref type="bibr" target="#b44">[46]</ref>, and data acquired at Neurospin research center; together, they were chosen to map a wide set of cognitive functions.</p><p>This collection contains 6,573 statistic maps corresponding to 45 unique mental concepts derived from 19 subterms (e.g., "visual, right hand, faces" for maps associated with the task of watching an image of a face and responding to a working memory task). These images were previously used to build a multiclass decoding model <ref type="bibr" target="#b46">[48]</ref>, and labels corresponded to the mental concepts associated with the statistic map (e.g., "visual," "language," or "objects"). Here we excluded the 9 classes that had fewer than 30 samples each, leaving 6,448 images corresponding to 36 classes. These 6,448 images were grouped together and referred to as the "<software ContextAttributes="used">BrainPedia</software>" dataset in the following.</p></div>
<div><head>Preprocessing</head><p>All statistic maps included in this study were downloaded from different collections of NeuroVault and therefore were processed using different pipelines (see the original studies for more details <ref type="bibr" target="#b46">[48,</ref><ref type="bibr" target="#b44">46]</ref>). We resampled all maps to dimensions <ref type="bibr" target="#b46">(48,</ref><ref type="bibr" target="#b54">56,</ref><ref type="bibr" target="#b46">48)</ref> using the MNI152 template available in <software ContextAttributes="used">Nilearn</software> <ref type="bibr" target="#b47">[49]</ref> (RRID:SCR_001362) as target image. A min-max normalization was also performed on all resampled maps to get statistical values between -1 and 1. Finally, the brain mask of the MNI152 template in <software ContextAttributes="used">Nilearn</software> was used to exclude statistical values outside the brain in all statistic maps.</p></div>
<div><head>Model architectures</head><p>All models were implemented using <software ContextAttributes="used">PyTorch</software> <ref type="bibr" target="#b48">[50]</ref> v1.12.0 (RRID:SC R_018536) with <software ContextAttributes="used">CUDA</software> [51] v10.2. For our model architectures, we chose to use 3D-convolutional feature extractors that take into account the 3 spatial dimensions of fMRI statistic maps. Schematic representations of the architectures are available in Fig. <ref type="figure" target="#fig_1">2</ref> and Supplementary Fig. <ref type="figure" target="#fig_0">S1</ref>, available at <ref type="bibr" target="#b50">[52]</ref>.</p></div>
<div><head>CAE</head><p>The base architecture of our CAE was inspired from <ref type="bibr" target="#b25">[26]</ref>. Two architectures were derived from this base: a 4-layer and a 5-layer architecture, respectively corresponding to the number of convolutional layers in each part of the CAE (encoder and decoder). In the 4-layer model, the encoder part consisted in four 3D convolutional layers with respectively 64, 128, 256, and 512 channels. Each layer had a kernel size of 3 × 3 × 3, a stride of 2 × 2 × 2, and a padding of 1 × 1 × 1. The 3D batch normalization layers <ref type="bibr" target="#b51">[53]</ref> followed each convolutional layer with respectively 64, 128, 256, and 512 channels, and a leaky rectified linear unit (ReLU) activation function was used for all layers. The decoding part of the CAE was symmetric to the encoder, except that 3D transposed convolutional layers were used instead of classic convolutional layers. Transposing convolutions is a method to upsample an output using learnable parameters. It can be seen as an opposite process to classical convolutions. To keep the number of features symmetric at each layer's output, the kernel size of the first layer was set to 4 × 3 × 4 and to 4 × 4 × 4 for all other transposed convolutional layers. Leaky ReLU activation function was also used for all layers except for the last one (i.e., the output one), for which a sigmoid function was used in order to obtain output values between -1 and 1. The latent space for this model was of size 512 × 3 × 4 × 3. A schematic representation of this architecture can be found in Fig. <ref type="figure" target="#fig_1">2A</ref>.</p><p>In the 5-layer model, 1 convolutional layer was added at the beginning of the encoder with 32 channels and similar parameters as the other layers of the encoder. A transposed convolutional layer was also added at the end of the decoder with 32 channels. The kernel sizes in the decoder were also modified to maintain the feature map sizes: the first and second layers of the decoder had kernel sizes of 3 × 4 × 3 and 4 × 3 × 4, respectively. All other parameters, batch normalization layers, and activation functions were the same. The latent space for this model was of size 512 × 2 × 2 × 2. A schematic representation of this architecture can be found in Supplementary Fig. <ref type="figure" target="#fig_0">S1A</ref>, available at <ref type="bibr" target="#b50">[52]</ref>.</p></div>
<div><head>CNN</head><p>The 3D CNNs used for classification followed the architecture of the encoder part of the CAEs. In the same way as for the CAEs, 2 CNN architectures were derived. For each one, we took the corresponding architecture of the encoder (4 or 5 layers) and added a fully connected layer at the end. The number of nodes in this layer varied depending on the number of classes. A softmax activation function was used for this output layer. Visual representation of the CNNs are available in Fig. <ref type="figure" target="#fig_1">2B</ref> and Supplementary Fig. <ref type="figure" target="#fig_0">S1B</ref>, available at <ref type="bibr" target="#b50">[52]</ref>.</p></div>
<div><head>CAE training</head><p>To train our CAEs to reconstruct the statistic maps of the Neu-roVault dataset, we used an Adam optimizer <ref type="bibr" target="#b52">[54]</ref> with a learning rate of 1e-04 and all other parameters with default values. The loss function was the mean squared error (MSE: the squared L2 norm), which is the standard reconstruction loss. </p></div>
<div><head>Dataset split</head></div>
<div><head>Architecture comparison</head><p>To limit the computational cost of our experiments, we fixed some of the hyperparameters of the CAE and only compared those that were of interest for the later experiments. Here, we use the term model "hyperparameters," to distinguish with model "parameters," to represent the values that cannot be learned during training but are set beforehand (e.g., the batch size or the number of hidden layers). Thus, a batch size of 32 and a learning rate of 1e-04 were chosen to train the CAE for a number of 200 epochs (i.e., values that are often used in experiments). The only hyperparameter for which different values were compared included the number of hidden layers of the model: 4 layers vs. 5 layers for each part (encoder/decoder) of the model.</p></div>
<div><head>Performance evaluation</head><p>To assess the performance of the CAEs, we estimated Pearson's correlation coefficient between the reconstructed statistic map and the original statistic map. The correlation coefficient was computed using numpy version 1.21.2 (RRID:SCR_008633) <ref type="bibr" target="#b53">[55]</ref>. The closer to 1 the correlation coefficient was, the stronger the relationship between the maps and the more accurate the reconstruction. Note that we did not use MSE in this context as its individual values (for each data point) were not easily interpreted.</p></div>
<div><head>Classifier training</head><p>We trained 2 types of classifiers for all the experiments: r the classifier with default algorithm initialized with the original algorithm from <ref type="bibr" target="#b54">[56]</ref> (i.e., Kaiming Uniform algorithm for convolutional and fully connected layers with a parameter of √ 5) and r the classifier with pretrained CAE initialized using the weights and bias of the convolutional layers of the CAE pretrained on the NeuroVault dataset.</p><p>The CNNs were trained using the Adam optimizer with a learning rate of 1e-04. We used the cross-entropy loss function for training the classifier. Both were implemented in <software>PyTorch</software>.</p></div>
<div><head>Dataset split</head><p>As described in Fig. <ref type="figure" target="#fig_0">1</ref> (on the right), the classification datasets were split in 2 disjoint subsets: the validation dataset used to optimize the hyperparameters and the test dataset used to test the performance. Each subset contained 50% of the subjects of the overall dataset with no overlap to avoid any data leakage (see <ref type="bibr" target="#b55">[57,</ref><ref type="bibr" target="#b56">58]</ref>).</p><p>For each experiment, the validation and test datasets were then split into 5 folds for cross-validation. Subjects were randomly sampled in each fold in order to ensure that there was no overlap of subjects across folds. The identifiers of the subjects included in the different folds were saved for reproducibility. More details on the methods used to perform the 5-fold split for each dataset are specified in section "Benefits of self-taught learning and impact of different factors".</p></div>
<div><head>Evaluation of performance</head><p>The performance of each model was measured using several metrics: accuracy (Acc), precision (P), recall (R), and F1-macro score (F1). All metrics were implemented using <software ContextAttributes="used">scikit-learn</software> <ref type="bibr" target="#b47">[49]</ref> with default parameters, except for F1-score, for which the "average" parameter was specified with "macro" to deal with multiclass classification.</p><p>To evaluate the performance of a model, all metrics were averaged among the 5 folds of cross-validation, and standard error of the mean was computed.</p><p>To compare the final performance of models with default initialization versus fine-tuned weights, we used paired 1-tailed 2sample t-tests between the performance values (accuracy or F1score) of the 5 models trained during cross-validation. The tstatistic and P value were provided, and a value of 0.05 was used for the P value significance threshold.</p></div>
<div><head>Hyperparameter optimization</head><p>To select the best hyperparameters for each dataset and each type of initialization, we evaluated the performance of each model by performing a 5-fold cross-validation on the validation dataset.</p><p>For each type of classifier (i.e., initialized with default algorithm vs. pretrained), we refined and optimized the hyperparameters using the largest datasets (Large <software>BrainPedia</software> and HCP). However, the large amount of training data made it computationally extremely costly to perform a full grid search. We therefore limited our research to predefined values of batch sizes (32 or 64), number of epochs (200 or 500), and model architectures (4 layers or 5 layers). All batch sizes, number of epochs, and architectures were tested for each type of classifier and each dataset. We did not perform any optimization on the learning rate to limit the computational cost of our experiments. Every model was trained using a learning rate of 1e-04.</p><p>We selected the best set of hyperparameters based on the performance of the corresponding model in terms of accuracy and F1-score, averaged across folds.</p></div>
<div><head>Benefits of self-taught learning and impact of different factors</head><p>To investigate the benefits of self-taught learning for neuroimaging data, different brain decoding experiments were studied. For all, after optimizing the hyperparameters of the 2 models (i.e., the model with default initialization or with pretrained CAE and finetuned weights), we assessed the performance of these optimized models on the test dataset using a 5-fold cross-validation.</p></div>
<div><head>Homogeneous dataset (single study)</head><p>The HCP dataset was used to compare the performance of the models for the task of decoding on a homogeneous dataset (i.e., from a single study). We studied the impact of 2 factors on the classification: sample size and number of target classes. For sample size, subsets of the global HCP dataset were created with different number of subjects: N = 50, 100, and 200, with each smaller subset being a subset of the immediately larger one. To create these subsets, we first split the global HCP test dataset into 5 folds, with different subjects in each fold. In each of these 5 folds, we randomly sampled 200/5 = 40 subjects and obtained 5 subfolds that together composed the smaller subset of 200 subjects. This process was repeated for subsamples N = 100 and 50 by sampling from their superset. This ensured that the 5 models trained on different combinations of the 4 folds of a smaller subset could be tested on the remaining fold of the global test dataset with no overlap between the training and test data. The process is illustrated in Fig. <ref type="figure">3A</ref>.</p><p>In the end, we obtained 4 datasets with respectively N = 50, 100, and 200 subjects in addition to the global dataset with all subjects (N = 393). These datasets respectively contained 1,150, 2,300, 4,590, and 9,017 statistic maps in the test subset and 1,150, 2,300, 4,591, and 9,053 in the validation subset (note: some contrasts were missing for part of subjects). Since we use a 5-fold validation scheme, the models were trained on approximately 80% of the statistic maps in the corresponding subset (i.e., validation for hyperparameter optimization and test for performance evaluation).</p><p>Three types of classification were investigated: first, the "contrast classification," which consisted in identifying the contrast associated with a statistic map (23 different contrasts); second, the "task classification," which consisted in identifying the task associated with a statistic map (7 different tasks, with multiple contrasts per task); and third, the "1-contrast task classification." This time, we selected a single contrast per task and classified the tasks (7 different tasks, with 1 contrast per task). The selected contrasts were "2-back places," "faces," "punish," "relational," "right hand," "story," and "tom," respectively for the tasks "working memory," "emotion," "gambling," "relational," "motor," "language," and "social." We selected these contrasts similarly to what was done in <ref type="bibr" target="#b7">[8]</ref>, in which the HCP dataset was used in a decoding model. For each task, the contrast that showed a greater association with the task had priority over the other (e.g., "punish" for the "gambling" task). For "working memory" and "motor" tasks, which contained more than 1 task condition, the authors of <ref type="bibr" target="#b7">[8]</ref> randomly chose one ("2back body" for working memory and "right hand" for motor). The dataset used for this third type of classification was thus smaller than the others (only 1 map per task per subject). For this classification task, the number of statistic maps was respectively 300, 598, 1,198, and 2,355 for N = 50, 100, and 200 and for the global dataset. </p></div>
<div><head>Heterogeneous dataset (multiple studies)</head><p>To study the benefits of self-taught learning on a heterogeneous dataset (i.e., from multiple studies), we used <software ContextAttributes="used">BrainPedia</software>. For these experiments, we focused on the classification of mental concepts (as available in NeuroVault metadata). Fig. <ref type="figure">3B</ref> illustrates the process used to split this dataset. To perform the split while maintaining the heterogeneity in each fold, we randomly sampled 50% of the subjects of each study to form the "validation" and "test" datasets of <software ContextAttributes="used">BrainPedia</software> (see Fig. <ref type="figure">3B</ref>). Then, each dataset, each study was split into 5 folds, and the nth folds of the different studies were combined to form the nth fold of the dataset. Validation and test datasets included N = 428 subjects and were respectively composed of 3,179 and 3,269 statistic maps.</p><p>We also studied the impact of sample size in the presence of heterogeneity by extracting smaller datasets. Among the 29 studies of the <software>BrainPedia</software> dataset, we only kept those that were composed of more than 20 subjects. In these remaining studies, already split into 5 folds in <software ContextAttributes="used">BrainPedia</software> validation and test subdatasets, 2 subjects were randomly drawn per fold per study per subdataset to obtain 10 subjects per study per subdataset. Like above, the nth folds of the different studies were combined to form the nth fold of each subdataset of the "Small <software ContextAttributes="used">BrainPedia</software>" dataset. In the end, this smaller dataset was composed of 1,844 maps, divided in 30 classes, from 11 studies and 220 subjects. This dataset was also split into test and validation subsets with 50% of the subjects in each (N = 110). The test and validation subsets were thus composed respectively of 917 and 927 maps.</p></div>
<div><head>Explainability</head></div>
<div><head>Exploring feature maps to understand the generalizability across subjects</head><p>To investigate the reasons for the difference in performance between the pretrained and default models, we visualized and analyzed the feature maps of the different convolutional layers of the model. Visualizing these features was useful to better understand how each model made its predictions. Differences in the observed features can help understand differences in terms of performance.</p><p>With a generalizable classifier, we hypothesized that features of different subjects from the same class should be similar (and therefore not be impacted by individual differences). To study this, we computed for each classifier, each layer, and each class the correlations between the feature maps for all pairs of subjects. A high mean correlation highlighted a higher similarity between the feature maps extracted by this layer for a classifier and thus a higher generalizability.</p></div>
<div><head>Investigating the contribution of each layer to the overall performance</head><p>We explored which pretrained layer had the strongest impact on the classification performance. This could be made at 2 stages: before and during training.</p><p>Before training, we only transferred the weights of some parts of the CAE. In particular, we kept the weights of the last convolutional layers with a default initialization and initialized the first layers with the weights of the pretrained CAE. Multiple configurations were explored: transferring only the weights of the first one up to the first 4 convolutional layers.</p><p>During training, we froze some layers of the model initialized with the weights of the pretrained CAE, that is, some layers (the first ones) were not fine-tuned. Multiple types of freezing were tested: freezing of the first 2 to the first 5 convolutional layers. </p></div>
<div><head>Results</head></div>
<div><head>AutoEncoder performance</head><p>Reconstruction performance of the CAEs is presented in Table <ref type="table" target="#tab_2">2</ref>.</p><p>When comparing the 2 CAE architectures (4 layers vs. 5 layers) trained on the NeuroVault dataset, the mean correlations between original and reconstructed maps were better for the 4-layer architecture (86.9% vs. 77.8%). These results suggest that the reconstruction capabilities of the CAEs are dependent on the model architecture and the size of the latent space. Fig. <ref type="figure" target="#fig_3">4</ref> shows the reconstruction of a statistic map randomly drawn from the NeuroVault test dataset with the 2 CAE architectures. With the 4-layer architecture, details of the map were better reconstructed than with the 5-layer architecture (see the green square on the map). This was due to the level of compression of the data that was higher in the 5-layer CAE and that learned only the most useful features with less emphasis in learning specific details. Both models were used as the pretrained model for classification to see if the benefits of the CAEs were related to their reconstruction performance.</p></div>
<div><head>Hyperparameter optimization for classifiers</head><p>The best hyperparameters and corresponding performance can be found in Table <ref type="table" target="#tab_4">3</ref>.</p></div>
<div><head>Choice of hyperparameters for HCP dataset</head><p>Performance of the different models trained with the different hyperparameters can be found in Supplementary Table <ref type="table" target="#tab_1">S1</ref>, available at <ref type="bibr" target="#b50">[52]</ref>. For the default algorithm initialization, the best model had 4 layers and was trained with a batch size of 32 for 500 epochs. This model achieved an accuracy of 90.8% on average of the 5 folds of cross-validation. For the pretrained CAE initialization, the best model had 5 layers and was trained with a batch size of 64 for 200 epochs (average accuracy of 91.8%). The best hyperparameters for each type of initialization (default and pretrained) were used in all subsequent experiments.</p></div>
<div><head>Choice of hyperparameters for BrainPedia dataset</head><p>Results for all sets of hyperparameters are available in Supplementary Table <ref type="table" target="#tab_2">S2</ref>, available at <ref type="bibr" target="#b50">[52]</ref>. For the default algorithm initialization, the model that achieved the best performance had 5 layers and a batch size of 64 for 500 epochs. This model classified the <software ContextAttributes="used">BrainPedia</software> dataset with an average accuracy of 67.1% and an average F1-score of 61%. The performance of the pretrained CAE was the best using a 5-layer architecture, a batch size of 64, and a training time of 200 epochs.   </p></div>
<div><head>Benefits of self-taught learning on a homogeneous dataset</head></div>
<div><head>Impact of the sample size</head><p>For all classification experiments, the size of the training set (in terms of number of subjects) had a strong impact on the benefits of self-taught learning. With 50 subjects, the performance of the pretrained CAE outperformed the performance of the classifier initialized with the default algorithm in all our experiments (improvements of 0.7% to 3.4% in mean accuracies). These improvements were always significant (P &lt; 0.05). When sample size increased, this improvement was reduced and was sometimes not significant. If we focus on contrast classification (Fig. <ref type="figure" target="#fig_4">5</ref>), which was the hardest classification task between the 3 presented here due to the higher number of classes, the difference between the performance of the 2 classifiers decreased with sample size (mean accuracies of 88.6% and 90.2%, respectively, for default initialization and pretrained model for N = 200, which corresponded to an improvement of 1.6% compared to almost 3% for N = 100). For N = 200, the difference of performance was not significant, probably due to the presence of an outlier value in the accuracies of the pretrained CAE. Indeed, accuracies of the pretrained CAE model were superior to the ones of the default model, except for the pre-trained model tested on the third fold of cross-validation, which was lower. This value was also significantly lower than those of models tested on other folds of cross-validation (see Supplementary Table <ref type="table" target="#tab_4">S3</ref>, available at <ref type="bibr" target="#b50">[52]</ref>).</p></div>
<div><head>Impact of the target classification task</head><p>For simpler classification experiments (i.e., with less classes to separate), pretraining was not always useful. In these experiments, performance was already nearly perfect (accuracies close to 1) and therefore difficult to improve. For large sample sizes (N &gt; 100), performance was close (difference between mean accuracies lower than 0.6%) between models initialized with default algorithm and pretrained models (see Figs. <ref type="figure" target="#fig_5">6</ref> and<ref type="figure" target="#fig_6">7</ref>). However, for smaller sample sizes (N = 50), pretraining improved classification-similarly to what had been shown for more complex tasks-with accuracies of the pretrained models higher than default models of 0.7% and 1.2% for task classification and 1contrast task classification, respectively. These results suggest that pretraining can be beneficial when studying difficult classification problems such as those with few training samples or complex classification tasks.   </p></div>
<div><head>Benefits of self-taught learning on a heterogeneous dataset</head><p>Table <ref type="table" target="#tab_7">5</ref> summarizes the results for the classification of mental concepts on the small and the large <software ContextAttributes="used">BrainPedia</software> datasets. These results are illustrated in Fig. <ref type="figure" target="#fig_7">8</ref>. On a the small <software ContextAttributes="used">BrainPedia</software> dataset, pretraining improved the performance of the classifier. When looking at the mean accuracies, respectively 56.8% and 64.5% for the classifier initialized with the default algorithm and the pretrained classifier, the difference was high (almost 8% of improvement). But in this case, the F1score was a better metric to assess the performance. Indeed, this metric focuses more on classification errors and is a better indicator of performance when classes are imbalanced, which was the case in this dataset in which some classes were more represented than others (e.g., in the small <software ContextAttributes="used">BrainPedia</software> training set, 205 maps corresponded to the class "visual words, language, visual," whereas only 19 are in the class "left foot, visual"). When focusing on this metric, the pretrained classifier performance was markedly higher than the ones of the classifier with default initialization (11.5% of improvement in mean F1-score). Performance (accuracies and F1-scores) was both significantly improved with the pretrained model compared to the default one (P &lt; 0.05).</p><p>On the global <software>BrainPedia</software> dataset, performance also increased with pretraining. Mean accuracy and F1-score were higher for the the pretrained model (F1-score of 73.6% against 64.9% for the model with default initialization) even if the sample size of the dataset was higher and more classes were represented. Indeed, the classification task was also more complex for this dataset since data were separated into 36 classes instead of 30 for Small Downloaded from https://academic.oup.com/gigascience/article/doi/10.1093/gigascience/giad029/7150396 by guest on 03 May 2023    <software ContextAttributes="used">BrainPedia</software> due to the presence of maps from other studies in the dataset.</p></div>
<div><head>How do we explain these benefits?</head></div>
<div><head>Features</head><p>To better understand the behavior of each model-in particular, on what features they based their predictions on-we visualized the mean features across subjects of each layer of the pretrained, default models and baseline CAE for each class label (i.e., contrast). Specifically, we studied the mean feature maps obtained across subjects in the test set (fold 1) of the N = 50 sample of the HCP dataset for different contrasts. This configuration was chosen due to the large difference between performance of default and pretrained models on this classification task. Our main interest was to see if the model would focus on general patterns of activation or more individual features. We focused on the contrasts that led to the most difficult classification tasks (i.e., had the lowest per-class accuracy [less than 80%]). Per-class accuracy for selected contrasts are shown in Table <ref type="table" target="#tab_8">6</ref> and for all contrasts in Supplementary Table <ref type="table" target="#tab_9">S7</ref>, available at <ref type="bibr" target="#b50">[52]</ref>. Eight contrasts were selected: "Working memory": "0-back body," "0-back places," "0-back tools," "2-back body," "2-back tools," "Gambling: punish," "Gambling: reward," and "Relational: relational" and among these 8 contrasts, 7 (all except "2-back body") had a better per-class accuracy with the pretrained CAE, see "Benefits of self-taught learning and impact of different factors". Fig. <ref type="figure" target="#fig_8">9</ref> shows the mean feature maps for two of the selected contrasts and for the first 4 convolutional layers of the models: CNN with default initialization, pretrained CNN, and CAE. The mean feature maps of all the selected contrasts and layers are displayed in Supplementary Fig. <ref type="figure" target="#fig_1">S2</ref>, available at <ref type="bibr" target="#b50">[52]</ref>. The first convolutional layer features (column 2 of Fig. <ref type="figure" target="#fig_8">9</ref>) were similar across models but different between the contrasts: see, for instance, the activation patterns of contrasts WM: "0-back body" and "Gambling: punish," which were localized in the same areas but had different shapes. These were high-level features: brain shape and main activation patterns. However, the second convolutional layer (third column) seemed to learn more important features for classification. The shape of the brain was still visible, but patterns of activation were more blurry, as if they were lower-resolution representations of the original statistic maps. However, features started to be different between models at this layer with some modifications of the shape of the main activation patterns between the default model (first row of each contrast) vs. the pretrained model and the CAE (second and third lines). The same observation was made for the third convolutional layer (fourth column), which began to learn deeper representations. Due to the size of the features (6 * 7 * 6), the brain shape and activation patterns were not visible, and these features were thus less interpretable and required a quantitative analysis.</p><p>Mean correlations between the feature maps of the same contrast were computed for each pair of subjects. A high mean correlation indicates a higher similarity between the feature maps produced in a given layer of a neural network and thus potentially a higher generalization power since the feature maps are less different between subjects and thus less sensitive to individual variations. Fig. <ref type="figure" target="#fig_0">10</ref> shows the mean correlations for the 8 selected contrasts and for the first 4 convolutional layers of the models (different values represent different contrasts). For layers 1 and 2, mean correlations were low (&lt;60%) and not very different between the models even if the pretrained CNN seemed to account more about individual differences than the default model and baseline CAE.</p><p>The main change was visible at layer 3, where there was an important difference (more than 30% for every contrast) between the mean correlation between the features learned by the default CNN and the pretrained one. The features of this layer seemed more similar between different subjects and more generalizable across subjects for the pretrained model (mean correlations &gt;80% for all contrasts) than for the default model for which the mean correlations were lower than 50% for every contrast. Correlations started to converge for the fourth layer but were still lower for the default model.</p></div>
<div><head>What layers benefit the most from weight transfer from the CAE?</head><p>To explore the impact of each layer and the benefits of the baseline weights of the CAE, we tried several experiments with different numbers of frozen layers and several weight transfer configurations: transferring only the weights of the first convolutional layer to transferring the weights of the first 4 convolutional layers. Performance of the different models with different numbers of transferred layers is shown in Table <ref type="table" target="#tab_9">7</ref>. When only the weights of the first layer were transferred, classification performance was lower than with other configurations (82.7% of accuracy compared to more than 84% for at least 2 transferred layers). This suggests that features learned by the CAE at this layer were less important for classification. However, when increasing the number of transferred layers, performance started to grow and became closer to the accuracy obtained when transferring all layers (87%). This growth was quite constant, and there was no large improvement of performance when transferring the weights of a layer in particular, except when moving from transferring the first layer to the first 2 layers. Thus, pretraining the deeper layers of the model was beneficial to improve classification performance, probably because of the ability of these layers to extract more general features, less sensitive to individual variations, as we saw above. Transferring Downloaded from https://academic.oup.com/gigascience/article/doi/10.1093/gigascience/giad029/7150396 by guest on 03 May 2023 Figure <ref type="figure" target="#fig_0">10</ref>: Boxplots of mean correlations between the feature maps of different subjects for the 8 selected contrasts ("Working memory": "0-back body," "0-back places," "0-back tools," "2-back body," "2-back tools," "Gambling: punish," "Gambling: reward," and "Relational: relational") for different models at layers 1, 2, 3 and 4. DA, default algorithm initialization; PT, pretraining initialization; AE: baseline autoencoder. For layers 3 and 4, pretrained CNN and baseline CAE show larger correlation between subjects than default CNN, meaning a lower attention to individual variabilities. </p></div>
<div><head>Faster fine-tuning: what happens if we freeze some layers?</head><p>Table <ref type="table" target="#tab_10">8</ref> shows the results of the different experiments with different numbers of frozen layers. When we froze the first con-volutional layers (from 2 to 4 frozen layers) on the pretrained model, the performance did not decrease. This suggests that the features extracted by the baseline autoencoder for these layers were general enough to perform a classification task with only 1 fine-tuned convolutional layer in addition to the dense layer. However, when freezing all convolutional layers of the model (5 layers), there was a large drop in terms of performance (86% to 80% of accuracy between freezing 2-4 layers vs. 5 layers), and this confirmed the observation made before on the difference between the features extracted by the fifth layer for reconstruction (CAE) and for classification (CNN). In conclusion, the first 4 convolutional layers of our model extracted more general features, whereas the last one extracted deeper and more specific features for classification.</p></div>
<div><head>Discussion</head></div>
<div><head>Summary of the results</head><p>In this work, we showed the benefits of self-taught learning with a large and variable database on the classification of 2 large public datasets with different sample sizes and classification tasks. In all cases, pretraining a classifier with an unsupervised task (in our case: reconstruction) was beneficial, but the level of improvement varied depending on the classification task and the size of the training dataset. When sample sizes were small, pretraining always improved the classification performance, regardless of whether the dataset was homogeneous or heterogeneous and the complexity of the classification task. In medical imaging, where the dimensions of the data are often very large and few samples are typically available due to high financial and human costs, learning a good representation of the data can be very difficult <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b57">59]</ref>. Unsupervised pretraining can thus be helpful by initializing the weights of the CNN to preserve the (brain) structure learned by the autoencoder and facilitate the learning process. However, when the sample size increases, benefits are less remarkable since the amount of available training data is probably sufficient to learn a good representation.</p><p>This observation can also be made for classification tasks. When trying to classify the data in a small number of classes, performance of the pretrained classifier was better but not with a high improvement of performance, even for small sample sizes (e.g., 100 subjects for task classification). But when trying to separate data into more classes, for a more fine-grained classification, the representation learned during the pretraining was beneficial.</p><p>Another benefit of self-taught learning we found was the reduction of the training time. Performance of the pretrained classifier was better even with less training epochs. This was the case for both dataset results, which were computed for 500 epochs for the default algorithm and 200 epochs for the pretrained model. This is in line with <ref type="bibr" target="#b58">[60]</ref> in which researchers showed that the pretrained models remain in the same basin of the loss function when trained on new data, and since the weights are already initialized close to a good representation of data, less epochs are necessary to adapt this representation for classification.</p><p>Architectures of the models also had an impact on the benefits of self-taught learning. With both datasets, pretrained models performed better using the 5-layer architecture. This effect was studied by <ref type="bibr" target="#b40">[41]</ref>, who showed that, while unsupervised pretraining helps for deep networks with more layers, it appears to hurt for too small networks. The size of the latent space of the CAE, with 5 layers being almost 5 times smaller than the 4-layer one, suggests that only a small subset of features of the input is relevant for predicting the class label.</p><p>However, the classification accuracies of the pretrained models were not related to the reconstruction performance of the CAE since the 4-layer CAE reconstructs maps with better precision than the 5-layer CAE. This confirms that the features learned by the 4-layer CAE for reconstruction were not all useful for classification, and focusing on a smaller number of features (with 5 layers) facilitates the learning process.</p><p>This observation was confirmed by the large drop in performance when freezing the first fifth convolutional layers of the pretrained model and when transferring only part of the layers. Deeper pretrained layers had more impact on classification performance, meaning that the features extracted by these layers were different from those learned by layers initialized with the default algorithm. In particular, the third and fourth convolutional layers showed the best benefits when being transferred, due to the generalizability of the extracted features. This was not the case for the fifth layer, for which features need to be specific to the classification task.</p><p>The pretrained model improved the performance in terms of classification due its ability to focus on more generalizable features. By pretraining a model on a large variable dataset such as NeuroVault, we built a model that is less sensitive to the training data and less sensitive to individual differences, thus more generalizable and applicable to new subjects. With parallel computing (use of 2 GPUs in parallel), we could hope to shorten this time to 24 hours, with the cost of using more computing resources. Other types of architectures with different number of fully connected or convolutional layers could have been tested to see the effect of other latent space sizes as it was done in <ref type="bibr" target="#b40">[41]</ref>.</p></div>
<div><head>Limitations and perspectives</head><p>The main limitation of our work is the classification experiments and datasets we chose. In fMRI, the number of possible labels and, thus, classification tasks is very high due to a lack of consensus in the field with respect to standardizing tasks, contrasts, and mental concepts <ref type="bibr" target="#b37">[38]</ref>. In our experiments, we used the labels provided by NeuroVault as specified in the original studies <ref type="bibr" target="#b44">[46,</ref><ref type="bibr" target="#b46">48]</ref>. We chose to compare multiple types of classification on the HCP dataset to illustrate different approaches are in use in the field or that were used by other studies <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b28">29]</ref>. For <software ContextAttributes="used">BrainPedia</software>, a multilabel decoding was performed in the original study since multiple concepts are associated with most maps. Labels we had access to were then the list of labels associated with each map. To be able to compare our results with those of the homogeneous dataset (HCP), we chose to classify these as unique labels, which was less complex and less precise in practice. This type of issue is due to the lack of harmonization in the way tasks and cognitive processes are defined. Using ontologies such as Cognitive Atlas <ref type="bibr" target="#b37">[38]</ref>, NeuroVault annotations could be harmonized and enriched, as it was done by <ref type="bibr" target="#b36">[37]</ref> by mapping the original labels to target ones from Cognitive Atlas or <ref type="bibr" target="#b59">[61]</ref> in which cognitive conditions were annotated by a group of expert using the same atlas.</p><p>In neuroimaging, many sources of variability can impact the results of an experiment and the generalizability of the results. Here, we investigated the generalizability of our model by assessing the benefits of pretraining on a heterogeneous dataset (<software ContextAttributes="used">BrainPedia</software>). While this dataset was heterogeneous in terms of the studies that were included, all maps were obtained using the same processing pipeline. Multiple studies have shown that the exact pipeline used to obtain an fMRI result can have a nonnegligible impact on fMRI statistic maps <ref type="bibr" target="#b60">[62,</ref><ref type="bibr" target="#b61">63]</ref>. In the future, investigating performance of classification on a more variable target dataset with statistic maps from different studies but also processed using different pipelines would be of great interest. In a recent study <ref type="bibr" target="#b9">[10]</ref>, the authors tried to compare the performance of different classifiers trained on fMRI 3D volumes series obtained with various scenarios of minimal preprocessing pipelines. A similar experiment was recently made by <ref type="bibr" target="#b62">[64]</ref>, who found that preprocessing pipeline selection can impact the performance of a supervised classifier. Comparing the adaptation capacities of models on volumes preprocessed with different pipelines could be also interesting to evaluate the impact of analytical variability on deep learning with fMRI and to see if the generalizability of our pretrained models also works for interpipeline differences.</p><p>Note that self-supervised (instead of self-taught) learning could have also been used to pretrain our model, as it was done by <ref type="bibr" target="#b32">[33]</ref>, who designed self-supervised learning frameworks, inspired by the field of natural language processing, to pretrain mental state decoding models. Self-supervised learning is a supervised machine learning setting where the supervision is generated directly from the data and the model is pretrained using a supervised surrogate task. Self-supervised learning is particularly relevant if the surrogate task is close to the final one targeted by the user (e.g., if they can share the same feature representation). It is possible that, by designing a relevant supervised surrogate task that could be relevant for all very diverse usage of our model, the pretrained model would have performed better than the one presented in this article. Designing and experimenting with such a surrogate supervised task could be interesting for future work.</p><p>In our self-taught context, using unsupervised models could allow us to build a space capturing the similarities and differences of statistic maps (i.e., to learn a robust latent representation of Downloaded from https://academic.oup.com/gigascience/article/doi/10.1093/gigascience/giad029/7150396 by guest on 03 May 2023</p><p>Self taught learning for brain decoding | 15 the important features of statistic maps in a specific context). By adding other constraints to this latent space and/or choosing an adapted pretraining dataset, we could use this for other purposes than brain decoding. For instance, building a space that captures the analytical variability in statistic maps could help us understand the difference between the pipelines but also identify the more robust pipelines. Future works will focus on building such a space with specific constraints to evaluate distance between different pipelines.</p></div>
<div><head>Conclusion</head><p>In this study, we compared the benefits of a self-taught learning framework in the task of classifying 3D fMRI statistic maps. We showed that unsupervised pretraining improves the performance of classification in experiments with few training samples and complex classification tasks, which is a very common setup in fMRI studies.</p></div><figure xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1:Flow diagram of the self-taught learning methodology. NeuroVault dataset is used to train a convolutional autoencoder (CAE). The encoder of this CAE is used to initialize a convolutional neural network (CNN) and to train it to classify other datasets. These classification datasets are split in 2 disjoints datasets: a "validation" one used to optimize hyperparameters and a "test" one to evaluate performance. In each one, a 5-fold cross-validation is performed.</figDesc></figure>
<figure xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Schematic visualization of the architectures of the convolutional autoencoder (CAE) (A) and convolutional neural network (CNN) (B) with 4 layers. The CAE is composed of an encoder and a decoder with respectively 4 convolutional and transposed convolutional layers. The size of the latent space is 512 * 3 * 4 * 3. The CNN has the same architecture as the encoder of the CAE with a fully connected layer added at the end of the network with different numbers of output node depending on the dataset and the classification performed.</figDesc></figure>
<figure xml:id="fig_2"><head>7 ABFigure 3 :</head><label>73</label><figDesc>Figure 3:Overview of the process used to split the datasets for cross-validation. (A) The method performed for the HCP dataset and its subsamples. (B) Method used for BrainPedia and Small BrainPedia datasets. In both cases, the global dataset is first split into 2 subdatasets: "validation" and "test" with respectively 50% of the subjects, and then each subdataset is divided into 5 folds for cross-validation.</figDesc></figure>
<figure xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Original version and reconstruction of a randomly drawn statistic map of the NeuroVault test dataset (image ID: 109) with the 2 CAEs (4 layers and 5 layers). The green square corresponds to a highlighted part of the map for which reconstruction performance is better using the 5-layer architecture.</figDesc><graphic coords="9,119.02,53.87,362.88,288.24" type="bitmap" /></figure>
<figure xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Mean accuracies and standard errors of the mean on contrast classification with the HCP dataset for the models initialized with default algorithm (blue) and pretrained CAE (orange). Pretraining improves contrast classification performance for small sample sizes and at a lower level of improvement, also for large sample sizes.</figDesc><graphic coords="10,50.78,330.49,226.90,171.96" type="bitmap" /></figure>
<figure xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Mean accuracies and standard errors of the mean on task classification with the HCP dataset for the models initialized with default algorithm (blue) and pretrained CAE (orange). Pretraining improves task classification performance for all sample sizes, but sample sizes did not have a huge influence on the level of improvement.</figDesc><graphic coords="10,312.03,330.49,226.90,171.96" type="bitmap" /></figure>
<figure xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Mean accuracies and standard errors of the mean on 1-contrast task classification with the HCP dataset for the models initialized with default algorithm (blue) and pretrained CAE (orange). Pretraining does not always improve 1-contrast task classification performance: for large sample sizes, pretraining and default initialization give very similar results.</figDesc><graphic coords="11,56.39,54.15,226.90,171.96" type="bitmap" /></figure>
<figure xml:id="fig_7"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Mean F1-scores and standard errors of the mean of the classification of mental concepts on BrainPedia datasets (small and large) for the models initialized with default algorithm (blue) and pretrained CAE (orange). Pretraining improves classification performance, in particular for the small dataset.</figDesc><graphic coords="11,56.39,517.76,226.90,149.04" type="bitmap" /></figure>
<figure xml:id="fig_8"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Original mean statistic maps (column 1) and mean feature maps across subjects of fold 1 of the test dataset of HCP 50 for the first 4 convolutional layers of each model (columns 2-5): CNN with default algorithm initialization (DA), pretrained CNN (PT), and CAE, for 2 of the 8 selected contrasts (WM: 0-back body and Gambling: punish).</figDesc><graphic coords="12,138.78,343.52,312.07,61.72" type="bitmap" /></figure>
<figure xml:id="fig_9"><head /><label /><figDesc>Due to the high computational time required to train a model, we only compared 2 model architectures (4 and 5 layers). Indeed, training a CAE model can be very time-consuming, particularly in our case since we use a large training dataset (N = 22,772) and high-dimensional data (k = 48 * 56 * 48). With the 4-layer model, for 200 epochs, it took approximately 48 hours to train on 1 GPU.</figDesc></figure>
<figure><head /><label /><figDesc /><graphic coords="13,51.01,54.39,498.86,198.72" type="bitmap" /></figure>
<figure type="table" xml:id="tab_0"><head /><label /><figDesc>Downloaded from https://academic.oup.com/gigascience/article/doi/10.1093/gigascience/giad029/7150396 by guest on 03 May 2023</figDesc><table /></figure>
<figure type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Overview of the datasets. For each dataset, number of statistic maps is presented, as well as the number of subjects, number of studies, and the type of labels (if available).</figDesc><table><row><cell>Dataset</cell><cell>Maps</cell><cell>Subjects</cell><cell>Studies</cell><cell>Labels</cell></row><row><cell>NeuroVault</cell><cell>28,532</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>HCP</cell><cell>18,070</cell><cell>787</cell><cell>1</cell><cell>Tasks (7)</cell></row><row><cell /><cell /><cell /><cell /><cell>Contrasts (23)</cell></row><row><cell>BrainPedia</cell><cell>6,448</cell><cell>826</cell><cell>29</cell><cell>Cognitive</cell></row><row><cell /><cell /><cell /><cell /><cell>processes</cell></row></table></figure>
<figure type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Reconstruction performance of the CAE depending on model architecture and training set. Values are the mean Pearson's correlation coefficients (standard error of the mean).</figDesc><table><row><cell>Model</cell><cell>4 layers</cell><cell>5 layers</cell></row><row><cell /><cell>Latent space 18,432</cell><cell>Latent space 4,096</cell></row><row><cell>Correlation</cell><cell>86.9</cell><cell>77.8</cell></row><row><cell>(standard error)</cell><cell>( 0.18)</cell><cell>( 0.23)</cell></row></table></figure>
<figure type="table" xml:id="tab_3"><head>Table 4</head><label>4</label><figDesc /><table><row><cell>Downloaded from https://academic.oup.com/gigascience/article/doi/10.1093/gigascience/giad029/7150396 by guest on 03 May 2023</cell></row></table><note><p>summarizes the results for the different classification experiments on the HCP datasets.</p></note></figure>
<figure type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Hyperparameters chosen for each dataset and corresponding performance of the classifier on the validation set of the dataset</figDesc><table><row><cell /><cell /><cell /><cell /><cell /><cell>Average</cell><cell /></row><row><cell>Dataset</cell><cell>Initialization</cell><cell>Model</cell><cell>Epochs</cell><cell>Batch</cell><cell>accuracy (%)</cell><cell>Average F1 (%)</cell></row><row><cell /><cell /><cell /><cell /><cell /><cell>(standard error)</cell><cell>( standard error)</cell></row><row><cell>HCP</cell><cell>Default algorithm</cell><cell>4 layers</cell><cell>500</cell><cell>32</cell><cell>90.8 (1.5)</cell><cell>90.8 (1.6)</cell></row><row><cell /><cell>Pretrained CAE</cell><cell>5 layers</cell><cell>200</cell><cell>64</cell><cell>91.8 (0.9)</cell><cell>91.8 (0.9)</cell></row><row><cell>BrainPedia</cell><cell>Default algorithm</cell><cell>5 layers</cell><cell>500</cell><cell>64</cell><cell>67.1 (1.7)</cell><cell>61.0 (1.6)</cell></row><row><cell /><cell>Pretrained CAE</cell><cell>5 layers</cell><cell>200</cell><cell>64</cell><cell>73.8 (2.7)</cell><cell>70.0 (2.3)</cell></row></table></figure>
<figure type="table" xml:id="tab_5"><head /><label /><figDesc>Downloaded from https://academic.oup.com/gigascience/article/doi/10.1093/gigascience/giad029/7150396 by guest on 03 May 2023</figDesc><table /></figure>
<figure type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Classification performance on HCP datasets of models initialized with default algorithm vs. with the weights of the pretrained CAE. Mean accuracies and standard errors of the means among the 5 folds of cross-validation are shown. Paired 2-sample t-tests are performed between the accuracies of the 5 models obtained with cross-validation for each type of initialization. DA, default algorithm initialization; PT, pretraining initialization.</figDesc><table><row><cell>Subjects</cell><cell>50</cell><cell /><cell>100</cell><cell /><cell>200</cell><cell /><cell cols="2">Global (393)</cell></row><row><cell>Maps</cell><cell>1,150</cell><cell /><cell>2,300</cell><cell /><cell>4,590</cell><cell /><cell>9,017</cell><cell /></row><row><cell>Init.</cell><cell>DA</cell><cell>PT</cell><cell>DA</cell><cell>PT</cell><cell>DA</cell><cell>PT</cell><cell>DA</cell><cell>PT</cell></row><row><cell cols="2">Contrast classification (23 classes)</cell><cell /><cell /><cell /><cell /><cell /><cell /><cell /></row><row><cell>Mean accuracy (%)</cell><cell>83.6</cell><cell>87.0</cell><cell>86.8</cell><cell>89.9</cell><cell>88.6</cell><cell>90.2</cell><cell>90.9</cell><cell>92.4</cell></row><row><cell>(standard error)</cell><cell>(0.61)</cell><cell>(0.51)</cell><cell>(0.69)</cell><cell>(0.34)</cell><cell>(0.84)</cell><cell>(1.46)</cell><cell>(0.38)</cell><cell>(0.44)</cell></row><row><cell>Paired t-test (4 dof)</cell><cell /><cell>-11.52</cell><cell /><cell>-4.77</cell><cell /><cell>-1.42</cell><cell /><cell>-4.74</cell></row><row><cell>P value</cell><cell /><cell>0.0003</cell><cell /><cell>0.009</cell><cell /><cell>0.23</cell><cell /><cell>0.009</cell></row><row><cell cols="3">Task classification (7 classes, multiple contrasts per class)</cell><cell /><cell /><cell /><cell /><cell /><cell /></row><row><cell>Mean accuracy (%)</cell><cell>96.6</cell><cell>97.3</cell><cell>95.4</cell><cell>98.0</cell><cell>97.9</cell><cell>98.5</cell><cell>98.4</cell><cell>99.0</cell></row><row><cell>(standard error)</cell><cell>(0.47)</cell><cell>(0.43)</cell><cell>(1.49)</cell><cell>(0.25)</cell><cell>(0.44)</cell><cell>(0.16)</cell><cell>(0.17)</cell><cell>(0.13)</cell></row><row><cell>Paired t-test (4 dof)</cell><cell /><cell>-3.57</cell><cell /><cell>-1.4</cell><cell /><cell>-1.5</cell><cell /><cell>-5.65</cell></row><row><cell>P value</cell><cell /><cell>0.02</cell><cell /><cell>0.2</cell><cell /><cell>0.2</cell><cell /><cell>0.005</cell></row><row><cell cols="3">One constrast task classification (7 classes, 1 contrast per class)</cell><cell /><cell /><cell /><cell /><cell /><cell /></row><row><cell>Mean accuracy (%)</cell><cell>97.9</cell><cell>99.1</cell><cell>98.9</cell><cell>99.4</cell><cell>99.3</cell><cell>99.6</cell><cell>99.4</cell><cell>99.6</cell></row><row><cell>(standard error)</cell><cell>(0.3)</cell><cell>(0.3)</cell><cell>(0.17)</cell><cell>(0.25)</cell><cell>(0.2)</cell><cell>(0.2)</cell><cell>(0.2)</cell><cell>(0.14)</cell></row><row><cell>Paired t-test (4 dof)</cell><cell /><cell>-4.17</cell><cell /><cell>-3.32</cell><cell /><cell>-2.33</cell><cell /><cell>-2.06</cell></row><row><cell>P value</cell><cell /><cell>0.01</cell><cell /><cell>0.03</cell><cell /><cell>0.08</cell><cell /><cell>0.1</cell></row></table></figure>
<figure type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Classification performance on BrainPedia datasets of models initialized with default algorithm vs. with the weights of a pretrained CAE. DA, default algorithm initialization; PT, pretraining initialization.</figDesc><table><row><cell>Dataset</cell><cell cols="2">Small BrainPedia</cell><cell cols="2">BrainPedia</cell></row><row><cell>Init.</cell><cell>DA</cell><cell>PT</cell><cell>DA</cell><cell>PT</cell></row><row><cell>Mean accuracy (%)</cell><cell>56.8</cell><cell>64.5</cell><cell>67.1</cell><cell>74.2</cell></row><row><cell>(standard error)</cell><cell>(1.5)</cell><cell>(2.1)</cell><cell>(0.9)</cell><cell>(2.3)</cell></row><row><cell>Paired t-test (4 dof)</cell><cell /><cell>-8.72</cell><cell /><cell>-3.43</cell></row><row><cell>P value</cell><cell /><cell>0.001</cell><cell /><cell>0.02</cell></row><row><cell>Mean F1-score (%)</cell><cell>50.5</cell><cell>62.0</cell><cell>64.9</cell><cell>73.6</cell></row><row><cell>(standard error)</cell><cell>(3.5)</cell><cell>(2.1)</cell><cell>(0.8)</cell><cell>(2.2)</cell></row><row><cell>Paired t-test (4 dof)</cell><cell /><cell>-4.89</cell><cell /><cell>-2.89</cell></row><row><cell>P value</cell><cell /><cell>0.008</cell><cell /><cell>0.04</cell></row></table></figure>
<figure type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Per-class accuracies for classification of contrasts with HCP dataset sample N = 50 for DA (default algorithm) and PT (pretrained CAE). Only lowest per-class accuracy (&lt;80%) is shown in. For other per-class accuracy, please refer to Supplementary TableS7, available at<ref type="bibr" target="#b50">[52]</ref>.</figDesc><table><row><cell>Contrast</cell><cell cols="2">Per-class accuracy</cell></row><row><cell /><cell>DA</cell><cell>PT</cell></row><row><cell>WM: 0BK BODY</cell><cell>57.7</cell><cell>60.3</cell></row><row><cell>WM: 0BK PLACE</cell><cell>70.5</cell><cell>79.5</cell></row><row><cell>WM: 0BK TOOL</cell><cell>57.8</cell><cell>66.7</cell></row><row><cell>WM: 2BK BODY</cell><cell>74.3</cell><cell>73.1</cell></row><row><cell>WM: 2BK TOOL</cell><cell>47.4</cell><cell>60.3</cell></row><row><cell>GAMBLING: PUNISH</cell><cell>55.1</cell><cell>67.9</cell></row><row><cell>RELATIONAL</cell><cell>58.9</cell><cell>75.6</cell></row><row><cell>GAMBLING: REWARD</cell><cell>57.7</cell><cell>66.7</cell></row></table></figure>
<figure type="table" xml:id="tab_9"><head>Table 7 :</head><label>7</label><figDesc>Classification performance (mean accuracy and standard error, in %) of pretrained models with different numbers of transferred layers on classification of contrasts for HCP dataset sample, N = 50</figDesc><table><row><cell /><cell>Mean classification accuracy</cell></row><row><cell>Number of transferred layers</cell><cell>(standard error) (%)</cell></row><row><cell>0 (Default initialization)</cell><cell>83.6 (0.61)</cell></row><row><cell>1</cell><cell>82.67 (0.45)</cell></row><row><cell>2</cell><cell>84.79 (0.52)</cell></row><row><cell>3</cell><cell>85.51 (0.8)</cell></row><row><cell>4</cell><cell>86.6 (0.4)</cell></row><row><cell>Full pretrained model</cell><cell>87.0 (0.51)</cell></row></table></figure>
<figure type="table" xml:id="tab_10"><head>Table 8 :</head><label>8</label><figDesc>Classification performance (mean accuracy and standard error, in %) of pretrained models with different numbers of frozen layers on classification of contrasts for HCP dataset sample, N = 50</figDesc><table><row><cell /><cell>Mean classification accuracy</cell></row><row><cell>Nb. of frozen layers</cell><cell>(standard error) (%)</cell></row><row><cell>2</cell><cell>86.7 (0.54)</cell></row><row><cell>3</cell><cell>86.82 (0.66)</cell></row><row><cell>4</cell><cell>86.1 (0.64)</cell></row><row><cell>5</cell><cell>80.42 (0.99)</cell></row><row><cell cols="2">the weights of the last convolutional layer (fifth) was, however, not</cell></row><row><cell cols="2">very impactful, and performance of model with 4 transferred lay-</cell></row><row><cell cols="2">ers was very close to the ones of the fully pretrained model (86.6%</cell></row><row><cell cols="2">vs. 87.0%). We suppose that this layer was important to extract</cell></row><row><cell cols="2">task-related features that were different from the ones learned</cell></row><row><cell cols="2">by the CAE, explaining the limited impacts of transferring the CAE</cell></row><row><cell>weights.</cell><cell /></row></table></figure>
			<note place="foot" xml:id="foot_0"><p>Downloaded from https://academic.oup.com/gigascience/article/doi/10.1093/gigascience/giad029/7150396 by guest on 03 May 2023</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This work was partially funded by <rs type="funder">Region Bretagne (ARED MAPIS)</rs> and <rs type="funder">Agence Nationale pour la Recherche</rs> for the program of doctoral <rs type="projectName">contracts in artificial intelligence</rs> (project <rs type="grantNumber">ANR-20-THIA-0018</rs>). We thank <rs type="person">Gregory Kiar</rs> who worked on a preliminary version of the autoencoder based on NeuroVault data. Data used in the HCP dataset were provided (in part) by the Human Connectome Project, <rs type="institution">WU-Minn Consortium</rs> (Principal Investigators: <rs type="person">David Van Essen</rs> and <rs type="person">Kamil Ugurbil</rs>; <rs type="grantNumber">1U54MH091657</rs>), funded by the <rs type="funder">16 NIH Institutes and Centers</rs> that support the <rs type="funder">NIH Blueprint for Neuroscience Research</rs>, and by the <rs type="funder">McDonnell Center for Systems Neuroscience at Washington University</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_kxuW6vU">
					<idno type="grant-number">ANR-20-THIA-0018</idno>
					<orgName type="project" subtype="full">contracts in artificial intelligence</orgName>
				</org>
				<org type="funding" xml:id="_bdJGsqq">
					<idno type="grant-number">1U54MH091657</idno>
				</org>
			</listOrg>

			<div type="availability">
<div><head>Data Availability</head><p>Supporting data, including supplementary materials (figures and tables) and additional links (code and derived data) is also available via the GigaScience repository, GigaDB <ref type="bibr" target="#b50">[52]</ref>. The data used in this study are openly available on NeuroVault <ref type="bibr" target="#b35">[36]</ref>. No experimental activity involving the human participants was made by the authors. Only publicly released data were used.</p><p>NeuroVault IDs of statistic maps included in each dataset (Neu-roVault, HCP, and <software>BrainPedia</software>) are available in the Derived Data section.</p></div>
<div><head>Code</head><p>The code produced to run the experiments and to create the figures and tables of this article is available in the Software Heritage public archive at <ref type="bibr" target="#b42">[43]</ref>.</p><p>r Programming language: Python3.9 r Licence: MIT r Requirements: numerous Python libraries described in the GitLab repository.</p></div>
<div><head>Derived Data</head><p>Derived data such as NeuroVault IDs of statistic maps used in the different datasets and the parameters of the trained models (for validation of the hyperparameter and test of performance) are available in Zenodo: <ref type="bibr" target="#b43">[44]</ref>.</p></div>
			</div>

			<div type="annex">
<div><head>Additional Files</head><p>Supplementary Table <ref type="table">S1</ref> -validation_model_hcp.csv -Classification performance (Accuracy and F1-score) of models trained on HCP validation dataset in a 5-fold cross-validation scheme with different hyperparameters (batch size, epochs, architecture). Supplementary Table <ref type="table">S2</ref> -validation_model_bp.csv -Classification performance (Accuracy and F1-score) of models trained on <software ContextAttributes="used">BrainPedia</software> validation dataset in a 5-fold cross-validation scheme with different hyperparameters (batch size, epochs, architecture). Supplementary Table <ref type="table">S3</ref> -test_hcp_contrast_sample_size.csv -Classification performance (Accuracy and F1-score) of models trained on HCP test dataset with different subsamples in a 5-fold cross-validation scheme for contrast classification. Supplementary Table <ref type="table">S4</ref> -test_hcp_task_sample_size.csv -Classification performance (Accuracy and F1-score) of models trained on HCP test dataset with different subsamples in a 5-fold crossvalidation scheme for task classification. Supplementary Table <ref type="table">S5</ref> -test_hcp_one_contrast_task _sam-ple_size.csv -Classification performance (Accuracy and F1-score) of models trained on HCP test dataset with different subsamples in a 5-fold cross-validation scheme for one-contrast task classification. Supplementary Table <ref type="table">S6</ref> -comparison_bp.csv -Classification performance (Accuracy and F1-score) of models trained on Brain-Pedia test dataset in a 5-fold cross-validation scheme. Supplementary Table <ref type="table">S7</ref> -comparison_small_bp.csv -Classification performance (Accuracy and F1-score) of models trained on Small <software ContextAttributes="used">BrainPedia</software> test dataset in a 5-fold cross-validation scheme. <ref type="table">S8</ref> -per_class_accuracies.csv -Classification performance per class of model trained on HCP dataset sample n=50 for the first fold. Supplementary Figure <ref type="figure">S1</ref> -Schematic visualisation of the architectures of the Convolutional <software ContextAttributes="used">AutoEncoder</software> (a) and Convolutional Neural Network (b) with 4 layers. Supplementary Figure <ref type="figure">S2</ref> -Original mean statistic maps (column 1) and mean feature maps across subjects of the fold 1 of the test dataset of HCP 50 for the first four convolutional layers of each model (columns 2-4): CNN with default algorithm initialization (DA), pre-trained CNN (PT) and CAE for the 8 selected contrasts: `Working Memory': `0-back body', `0-back places', `0-back tools', `2-back body', `2-back tools' , `Gambling: punish', `Gambling: reward and `Relational.</p></div>
<div><head>Supplementary Table</head></div>
<div><head>Authors' Contributions</head><p>E.G. developped the project (wrote the code, performed the experiments and analyzed results) and wrote the manuscript. E.F. and C.M. supervised the project and read back the manuscript.</p></div>
<div><head>Abbreviations</head><p>2D: 2-dimensional; 3D: 3-dimensional; CAE: convolutional autoencoder; CNN: convolutional neural network; DL: deep learning; fMRI: functional magnetic resonance imaging HCP: Human Connectome Project; MSE: mean squared error; ReLU: rectified linear unit.</p></div>
<div><head>Ethics</head><p>The data used in this study are openly available on NeuroVault <ref type="bibr" target="#b35">[36]</ref>. No experimental activity involving the human participants was made by the authors. Only publicly released data were used.</p><p>HCP: Written informed consent was obtained from all participants of HCP, and the original study was approved by the Washington University Institutional Review Board.</p><p><software ContextAttributes="used">BrainPedia</software>: <software ContextAttributes="used">BrainPedia</software> database comprises data from 29 studies, assembled from various sources (OpenNeuro, NeuroSpin research center, etc.). Subject-level maps resulting from first-level statistical analysis were uploaded on NeuroVault by <ref type="bibr" target="#b46">[48]</ref>. The list of the original dataset used is available on <ref type="bibr" target="#b46">[48]</ref>, S1 Text <ref type="bibr" target="#b63">[65]</ref>.</p></div>
<div><head>Competing Interests</head><p>The authors declare that they have no competing interests.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep learning encodes robust discriminative neuroimaging representations to outperform standard machine learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Abrol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Salman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat Commun</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">353</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep learning for brain disorder diagnosis based on fMRI images</title>
		<author>
			<persName><forename type="first">W</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">X</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">469</biblScope>
			<biblScope unit="page" from="332" to="345" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep learning for brain decoding</title>
		<author>
			<persName><forename type="first">O</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Oztekin</surname></persName>
		</author>
		<author>
			<persName><surname>Vural</surname></persName>
		</author>
		<author>
			<persName><surname>Fty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 IEEE International Conference on Image Processing (ICIP)</title>
		<meeting><address><addrLine>Paris, France</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="2784" to="2788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">3D convolutional neural network for feature extraction and classification of fMRI volumes</title>
		<author>
			<persName><forename type="first">H</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><surname>Hc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ternational Workshop on Pattern Recognition in Neuroimaging (PRNI)</title>
		<meeting><address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A multichannel 2D convolutional neural network model for task-evoked fMRI data classification</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Liao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput Int Neurosci</title>
		<imprint>
			<biblScope unit="volume">2019</biblScope>
			<biblScope unit="page" from="1" to="9" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Brain decoding using fMRI images for multiple subjects through deep learning</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">B</forename><surname>Qureshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Azad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Qureshi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput Math Methods Med</title>
		<imprint>
			<biblScope unit="volume">2022</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Deep learning of fMRI big data: a novel approach to subject-transfer decoding</title>
		<author>
			<persName><forename type="first">S</forename><surname>Koyamada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shikauchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Nakae</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.1502.00093</idno>
		<idno type="arXiv">arXiv:1502.00093</idno>
		<ptr target="https://doi.org/10.48550/arXiv.1502.00093" />
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Decoding and mapping task states of the human brain via deep learning</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Hum Brain Mapp</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="1505" to="1519" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Design of deep learning model for taskevoked fMRI data classification</title>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput Int Neurosci</title>
		<imprint>
			<biblScope unit="volume">2021</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">fMRI volume classification using a 3D convolutional neural network robust to shifted and scaled neuronal activations</title>
		<author>
			<persName><forename type="first">H</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><surname>Hc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeuroImage</title>
		<imprint>
			<biblScope unit="volume">223</biblScope>
			<biblScope unit="page">117328</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Classification of schizophrenia and normal controls using 3D convolutional neural network and outcome visualization</title>
		<author>
			<persName><forename type="first">K</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Schizophrenia Res</title>
		<imprint>
			<biblScope unit="volume">212</biblScope>
			<biblScope unit="page" from="186" to="195" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Challenges for cognitive decoding using deep learning methods</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ré</surname></persName>
		</author>
		<author>
			<persName><surname>Poldrack</surname></persName>
		</author>
		<author>
			<persName><surname>Ra</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2108.06896</idno>
		<idno type="arXiv">arXiv:2108.06896</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2108.06896" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Deep learning: From natural to medical images</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kooi</surname></persName>
		</author>
		<ptr target="https://medium.com/merantix/deep-learning-from-natural-to-medical-images-74827bf51d6b" />
		<imprint>
			<date type="published" when="2018-09">2018. September 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Intelligence artificielle et imagerie médicale</title>
		<author>
			<persName><forename type="first">R</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Deutsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fournier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bull Cancer</title>
		<imprint>
			<biblScope unit="volume">109</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="83" to="88" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Scanning the horizon: towards transparent and reproducible neuroimaging research</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Poldrack</surname></persName>
		</author>
		<author>
			<persName><surname>Baker</surname></persName>
		</author>
		<author>
			<persName><surname>Ci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Durnez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat Rev Neurosci</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="115" to="126" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Power failure: why small sample size undermines the reliability of neuroscience</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">S</forename><surname>Button</surname></persName>
		</author>
		<author>
			<persName><surname>Ioannidis</surname></persName>
		</author>
		<author>
			<persName><surname>Jpa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Mokrysz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat Rev Neurosci</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="365" to="376" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">How much data is needed to train a medical image deep learning system to achieve necessary high accuracy?</title>
		<author>
			<persName><forename type="first">J</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shin</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.1511.06348</idno>
		<idno type="arXiv">arXiv:1511.06348</idno>
		<ptr target="https://doi.org/10.48550/arXiv.1511.06348" />
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Addressing fairness in artificial intelligence for medical imaging</title>
		<author>
			<persName><forename type="first">Ricci</forename><surname>Lara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Echeveste</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ferrante</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat Commun</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">4581</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A survey on transfer learning</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Know Data Eng</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="1345" to="1359" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">ImageNet: A large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Miami, Florida</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009">2009. 2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The Human Connectome Project's neuroimaging approach</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Glasser</surname></persName>
		</author>
		<author>
			<persName><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><surname>Sm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Marcus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat Neurosci</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="1175" to="1187" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">UK Biobank: An open access resource for identifying the causes of a wide range of complex diseases of middle and old age</title>
		<author>
			<persName><forename type="first">C</forename><surname>Sudlow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gallacher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Allen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS Med</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">1001779</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Increasing diversity in connectomics with the Chinese Human Connectome Project</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat Neurosci</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="163" to="172" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Few-shot decoding of brain activation maps</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bontonou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lioi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Farrugia</surname></persName>
		</author>
		<idno type="DOI">10.23919/EUSIPCO54536.2021.9616158</idno>
	</analytic>
	<monogr>
		<title level="m">2021 29th European Signal Processing Conference (EUSIPCO)</title>
		<meeting><address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Evaluation of task fMRI decoding with deep learning on a small sample dataset</title>
		<author>
			<persName><forename type="first">S</forename><surname>Yotsutsuji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Akama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Front Neuroinform</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">577451</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">fMRI data augmentation via synthesis</title>
		<author>
			<persName><forename type="first">P</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Koyejo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE 16th International Symposium on Biomedical Imaging (ISBI 2019)</title>
		<meeting><address><addrLine>Venice, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1783" to="1787" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Extracting representations of cognition across neuroimaging studies improves brain decoding</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mensch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Thirion</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS Comput Biol</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page">1008795</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Evaluating deep transfer learning for whole-brain cognitive decoding</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Lindenberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Samek</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2111.01562</idno>
		<idno type="arXiv">arXiv:2111.01562</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2111.01562" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Decoding behavior tasks from brain activity using deep transfer learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="43222" to="43232" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Transfer learning of deep neural network representations for fMRI decoding</title>
		<author>
			<persName><forename type="first">M</forename><surname>Svanera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Savardi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Benini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Neurosci Methods</title>
		<imprint>
			<biblScope unit="volume">328</biblScope>
			<biblScope unit="page">108319</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">From YouTube to the brain: transfer learning can improve brain-imaging predictions with deep learning</title>
		<author>
			<persName><forename type="first">N</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bzdok</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Netw</title>
		<imprint>
			<biblScope unit="volume">153</biblScope>
			<biblScope unit="page" from="325" to="338" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">The kinetics human action video dataset</title>
		<author>
			<persName><forename type="first">W</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.1705.06950</idno>
		<idno type="arXiv">arXiv:1705.06950</idno>
		<ptr target="https://doi.org/10.48550/arXiv.1705.06950" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Self-supervised learning of brain dynamics from broad neuroimaging data</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ré</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Poldrack</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2206.11417</idno>
		<idno type="arXiv">arXiv:2206.11417</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2206.11417" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Making big data open: data sharing in neuroimaging</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Poldrack</surname></persName>
		</author>
		<author>
			<persName><surname>Gorgolewski</surname></persName>
		</author>
		<author>
			<persName><surname>Kj</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat Neurosci</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="1510" to="1517" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">The Open-Neuro resource for sharing of neuroscience data</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Markiewicz</surname></persName>
		</author>
		<author>
			<persName><surname>Gorgolewski</surname></persName>
		</author>
		<author>
			<persName><surname>Kj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Feingold</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">eLife</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">71774</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">org: a web-based repository for collecting and sharing unthresholded statistical maps of the human brain</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Gorgolewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Rivera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Front Neuroinform</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">8</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Comprehensive decoding mental processes from web repositories of functional brain images</title>
		<author>
			<persName><forename type="first">R</forename><surname>Menuet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Meudec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dockès</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci Rep</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">7050</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">The cognitive atlas: toward a knowledge foundation for cognitive neuroscience</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Poldrack</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kittur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kalar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Front Neuroinform</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">17</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Self-taught learning: transfer learning from unlabeled data</title>
		<author>
			<persName><forename type="first">R</forename><surname>Raina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Battle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Conference on Machine Learning</title>
		<meeting>the 24th International Conference on Machine Learning<address><addrLine>Corvalis, Oregon</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="759" to="766" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Robust and discriminative self-taught learning</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<ptr target="https://proceedings.mlr.press/v28/wang13g.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Conference on Machine Learning. JMLR.org</title>
		<meeting>the 30th International Conference on Machine Learning. JMLR.org<address><addrLine>Atlanta, Georgia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Why does unsupervised pre-training help deep learning?</title>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Mach Learn Res</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="625" to="660" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Taskrelevant autoencoding" enhances machine learning for human neuroscience</title>
		<author>
			<persName><forename type="first">S</forename><surname>Orouji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Taschereau-Dumouchel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cortese</forename></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2208.08478</idno>
		<idno type="arXiv">arXiv:2208.08478</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2208.08478" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Software Heritage archive for the GitLab repository "self_taught_decoding</title>
		<author>
			<persName><forename type="first">E</forename><surname>Germani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Fromont</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Maumet</surname></persName>
		</author>
		<idno>rev:590ad8461f8ba311f17049a91419261c194c8445</idno>
		<ptr target="origin=https://gitlab.inria.fr/egermani/self_taught_decoding;visit=swh:1:snp:289ee6" />
	</analytic>
	<monogr>
		<title level="j">anchor=swh</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2023">2023. f81cd88d26fa3f332eecfb86d3df1f114f</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">On the benefits of selftaught learning for brain decoding-data</title>
		<author>
			<persName><forename type="first">E</forename><surname>Germani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Fromont</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Maumet</surname></persName>
		</author>
		<idno type="DOI">10.5281/zenodo.7566172</idno>
		<ptr target="https://doi.org/10.5281/zenodo.7566172" />
	</analytic>
	<monogr>
		<title level="j">Zenodo</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">The WU-Minn Human Connectome Project: an overview</title>
		<author>
			<persName><surname>Van Essen</surname></persName>
		</author>
		<author>
			<persName><surname>Dc</surname></persName>
		</author>
		<author>
			<persName><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><surname>Sm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Barch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mapp Connect</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="62" to="79" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">NeuroVault Collection</title>
		<ptr target="https://identifiers.org/neurovault.collection" />
		<imprint>
			<date type="published" when="1952-01-19">1952. 1952. 1952. 19 Jan 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Atlases of cognition with large-scale human brain mapping</title>
		<author>
			<persName><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName><surname>Poldrack</surname></persName>
		</author>
		<author>
			<persName><surname>Ra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS Comput Biol</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">1006565</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Machine learning for neuroimaging with scikit-learn</title>
		<author>
			<persName><forename type="first">A</forename><surname>Abraham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Eickenberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Front Neuroinform</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">14</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">PyTorch: an imperative style, high-performance deep learning library</title>
		<author>
			<persName><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv Neu Inf Process Syst</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="8024" to="8035" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">CUDA Programming: A Developer's Guide to Parallel Computing with GPUs</title>
		<author>
			<persName><forename type="first">S</forename><surname>Cook</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<biblScope unit="page" from="978" to="978" />
		</imprint>
	</monogr>
	<note>1st. ed.</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Supporting data for "On the Benefits of Self-Taught Learning for Brain Decoding</title>
		<author>
			<persName><forename type="first">E</forename><surname>Germani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Fromont</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Maumet</surname></persName>
		</author>
		<idno type="DOI">10.5524/102377</idno>
		<ptr target="https://doi.org/10.5524/102377" />
	</analytic>
	<monogr>
		<title level="j">GigaScience Database</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Batch normalization: accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning. JMLR.org</title>
		<meeting>the 32nd International Conference on Machine Learning. JMLR.org<address><addrLine>Lille, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Adam: a method for stochastic optimization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.1412.6980</idno>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<ptr target="https://doi.org/10.48550/arXiv.1412.6980" />
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Array programming with NumPy</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName><surname>Millman</surname></persName>
		</author>
		<author>
			<persName><surname>Kj</surname></persName>
		</author>
		<author>
			<persName><surname>Walt</surname></persName>
		</author>
		<author>
			<persName><surname>Sjvd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">585</biblScope>
			<biblScope unit="page" from="357" to="362" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: surpassing human-level performance on imagenet classification</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision<address><addrLine>Santiago, Chile</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Machine learning for medical imaging: methodological failures and recommendations for the future</title>
		<author>
			<persName><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Cheplygina</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">npj Digital Med</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">48</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Leakage and the reproducibility crisis in ML-based science</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kapoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Narayanan</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2207.07048</idno>
		<idno type="arXiv">arXiv:2207.07048</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2207.07048" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Revisiting unreasonable effectiveness of data in deep learning Era</title>
		<author>
			<persName><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)<address><addrLine>Venice, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="843" to="852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">What is being transferred in transfer learning?</title>
		<author>
			<persName><forename type="first">B</forename><surname>Neyshabur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sedghi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv Neu Inf Proc Syst</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="512" to="523" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Predicting brain activation maps for arbitrary tasks with cognitive encoding models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Walters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName><surname>Bissett</surname></persName>
		</author>
		<author>
			<persName><surname>Pg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neu-roImage</title>
		<imprint>
			<biblScope unit="volume">263</biblScope>
			<biblScope unit="page">119610</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">On the plurality of (methodological) worlds: estimating the analytic flexibility of fMRI experiments</title>
		<author>
			<persName><forename type="first">J</forename><surname>Carp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Front Neurosci</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">149</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Variability in the analysis of a single neuroimaging dataset by many teams</title>
		<author>
			<persName><forename type="first">R</forename><surname>Botvinik-Nezer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Holzmeister</surname></persName>
		</author>
		<author>
			<persName><surname>Camerer</surname></persName>
		</author>
		<author>
			<persName><surname>Cf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">582</biblScope>
			<biblScope unit="issue">7810</biblScope>
			<biblScope unit="page" from="84" to="88" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Pipeline-Invariant Representation Learning for Neuroimaging</title>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fedorov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mathur</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2208.12909</idno>
		<idno type="arXiv">arXiv:2208.12909</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2208.12909" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Distribution of terms in BrainPedia database</title>
		<author>
			<persName><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName><surname>Poldrack</surname></persName>
		</author>
		<author>
			<persName><surname>Ra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS Comput Biol</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">1006565</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</teiCorpus></text>
</TEI>