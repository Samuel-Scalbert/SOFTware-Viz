<?xml version='1.0' encoding='utf-8'?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" version="1.1" xsi:schemaLocation="http://www.tei-c.org/ns/1.0 http://api.archives-ouvertes.fr/documents/aofr-sword.xsd">
  <teiHeader>
    <fileDesc>
      <titleStmt>
        <title>HAL TEI export of hal-04317927</title>
      </titleStmt>
      <publicationStmt>
        <distributor>CCSD</distributor>
        <availability status="restricted">
          <licence target="http://creativecommons.org/licenses/by/4.0/">Distributed under a Creative Commons Attribution 4.0 International License</licence>
        </availability>
        <date when="2024-04-29T11:47:17+02:00" />
      </publicationStmt>
      <sourceDesc>
        <p part="N">HAL API platform</p>
      </sourceDesc>
    </fileDesc>
  </teiHeader>
  <text>
    <body>
      <listBibl>
        <biblFull>
          <titleStmt>
            <title xml:lang="en">Dance, Dance, Dance With My Hands: Third-Party Human Robot-Human Interactions</title>
            <author role="aut">
              <persName>
                <forename type="first">Sorina-Silvia</forename>
                <surname>Cîrcu</surname>
              </persName>
              <email type="md5">efc4484d76c1af04d0349dd9a4486e82</email>
              <email type="domain">lirmm.fr</email>
              <idno type="idhal" notation="numeric">1323308</idno>
              <idno type="halauthorid" notation="string">2976050-1323308</idno>
              <affiliation ref="#struct-1100638" />
            </author>
            <author role="aut">
              <persName>
                <forename type="first">Bruno</forename>
                <surname>Yun</surname>
              </persName>
              <email type="md5">dc02d6277b304ac507946a14c429500f</email>
              <email type="domain">lirmm.fr</email>
              <idno type="idhal" notation="string">brunoyun</idno>
              <idno type="idhal" notation="numeric">1280752</idno>
              <idno type="halauthorid" notation="string">1083304-1280752</idno>
              <idno type="ORCID">https://orcid.org/0000-0001-9370-3917</idno>
              <affiliation ref="#struct-302758" />
            </author>
            <author role="aut">
              <persName>
                <forename type="first">Abderrahmane</forename>
                <surname>Kheddar</surname>
              </persName>
              <email type="md5">89482b959651fa7c9ed69f14dd28bdb9</email>
              <email type="domain">ibisc.univ-evry.fr</email>
              <idno type="idhal" notation="string">kheddar-abderrahmane</idno>
              <idno type="idhal" notation="numeric">176001</idno>
              <idno type="halauthorid" notation="string">34447-176001</idno>
              <idno type="ORCID">https://orcid.org/0000-0001-9033-9742</idno>
              <idno type="IDREF">https://www.idref.fr/068911890</idno>
              <affiliation ref="#struct-226175" />
              <affiliation ref="#struct-1100638" />
            </author>
            <author role="aut">
              <persName>
                <forename type="first">Chu-Yin</forename>
                <surname>Chen</surname>
              </persName>
              <email type="md5">209fef80cf3a2282b26e069862dd3c9e</email>
              <email type="domain">univ-paris8.fr</email>
              <idno type="idhal" notation="string">chu-yin-chen</idno>
              <idno type="idhal" notation="numeric">1179042</idno>
              <idno type="halauthorid" notation="string">862379-1179042</idno>
              <idno type="ORCID">https://orcid.org/0000-0001-5491-0673</idno>
              <affiliation ref="#struct-205177" />
            </author>
            <author role="aut">
              <persName>
                <forename type="first">Madalina</forename>
                <surname>Croitoru</surname>
              </persName>
              <email type="md5">1c0d9fd64e87edf227ddf57482ebc5a4</email>
              <email type="domain">lirmm.fr</email>
              <idno type="idhal" notation="string">madalina-croitoru</idno>
              <idno type="idhal" notation="numeric">742676</idno>
              <idno type="halauthorid" notation="string">14126-742676</idno>
              <idno type="IDREF">https://www.idref.fr/175044694</idno>
              <idno type="ORCID">https://orcid.org/0000-0001-5456-7684</idno>
              <affiliation ref="#struct-1102911" />
            </author>
            <editor role="depositor">
              <persName>
                <forename>Bruno</forename>
                <surname>YUN</surname>
              </persName>
              <email type="md5">69079229b79a808a8043671f43ab79ba</email>
              <email type="domain">univ-lyon1.fr</email>
            </editor>
          </titleStmt>
          <editionStmt>
            <edition n="v1" type="current">
              <date type="whenSubmitted">2023-12-01 13:38:52</date>
              <date type="whenModified">2024-03-07 14:26:04</date>
              <date type="whenReleased">2023-12-04 13:36:43</date>
              <date type="whenProduced">2023-08-28</date>
              <date type="whenEndEmbargoed">2023-12-01</date>
              <ref type="file" target="https://hal.science/hal-04317927/document">
                <date notBefore="2023-12-01" />
              </ref>
              <ref type="file" subtype="author" n="1" target="https://hal.science/hal-04317927/file/ROMAN2023.pdf">
                <date notBefore="2023-12-01" />
              </ref>
            </edition>
            <respStmt>
              <resp>contributor</resp>
              <name key="1448536">
                <persName>
                  <forename>Bruno</forename>
                  <surname>YUN</surname>
                </persName>
                <email type="md5">69079229b79a808a8043671f43ab79ba</email>
                <email type="domain">univ-lyon1.fr</email>
              </name>
            </respStmt>
          </editionStmt>
          <publicationStmt>
            <distributor>CCSD</distributor>
            <idno type="halId">hal-04317927</idno>
            <idno type="halUri">https://hal.science/hal-04317927</idno>
            <idno type="halBibtex">circu:hal-04317927</idno>
            <idno type="halRefHtml">&lt;i&gt;RO-MAN 2023 - 32nd IEEE International Conference on Robot and Human Interactive Communication&lt;/i&gt;, Aug 2023, Busan, South Korea. pp.1997-2002, &lt;a target="_blank" href="https://dx.doi.org/10.1109/RO-MAN57019.2023.10309353"&gt;&amp;#x27E8;10.1109/RO-MAN57019.2023.10309353&amp;#x27E9;&lt;/a&gt;</idno>
            <idno type="halRef">RO-MAN 2023 - 32nd IEEE International Conference on Robot and Human Interactive Communication, Aug 2023, Busan, South Korea. pp.1997-2002, &amp;#x27E8;10.1109/RO-MAN57019.2023.10309353&amp;#x27E9;</idno>
          </publicationStmt>
          <seriesStmt>
            <idno type="stamp" n="SHS">Sciences de l'Homme et de la Société</idno>
            <idno type="stamp" n="UNIV-PARIS8" corresp="UNIV-PARIS-LUMIERES">Université Paris VIII Vincennes-Saint Denis</idno>
            <idno type="stamp" n="CNRS">CNRS - Centre national de la recherche scientifique</idno>
            <idno type="stamp" n="INRIA">INRIA - Institut National de Recherche en Informatique et en Automatique</idno>
            <idno type="stamp" n="INRIA-SOPHIA">INRIA Sophia Antipolis - Méditerranée</idno>
            <idno type="stamp" n="IATE">Ingénierie des agropolymères et technologies émergentes</idno>
            <idno type="stamp" n="INRIASO">INRIA-SOPHIA</idno>
            <idno type="stamp" n="AO-MUSICOLOGIE" corresp="SHS">AO-MUSICOLOGIE</idno>
            <idno type="stamp" n="INRIA_TEST">INRIA - Institut National de Recherche en Informatique et en Automatique</idno>
            <idno type="stamp" n="AIAC" corresp="UNIV-PARIS8">Arts des Images et Art Contemporain</idno>
            <idno type="stamp" n="TESTALAIN1">TESTALAIN1</idno>
            <idno type="stamp" n="IDH" corresp="LIRMM">Interactive Digital Humans</idno>
            <idno type="stamp" n="LIRMM">Laboratoire d'Informatique de Robotique et de Microélectronique de Montpellier</idno>
            <idno type="stamp" n="INRIA2">INRIA 2</idno>
            <idno type="stamp" n="TDS-MACS">Réseau de recherche en Théorie des Systèmes Distribués, Modélisation, Analyse et Contrôle des Systèmes</idno>
            <idno type="stamp" n="UNIV-MONTPELLIER">Université de Montpellier</idno>
            <idno type="stamp" n="UNIV-PARIS-LUMIERES">Université Paris Lumières</idno>
            <idno type="stamp" n="INSTITUT-AGRO-MONTPELLIER">Institut Agro Montpellier</idno>
            <idno type="stamp" n="INRAE">Institut National de Recherche en Agriculture, Alimentation et Environnement</idno>
            <idno type="stamp" n="INRAEOCCITANIEMONTPELLIER" corresp="INRAE">INRAE Occitanie Montpellier</idno>
            <idno type="stamp" n="UNIV-PARIS8-OA" corresp="UNIV-PARIS8">HAL UNIV-PARIS8 - open access</idno>
            <idno type="stamp" n="INRIA-JAPON">Co-publications INRIA Japon</idno>
            <idno type="stamp" n="UM-2015-2021" corresp="UNIV-MONTPELLIER">Université de Montpellier (2015-2021)</idno>
            <idno type="stamp" n="UM-EPE" corresp="UNIV-MONTPELLIER">Université de Montpellier - EPE</idno>
            <idno type="stamp" n="BOREAL" corresp="LIRMM">Représentation de Connaissances et Langages à Base de Règles pour Raisonner sur les Données</idno>
            <idno type="stamp" n="INRIA-ROYAUMEUNI">INRIA-ROYAUMEUNI</idno>
            <idno type="stamp" n="INSTITUT-AGRO">Institut Agro</idno>
          </seriesStmt>
          <notesStmt>
            <note type="audience" n="2">International</note>
            <note type="invited" n="0">No</note>
            <note type="popular" n="0">No</note>
            <note type="peer" n="1">Yes</note>
            <note type="proceedings" n="1">Yes</note>
          </notesStmt>
          <sourceDesc>
            <biblStruct>
              <analytic>
                <title xml:lang="en">Dance, Dance, Dance With My Hands: Third-Party Human Robot-Human Interactions</title>
                <author role="aut">
                  <persName>
                    <forename type="first">Sorina-Silvia</forename>
                    <surname>Cîrcu</surname>
                  </persName>
                  <email type="md5">efc4484d76c1af04d0349dd9a4486e82</email>
                  <email type="domain">lirmm.fr</email>
                  <idno type="idhal" notation="numeric">1323308</idno>
                  <idno type="halauthorid" notation="string">2976050-1323308</idno>
                  <affiliation ref="#struct-1100638" />
                </author>
                <author role="aut">
                  <persName>
                    <forename type="first">Bruno</forename>
                    <surname>Yun</surname>
                  </persName>
                  <email type="md5">dc02d6277b304ac507946a14c429500f</email>
                  <email type="domain">lirmm.fr</email>
                  <idno type="idhal" notation="string">brunoyun</idno>
                  <idno type="idhal" notation="numeric">1280752</idno>
                  <idno type="halauthorid" notation="string">1083304-1280752</idno>
                  <idno type="ORCID">https://orcid.org/0000-0001-9370-3917</idno>
                  <affiliation ref="#struct-302758" />
                </author>
                <author role="aut">
                  <persName>
                    <forename type="first">Abderrahmane</forename>
                    <surname>Kheddar</surname>
                  </persName>
                  <email type="md5">89482b959651fa7c9ed69f14dd28bdb9</email>
                  <email type="domain">ibisc.univ-evry.fr</email>
                  <idno type="idhal" notation="string">kheddar-abderrahmane</idno>
                  <idno type="idhal" notation="numeric">176001</idno>
                  <idno type="halauthorid" notation="string">34447-176001</idno>
                  <idno type="ORCID">https://orcid.org/0000-0001-9033-9742</idno>
                  <idno type="IDREF">https://www.idref.fr/068911890</idno>
                  <affiliation ref="#struct-226175" />
                  <affiliation ref="#struct-1100638" />
                </author>
                <author role="aut">
                  <persName>
                    <forename type="first">Chu-Yin</forename>
                    <surname>Chen</surname>
                  </persName>
                  <email type="md5">209fef80cf3a2282b26e069862dd3c9e</email>
                  <email type="domain">univ-paris8.fr</email>
                  <idno type="idhal" notation="string">chu-yin-chen</idno>
                  <idno type="idhal" notation="numeric">1179042</idno>
                  <idno type="halauthorid" notation="string">862379-1179042</idno>
                  <idno type="ORCID">https://orcid.org/0000-0001-5491-0673</idno>
                  <affiliation ref="#struct-205177" />
                </author>
                <author role="aut">
                  <persName>
                    <forename type="first">Madalina</forename>
                    <surname>Croitoru</surname>
                  </persName>
                  <email type="md5">1c0d9fd64e87edf227ddf57482ebc5a4</email>
                  <email type="domain">lirmm.fr</email>
                  <idno type="idhal" notation="string">madalina-croitoru</idno>
                  <idno type="idhal" notation="numeric">742676</idno>
                  <idno type="halauthorid" notation="string">14126-742676</idno>
                  <idno type="IDREF">https://www.idref.fr/175044694</idno>
                  <idno type="ORCID">https://orcid.org/0000-0001-5456-7684</idno>
                  <affiliation ref="#struct-1102911" />
                </author>
              </analytic>
              <monogr>
                <title level="m">IEEE Xplore</title>
                <meeting>
                  <title>RO-MAN 2023 - 32nd IEEE International Conference on Robot and Human Interactive Communication</title>
                  <date type="start">2023-08-28</date>
                  <date type="end">2023-08-31</date>
                  <settlement>Busan</settlement>
                  <country key="KR">South Korea</country>
                </meeting>
                <imprint>
                  <publisher>IEEE</publisher>
                  <biblScope unit="serie">2023 32nd IEEE International Conference on Robot and Human Interactive Communication (RO-MAN)</biblScope>
                  <biblScope unit="pp">1997-2002</biblScope>
                </imprint>
              </monogr>
              <idno type="doi">10.1109/RO-MAN57019.2023.10309353</idno>
              <ref type="publisher">https://ro-man2023.org/main</ref>
            </biblStruct>
          </sourceDesc>
          <profileDesc>
            <langUsage>
              <language ident="en">English</language>
            </langUsage>
            <textClass>
              <keywords scheme="author">
                <term xml:lang="en">Gradual Semantics</term>
                <term xml:lang="en">Argumentation</term>
                <term xml:lang="en">Complexity</term>
              </keywords>
              <classCode scheme="halDomain" n="spi.auto">Engineering Sciences [physics]/Automatic</classCode>
              <classCode scheme="halDomain" n="shs.musiq">Humanities and Social Sciences/Musicology and performing arts</classCode>
              <classCode scheme="halTypology" n="COMM">Conference papers</classCode>
              <classCode scheme="halOldTypology" n="COMM">Conference papers</classCode>
              <classCode scheme="halTreeTypology" n="COMM">Conference papers</classCode>
            </textClass>
            <abstract xml:lang="en">
              <p>A robot can affect its social environment beyond the person who is interacting with it. Within this context, we believe it is important to explore Human-Robot Interactions (HRI) in complex social settings. To this end we examine the effect of different robot shapes in a multi-person context during dance routines, to understand how the design of the robot enhances the artistic process and through which factors human preferences are being shaped within a novel third party setting human-robot-human interaction (HRHI).</p>
            </abstract>
          </profileDesc>
        </biblFull>
      </listBibl>
    </body>
    <back>
      <listOrg type="structures">
        <org type="researchteam" xml:id="struct-1100638" status="VALID">
          <orgName>Interactive Digital Humans</orgName>
          <orgName type="acronym">LIRMM | IDH</orgName>
          <date type="start">2022-01-01</date>
          <desc>
            <address>
              <addrLine>LIRMM, 161 rue Ada, 34000 Montpellier</addrLine>
              <country key="FR" />
            </address>
            <ref type="url">https://www.lirmm.fr/equipes/IDH/</ref>
          </desc>
          <listRelation>
            <relation active="#struct-1100620" type="direct" />
            <relation name="UMR5506" active="#struct-441569" type="indirect" />
            <relation name="UMR5506" active="#struct-1100589" type="indirect" />
          </listRelation>
        </org>
        <org type="institution" xml:id="struct-302758" status="VALID">
          <idno type="ROR">https://ror.org/016476m91</idno>
          <orgName>University of Aberdeen</orgName>
          <desc>
            <address>
              <addrLine>King's College, Aberdeen AB24 3FX</addrLine>
              <country key="GB" />
            </address>
            <ref type="url">http://www.abdn.ac.uk/</ref>
          </desc>
        </org>
        <org type="laboratory" xml:id="struct-226175" status="VALID">
          <idno type="RNSR">200819748K</idno>
          <orgName>Joint Robotics Laboratory</orgName>
          <orgName type="acronym">CNRS-AIST JRL </orgName>
          <date type="start">2009-01-01</date>
          <desc>
            <address>
              <addrLine>National Institute of Advanced Industrial Science and Technology (AIST)Tsukuba Central 1, 1-1-1 Umezono, Tsukuba, Ibaraki 305-8560 Japon</addrLine>
              <country key="JP" />
            </address>
            <ref type="url">https://jrl-umi3218.github.io</ref>
          </desc>
          <listRelation>
            <relation active="#struct-302425" type="direct" />
            <relation name="IRL3218 / UMI3218" active="#struct-441569" type="direct" />
          </listRelation>
        </org>
        <org type="laboratory" xml:id="struct-205177" status="VALID">
          <idno type="IdRef">177322586</idno>
          <idno type="RNSR">200515265A</idno>
          <orgName>Arts des Images et Art Contemporain</orgName>
          <orgName type="acronym">AIAC</orgName>
          <date type="start">2005-01-01</date>
          <desc>
            <address>
              <addrLine>Université Paris 8 Vincennes-Saint-Denis - 2 rue de la Liberté - 93526 Saint-Denis cedex </addrLine>
              <country key="FR" />
            </address>
            <ref type="url">http://www.ai-ac.fr</ref>
          </desc>
          <listRelation>
            <relation name="EA4010" active="#struct-11141" type="direct" />
          </listRelation>
        </org>
        <org type="researchteam" xml:id="struct-1102911" status="VALID">
          <idno type="RNSR">202224285F</idno>
          <orgName>Représentation de Connaissances et Langages à Base de Règles pour Raisonner sur les Données</orgName>
          <orgName type="acronym">BOREAL</orgName>
          <date type="start">2022-06-01</date>
          <desc>
            <address>
              <addrLine>LIRMM, 161 rue Ada, 34000 Montpellier</addrLine>
              <country key="FR" />
            </address>
            <ref type="url">https://www.lirmm.fr/equipes/BOREAL</ref>
          </desc>
          <listRelation>
            <relation active="#struct-34586" type="direct" />
            <relation active="#struct-300009" type="indirect" />
            <relation active="#struct-1100620" type="direct" />
            <relation name="UMR5506" active="#struct-441569" type="indirect" />
            <relation name="UMR5506" active="#struct-1100589" type="indirect" />
            <relation active="#struct-1100827" type="direct" />
            <relation name="UMR1208" active="#struct-577435" type="indirect" />
            <relation active="#struct-1096330" type="indirect" />
            <relation active="#struct-1042499" type="indirect" />
            <relation active="#struct-1100589" type="indirect" />
          </listRelation>
        </org>
        <org type="laboratory" xml:id="struct-1100620" status="VALID">
          <idno type="IdRef">139590827</idno>
          <idno type="ISNI">0000000405990488</idno>
          <idno type="RNSR">199111950H</idno>
          <idno type="ROR">https://ror.org/013yean28</idno>
          <orgName>Laboratoire d'Informatique de Robotique et de Microélectronique de Montpellier</orgName>
          <orgName type="acronym">LIRMM</orgName>
          <date type="start">2022-01-01</date>
          <desc>
            <address>
              <addrLine>161 rue Ada - 34095 Montpellier</addrLine>
              <country key="FR" />
            </address>
            <ref type="url">https://www.lirmm.fr</ref>
          </desc>
          <listRelation>
            <relation name="UMR5506" active="#struct-441569" type="direct" />
            <relation name="UMR5506" active="#struct-1100589" type="direct" />
          </listRelation>
        </org>
        <org type="regroupinstitution" xml:id="struct-441569" status="VALID">
          <idno type="IdRef">02636817X</idno>
          <idno type="ISNI">0000000122597504</idno>
          <idno type="ROR">https://ror.org/02feahw73</idno>
          <orgName>Centre National de la Recherche Scientifique</orgName>
          <orgName type="acronym">CNRS</orgName>
          <date type="start">1939-10-19</date>
          <desc>
            <address>
              <country key="FR" />
            </address>
            <ref type="url">https://www.cnrs.fr/</ref>
          </desc>
        </org>
        <org type="regroupinstitution" xml:id="struct-1100589" status="VALID">
          <idno type="ROR">https://ror.org/051escj72</idno>
          <orgName>Université de Montpellier</orgName>
          <orgName type="acronym">UM</orgName>
          <date type="start">2022-01-01</date>
          <desc>
            <address>
              <addrLine>163 rue Auguste Broussonnet - 34090 Montpellier</addrLine>
              <country key="FR" />
            </address>
            <ref type="url">http://www.umontpellier.fr/</ref>
          </desc>
        </org>
        <org type="institution" xml:id="struct-302425" status="VALID">
          <idno type="ROR">https://ror.org/01703db54</idno>
          <orgName>National Institute of Advanced Industrial Science and Technology</orgName>
          <orgName type="acronym">AIST</orgName>
          <desc>
            <address>
              <addrLine>Higashi, Tsukuba, Ibaraki 305-8561</addrLine>
              <country key="JP" />
            </address>
            <ref type="url">http://www.aist.go.jp/index_en.html</ref>
          </desc>
        </org>
        <org type="institution" xml:id="struct-11141" status="VALID">
          <idno type="IdRef">026403552</idno>
          <idno type="ISNI">0000000121083026</idno>
          <orgName>Université Paris 8 Vincennes-Saint-Denis</orgName>
          <orgName type="acronym">UP8</orgName>
          <date type="start">1971-01-01</date>
          <desc>
            <address>
              <addrLine>2 rue de la Liberté - 93526 Saint-Denis cedex</addrLine>
              <country key="FR" />
            </address>
            <ref type="url">http://www.univ-paris8.fr/</ref>
          </desc>
        </org>
        <org type="laboratory" xml:id="struct-34586" status="VALID">
          <idno type="RNSR">198318250R</idno>
          <idno type="ROR">https://ror.org/01nzkaw91</idno>
          <orgName>Inria Sophia Antipolis - Méditerranée</orgName>
          <orgName type="acronym">CRISAM</orgName>
          <desc>
            <address>
              <addrLine>2004 route des Lucioles BP 93 06902 Sophia Antipolis</addrLine>
              <country key="FR" />
            </address>
            <ref type="url">http://www.inria.fr/centre/sophia/</ref>
          </desc>
          <listRelation>
            <relation active="#struct-300009" type="direct" />
          </listRelation>
        </org>
        <org type="institution" xml:id="struct-300009" status="VALID">
          <idno type="ROR">https://ror.org/02kvxyf05</idno>
          <orgName>Institut National de Recherche en Informatique et en Automatique</orgName>
          <orgName type="acronym">Inria</orgName>
          <desc>
            <address>
              <addrLine>Domaine de VoluceauRocquencourt - BP 10578153 Le Chesnay Cedex</addrLine>
              <country key="FR" />
            </address>
            <ref type="url">http://www.inria.fr/en/</ref>
          </desc>
        </org>
        <org type="laboratory" xml:id="struct-1100827" status="VALID">
          <idno type="ISNI">0000000403736662</idno>
          <idno type="RNSR">200317667V</idno>
          <idno type="ROR">https://ror.org/0000n5x09</idno>
          <orgName>Ingénierie des Agro-polymères et Technologies Émergentes</orgName>
          <orgName type="acronym">UMR IATE</orgName>
          <date type="start">2022-01-01</date>
          <desc>
            <address>
              <addrLine>Campus de la Gaillarde 2, place Pierre Viala 34 060 Montpellier Cédex 02 - FRANCE</addrLine>
              <country key="FR" />
            </address>
            <ref type="url">http://umr-iate.cirad.fr/</ref>
          </desc>
          <listRelation>
            <relation name="UMR1208" active="#struct-577435" type="direct" />
            <relation active="#struct-1096330" type="direct" />
            <relation active="#struct-1042499" type="indirect" />
            <relation active="#struct-1100589" type="direct" />
          </listRelation>
        </org>
        <org type="institution" xml:id="struct-577435" status="VALID">
          <idno type="ROR">https://ror.org/003vg9w96</idno>
          <orgName>Institut National de Recherche pour l’Agriculture, l’Alimentation et l’Environnement</orgName>
          <orgName type="acronym">INRAE</orgName>
          <date type="start">2020-01-01</date>
          <desc>
            <address>
              <country key="FR" />
            </address>
          </desc>
        </org>
        <org type="institution" xml:id="struct-1096330" status="VALID">
          <idno type="IdRef">261038990</idno>
          <idno type="ROR">https://ror.org/03rnk6m14</idno>
          <orgName>Institut Agro Montpellier</orgName>
          <date type="start">2022-01-01</date>
          <desc>
            <address>
              <addrLine>2, place Viala - Montpellier</addrLine>
              <country key="FR" />
            </address>
            <ref type="url">https://www.institut-agro-montpellier.fr/</ref>
          </desc>
          <listRelation>
            <relation active="#struct-1042499" type="direct" />
          </listRelation>
        </org>
        <org type="regroupinstitution" xml:id="struct-1042499" status="VALID">
          <idno type="IdRef">260373249</idno>
          <orgName>Institut national d'enseignement supérieur pour l'agriculture, l'alimentation et l'environnement</orgName>
          <orgName type="acronym">Institut Agro</orgName>
          <date type="start">2020-01-01</date>
          <desc>
            <address>
              <country key="FR" />
            </address>
            <ref type="url">https://www.institut-agro.fr</ref>
          </desc>
        </org>
      </listOrg>
    </back>
  <teiCorpus>
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Dance, Dance, Dance With My Hands: Third-Party Human Robot-Human Interactions</title>
			</titleStmt>
			<publicationStmt>
				<publisher />
				<availability status="unknown"><licence /></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Sorina-Silvia</forename><surname>Cîrcu</surname></persName>
							<email>sorina-silvia.circu@lirmm.fr</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">CNRS-University of Montpellier</orgName>
								<orgName type="institution" key="instit2">LIRMM UMR5506</orgName>
								<address>
									<settlement>Montpellier</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">University of Paris 8</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Bruno</forename><surname>Yun</surname></persName>
							<email>bruno.yun@abdn.ac.uk</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Aberdeen</orgName>
								<address>
									<country>Scotland, United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Abderrahmane</forename><surname>Kheddar</surname></persName>
							<email>kheddar@lirmm.fr</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">CNRS-University of Montpellier</orgName>
								<orgName type="institution" key="instit2">LIRMM UMR5506</orgName>
								<address>
									<settlement>Montpellier</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">-AIST Joint Robotics Laboratory IRL3218</orgName>
								<orgName type="institution">CNRS</orgName>
								<address>
									<settlement>Tsukuba</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chu-Yin</forename><surname>Chen</surname></persName>
							<email>chu-yin.chen@univ-paris8.fr</email>
							<affiliation key="aff3">
								<orgName type="institution">University of Paris 8</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Madalina</forename><surname>Croitoru</surname></persName>
							<email>croitoru@lirmm.fr</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">CNRS-University of Montpellier</orgName>
								<orgName type="institution" key="instit2">LIRMM UMR5506</orgName>
								<address>
									<settlement>Montpellier</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Dance, Dance, Dance With My Hands: Third-Party Human Robot-Human Interactions</title>
					</analytic>
					<monogr>
						<imprint>
							<date />
						</imprint>
					</monogr>
					<idno type="MD5">31E6CAF607923BE670BBB309B4D6B99C</idno>
					<idno type="DOI">10.1109/RO-MAN57019.2023.10309353</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-04-12T14:50+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid" />
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div><p>A robot can affect its social environment beyond the person who is interacting with it. Within this context, we believe it is important to explore Human-Robot Interactions (HRI) in complex social settings. To this end we examine the effect of different robot shapes in a multi-person context during dance routines, to understand how the design of the robot enhances the artistic process and through which factors human preferences are being shaped within a novel third party setting human-robot-human interaction (HRHI).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div><head>I. INTRODUCTION</head><p>Questioning how knowledge gained through art practices "can relate to other forms of knowledge regarded by the public as more or less authoritative or trustworthy" <ref type="bibr" target="#b0">[1]</ref>, art is seen as a perfect frame for distributed knowledge.</p><p>Current research suggesting that people's behaviors towards robots are influenced by the observation of thirdparty encounters between robots and other people <ref type="bibr" target="#b1">[2]</ref>, offers important insights. To the extent of our knowledge, we are the first to investigate third-party (i.e. robot -robothuman) interactions <ref type="bibr" target="#b2">[3]</ref> within a dance imitation setting. Such interactions will become more and more common with the rise of the robot <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>. In parallel, nonverbal behaviours <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref> prove their efficiency in collaborative processes between humans and robots. According to <ref type="bibr" target="#b9">[10]</ref>, a robot can affect its social environment beyond the person who is interacting with it. Within this context, we believe it is important to explore Human-Robot Interactions (HRI) in complex social settings. Our initial study was based on a collective HRI done with virtual avatars of robots during a dance workshop. The next step of our research, allows us to tackle new interaction possibilities and verify a new hypothesis about the human-robot social dynamics: from the perspective of another human interacting with a humanrobot dyad, our hypothesis is the embodiment type influences how the robot is being perceived and its impact on the creative environment.Through our approach, we examine the effect of different robot shapes in a multi-person context during dance routines, to understand how the design of the robot enhances the artistic process and through which factors human preferences are being shaped within a third party setting human-robot-human.</p><p>In this paper, motivated by the flourishing perspective of staging robots in art performances, we introduce a novel third-party human-robot-human interaction (HRHI) setting as defined in <ref type="bibr" target="#b2">[3]</ref>. Traditionally third-party interactions are notably less studied than their two-party counterparts (see for instance mediation studies with respect to negotiation studies etc). We restrict ourselves to the setting of collaborative dance routines involving upper body movements between a professional dancer, a robot, and a human. Through this setting, we are interested in the creative potential of such interaction partners that we propose to measure by the improvising capacity of the human spectator. As noted by <ref type="bibr" target="#b10">[11]</ref>, creativity in works of art involves skill defined as a certain "plasticity of the control", i.e., being able to see beyond the specific problem with which one is dealing and having a real understanding of the methods and procedures of the discipline and the principles that lay behind them.</p><p>We propose to analyze the improvisation capacity induced by the third-party creative state from the following key points: substitutability <ref type="bibr" target="#b11">[12]</ref>, synchrony <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref> and kinaesthetic awareness <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref> of the participants:</p><p>• In dance, remembering a phrase or a gesture involves a process of assimilation known as marking, where each dancer reproduces the movement material by activating different body parts. The observations of <ref type="bibr" target="#b11">[12]</ref> propose the marking technique as a scaffold to mentally project a more detailed structure than one could otherwise be held in mind. Similar to an interactive strategy augmenting cognition, dancers mark with their bodies, dance sequences to remember and transmit them. One of the proprieties of the marking is substitutability, describing how a movement in one body part can represent the movement in another. Following Kirsh's observations that "hand movements and head tilts regularly stand for the motion of different body parts", we propose a "dance with the hands" experiment translated by robots, that encodes different body parts in the human partners. In robotics this can have equivalent implications with "the correspondence problem" as defined in <ref type="bibr" target="#b13">[14]</ref>.</p><p>• Synchrony refers to simultaneous actions in our study, playing an important role in collaborative practices <ref type="bibr" target="#b17">[18]</ref>. As defined in <ref type="bibr" target="#b18">[19]</ref>, this process corresponds to "the dynamic and reciprocal adaptation of the temporal structure of behaviors between interactive partners". Moreover, synchrony influences the interaction quality to a greater extent than imitation because "unlike mirroring or mimicry, it is dynamic in [...] the timing, rather than the nature of the behaviors <ref type="bibr" target="#b18">[19]</ref>."</p><p>• Alternatively, the kineasthetic awareness is a very important part of human physical education, involving inner physical sensations, among which the sense of balance and muscle tension. Seen as an ability to learn new things by understanding and controlling the position and movement of ones' body, it is a key factor in dance improvisation.</p><p>According to <ref type="bibr" target="#b19">[20]</ref> this type of awareness is related to embodiment <ref type="bibr" target="#b20">[21]</ref> in human performers. Further on <ref type="bibr" target="#b15">[16]</ref> introduces the concept of kinaesthetic empathy, "which explores the affective potential of movement and, with it, our innate capacity to kinaesthetically perceive other bodies." To tackle this hypothesis, the authors set up a study involving the BodyWeather dance training technique developed by choreographer and dancer Tess de Quincey. This technique's aim is to cultivate body's sensitivity and connectedness with its environment, while generating original movement material. In a similar approach, we focus on dance improvisation mixed with spontaneous gestures to determine how participants with little experience in dancing are relying on the robot to improvise.</p><p>Transposing these observations to human-robot interactions, allows us to determine the impact of robtos on an overall artistic process and analyze how the embodiment type influences the spontaneous responses in humans.</p></div>
<div><head>II. THE HRHI EXPERIMENT</head></div>
<div><head>A. Preliminary experiment</head><p>In the first preliminary experiment, we recruited 25 French participants (24 females and 1 male) from a dance class at Lycee La Mercy in Montpellier. The experiment took place in November 2022. The mean age of the participants was 16.44 (SD=0.58, MD=16). The participants had a group experience of HRI, during a collective dance training session. We prepared three video sequences of one minute each where a series of gestures was executed, following the same order. The 25 participating students were instructed to imitate and then improvise with the virtual versions of an industrial (Franka) and a humanoid robot (HRP-4), see Figure <ref type="figure" target="#fig_0">1</ref>. We asked them to collectively imitate each sequence in the following order: human, industrial robot and a humanoid robot. Once this imitation process occurred, we switched off the projection and asked the participants to collectively improvise using the gestures learnt during the video trials. The improvisation lasted for about 15 minutes and at their demand, we used recorded music to enhance expressive states. Videos of this experiment are available online (https://vimeo.com/779347404 and https: //vimeo.com/779363288). At the end of this process, we asked the participants to fill in a form with 23 questions. The feedback after the collective improvisation was that learning by imitation facilitates the emergence of a creative interaction type, improving the quality of movement.</p><p>Our goal was to understand how digital anthropomorphism triggers creativity and the experiment provided useful insights giving us the opportunity to tackle this concept in a broader context; but, more importantly, it put the basis for investigating the innovative HRHI setting introduced in this paper and described in the next section. Among our observations, we noted that the place the robot has within the experiment depends on the familiarity with the subject.</p></div>
<div><head>B. Participant recruitment</head><p>For investigating HRHI, we have recruited 21 French participants (7 females and 14 males) from Université de Montpellier. The experiment took place in March 2023 at the Laboratory of Computer Science, Robotics and Microelectronics of Montpellier. The mean age of the participants was 24.9 (SD=3.11, MD=24). The study was conducted under the ethical approval of the University of Montpellier.</p></div>
<div><head>C. Methodology</head><p>Each of our participants had to attend two sessions, one with the Franka robot and one with the HRP-4 robot (in this order). Those two sessions were split into an imitation phase (phase 1) and an improvisation phase (phase 2).</p><p>The third-party interactions used a minute-long movement sequence for each of the robots. We first programmed the humanoid robot, then adapted the sequence to the physical limitation (i.e., the number of degrees of freedom and joint order) of the industrial arm, having an identical time grid (i.e., a pause -freeze of motion -occurred at the same moment and lasted for about 5 seconds). The sequence order was identical for each interaction during the imitation phase, while the order of gestures was performed randomly during the improvisation phase. At the end of each session (after the two phases), the participants had to answer 29 questions, referred to as Qi, 1 ≤ i ≤ 29. The list of questions and In my opinion, X robots can be creative if their movements are. . .</p></div>
<div><head>TABLE I SUMMARY OF THE QUESTIONNAIRE STATISTICS</head><p>some statistics about their answers are detailed in Table <ref type="table">I</ref>. Please note that Q28 and Q29 are open questions with freetext answers; in Q18, the participant had to choose between "did not respect the instruction to improvise", "alternatively improvised a few times with the robot and a few times with the human", "completely improvised by myself not giving attention to either human or robot", "imitated the human who was improving with the robot", "felt both improvising with the human and the robot at the same time", "improvised only with the robot", and "improvised only with the human"; in Q6/Q7, the participants had to choose between "No" (1), "Maybe" (2), and "Yes" <ref type="bibr" target="#b2">(3)</ref>. In all the other questions, the participants had to answer using a 5-point Likert scale, from 1 (strongly disagree) to 5 (strongly agree).</p><p>Questions Q2-4, Q10-11, Q13, and Q15-17 (light grey) analyze the imitation context through synchrony and substitutability, whereas Q5-7, Q18-20, and Q23 (dark grey) focus on kinaesthetic awareness during the improvisational part.</p><p>Working with real robots instead of their digital twins <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref> is motivated by the fact that dancing sequences appearing smooth in simulations are altered by live noise, hazardous physical contingencies, and subject to mechanical constraints when enacted through robot bodies. As <ref type="bibr" target="#b20">[21]</ref> states, "such unpredictability is related to a robot's material embodiment, and part of their idiosyncratic charm as performers". Additionally, the professional dancer used a combination of intentional, aesthetic gestures and reflexive and everyday gestures (like yawning) during the improvisation phase, adapting its behaviour to each participant. When the dance improvisation was less comfortable, she engaged in direct contact with the participant, changing place accordingly or performing fewer movements. When the participants expressed agency and autonomy during the improvisation, she deliberately imitated the participant, then performed a series of expressive motions to see if the participant continues the process further. These differences are addressed in Q8 and Q9 (green).</p><p>As the humanoid robot has gestures and movements that are easily recognized as everyday gestures by the partici-pants, we added an extra movement type to the sequence, having the robot shake some body parts during the sequence. This movement type helped the participant distinguish intentional from unintentional (or reflex movements) in the humanoid. It also produced occasionally a loud noise. Alternatively, the industrial robot performed the sequence without symmetrical body parts and the equivalent of shaking expressed through jerk during some joint rotations. Our choice to develop an interaction involving only upper-body dance routines is influenced by the fact that working with constraints enlarges expression possibilities in humans and reduces the risks of robot malfunctioning. As the industrial arm is set on a fixed base, it seemed logical to get the humanoid robot immobile, on a chair. A second chair was available during the humanoid robot experiments, and participants were instructed to choose between using it or not at the beginning of each session. Depending on the choice of each participant, the dancer chose accordingly. As a result, the setting of each experience was different, having the participant and the humanoid sitting, the dancer and the humanoid sitting, or both humans standing and the humanoid sitting. The affordance characteristic <ref type="bibr" target="#b23">[24]</ref> involving a chair requires adopting a reflexive, static position while the human has to compensate with the hands to generate expressive movements. The aspects of comfort and constraints were addressed in the Q10, Q20 and Q24 questions.</p><p>The rest of the questions are oriented towards the comprehension of creativity involving the three key points mentioned in Section I alongside subjective input expressed through questions oriented towards emotions.</p></div>
<div><head>III. RESULTS AND DISCUSSION</head><p>Considering creativity is a combination of skill and intention <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b26">[27]</ref>, our methodology identifies the factors that differentiated a result from either noncreative or creative. Given these premises, we proposed a model of HRHI based on two phases (dance imitation and dance improvisation) to analyze the human expectations within. We analyzed through our questionnaire the kinaesthetic awareness, the substitutability as well as the synchrony potential of two robots operating in an identical laboratory setting. Although the motion algorithms were identical, the robots' moves elicited different reactions in the participants. Their feedback might be related to their familiarity with robots (i.e., participants less accustomed to robots were more enthusiastic about their creative potential in dance). We summarize our results and identify some of the challenges as follows. A detailed version of the data is available here: https://osf.io/rkqwt/?view_ only=ac3cb3ca6e4b4da09052ef5ffeb4d1c2.</p></div>
<div><head>A. Numerical analysis of results</head><p>During the imitation phase, 95.3% of the participants easily imitated the dance sequence for the humanoid robot, compared to 61.9% for the industrial arm. Moreover, 71.5% of the participants in the interaction with the humanoid robot and 61.9% for the industrial arm considered it not suitable to use improvisation movements during this phase. These participants were engaged in what Dourish calls "absorbed coping" <ref type="bibr" target="#b20">[21]</ref>, or a full engagement in the interaction, with 47.6% of participants for the humanoid and 33.3% participants for the industrial arm, not imitating the professional dancer at all. A similar distribution of answers regarding comfort during the imitation: with 85.7% participants feeling comfortable while imitating the humanoid robot and 38.1% the dancer, compared to 66.7% for the robot and 42.85% for the dancer -for the industrial arm robot. This might be correlated with the fact that for the humanoid robot, 71.4% of the participants decided to sit on the chair during either the improvisation or the imitation phase. The feeling of being inspired by the human dancer's movements compared to the robot follows the same trend, with 42.8% of participants agreeing this is the case for the humanoid robot, compared to 38.9% for the industrial one. Overall, the participants found it more interesting to follow the movements of the robot, instead of the dancer-with 66.7% for both the industrial and the humanoid robots. Only a small minority-19% for the industrial and 14.3% for the humanoid identified emotions in the robot's dance, compared to the human's -61.9% for the industrial session and 57.1% for the humanoid one. According to <ref type="bibr" target="#b27">[28]</ref> in bodily emotional expression modes, humans can recognize on average about 50% of a robot's emotions correctly. The lower score in our study can be explained by the fact that the robots were not reactive. This observation emphasizes our intuition that participants engaged in dance (either imitating or improvising) were less likely to contemplate the performative act and ascribe emotions to either human or robot. On another note, during the imitation phase, 66.7% of the participants felt in synchrony with the humanoid robot compared to 57.1% for the industrial robot. Comparing these results to the same question regarding the improvisation phase, we note a difference, as only 14.3% for the industrial robot and 23.8% for the humanoid robot responded positively. This finding proves that synchrony is less established through dance improvisation, regardless of the robot type or the dancer.</p><p>Another interesting fact to mention is that while improvising, the participants did not feel the need to engage in physical contact with the robot (66.7% for the industrial arm and 90.5% with the humanoid robot) with a slight indecision percentage for the industrial arm (19%), compared to (9.5%) for the humanoid robot. Alternatively, 14.3% of the participants in the experiment with the industrial arm responded affirmatively to this question, whereas none for the humanoid robot. This interest in physical contact, reinforcing the kinaesthetic awareness during the improvisation phase, can facilitate a dance creative state in HRHI dance.</p><p>While improvising, the majority of the participants (80.95% for the industrial robot and 90.4% for the humanoid) rejected the idea the robot was reactive, regardless of its shape. However, 42.85% of the participants for the industrial robot and 23.8% for the humanoid to make the robot react to their gestures.</p><p>Since we included in the dancer's sequence a set of unintentional movements (type yawning and shaking) among the standard, aesthetic movements-we wanted to see if the participants made the distinction between them during the third-party interaction. In both cases, 52.4% for the industrial arm and 47.6% for the humanoid one, participants managed to identify them. For the robots, these movements were simulated during the improvisation phase sessions, where each sequence was performed in a random order compared to the imitation phase. For the humanoid robot, a state inspired by human shaking was added. Only 19% of the participants for the industrial arm identified these movements, compared to 85.7% for the humanoid robot. We explain this difference by the fact that the shaking movement was relatively different from the rest of the movements of the sequence, and also analogical to states of neurological dysfunctions in the human body. The occasional jerk from the industrial robot was less attributed to a human-like characteristic.</p><p>When asked whom they have improvised with during the second session, the answers had a similar distribution for both robots. For the humanoid robot, 33.3% of the participants improvised alternatively with both human and robot, 28.6% only with the robot, 23.8% improvised with both at the same time, while 9.5% completely improvised by themselves and 4.8% did not respect the instruction to improvise. For the industrial arm, 28.6% of the participants improvised with both human and robot at the same time, another 28.57% improvised only with the robot, 23.8% improvised alternatively with both human and robot, while 9.5% completely improvised by themselves and others 9.5% did not respect the instruction to improvise. An explanation of these dynamics is illustrated in Fig. <ref type="figure" target="#fig_2">3</ref>. We note that none of our participants responded they improvised with or imitated the dancer exclusively, regardless of the robot shape, proving the focus in an HRHI remains on the robot.</p></div>
<div><head>B. Limitations and future work</head><p>One shortcoming of our experiment is the order in which the trials occurred. Most of the participants started with the industrial arm experiment and then in the humanoid one. Our intention would have been to have the participants interact in random order with the two robots, but for logistic reasons, it was not possible. The imitation phase and its incidence in the participants regarding the humanoid robot might be explained by the fact that seeing the context of the experiments before, got the participants more accustomed to the constraints. Alternatively, these participants were less engaged in the need for a physical contact with the robot, challenging the idea of creativity emerging from collaborative practices like dance contact. Overall, the number of participants might also be a limitation for our analysis, as the data set is small compared to other studies.</p><p>As for our key factors, if substitutability and synchrony are verifiable (i.e through video analysis or direct observation), working with kinaesthetic awareness has a dual nature and is more difficult to measure. According to <ref type="bibr" target="#b16">[17]</ref>: "sensorimotor processes can be characterized as both opaque and transparent. The apparent contradiction is resolved when considering that in both cases there is an issue of not seeing: opacity prevents us from seeing what we try to see, transparency is not seeing that through which we see." To understand its incidence in the improvisation process we rely on our participants' feedback but hope that for our future studies, some sensor measurements could complement our analysis. Furthermore getting both robots to react to participant's movements (i.e., similarly to the human dancer) could also positively affect the overall feedback regarding creativity.</p></div>
<div><head>IV. CONCLUSION</head><p>Unlike other art forms, dance co-exists with traditions involving common, everyday gestures that later inspire aesthetic processes. Throughout its history, post-modern dance has renewed its expression by mixing these everyday gestures with aesthetical ones on stage, through a process of imitation and improvisation. Imitation in human-robot interactive dance has been long researched in the literature <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref>, especially in an embodiment setting <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b33">[34]</ref>. While improvisation was mostly used to improve robots' reactivity and social acceptance <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b36">[37]</ref> artists get robots to improvise on stage whether it is for dancing <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b15">[16]</ref> or playing music <ref type="bibr" target="#b38">[39]</ref>. It is important to specify that in our current study, rather than improvising, the robots are emulating improvisation according to pre-programmed algorithms. Originally a tool for composition, HRI dance improvisation is becoming a live performance technique enhancing creativity in the performers. Inspired by the work of <ref type="bibr" target="#b20">[21]</ref>, we used improvisation as a bottom-up approach for analyzing embodied interaction in dance. As <ref type="bibr" target="#b39">[40]</ref> states, robot design changes rapidly while models become obsolete once companies stop developing them further. Consequently, the robotic community has difficulties in sharing common ground on what the term "robot" currently implies, including a large spectrum of shapes like "android, humanoid, mechanoid, machine-like, zoomorphic or anthropomorphic". Within such categories, understanding the interaction possibilities between humans and robots is a complex challenge, at the core of several disciplines involving robotics, neuroscience, psychology, ethology, philosophy of mind, and possibly arts. Recycling current practices, technologies, and protocols is less investigated than innovative models, leading to an oversimplified view of HRI. Consequently, we expanded the original settings of HRI to a third-party interaction model, in order to develop further emergent concepts related to arts and creativity that could increase social acceptance of robots. The authors in <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b20">[21]</ref>, cite the computer scientist Paul Dourish for whom "embodiment is not a property of systems, technologies, or artefacts; it is a property of interaction". The type of embodied interaction we studied aims at "the creation and sharing of meaning" as defined by Dourish. In conclusion, the concept of embodiment is not limited to the physical manifestation of people and objects, being expanded to collaborative relationships between people and things. Through our study, we highlighted the importance of third party interactions in art experiments involving robots, hoping these models would increase the quality of exchange between them and humans in different social contexts.</p></div><figure xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Representation of the Franka (left) and the HRP-4 (right) robots.</figDesc><graphic coords="3,350.57,135.12,170.07,85.04" type="bitmap" /></figure>
<figure xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Still from the HRP-4 experiments. Imitation phase above and improvisation phase below</figDesc></figure>
<figure xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Number of participants for each answer to Q18 in the two sessions (humanoid and industrial robots).</figDesc><graphic coords="6,54.00,50.08,255.12,219.31" type="bitmap" /></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Learning as understanding in practice: Exploring interrelations between perception, creativity and skill</title>
		<author>
			<persName><forename type="first">W</forename><surname>Gunn</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="26" to="28" />
			<pubPlace>Solstrand, Norway</pubPlace>
		</imprint>
	</monogr>
	<note>Sensuous Knowledge: Creating a Tradition</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Individuals expend more effort to compete against robots than humans after observing competitive human-robot interactions</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">H</forename><surname>Timmerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Henschel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hortensius</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">S</forename><surname>Cross</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Social Robotics</title>
		<meeting><address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021-11-13">10-13 November 2021</date>
			<biblScope unit="page" from="685" to="696" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Initial studies in human-robot-human interaction: Fitts' law for two people</title>
		<author>
			<persName><forename type="first">K</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Peshkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Colgate</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Patton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Robotics and Automation</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="2333" to="2338" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A paradigm shift for robot ethics: From HRI to human-robot-system interaction (HRSI)</title>
		<author>
			<persName><forename type="first">A</forename><surname>Van Wynsberghe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medicolegal and Bioethics</title>
		<imprint>
			<biblScope unit="page" from="11" to="21" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Human-robot interaction: status and challenges</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">B</forename><surname>Sheridan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Human factors</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="525" to="532" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A taxonomy for human-robot interaction</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">A</forename><surname>Yanco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Drury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI fall symposium on human-robot interaction</title>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="111" to="119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A quantitative technique for analysing robot-human interactions</title>
		<author>
			<persName><forename type="first">K</forename><surname>Dautenhahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Werry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/RSJ International Conference on Intelligent Robots and Systems</title>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1132" to="1138" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Robot nonverbal behavior improves task performance in difficult collaborations</title>
		<author>
			<persName><forename type="first">H</forename><surname>Admoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hayes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Scassellati</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM/IEEE International Conference on Human-Robot Interaction</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="51" to="58" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Evaluation of expressive motions based on the framework of laban effort features for social attributes of robots</title>
		<author>
			<persName><forename type="first">E</forename><surname>Emir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Burns</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">31st IEEE International Conference on Robot and Human Interactive Communication (RO-MAN)</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1548" to="1553" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Robots in the wild: A time for more robust theories of human-robot interaction</title>
		<author>
			<persName><forename type="first">M</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hinds</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Creativity and skill</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bailin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Thinking with the body</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kirsh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A dancing robot for rhythmic social interaction</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Michalowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sabanovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kozima</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM/IEEE international conference on Human-robot interaction</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="89" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Synchrony and perception in robotic imitation across embodiments</title>
		<author>
			<persName><forename type="first">A</forename><surname>Alissandrakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Nehaniv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Dautenhahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Symposium on Computational Intelligence in Robotics and Automation</title>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="923" to="930" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Perception of intersensory synchrony: a tutorial review</title>
		<author>
			<persName><forename type="first">J</forename><surname>Vroomen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Keetels</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Attention, Perception, &amp; Psychophysics</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="871" to="884" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Towards socializing nonanthropomorphic robots by harnessing dancers' kinesthetic awareness</title>
		<author>
			<persName><forename type="first">P</forename><surname>Gemeinboeck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Saunders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Cultural Robotics</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="85" to="97" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Cultivating kinaesthetic awareness through interaction: Perspectives from somatic practices and embodied cognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Candau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Franc ¸oise</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Fdili</forename><surname>Alaoui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Schiphorst</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">06</biblScope>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Synchrony and cooperation</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Wiltermuth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Heath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological science</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="5" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Interpersonal synchrony: A survey of evaluation methods across disciplines</title>
		<author>
			<persName><forename type="first">E</forename><surname>Delaherche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chetouani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mahdhaoui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Saint-Georges</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Viaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions on Affective Computing</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="349" to="365" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Action research: Dance improvisation as dance technique</title>
		<author>
			<persName><forename type="first">P</forename><surname>Schwartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Physical Education, Recreation &amp; Dance</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="42" to="46" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Tonight we improvise! real-time tracking for human-robot improvisational dance</title>
		<author>
			<persName><forename type="first">E</forename><surname>Jochum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Derks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Movement and Computing</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">How digital anthropomorphism enhances creativity in human-to-robot dance interactivity</title>
		<author>
			<persName><forename type="first">S.-S</forename><surname>Cîrcu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-Y</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Electronic Art</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Towards a dance co-creation with robots</title>
		<author>
			<persName><forename type="first">S.-S</forename><surname>Cîrcu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Teles De Castro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kheddar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium "Limits of the human, machines without limits?</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Affordance research in developmental robotics: A survey</title>
		<author>
			<persName><forename type="first">H</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Cognitive and Developmental Systems</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="237" to="255" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Chapter five. creativity and skill</title>
		<author>
			<persName><forename type="first">B</forename><surname>Gaut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The idea of creativity</title>
		<imprint>
			<publisher>Brill</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="83" to="103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">The philosophy of creativity</title>
	</analytic>
	<monogr>
		<title level="j">Philosophy Compass</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1034" to="1046" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Improvisation in the arts</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bresnahan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Philosophy Compass</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="573" to="582" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Survey of emotions in human-robot interactions: Perspectives from robotic psychology on 20 years of research</title>
		<author>
			<persName><forename type="first">R</forename><surname>Stock-Homburg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ternational Journal of Social Robotics</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="389" to="411" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Adaptive human-robot interaction in sensorimotor task instruction: From human to robot dance tutors</title>
		<author>
			<persName><forename type="first">R</forename><surname>Ros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Demiris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Robotics and Autonomous Systems</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="707" to="720" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Imitation system of humanoid robots and its applications</title>
		<author>
			<persName><forename type="first">Z.-F</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-P</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Open Journal of Circuits and Systems</title>
		<imprint>
			<biblScope unit="volume">01</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Robotic dance in social robotics-a taxonomy</title>
		<author>
			<persName><forename type="first">H</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Human-Machine Systems</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="281" to="293" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Imitation with alice: learning to imitate corresponding actions across dissimilar embodiments</title>
		<author>
			<persName><forename type="first">A</forename><surname>Alissandrakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Nehaniv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Dautenhahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Systems, Man, and Cybernetics -Part A: Systems and Humans</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="482" to="496" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Action, state and effect metrics for robot imitation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Alissandrakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Nehaniv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Dautenhahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Symposium on Robot and Human Interactive Communication</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="232" to="237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Embodied imitationenhanced reinforcement learning in multi-agent systems</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Erbas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Winfield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bull</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adaptive Behavior</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="31" to="50" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Embodied cognition for autonomous interactive robots</title>
		<author>
			<persName><forename type="first">G</forename><surname>Hoffman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Topics in cognitive science</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="759" to="772" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">The interactive robotic percussionist: new developments in form, mechanics, perception and interaction design</title>
		<author>
			<persName><forename type="first">G</forename><surname>Weinberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Driscoll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM/IEEE international conference on Human-robot interaction</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="97" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Robot dance generation with music based trajectory optimization</title>
		<author>
			<persName><forename type="first">M</forename><surname>Boukheddimi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Harnack</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vyas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Arriaga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Kirchner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE-RSJ International Conference on Intelligent Robots and Systems</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="3069" to="3076" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">The multiple bodies of a machine performer</title>
		<author>
			<persName><forename type="first">L.-P</forename><surname>Demers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Robots and Art: Exploring an Unlikely Symbiosis</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="273" to="306" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">On stage: robots as performers</title>
		<author>
			<persName><forename type="first">G</forename><surname>Hoffman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Human-Robot Interaction: Perspectives and Contributions to Robotics from the Human Sciences</title>
		<meeting><address><addrLine>Los Angeles, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">21</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Some brief thoughts on the past and future of humanrobot interaction</title>
		<author>
			<persName><forename type="first">K</forename><surname>Dautenhahn</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1" to="3" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Understanding kinaesthetic creativity in dance</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hsueh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">F</forename><surname>Alaoui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">E</forename><surname>Mackay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CHI Conference on Human Factors in Computing Systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</teiCorpus></text>
</TEI>