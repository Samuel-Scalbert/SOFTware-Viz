<?xml version='1.0' encoding='utf-8'?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" version="1.1" xsi:schemaLocation="http://www.tei-c.org/ns/1.0 http://api.archives-ouvertes.fr/documents/aofr-sword.xsd">
  <teiHeader>
    <fileDesc>
      <titleStmt>
        <title>HAL TEI export of hal-03346196</title>
      </titleStmt>
      <publicationStmt>
        <distributor>CCSD</distributor>
        <availability status="restricted">
          <licence target="http://creativecommons.org/licenses/by/4.0/">Distributed under a Creative Commons Attribution 4.0 International License</licence>
        </availability>
        <date when="2024-04-22T04:03:54+02:00" />
      </publicationStmt>
      <sourceDesc>
        <p part="N">HAL API platform</p>
      </sourceDesc>
    </fileDesc>
  </teiHeader>
  <text>
    <body>
      <listBibl>
        <biblFull>
          <titleStmt>
            <title xml:lang="en">Benchmarking and challenges in security and privacy for voice biometrics</title>
            <author role="aut">
              <persName>
                <forename type="first">Jean-Francois</forename>
                <surname>Bonastre</surname>
              </persName>
              <idno type="halauthorid">1208911-0</idno>
              <affiliation ref="#struct-100376" />
            </author>
            <author role="aut">
              <persName>
                <forename type="first">Hector</forename>
                <surname>Delgado</surname>
              </persName>
              <idno type="halauthorid">2214718-0</idno>
              <affiliation ref="#struct-1069974" />
            </author>
            <author role="aut">
              <persName>
                <forename type="first">Nicholas</forename>
                <surname>Evans</surname>
              </persName>
              <email type="md5">8875e075e61779bb3539950f1dd3db19</email>
              <email type="domain">eurecom.fr</email>
              <idno type="idhal" notation="numeric">938450</idno>
              <idno type="halauthorid" notation="string">565685-938450</idno>
              <affiliation ref="#struct-421532" />
            </author>
            <author role="aut">
              <persName>
                <forename type="first">Tomi</forename>
                <surname>Kinnunen</surname>
              </persName>
              <idno type="halauthorid">552474-0</idno>
              <affiliation ref="#struct-303425" />
            </author>
            <author role="aut">
              <persName>
                <forename type="first">Kong Aik</forename>
                <surname>Lee</surname>
              </persName>
              <idno type="halauthorid">1152669-0</idno>
              <affiliation ref="#struct-452406" />
            </author>
            <author role="aut">
              <persName>
                <forename type="first">Xuechen</forename>
                <surname>Liu</surname>
              </persName>
              <idno type="halauthorid">2005740-0</idno>
              <affiliation ref="#struct-303425" />
              <affiliation ref="#struct-420403" />
            </author>
            <author role="aut">
              <persName>
                <forename type="first">Andreas</forename>
                <surname>Nautsch</surname>
              </persName>
              <idno type="idhal" notation="numeric">795565</idno>
              <idno type="halauthorid" notation="string">1612971-795565</idno>
              <idno type="ORCID">https://orcid.org/0000-0002-3405-4416</idno>
              <affiliation ref="#struct-421532" />
            </author>
            <author role="aut">
              <persName>
                <forename type="first">Paul-Gauthier</forename>
                <surname>Noe</surname>
              </persName>
              <email type="md5">007deff708a56c9abf330cbb5354080d</email>
              <email type="domain">inria.fr</email>
              <idno type="idhal" notation="string">paul-gauthier-noe</idno>
              <idno type="idhal" notation="numeric">182438</idno>
              <idno type="halauthorid" notation="string">2285349-182438</idno>
              <idno type="ORCID">https://orcid.org/0000-0002-2304-9830</idno>
              <affiliation ref="#struct-100376" />
            </author>
            <author role="aut">
              <persName>
                <forename type="first">Jose</forename>
                <surname>Patino</surname>
              </persName>
              <idno type="halauthorid">40954-0</idno>
              <affiliation ref="#struct-421532" />
            </author>
            <author role="aut">
              <persName>
                <forename type="first">Md</forename>
                <surname>Sahidullah</surname>
              </persName>
              <email type="md5">d1dd516e111d97a16bc76e7d6e394cce</email>
              <email type="domain">inria.fr</email>
              <idno type="idhal" notation="string">sahid</idno>
              <idno type="idhal" notation="numeric">737397</idno>
              <idno type="halauthorid" notation="string">41544-737397</idno>
              <affiliation ref="#struct-420403" />
            </author>
            <author role="aut">
              <persName>
                <forename type="first">Brij Mohan Lal</forename>
                <surname>Srivastava</surname>
              </persName>
              <idno type="halauthorid">1372713-0</idno>
              <affiliation ref="#struct-432650" />
            </author>
            <author role="aut">
              <persName>
                <forename type="first">Massimiliano</forename>
                <surname>Todisco</surname>
              </persName>
              <email type="md5">4b6c7d10c08ba9415437dc1717763744</email>
              <email type="domain">eurecom.fr</email>
              <idno type="idhal" notation="numeric">1199117</idno>
              <idno type="halauthorid" notation="string">1426720-1199117</idno>
              <affiliation ref="#struct-421532" />
            </author>
            <author role="aut">
              <persName>
                <forename type="first">Natalia</forename>
                <surname>Tomashenko</surname>
              </persName>
              <email type="md5">d0c50ddf75b4f251eecb4f1e31b82d94</email>
              <email type="domain">inria.fr</email>
              <idno type="idhal" notation="string">natalia-tomashenko</idno>
              <idno type="idhal" notation="numeric">17002</idno>
              <idno type="halauthorid" notation="string">33762-17002</idno>
              <idno type="ARXIV">https://arxiv.org/a/tomashenko_n_1</idno>
              <idno type="IDREF">https://www.idref.fr/223393304</idno>
              <affiliation ref="#struct-100376" />
            </author>
            <author role="aut">
              <persName>
                <forename type="first">Emmanuel</forename>
                <surname>Vincent</surname>
              </persName>
              <email type="md5">06c8a35e507be1d816f58461ca2239ea</email>
              <email type="domain">inria.fr</email>
              <idno type="idhal" notation="string">emmanuelv</idno>
              <idno type="idhal" notation="numeric">1256</idno>
              <idno type="halauthorid" notation="string">4290-1256</idno>
              <idno type="ORCID">https://orcid.org/0000-0002-0183-7289</idno>
              <idno type="IDREF">https://www.idref.fr/089360176</idno>
              <orgName ref="#struct-300009" />
              <affiliation ref="#struct-420403" />
            </author>
            <author role="aut">
              <persName>
                <forename type="first">Xin</forename>
                <surname>Wang</surname>
              </persName>
              <idno type="idhal" notation="numeric">761950</idno>
              <idno type="halauthorid" notation="string">428988-761950</idno>
              <idno type="ORCID">https://orcid.org/0000-0002-3891-2684</idno>
              <affiliation ref="#struct-6501" />
            </author>
            <author role="aut">
              <persName>
                <forename type="first">Junichi</forename>
                <surname>Yamagishi</surname>
              </persName>
              <idno type="halauthorid">556784-0</idno>
              <affiliation ref="#struct-6501" />
            </author>
            <editor role="depositor">
              <persName>
                <forename>Centre De Documentation</forename>
                <surname>Eurecom</surname>
              </persName>
              <email type="md5">808fb6c747947adf4fb3953c09dd1636</email>
              <email type="domain">eurecom.fr</email>
            </editor>
            <funder ref="#projanr-50446" />
            <funder ref="#projanr-48401" />
            <funder ref="#projanr-49018" />
            <funder ref="#projeurop-712682" />
          </titleStmt>
          <editionStmt>
            <edition n="v1" type="current">
              <date type="whenSubmitted">2024-04-10 17:07:41</date>
              <date type="whenWritten">2021-09-01</date>
              <date type="whenModified">2024-04-11 07:48:11</date>
              <date type="whenReleased">2024-04-11 07:48:11</date>
              <date type="whenProduced">2021-11-10</date>
              <date type="whenEndEmbargoed">2024-04-10</date>
              <ref type="file" target="https://hal.science/hal-03346196/document">
                <date notBefore="2024-04-10" />
              </ref>
              <ref type="file" subtype="author" n="1" target="https://hal.science/hal-03346196/file/2109.00281.pdf">
                <date notBefore="2024-04-10" />
              </ref>
              <ref type="externalLink" target="http://arxiv.org/pdf/2109.00281" />
            </edition>
            <respStmt>
              <resp>contributor</resp>
              <name key="118606">
                <persName>
                  <forename>Centre De Documentation</forename>
                  <surname>Eurecom</surname>
                </persName>
                <email type="md5">808fb6c747947adf4fb3953c09dd1636</email>
                <email type="domain">eurecom.fr</email>
              </name>
            </respStmt>
          </editionStmt>
          <publicationStmt>
            <distributor>CCSD</distributor>
            <idno type="halId">hal-03346196</idno>
            <idno type="halUri">https://hal.science/hal-03346196</idno>
            <idno type="halBibtex">bonastre:hal-03346196</idno>
            <idno type="halRefHtml">&lt;i&gt;SPSC 2021, 1st ISCA Symposium on Security and Privacy in Speech Communication&lt;/i&gt;, ISCA, Nov 2021, Magdeburg, Germany. &lt;a target="_blank" href="https://dx.doi.org/10.21437/SPSC.2021-11"&gt;&amp;#x27E8;10.21437/SPSC.2021-11&amp;#x27E9;&lt;/a&gt;</idno>
            <idno type="halRef">SPSC 2021, 1st ISCA Symposium on Security and Privacy in Speech Communication, ISCA, Nov 2021, Magdeburg, Germany. &amp;#x27E8;10.21437/SPSC.2021-11&amp;#x27E9;</idno>
          </publicationStmt>
          <seriesStmt>
            <idno type="stamp" n="UNIV-AVIGNON">Université d'Avignon</idno>
            <idno type="stamp" n="UNIV-RENNES1">Université de Rennes 1</idno>
            <idno type="stamp" n="CNRS">CNRS - Centre national de la recherche scientifique</idno>
            <idno type="stamp" n="INRIA">INRIA - Institut National de Recherche en Informatique et en Automatique</idno>
            <idno type="stamp" n="IRISA">Irisa</idno>
            <idno type="stamp" n="EURECOM">EURECOM</idno>
            <idno type="stamp" n="INRIA-LILLE">INRIA Lille - Nord Europe</idno>
            <idno type="stamp" n="OPENAIRE">OpenAIRE</idno>
            <idno type="stamp" n="INRIA_TEST">INRIA - Institut National de Recherche en Informatique et en Automatique</idno>
            <idno type="stamp" n="INRIA-LORRAINE">INRIA Nancy - Grand Est</idno>
            <idno type="stamp" n="LORIA2">Publications du LORIA</idno>
            <idno type="stamp" n="INRIA-NANCY-GRAND-EST">INRIA Nancy - Grand Est</idno>
            <idno type="stamp" n="TESTALAIN1">TESTALAIN1</idno>
            <idno type="stamp" n="CRISTAL">Centre de Recherche en Informatique, Signal et Automatique de Lille (CRISTAL)</idno>
            <idno type="stamp" n="UNIV-LORRAINE">Université de Lorraine</idno>
            <idno type="stamp" n="INRIA2">INRIA 2</idno>
            <idno type="stamp" n="CRISTAL-MAGNET" corresp="CRISTAL">CRISTAL-MAGNET</idno>
            <idno type="stamp" n="UR1-HAL">Publications labos UR1 dans HAL-Rennes 1</idno>
            <idno type="stamp" n="LORIA">Laboratoire Lorrain de Recherche en Informatique et ses Applications</idno>
            <idno type="stamp" n="LORIA-NLPKD" corresp="LORIA">Department of Natural Language Processing &amp; Knowledge Discovery</idno>
            <idno type="stamp" n="UR1-MATH-STIC">UR1 - publications Maths-STIC</idno>
            <idno type="stamp" n="UR1-UFR-ISTIC">UFR ISTIC Informatique et électronique</idno>
            <idno type="stamp" n="LIA" corresp="UNIV-AVIGNON">Laboratoire Informatique d'Avignon</idno>
            <idno type="stamp" n="TEST-UR-CSS">TEST Université de Rennes CSS</idno>
            <idno type="stamp" n="UNIV-RENNES">Université de Rennes</idno>
            <idno type="stamp" n="UNIV-LILLE">Université de Lille</idno>
            <idno type="stamp" n="INRIA-300009">Inria 300009</idno>
            <idno type="stamp" n="TEST-HALCNRS">Collection test HAL CNRS</idno>
            <idno type="stamp" n="HYAIAI">Hybrid Approaches for Interpretable Artificial Intelligence</idno>
            <idno type="stamp" n="ANR">ANR</idno>
            <idno type="stamp" n="UR1-MATH-NUM">Pôle UnivRennes - Mathématiques - Numérique </idno>
            <idno type="stamp" n="INRIA-JAPON">Co-publications INRIA Japon</idno>
            <idno type="stamp" n="INRIA-SINGAPOUR">INRIA-SINGAPOUR</idno>
          </seriesStmt>
          <notesStmt>
            <note type="commentary">2021 ISCA Symposium on Security and Privacy in Speech Communication</note>
            <note type="audience" n="2">International</note>
            <note type="invited" n="0">No</note>
            <note type="popular" n="0">No</note>
            <note type="peer" n="1">Yes</note>
            <note type="proceedings" n="1">Yes</note>
          </notesStmt>
          <sourceDesc>
            <biblStruct>
              <analytic>
                <title xml:lang="en">Benchmarking and challenges in security and privacy for voice biometrics</title>
                <author role="aut">
                  <persName>
                    <forename type="first">Jean-Francois</forename>
                    <surname>Bonastre</surname>
                  </persName>
                  <idno type="halauthorid">1208911-0</idno>
                  <affiliation ref="#struct-100376" />
                </author>
                <author role="aut">
                  <persName>
                    <forename type="first">Hector</forename>
                    <surname>Delgado</surname>
                  </persName>
                  <idno type="halauthorid">2214718-0</idno>
                  <affiliation ref="#struct-1069974" />
                </author>
                <author role="aut">
                  <persName>
                    <forename type="first">Nicholas</forename>
                    <surname>Evans</surname>
                  </persName>
                  <email type="md5">8875e075e61779bb3539950f1dd3db19</email>
                  <email type="domain">eurecom.fr</email>
                  <idno type="idhal" notation="numeric">938450</idno>
                  <idno type="halauthorid" notation="string">565685-938450</idno>
                  <affiliation ref="#struct-421532" />
                </author>
                <author role="aut">
                  <persName>
                    <forename type="first">Tomi</forename>
                    <surname>Kinnunen</surname>
                  </persName>
                  <idno type="halauthorid">552474-0</idno>
                  <affiliation ref="#struct-303425" />
                </author>
                <author role="aut">
                  <persName>
                    <forename type="first">Kong Aik</forename>
                    <surname>Lee</surname>
                  </persName>
                  <idno type="halauthorid">1152669-0</idno>
                  <affiliation ref="#struct-452406" />
                </author>
                <author role="aut">
                  <persName>
                    <forename type="first">Xuechen</forename>
                    <surname>Liu</surname>
                  </persName>
                  <idno type="halauthorid">2005740-0</idno>
                  <affiliation ref="#struct-303425" />
                  <affiliation ref="#struct-420403" />
                </author>
                <author role="aut">
                  <persName>
                    <forename type="first">Andreas</forename>
                    <surname>Nautsch</surname>
                  </persName>
                  <idno type="idhal" notation="numeric">795565</idno>
                  <idno type="halauthorid" notation="string">1612971-795565</idno>
                  <idno type="ORCID">https://orcid.org/0000-0002-3405-4416</idno>
                  <affiliation ref="#struct-421532" />
                </author>
                <author role="aut">
                  <persName>
                    <forename type="first">Paul-Gauthier</forename>
                    <surname>Noe</surname>
                  </persName>
                  <email type="md5">007deff708a56c9abf330cbb5354080d</email>
                  <email type="domain">inria.fr</email>
                  <idno type="idhal" notation="string">paul-gauthier-noe</idno>
                  <idno type="idhal" notation="numeric">182438</idno>
                  <idno type="halauthorid" notation="string">2285349-182438</idno>
                  <idno type="ORCID">https://orcid.org/0000-0002-2304-9830</idno>
                  <affiliation ref="#struct-100376" />
                </author>
                <author role="aut">
                  <persName>
                    <forename type="first">Jose</forename>
                    <surname>Patino</surname>
                  </persName>
                  <idno type="halauthorid">40954-0</idno>
                  <affiliation ref="#struct-421532" />
                </author>
                <author role="aut">
                  <persName>
                    <forename type="first">Md</forename>
                    <surname>Sahidullah</surname>
                  </persName>
                  <email type="md5">d1dd516e111d97a16bc76e7d6e394cce</email>
                  <email type="domain">inria.fr</email>
                  <idno type="idhal" notation="string">sahid</idno>
                  <idno type="idhal" notation="numeric">737397</idno>
                  <idno type="halauthorid" notation="string">41544-737397</idno>
                  <affiliation ref="#struct-420403" />
                </author>
                <author role="aut">
                  <persName>
                    <forename type="first">Brij Mohan Lal</forename>
                    <surname>Srivastava</surname>
                  </persName>
                  <idno type="halauthorid">1372713-0</idno>
                  <affiliation ref="#struct-432650" />
                </author>
                <author role="aut">
                  <persName>
                    <forename type="first">Massimiliano</forename>
                    <surname>Todisco</surname>
                  </persName>
                  <email type="md5">4b6c7d10c08ba9415437dc1717763744</email>
                  <email type="domain">eurecom.fr</email>
                  <idno type="idhal" notation="numeric">1199117</idno>
                  <idno type="halauthorid" notation="string">1426720-1199117</idno>
                  <affiliation ref="#struct-421532" />
                </author>
                <author role="aut">
                  <persName>
                    <forename type="first">Natalia</forename>
                    <surname>Tomashenko</surname>
                  </persName>
                  <email type="md5">d0c50ddf75b4f251eecb4f1e31b82d94</email>
                  <email type="domain">inria.fr</email>
                  <idno type="idhal" notation="string">natalia-tomashenko</idno>
                  <idno type="idhal" notation="numeric">17002</idno>
                  <idno type="halauthorid" notation="string">33762-17002</idno>
                  <idno type="ARXIV">https://arxiv.org/a/tomashenko_n_1</idno>
                  <idno type="IDREF">https://www.idref.fr/223393304</idno>
                  <affiliation ref="#struct-100376" />
                </author>
                <author role="aut">
                  <persName>
                    <forename type="first">Emmanuel</forename>
                    <surname>Vincent</surname>
                  </persName>
                  <email type="md5">06c8a35e507be1d816f58461ca2239ea</email>
                  <email type="domain">inria.fr</email>
                  <idno type="idhal" notation="string">emmanuelv</idno>
                  <idno type="idhal" notation="numeric">1256</idno>
                  <idno type="halauthorid" notation="string">4290-1256</idno>
                  <idno type="ORCID">https://orcid.org/0000-0002-0183-7289</idno>
                  <idno type="IDREF">https://www.idref.fr/089360176</idno>
                  <orgName ref="#struct-300009" />
                  <affiliation ref="#struct-420403" />
                </author>
                <author role="aut">
                  <persName>
                    <forename type="first">Xin</forename>
                    <surname>Wang</surname>
                  </persName>
                  <idno type="idhal" notation="numeric">761950</idno>
                  <idno type="halauthorid" notation="string">428988-761950</idno>
                  <idno type="ORCID">https://orcid.org/0000-0002-3891-2684</idno>
                  <affiliation ref="#struct-6501" />
                </author>
                <author role="aut">
                  <persName>
                    <forename type="first">Junichi</forename>
                    <surname>Yamagishi</surname>
                  </persName>
                  <idno type="halauthorid">556784-0</idno>
                  <affiliation ref="#struct-6501" />
                </author>
              </analytic>
              <monogr>
                <meeting>
                  <title>SPSC 2021, 1st ISCA Symposium on Security and Privacy in Speech Communication</title>
                  <date type="start">2021-11-10</date>
                  <date type="end">2021-11-12</date>
                  <settlement>Magdeburg</settlement>
                  <country key="DE">Germany</country>
                </meeting>
                <respStmt>
                  <resp>conferenceOrganizer</resp>
                  <name>ISCA</name>
                </respStmt>
                <imprint>
                  <date type="datePub">2021-11-10</date>
                </imprint>
              </monogr>
              <idno type="arxiv">2109.00281</idno>
              <idno type="doi">10.21437/SPSC.2021-11</idno>
              <ref type="publisher">https://spsc-symposium2021.de/#home</ref>
            </biblStruct>
          </sourceDesc>
          <profileDesc>
            <langUsage>
              <language ident="en">English</language>
            </langUsage>
            <textClass>
              <classCode scheme="classification">SPSC</classCode>
              <classCode scheme="halDomain" n="info.info-ia">Computer Science [cs]/Computer Aided Engineering</classCode>
              <classCode scheme="halTypology" n="COMM">Conference papers</classCode>
              <classCode scheme="halOldTypology" n="COMM">Conference papers</classCode>
              <classCode scheme="halTreeTypology" n="COMM">Conference papers</classCode>
            </textClass>
            <abstract xml:lang="en">
              <p>For many decades, research in speech technologies has focused upon improving reliability. With this now meeting user expectations for a range of diverse applications, speech technology is today omni-present. As result, a focus on security and privacy has now come to the fore. Here, the research effort is in its relative infancy and progress calls for greater, multidisciplinary collaboration with security, privacy, legal and ethical experts among others. Such collaboration is now underway. To help catalyse the efforts, this paper provides a high-level overview of some related research. It targets the non-speech audience and describes the benchmarking methodology that has spearheaded progress in traditional research and which now drives recent security and privacy initiatives related to voice biometrics. We describe: the ASVspoof challenge relating to the development of spoofing countermeasures; the VoicePrivacy initiative which promotes research in anonymisation for privacy preservation.</p>
            </abstract>
          </profileDesc>
        </biblFull>
      </listBibl>
    </body>
    <back>
      <listOrg type="structures">
        <org type="laboratory" xml:id="struct-100376" status="VALID">
          <orgName>Laboratoire Informatique d'Avignon</orgName>
          <orgName type="acronym">LIA</orgName>
          <desc>
            <address>
              <addrLine>339 Chemin des Meinajaries Agroparc BP 1228 84911 Avignon cedex 9</addrLine>
              <country key="FR" />
            </address>
            <ref type="url">http://lia.univ-avignon.fr/</ref>
          </desc>
          <listRelation>
            <relation active="#struct-195507" type="direct" />
            <relation active="#struct-302221" type="direct" />
          </listRelation>
        </org>
        <org type="institution" xml:id="struct-1069974" status="VALID">
          <orgName>Nuance Communications [Spain]</orgName>
          <desc>
            <address>
              <addrLine>Nuance Communications, Inc. C/ Gran Via 39, 8th floor, 28013 Madrid </addrLine>
              <country key="ES" />
            </address>
            <ref type="url">https://www.nuance.com/es-es/index.html</ref>
          </desc>
        </org>
        <org type="institution" xml:id="struct-421532" status="VALID">
          <orgName>Eurecom [Sophia Antipolis]</orgName>
          <desc>
            <address>
              <addrLine>Campus SophiaTech, 450 Route des Chappes, CS 50193 - 06904 Biot Sophia Antipolis cedex</addrLine>
              <country key="FR" />
            </address>
            <ref type="url">http://www.eurecom.fr/</ref>
          </desc>
        </org>
        <org type="institution" xml:id="struct-303425" status="VALID">
          <orgName>University of Eastern Finland</orgName>
          <date type="start">2010-01-01</date>
          <desc>
            <address>
              <addrLine> Educa, Tulliportinkatu 1, 80130 Joensuu, Finland</addrLine>
              <country key="FI" />
            </address>
            <ref type="url">http://www.uef.fi/en/etusivu</ref>
          </desc>
        </org>
        <org type="institution" xml:id="struct-452406" status="VALID">
          <orgName>Institute for Infocomm Research - I²R [Singapore]</orgName>
          <desc>
            <address>
              <addrLine>1 Fusionopolis Way, Singapore 138632</addrLine>
              <country key="SG" />
            </address>
            <ref type="url">http://www.i2r.a-star.edu.sg/</ref>
          </desc>
        </org>
        <org type="researchteam" xml:id="struct-420403" status="VALID">
          <idno type="RNSR">201421147E</idno>
          <orgName>Speech Modeling for Facilitating Oral-Based Communication</orgName>
          <orgName type="acronym">MULTISPEECH</orgName>
          <desc>
            <address>
              <country key="FR" />
            </address>
            <ref type="url">http://www.inria.fr/equipes/multispeech</ref>
          </desc>
          <listRelation>
            <relation active="#struct-129671" type="direct" />
            <relation active="#struct-300009" type="indirect" />
            <relation active="#struct-423086" type="direct" />
            <relation active="#struct-206040" type="indirect" />
            <relation active="#struct-413289" type="indirect" />
            <relation name="UMR7503" active="#struct-441569" type="indirect" />
          </listRelation>
        </org>
        <org type="researchteam" xml:id="struct-432650" status="VALID">
          <idno type="RNSR">201321079K</idno>
          <orgName>Machine Learning in Information Networks</orgName>
          <orgName type="acronym">MAGNET</orgName>
          <date type="start">2015-01-01</date>
          <desc>
            <address>
              <country key="FR" />
            </address>
            <ref type="url">http://www.inria.fr/equipes/magnet</ref>
          </desc>
          <listRelation>
            <relation active="#struct-104752" type="direct" />
            <relation active="#struct-300009" type="indirect" />
            <relation active="#struct-410272" type="direct" />
            <relation name="UMR9189" active="#struct-120930" type="indirect" />
            <relation name="UMR9189" active="#struct-374570" type="indirect" />
            <relation name="UMR9189" active="#struct-441569" type="indirect" />
          </listRelation>
        </org>
        <org type="institution" xml:id="struct-6501" status="VALID">
          <idno type="ROR">https://ror.org/04ksd4g47</idno>
          <orgName>National Institute of Informatics</orgName>
          <orgName type="acronym">NII</orgName>
          <desc>
            <address>
              <addrLine>2-1-2 Hitotsubashi, Chiyoda-ku, Tokyo 101-8430</addrLine>
              <country key="JP" />
            </address>
            <ref type="url">http://www.nii.ac.jp/en/</ref>
          </desc>
        </org>
        <org type="institution" xml:id="struct-195507" status="VALID">
          <idno type="ROR">https://ror.org/00mfpxb84</idno>
          <orgName>Avignon Université</orgName>
          <orgName type="acronym">AU</orgName>
          <desc>
            <address>
              <addrLine>74 rue Louis Pasteur - 84 029 Avignon cedex 1</addrLine>
              <country key="FR" />
            </address>
            <ref type="url">http://www.univ-avignon.fr/</ref>
          </desc>
        </org>
        <org type="institution" xml:id="struct-302221" status="VALID">
          <orgName>Centre d'Enseignement et de Recherche en Informatique - CERI</orgName>
          <desc>
            <address>
              <country key="FR" />
            </address>
          </desc>
        </org>
        <org type="laboratory" xml:id="struct-129671" status="VALID">
          <idno type="RNSR">198618246Y</idno>
          <idno type="ROR">https://ror.org/03fcjvn64</idno>
          <orgName>Inria Nancy - Grand Est</orgName>
          <desc>
            <address>
              <addrLine>615 rue du Jardin Botanique 54600 Villers-lès-Nancy</addrLine>
              <country key="FR" />
            </address>
            <ref type="url">http://www.inria.fr/nancy</ref>
          </desc>
          <listRelation>
            <relation active="#struct-300009" type="direct" />
          </listRelation>
        </org>
        <org type="institution" xml:id="struct-300009" status="VALID">
          <idno type="ROR">https://ror.org/02kvxyf05</idno>
          <orgName>Institut National de Recherche en Informatique et en Automatique</orgName>
          <orgName type="acronym">Inria</orgName>
          <desc>
            <address>
              <addrLine>Domaine de VoluceauRocquencourt - BP 10578153 Le Chesnay Cedex</addrLine>
              <country key="FR" />
            </address>
            <ref type="url">http://www.inria.fr/en/</ref>
          </desc>
        </org>
        <org type="department" xml:id="struct-423086" status="VALID">
          <orgName>Department of Natural Language Processing &amp; Knowledge Discovery</orgName>
          <orgName type="acronym">LORIA - NLPKD</orgName>
          <desc>
            <address>
              <country key="FR" />
            </address>
            <ref type="url">http://www.loria.fr/en/research/departments/natural-language-processing-and-knowledge-discovery/</ref>
          </desc>
          <listRelation>
            <relation active="#struct-206040" type="direct" />
            <relation active="#struct-300009" type="indirect" />
            <relation active="#struct-413289" type="indirect" />
            <relation name="UMR7503" active="#struct-441569" type="indirect" />
          </listRelation>
        </org>
        <org type="laboratory" xml:id="struct-206040" status="VALID">
          <idno type="IdRef">067077927</idno>
          <idno type="ISNI">0000000121795429</idno>
          <idno type="RNSR">198912571S</idno>
          <idno type="IdUnivLorraine">[UL]RSI--</idno>
          <idno type="ROR">https://ror.org/02vnf0c38</idno>
          <orgName>Laboratoire Lorrain de Recherche en Informatique et ses Applications</orgName>
          <orgName type="acronym">LORIA</orgName>
          <date type="start">2012-01-01</date>
          <desc>
            <address>
              <addrLine>Campus Scientifique BP 239 54506 Vandoeuvre-lès-Nancy Cedex</addrLine>
              <country key="FR" />
            </address>
            <ref type="url">http://www.loria.fr</ref>
          </desc>
          <listRelation>
            <relation active="#struct-300009" type="direct" />
            <relation active="#struct-413289" type="direct" />
            <relation name="UMR7503" active="#struct-441569" type="direct" />
          </listRelation>
        </org>
        <org type="institution" xml:id="struct-413289" status="VALID">
          <idno type="IdRef">157040569</idno>
          <idno type="IdUnivLorraine">[UL]100--</idno>
          <idno type="ROR">https://ror.org/04vfs2w97</idno>
          <orgName>Université de Lorraine</orgName>
          <orgName type="acronym">UL</orgName>
          <date type="start">2012-01-01</date>
          <desc>
            <address>
              <addrLine>34 cours Léopold - CS 25233 - 54052 Nancy cedex</addrLine>
              <country key="FR" />
            </address>
            <ref type="url">http://www.univ-lorraine.fr/</ref>
          </desc>
        </org>
        <org type="regroupinstitution" xml:id="struct-441569" status="VALID">
          <idno type="IdRef">02636817X</idno>
          <idno type="ISNI">0000000122597504</idno>
          <idno type="ROR">https://ror.org/02feahw73</idno>
          <orgName>Centre National de la Recherche Scientifique</orgName>
          <orgName type="acronym">CNRS</orgName>
          <date type="start">1939-10-19</date>
          <desc>
            <address>
              <country key="FR" />
            </address>
            <ref type="url">https://www.cnrs.fr/</ref>
          </desc>
        </org>
        <org type="laboratory" xml:id="struct-104752" status="VALID">
          <idno type="RNSR">200818245B</idno>
          <idno type="ROR">https://ror.org/04eej9726</idno>
          <orgName>Inria Lille - Nord Europe</orgName>
          <desc>
            <address>
              <addrLine>Parc Scientifique de la Haute Borne 40, avenue Halley Bât.A, Park Plaza 59650 Villeneuve d'Ascq</addrLine>
              <country key="FR" />
            </address>
            <ref type="url">http://www.inria.fr/lille/</ref>
          </desc>
          <listRelation>
            <relation active="#struct-300009" type="direct" />
          </listRelation>
        </org>
        <org type="laboratory" xml:id="struct-410272" status="VALID">
          <idno type="IdRef">18388695X</idno>
          <idno type="RNSR">201521249L</idno>
          <idno type="ROR">https://ror.org/05vrs3189</idno>
          <orgName>Centre de Recherche en Informatique, Signal et Automatique de Lille - UMR 9189</orgName>
          <orgName type="acronym">CRIStAL</orgName>
          <date type="start">2015-01-01</date>
          <desc>
            <address>
              <addrLine>Université de Lille - Campus scientifique - Bâtiment ESPRIT - Avenue Henri Poincaré - 59655 Villeneuve d’Ascq</addrLine>
              <country key="FR" />
            </address>
            <ref type="url">https://www.cristal.univ-lille.fr/</ref>
          </desc>
          <listRelation>
            <relation name="UMR9189" active="#struct-120930" type="direct" />
            <relation name="UMR9189" active="#struct-374570" type="direct" />
            <relation name="UMR9189" active="#struct-441569" type="direct" />
          </listRelation>
        </org>
        <org type="institution" xml:id="struct-120930" status="VALID">
          <idno type="IdRef">256304629</idno>
          <idno type="ISNI">0000000122034461</idno>
          <idno type="ROR">https://ror.org/01x441g73</idno>
          <orgName>Centrale Lille</orgName>
          <desc>
            <address>
              <addrLine>École Centrale de Lille - Cité Scientifique - CS 20048 59651 Villeneuve d'Ascq Cedex</addrLine>
              <country key="FR" />
            </address>
            <ref type="url">https://centralelille.fr/</ref>
          </desc>
        </org>
        <org type="regroupinstitution" xml:id="struct-374570" status="VALID">
          <idno type="IdRef">223446556</idno>
          <idno type="ISNI">0000 0001 2242 6780</idno>
          <idno type="ROR">https://ror.org/02kzqn938</idno>
          <idno type="Wikidata">Q3551621</idno>
          <orgName>Université de Lille</orgName>
          <desc>
            <address>
              <addrLine>EPE Université de Lille. -- 42 rue Paul Duez, 59000 Lille</addrLine>
              <country key="FR" />
            </address>
            <ref type="url">https://www.univ-lille.fr/</ref>
          </desc>
        </org>
      </listOrg>
      <listOrg type="projects">
        <org type="anrProject" xml:id="projanr-50446" status="VALID">
          <idno type="anr">ANR-19-DATA-0008</idno>
          <orgName>Harpocrates</orgName>
          <desc>Open data, outils et challenges pour l'anonymisation des voix</desc>
          <date type="start">2019</date>
        </org>
        <org type="anrProject" xml:id="projanr-48401" status="VALID">
          <idno type="anr">ANR-18-CE23-0018</idno>
          <idno type="program">APPEL À PROJETS GÉNÉRIQUE 2018</idno>
          <orgName>DEEP-PRIVACY</orgName>
          <desc>Apprentissage distribué, personnalisé, préservant la privacité pour le traitement de la parole</desc>
          <date type="start">2018</date>
        </org>
        <org type="anrProject" xml:id="projanr-49018" status="VALID">
          <idno type="anr">ANR-18-JSTS-0001</idno>
          <idno type="program">APPEL À PROJETS FRANCO-JAPONAIS : INTERACTION SYMBIOTIQUE</idno>
          <orgName>VoicePersonae</orgName>
          <desc>Clonage et protection de l'identité vocale</desc>
          <date type="start">2018</date>
        </org>
        <org type="europeanProject" xml:id="projeurop-712682" status="VALID">
          <idno type="number">825081</idno>
          <idno type="program">H2020</idno>
          <orgName>COMPRISE</orgName>
          <desc>Cost effective, Multilingual, Privacy-driven voice-enabled Services</desc>
          <date type="start">2018-12-01</date>
          <date type="end">2021-11-30</date>
        </org>
      </listOrg>
    </back>
  <teiCorpus>
	<teiHeader xml:lang="fr">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Benchmarking and challenges in security and privacy for voice biometrics</title>
				<funder>
					<orgName type="full">COMPRISE)</orgName>
				</funder>
				<funder ref="#_f6rF3cF">
					<orgName type="full">Japan Science and Technology Agency</orgName>
					<orgName type="abbreviated">JST</orgName>
				</funder>
				<funder ref="#_UU58R3D">
					<orgName type="full">Region Grand Est, France</orgName>
				</funder>
				<funder ref="#_C84xXTt">
					<orgName type="full">Academy of Finland</orgName>
				</funder>
				<funder ref="#_vKPwHRH">
					<orgName type="full">European Commission</orgName>
				</funder>
				<funder>
					<orgName type="full">French ANR (VoicePrivacy, VoicePersonae</orgName>
				</funder>
				<funder ref="#_XdHb7pz">
					<orgName type="full">JSPS</orgName>
				</funder>
				<funder>
					<orgName type="full">DEEP-PRIVACY, Harpocrates)</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher />
				<availability status="unknown"><licence /></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jean-Francois</forename><surname>Bonastre</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Hector</forename><surname>Delgado</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Nicholas</forename><surname>Evans</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Tomi</forename><surname>Kinnunen</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Aik</forename><surname>Kong</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Xuechen</forename><surname>Lee</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Andreas</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Paul-Gauthier</forename><surname>Nautsch</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jose</forename><surname>Noe</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Md</forename><surname>Patino</surname></persName>
						</author>
						<author>
							<persName><surname>Sahidullah</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Hector</forename><surname>Delgado</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Héctor</forename><surname>Delgado</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jose</forename><surname>Noé</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Brij</forename><surname>Sahidullah</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Lal</forename><surname>Mohan</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Massimiliano</forename><surname>Srivastava</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Natalia</forename><surname>Todisco</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Emmanuel</forename><surname>Tomashenko</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Xin</forename><surname>Vincent</surname></persName>
						</author>
						<author>
							<persName><surname>Wang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yamagishi</forename><surname>Junichi</surname></persName>
						</author>
						<author>
							<persName><surname>Asvspoof</surname></persName>
						</author>
						<title level="a" type="main">Benchmarking and challenges in security and privacy for voice biometrics</title>
					</analytic>
					<monogr>
						<imprint>
							<date />
						</imprint>
					</monogr>
					<idno type="MD5">72AE110C98CB04CA0A1784EC9ABD205D</idno>
					<idno type="DOI">10.21437/SPSC.2021-11</idno>
					<note type="submission">Submitted on 10 Apr 2024</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-04-12T14:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid" />
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div><p>niveau recherche, publiés ou non, émanant des établissements d'enseignement et de recherche français ou étrangers, des laboratoires publics ou privés.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="fr">
		<body>
<div><head n="1.">Introduction</head><p>The voice is among the most natural and convenient means to human-machine interaction and biometric authentication. In some scenarios, particularly telephony or teleconferencing applications, voice can be the only available biometric. While automatic speaker verification (ASV) systems can provide a reliable means to authentication, like all biometric technologies, it is not without security and privacy concerns. Arguably, these concerns are potentially greater for voice biometrics than they can be for authentication systems that use alternative biometric characteristics.</p><p>Security concerns relate to the potential for ASV systems to be manipulated by adversaries through spoofing attacks <ref type="bibr" target="#b0">[1]</ref>, now referred to as presentation attacks <ref type="bibr" target="#b1">[2]</ref>. Fraudsters can launch spoofing attacks to gain illegitimate access to protected services or resources by presenting to the ASV system a speech recording which has been manipulated to sound<ref type="foot" target="#foot_0">1</ref> like another speaker. Without adequate protection, spoofing attacks can substantially degrade the reliability of almost any ASV system.</p><p>While not related specifically to voice biometrics, but to speech technology more generally, privacy concerns relate to the potential for speech data to be exploited for purposes other than those to which an individual might have given consent <ref type="bibr" target="#b2">[3]</ref>. Speech signals are a rich source of personal, private information. In providing recordings of speech to a particular voice service, the speaker usually furnishes the service provider with much more information than is strictly necessary in order to perform the expected task, hence the need for privacy preservation.</p><p>In this article we describe two specific benchmarking challenges launched by the speech processing research community to expedite solutions to security and privacy concerns. The first involves solutions to protect ASV systems from being manipulated by spoofing in the form of spoofing countermeasures or presentation attack detection systems <ref type="bibr" target="#b1">[2]</ref> whose development is spearheaded through the ASVspoof initiative launched in 2015 <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6]</ref>. The second relates to the <software ContextAttributes="used">VoicePrivacy</software> initiative, launched in 2020 <ref type="bibr" target="#b6">[7]</ref>, which aims to promote the development of privacy preservation solutions for speech technology. The inaugural <software ContextAttributes="used">VoicePrivacy</software> challenge focused upon anonymisation, namely techniques to manipulate speech data in order that it cannot be used with ASV systems to recognise the speaker.</p><p>The article is intended as a contribution to the efforts to build bridges between the speech community and, e.g., the legal and ethical communities and, in particular, to support the recently formed Security and Privacy in Speech Communication (SPSC) special interest group of the International Speech Communication Association (ISCA). It targets the non-specialist and is hence intentionally high-level, with a focus upon the essentials and clarity, rather than upon scientific and technical rigour. We describe the importance of benchmarking campaigns, classifier fundamentals and the early, classical approaches to performance estimation. After presenting a high-level overview of ASV systems, the remainder of the article introduces the ASVspoof and <software>VoicePrivacy</software> initiatives.</p></div>
<div><head n="2.">Evaluation-driven research</head><p>In the early days, researchers collected their own datasets to develop methods (algorithms and software). To make technological progress in complex tasks such as speech or speaker recognition, and to be able to meaningfully compare different methods, the need for commensurable performance benchmarking was quickly recognised. From the mid-1990s, the National Institute of Standards and Technology (NIST) -a US-based standardisation body -pioneered evaluation-driven research <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref>. The key ingredients are: (1) commonly agreed (often public) data, evaluation rules, and performance metrics; (2) disentangled roles for researchers (evaluees) and evaluators. Performance claims should not be reported by evaluees but instead by independent evaluators who provide common infrastructure (data, rules, metrics) in the form of an evaluation campaign or challenge. Only with such a level playing field can competing methods can be meaningfully compared. There are many such  challenges <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref> within the speech field. Typically, they require participants <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref> to design special-purpose software to solve some specific tasks. For many, the software takes the form of a binary classifier, namely a system which must choose between two mutually exclusive hypotheses.</p><p>Even if classifiers are based on well-established statistical principles and are trained objectively via numerical optimisation techniques, they can (and do) make errors. Errors are the result of over-simplified modelling assumptions, natural variability in the input data, sources of external nuisance variation, or as a result of limited training data, etc. The design of evermore reliable classifiers is the traditional bread and butter of machine learning and speech research.</p><p>A binary classifier makes two types of errors: misses (false rejections) and false alarms (false acceptances). Misses imply that the classifier rejects a positive input that should be accepted. A false alarm results from the classifier accepting a negative input that should be rejected. Classifier performance can be estimated by dividing the number of misses and false alarms by the number of tested positive and negative cases respectively. The resulting miss rate (Pmiss) and false alarm rate (PF A) can be seen as proxies for user convenience and security.</p><p>Binary classifier decisions are derived in two stages. The first involves computation of a 'soft decision' (the score)a real number that expresses the classifier's confidence in the positive case. Example raw score distributions for such a binary classifier are illustrated in Fig. <ref type="figure" target="#fig_0">1</ref>(a) were the positive case is the target class and the negative case is the impostor class. In practice, the score is often a logarithmic likelihood ratio (LLR) and expresses the relative strength between the two competing hypotheses. Second, the score is compared with a pre-set threshold δ. Scores above the threshold imply 'accept', whereas scores below the threshold imply 'reject'. By increasing the threshold, the false alarm rate (PF A, red shaded area in Fig. <ref type="figure" target="#fig_0">1</ref>) can be reduced at the expense of a higher miss rate (Pmiss, green shaded area) and vice versa. The trade-off can be readily visualised in so-called detection error trade-off (DET) plots <ref type="bibr" target="#b13">[14]</ref> such as that illustrated in Fig. <ref type="figure" target="#fig_2">2</ref> </p></div>
<div><head>(described below).</head><p>Since there are two error rates, which do we report? Or do we report both? What threshold / how do we set it? The answer to these questions is rather subtle and a detailed treatment is outside the scope of this paper. One particular metric, namely the equal error rate (EER) is adopted for a broad range of tasks. It is computed using the threshold that makes miss and false alarm rates equal (see Fig. <ref type="figure" target="#fig_2">2</ref>) -thereby yielding a single number to report. The lower the EER, the more reliable the classifier.</p><p>The authors acknowledge that, even if the EER is deprecated in ISO standards <ref type="bibr" target="#b14">[15]</ref>, it provides a compact summary of the discrimination capabilities of a classifier -how well it is capable of observing (in speech, 'hearing') differences between positive and negative inputs. In practice, however, the EER does not provide a full picture. More comprehensive approaches to assessment have been developed and have been broadly adopted by the community. These provide a view of classifier performance through the lens of a formal decision policy which represents the effective trade-off between the two decision outcomes <ref type="bibr" target="#b15">[16]</ref>.</p></div>
<div><head n="3.">Automatic speaker verification</head><p>ASV systems provide one of the most natural and convenient means to biometric person authentication. Test recordings (probes) are compared with enrolment recordings (references) to verify (or not) a claimed identity. The reference is used to create a model 2 which is stored in a reference database. At test time, the model corresponding to the claimed identity is compared to the test utterance resulting in a soft score. A hard accept/reject decision can be fully automated (e.g., online banking) or semi-automated (with some human intervention, e.g., when forensic practitioners present voice evidence in court). Scores should reflect reliably the extent to which the strengthof-evidence supports the same/different identity propositions: the higher the score, the greater the similarity and vice versa.</p><p>Detection error trade-off plots for two different ASV systems assessed using the ASVspoof 2019 logical access database and the <software ContextAttributes="used">VoicePrivacy</software> 2020 database are illustrated in Figs. <ref type="figure" target="#fig_2">2</ref> and<ref type="figure">4</ref> and show EERs of 2.5% and 1.1% respectively (blue profiles). Each point on any one profile corresponds to a different decision threshold (operating point). The profiles show how misses can be traded off against false alarms in order to meet different application requirements.</p></div>
<div><head n="4.">Security vulnerabilities: ASVspoof</head><p>Without adequate protections, the reliability of ASV systems can be compromised by the presentation of synthetic or converted voice, replayed speech and impersonation <ref type="bibr" target="#b5">[6]</ref>. Generated automatically from a text input, today's state-of-the-art synthesis systems are capable of producing speech that the human cannot distinguish from bona fide speech <ref type="bibr" target="#b16">[17]</ref>. Voice conversion systems operate directly upon an input speech signal and alter the voice to that of another speaker <ref type="bibr" target="#b17">[18]</ref>. Unlike synthetic and converted voice spoofing attacks, which both demand a certain technical expertise and suitable training and adaptation data, 2 On account of their dynamic nature and the variability in speech signals, we refer to models, not templates.  replay attacks can be launched by the layman, requiring only consumer-grade recording and replaying devices. All can substantially degrade ASV reliability. Impersonation, while still a threat <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20]</ref>, is less effective.</p><p>The impact of spoofing attacks upon an ASV system is illustrated in Fig. <ref type="figure" target="#fig_0">1(b)</ref>. Spoofing attacks introduce a third class of input so that the ASV system must now cope with target (matching), impostor (non-matching) and spoofed utterances. A successful spoofing attack circumvents the ASV system by provoking a score above the decision threshold. The score distribution for spoofing attacks is illustrated in the middle of Fig. <ref type="figure" target="#fig_0">1(b)</ref>. Spoofing attacks provoke a false alarm rate P spoof that is greater than the original false alarm rate PF A. One can think of spoofing attacks as a special case of impostors where there is a concerted effort to deceive the ASV system.</p><p>Without the capacity to distinguish between spoofed and bona fide speech, ASV reliability will degrade as a result of spoofing attacks, sometimes substantially. This degradation assessed using the ASVspoof 2019 logical access database (synthetic and converted voice spoofing attacks) is illustrated in Fig. <ref type="figure" target="#fig_2">2</ref>. The baseline EER (with no spoofing attacks) of 2.5% increases to over 50% when impostor trials are replaced by the most effective synthetic and converted voice spoofing attacks. These results should be interpreted with caution, however. In practice, one must consider the relative likelihood of the ASV system being presented with target and impostor trials, versus that of spoofing attacks. Without this consideration, the results in Fig. <ref type="figure" target="#fig_2">2</ref> might convey an overly pessimistic view of the vulnerabilities to spoofing. There is, in any case, potential to detect attacks automatically using countermeasures.</p><p>The series of ASVspoof challenges held bi-annually since 2015 have spearheaded the development of spoofing countermeasures or presentation attack detection (PAD) solutions for ASV. Spoofing countermeasures can be applied prior to ASV in order to detect attacks and prevent them from reaching the ASV system. The most recently completed challenge was held in 2019 and included separate logical access (synthetic and converted voice attacks) and physical access (replay attacks) Miss probability (in %)</p><p>Team 05 (EER=0.22 %) Team 45 (EER=1.86 %) Team 60 (EER=2.64 %) Team 24 (EER=3.45 %) Team 50 (EER=3.56 %)</p><p>Figure <ref type="figure">3</ref>: As for Fig. <ref type="figure" target="#fig_2">2</ref> except for spoofing countermeasures.</p><p>Profiles shown for the top five performing systems.</p><p>tasks <ref type="bibr" target="#b5">[6]</ref>. DET plots for the top-five performing spoofing countermeasures for the ASVspoof 2019 logical access task are shown in Fig. <ref type="figure">3</ref>. They show EERs of as low as 0.2%. Thus, while spoofing attacks can present a substantial threat to reliability, and while human listeners may not be able to detect spoofing attacks, countermeasures can be effective. Even so, while ASV countermeasures have proven potential to detect attacks, they can also degrade usability; they can erroneously classify bona fide speech as spoofed speech. Assessment and performance estimates should hence reflect the impact of both spoofing and countermeasures upon the ASV system; countermeasures should be assessed in tandem with ASV. Such more elaborate approaches to assessment, e.g. <ref type="bibr" target="#b20">[21]</ref>, are outside the scope of the current article.</p></div>
<div><head n="5.">Privacy implications: VoicePrivacy</head><p>Speech signals contain much more than just the spoken message or the voice identity <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23]</ref>. The speaker's sex/gender, age, socio-economic and geographical background, emotion and health condition etc. can all be estimated automatically using recordings of speech <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25]</ref>. With much of this information being personal and private there is hence an interest to develop privacy safeguards for speech technology. This is the goal of the <software ContextAttributes="created">VoicePrivacy</software> initiative, founded in 2020. While solutions to privacy preservation can take many different forms, and while <software ContextAttributes="created">VoicePrivacy</software> may explore different approaches in the future, the inaugural challenge focused upon the development of anonymization solutions <ref type="bibr" target="#b6">[7]</ref>.</p><p>The idea is to protect privacy by distorting speech data such that it cannot be used by ASV systems to recognise the speaker. Anonymisation is achieved by suppressing the information in speech signals that is typically used by machines to infer identity. On its own, this is relatively straightforward, e.g. by adding sufficient levels of background noise, or by replacing speech with silence. The challenge comes from the requirement to suppress personally identifiable attributes contained within the speech signal while leaving all other attributes intact. These requirements imply that anonymisation should not interfere with the application of some down stream tasks such as automatic speech recognition, nor should it introduce processing artefacts that might degrade subjective intelligibility or naturalness. Last, anonymised voices should remain distinctive, meaning the task ASV baseline (EER=1.11% ASV trained on anonymised data, anonymised test (EER=10.69%) Anonymised test (EER=52.12%</p><p>Figure <ref type="figure">4</ref>: Detection error trade-off plot for the <software ContextAttributes="used">VoicePrivacy</software> 2020 challenge (LibriSpeech test, male trials). Profiles shown for the baseline ASV system (blue profile) and the same system presented with anonymised test utterances (orange profile) and anonymised test utterances when the ASV system is re-trained using similarly anonymised training data (red profile).</p><p>might better be referred to as pseudonymisation. Rather than operating upon the speech signal so that it reflects the voice of another, specific speaker, anonymisation aims to prevent the speaker identity from being recognised. Anonymisation acts to increase the confusion between utterances produced by the same and different speakers. For a perfect anonymisation system, the score distributions corresponding to impostor and target trials overlap. The desired effect of anonymisation upon an ASV system is illustrated in Fig. <ref type="figure" target="#fig_0">1(c</ref>). In this case, the ASV system cannot produce simultaneously both a low false alarm rate and a low miss rate, no matter what the decision threshold, and the EER is 50%. Real anonymisation solutions are less effective.</p><p>The first <software ContextAttributes="used">VoicePrivacy</software> challenge was held in 2020 and attracted submissions from 7 independent teams. A DET plot for the primary challenge baseline <ref type="bibr" target="#b6">[7]</ref> is illustrated in Fig. <ref type="figure">4</ref>. It shows an increase in the EER from a baseline of 1% (blue profile) to 52% after anonymisation (orange profile). The EER of over 50% suggests that the anonymisation goal is met. However, if an anonymisation adversary were to adapt the ASV system in light of anonymisation, then performance is less effective; anonymised utterances still contain some personally identifiable information (PII) and the potential to re-identify the speaker remains. When the ASV system is re-trained or adapted using similarly anonymised training data, the result is a lower EER of 10.7% (red profile); true anonymisation remains elusive.</p><p>The above treatment does not reflect impacts upon intelligibility/naturalness, nor voice distinctiveness. In practice, the multiple objectives and complex nature of anonymisation means several different metrics are used in practice. The development of more suitable approaches to assessment are the focus of current research <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29]</ref>.</p></div>
<div><head n="6.">Reflections and further considerations</head><p>The community has made substantial progress to address the security and privacy implications of speech technology and rapid progress has been made in the last half-decade. We have: pro-posed definitions for a number of distinct tasks and solutions; established evaluation driven research initiatives as a vehicle to successfully raise the profile of security and privacy research and to build new research communities; developed protocols, criteria and metrics for the assessment of security and privacy safeguards. Nonetheless, we are perhaps still far from having a comprehensive appreciation of the implications as well as the potential of safeguards.</p><p>Will spoofing ever be a solved problem? Possibly not. Speech synthesis and voice conversion are long-established research fields with genuine applications, such as anonymisation, yet the same technology poses a threat to the security of ASV. The progress speech synthesis and voice conversion in recent years has been impressive. Today's technology produces synthetic speech that humans cannot distinguish from bona fide speech. Whether or not similar advances may one day result in machines that produce synthetic speech that other machines cannot detect is an intriguing question. For the time being it is clear that, if we seek reliable, secure approaches to person authentication using ASV, we must intensify our efforts in antispoofing. Future directions include a focus on more adversarial attacks, the development of countermeasures that function reliably in the wild, e.g. in the face of background noise and other sources of nuisance variation such as bandwidth and channel variability which typify telephony ASV scenarios, as well as the approach used to estimate performance.</p><p>The research effort in anonymisation is relatively embryonic. While there is an opportunity to provide some level of protection, current solutions fall short of delivering true anonymisation. Furthermore, we have observed differences in the level of privacy that a given anonymisation solution provides to different individuals. Whereas the protection for some can be strong, others are left with relatively little protection at all. Since personally identifiable information encapsulates far more than that used by most ASV systems to infer identity, since other alternative attributes can be used instead, and since current anonymisation solutions do not necessarily suppress them, is full anonymisation even technically possible?</p><p>Anonymisation solutions that focus only upon the attributes used by typical ASV systems already degrade intelligibility and naturalness. If all personally identifiable information can be successfully suppressed, then what remains? Is personally identifiable information so inextricably, indelibly embedded in speech that it cannot be (fully) removed? Even if a speech signal can be fully anonymised, will anything resembling speech remain? Or, for a given application, what level of intelligibility/naturalness can be sacrificed in order to achieve anonymisation? How should we even measure anonymisation performance and privacy? Are the methodology and metrics adequate? How does this research fit with broader solutions to privacy preservation, e.g. encryption and distributed learning?</p><p>That we have certainly raised more questions above than we have answered serves to show that our community's journey in security and privacy research is only just beginning. Our efforts must be redoubled looking to the future.</p></div><figure xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Illustrative score distributions for (a) automatic speaker verification (ASV), (b) an ASV system subjected to spoofing attacks and (c) an ASV system presented with anonymised data.</figDesc></figure>
<figure xml:id="fig_1"><head /><label /><figDesc>(EER=2.46 %) worst VC spoofing attack (EER=65.25 %) worst TTS spoofing attack (EER=66.42 %) EER=2.46%</figDesc></figure>
<figure xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Detection error trade-off plot for the ASVspoof 2019 logical access task. Profiles shown for the baseline ASV system (blue profile) and the same system subjected to the most effective synthetic (text-to-speech, TTS) speech spoofing attack (orange profile) and the most effective voice conversion (VC) spoofing attack (red profile).</figDesc></figure>
			<note place="foot" n="1" xml:id="foot_0"><p>We refer to machine perception rather than human perception; humans and machines do not hear in the same way.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="7.">Acknowledgements</head><p>The work reported in this paper was supported by: the <rs type="funder">French ANR (VoicePrivacy, VoicePersonae</rs>, <rs type="funder">DEEP-PRIVACY, Harpocrates)</rs>; the <rs type="funder">European Commission</rs> (<rs type="grantNumber">H2020</rs> <rs type="funder">COMPRISE)</rs>; the <rs type="funder">Japan Science and Technology Agency (JST)</rs> with grant No. <rs type="grantNumber">JPMJCR18A6</rs>; <rs type="funder">JSPS</rs> <rs type="grantNumber">KAKENHI</rs> No.<rs type="grantNumber">21K17775</rs>; the <rs type="funder">Academy of Finland</rs> (proj. <rs type="grantNumber">309629</rs>); <rs type="funder">Region Grand Est, France</rs>.<rs type="funder">French ANR (<software>VoicePrivacy</software>, VoicePersonae</rs>, <rs type="funder">DEEP-PRIVACY, Harpocrates)</rs>; the <rs type="funder">European Commission</rs> (<rs type="grantNumber">H2020</rs> <rs type="funder">COMPRISE)</rs>; the <rs type="funder">Japan Science and Technology Agency (JST)</rs> with grant No. <rs type="grantNumber">JPMJCR18A6</rs>; <rs type="funder">JSPS</rs> <rs type="grantNumber">KAKENHI</rs> No.<rs type="grantNumber">21K17775</rs>; the <rs type="funder">Academy of Finland</rs> (proj. <rs type="grantNumber">309629</rs>); <rs type="funder">Region Grand Est, France</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_vKPwHRH">
					<idno type="grant-number">H2020</idno>
				</org>
				<org type="funding" xml:id="_f6rF3cF">
					<idno type="grant-number">JPMJCR18A6</idno>
				</org>
				<org type="funding" xml:id="_XdHb7pz">
					<idno type="grant-number">KAKENHI</idno>
				</org>
				<org type="funding" xml:id="_C84xXTt">
					<idno type="grant-number">21K17775</idno>
				</org>
				<org type="funding" xml:id="_UU58R3D">
					<idno type="grant-number">309629</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Introduction to voice presentation attack detection and recent advances</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sahidullah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Delgado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Todisco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kinnunen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yamagishi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-A</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Handbook of Biometric Anti-Spoofing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="321" to="361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<idno>ISO/IEC 30107</idno>
		<title level="m">Information Technology -Biometric presentation attack detection</title>
		<meeting><address><addrLine>Geneva, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note>Standard, International Organization for Standardization</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The GDPR &amp; Speech Data: Reflections of Legal and Technology Communities, First Steps Towards a Common Understanding</title>
		<author>
			<persName><forename type="first">A</forename><surname>Nautsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Jasserand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Kindt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Todisco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Trancoso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Evans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech 2019</title>
		<meeting>Interspeech 2019</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3695" to="3699" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">ASVspoof 2015: the first automatic speaker verification spoofing and countermeasures challenge</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kinnunen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yamagishi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hanilc ¸i</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sahidullah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sizov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2037" to="2041" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The ASVspoof 2017 Challenge: Assessing the Limits of Replay Spoofing Attack Detection</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kinnunen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sahidullah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Delgado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Todisco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yamagishi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">ASVspoof 2019: future horizons in spoofed and fake audio detection</title>
		<author>
			<persName><forename type="first">M</forename><surname>Todisco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vestman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sahidullah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Delgado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nautsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yamagishi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">H</forename><surname>Kinnunen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1008" to="1012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Introducing the VoicePrivacy Initiative</title>
		<author>
			<persName><forename type="first">N</forename><surname>Tomashenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M L</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nautsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yamagishi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Patino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-F</forename><surname>Bonastre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-G</forename><surname>Noé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Todisco</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2020-10">oct 2020</date>
			<biblScope unit="page" from="1693" to="1697" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Two decades into speaker recognition evaluation -are we there yet?</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Sadjadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Reynolds</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech &amp; Language</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page">101058</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Two decades of speaker recognition evaluation at the National Institute of Standards and Technology</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Greenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">P</forename><surname>Mason</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech &amp; Language</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page">101032</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Voxceleb: Large-scale speaker verification in the wild</title>
		<author>
			<persName><forename type="first">A</forename><surname>Nagrani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech &amp; Language</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page">101027</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">SdSV Challenge 2020: Large-Scale Evaluation of Short-Duration Speaker Verification</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zeinali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Alam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Burget</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech 2020</title>
		<meeting>Interspeech 2020</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="731" to="735" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">I4U submission to NIST SRE 2018: Leveraging from a decade of shared experiences</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Hautamaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kinnunen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yamamoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1497" to="1501" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">State-of-the-art speaker recognition for telephone and video speech: the JHU-MIT submission for NIST SRE18</title>
		<author>
			<persName><forename type="first">J</forename><surname>Villalba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Snyder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Garcia-Romero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1488" to="1492" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The DET Curve in Assessment of Detection Task Performance</title>
		<author>
			<persName><forename type="first">A</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Doddington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kamm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ordowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Przybocki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eurospeech</title>
		<meeting>Eurospeech</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="1895" to="1898" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<idno>ISO/IEC IS 19795-1</idno>
		<title level="m">Information Technology -Biometric performance testing and reporting -Part 1: Principles and framework</title>
		<meeting><address><addrLine>Geneva, Switzerland</addrLine></address></meeting>
		<imprint>
			<publisher>Standard, International Organization for Standardization</publisher>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Nautsch</surname></persName>
		</author>
		<title level="m">Speaker Recognition in Unconstrained Environments</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
		<respStmt>
			<orgName>Technische Universität Darmstadt</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Natural TTS synthesis by conditioning WaveNet on Mel spectrogram predictions</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4779" to="4783" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Voice Conversion Challenge 2020 -Intra-lingual semi-parallel and cross-lingual voice conversion</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yamagishi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Joint Workshop for the Blizzard Challenge and Voice Conversion Challenge</title>
		<meeting>Joint Workshop for the Blizzard Challenge and Voice Conversion Challenge</meeting>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="page" from="80" to="98" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">I-vectors meet imitators: on vulnerability of speaker verification systems against voice mimicry</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">G</forename><surname>Hautamäki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kinnunen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Hautamäki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Leino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A.-M</forename><surname>Laukkanen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="930" to="934" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Spoofing and countermeasures for speaker verification: A survey</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kinnunen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yamagishi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Alegre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech Communication</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="page" from="130" to="153" />
			<date type="published" when="2015-02">feb 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Tandem Assessment of Spoofing Countermeasures and Automatic Speaker Verification: Fundamentals</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kinnunen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Delgado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="2195" to="2210" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Preserving privacy in speaker and speech characterisation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Nautsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jiménez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Treiber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kolberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech &amp; Language</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page" from="441" to="480" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><surname>Comprise</surname></persName>
		</author>
		<ptr target="https://www.compriseh2020.eu/files/2019/06/d5.1.pdf" />
		<title level="m">Deliverable Nº5.1: Data protection and GDPR requirements</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Voice signatures</title>
		<author>
			<persName><forename type="first">I</forename><surname>Shafran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mohri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ASRU</title>
		<meeting>ASRU</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="31" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Speaker characteristics</title>
		<author>
			<persName><forename type="first">T</forename><surname>Schultz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Speaker classification I</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="47" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Privacy-Preserving Adversarial Representation Learning in ASR: Reality or Illusion?</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M L</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bellet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tommasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Interspeech 2019</title>
		<imprint>
			<date type="published" when="2019-09">sep 2019</date>
			<biblScope unit="page" from="3700" to="3704" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The Privacy ZEBRA: Zero Evidence Biometric Recognition Assessment</title>
		<author>
			<persName><forename type="first">A</forename><surname>Nautsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Patino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Tomashenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yamagishi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-G</forename><surname>Noé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-F</forename><surname>Bonastre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Todisco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Evans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1698" to="1702" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A Comparative Study of Speech Anonymization Metrics</title>
		<author>
			<persName><forename type="first">M</forename><surname>Maouche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M L</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Vauquier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bellet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tommasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1708" to="1712" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Speech Pseudonymisation Assessment Using Voice Similarity Matrices</title>
		<author>
			<persName><forename type="first">P.-G</forename><surname>Noé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-F</forename><surname>Bonastre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Matrouf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Tomashenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nautsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Evans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech 2020</title>
		<meeting>Interspeech 2020</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1718" to="1722" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</teiCorpus></text>
</TEI>