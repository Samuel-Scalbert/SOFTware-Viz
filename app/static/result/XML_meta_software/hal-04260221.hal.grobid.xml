<?xml version='1.0' encoding='utf-8'?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" version="1.1" xsi:schemaLocation="http://www.tei-c.org/ns/1.0 http://api.archives-ouvertes.fr/documents/aofr-sword.xsd">
  <teiHeader>
    <fileDesc>
      <titleStmt>
        <title>HAL TEI export of hal-04260221</title>
      </titleStmt>
      <publicationStmt>
        <distributor>CCSD</distributor>
        <availability status="restricted">
          <licence target="http://creativecommons.org/licenses/by/4.0/">Distributed under a Creative Commons Attribution 4.0 International License</licence>
        </availability>
        <date when="2024-04-29T11:46:56+02:00" />
      </publicationStmt>
      <sourceDesc>
        <p part="N">HAL API platform</p>
      </sourceDesc>
    </fileDesc>
  </teiHeader>
  <text>
    <body>
      <listBibl>
        <biblFull>
          <titleStmt>
            <title xml:lang="en">Argument-based Detection and Classification of Fallacies in Political Debates</title>
            <author role="aut">
              <persName>
                <forename type="first">Pierpaolo</forename>
                <surname>Goffredo</surname>
              </persName>
              <email type="md5">542f193124a15940d78b84e5284e6967</email>
              <email type="domain">i3s.unice.fr</email>
              <idno type="idhal" notation="numeric">1251220</idno>
              <idno type="halauthorid" notation="string">2659038-1251220</idno>
              <affiliation ref="#struct-1039632" />
              <affiliation ref="#struct-13009" />
              <affiliation ref="#struct-1827" />
              <affiliation ref="#struct-178918" />
            </author>
            <author role="aut">
              <persName>
                <forename type="first">Mariana</forename>
                <surname>Chaves</surname>
              </persName>
              <email type="md5">fd7340137f7039f3dc393e8995611ee6</email>
              <email type="domain">gmail.com</email>
              <idno type="idhal" notation="string">mariana-chaves</idno>
              <idno type="idhal" notation="numeric">1326934</idno>
              <idno type="halauthorid" notation="string">1718730-1326934</idno>
              <idno type="ORCID">https://orcid.org/0009-0001-8086-9034</idno>
              <affiliation ref="#struct-1039632" />
              <affiliation ref="#struct-13009" />
              <affiliation ref="#struct-1827" />
              <affiliation ref="#struct-178918" />
            </author>
            <author role="aut">
              <persName>
                <forename type="first">Serena</forename>
                <surname>Villata</surname>
              </persName>
              <email type="md5">344072e78dc9a8131294cd6155a40995</email>
              <email type="domain">i3s.unice.fr</email>
              <idno type="idhal" notation="string">serena-villata</idno>
              <idno type="idhal" notation="numeric">9409</idno>
              <idno type="halauthorid" notation="string">28004-9409</idno>
              <idno type="ORCID">https://orcid.org/0000-0003-3495-493X</idno>
              <idno type="IDREF">https://www.idref.fr/200242911</idno>
              <idno type="VIAF">https://viaf.org/viaf/28150202128003110141</idno>
              <idno type="ISNI">http://isni.org/isni/0000000498435124</idno>
              <idno type="GOOGLE SCHOLAR">https://scholar.google.fr/citations?user=Sy9G0pUAAAAJ</idno>
              <affiliation ref="#struct-1039632" />
              <affiliation ref="#struct-1827" />
              <affiliation ref="#struct-13009" />
              <affiliation ref="#struct-178918" />
            </author>
            <author role="aut">
              <persName>
                <forename type="first">Elena</forename>
                <surname>Cabrio</surname>
              </persName>
              <email type="md5">0ce89e77e57cee1ae7625ba94ab93f06</email>
              <email type="domain">inria.fr</email>
              <idno type="idhal" notation="string">elena-cabrio</idno>
              <idno type="idhal" notation="numeric">948573</idno>
              <idno type="halauthorid" notation="string">779246-948573</idno>
              <idno type="ORCID">https://orcid.org/0000-0001-9374-7872</idno>
              <affiliation ref="#struct-1039632" />
              <affiliation ref="#struct-441569" />
              <affiliation ref="#struct-13009" />
              <affiliation ref="#struct-178918" />
            </author>
            <editor role="depositor">
              <persName>
                <forename>Pierpaolo</forename>
                <surname>Goffredo</surname>
              </persName>
              <email type="md5">ceddf953f341b0470b75c275f714eed4</email>
              <email type="domain">inria.fr</email>
            </editor>
            <funder ref="#projanr-50589" />
            <funder ref="#projanr-57260" />
          </titleStmt>
          <editionStmt>
            <edition n="v1" type="current">
              <date type="whenSubmitted">2023-12-20 10:36:10</date>
              <date type="whenModified">2024-02-26 11:22:07</date>
              <date type="whenReleased">2023-12-20 10:40:08</date>
              <date type="whenProduced">2023-12-06</date>
              <date type="whenEndEmbargoed">2023-12-18</date>
              <ref type="file" target="https://hal.science/hal-04260221/document">
                <date notBefore="2023-12-18" />
              </ref>
              <ref type="file" subtype="publisherAgreement" n="1" target="https://hal.science/hal-04260221/file/2023.emnlp-main.684.pdf">
                <date notBefore="2023-12-18" />
              </ref>
            </edition>
            <respStmt>
              <resp>contributor</resp>
              <name key="1323709">
                <persName>
                  <forename>Pierpaolo</forename>
                  <surname>Goffredo</surname>
                </persName>
                <email type="md5">ceddf953f341b0470b75c275f714eed4</email>
                <email type="domain">inria.fr</email>
              </name>
            </respStmt>
          </editionStmt>
          <publicationStmt>
            <distributor>CCSD</distributor>
            <idno type="halId">hal-04260221</idno>
            <idno type="halUri">https://hal.science/hal-04260221</idno>
            <idno type="halBibtex">goffredo:hal-04260221</idno>
            <idno type="halRefHtml">&lt;i&gt;EMNLP 2023 - Conference on Empirical Methods in Natural Language Processing&lt;/i&gt;, Dec 2023, Singapore (SG), Singapore. pp.11101-11112, &lt;a target="_blank" href="https://dx.doi.org/10.18653/v1/2023.emnlp-main.684"&gt;&amp;#x27E8;10.18653/v1/2023.emnlp-main.684&amp;#x27E9;&lt;/a&gt;</idno>
            <idno type="halRef">EMNLP 2023 - Conference on Empirical Methods in Natural Language Processing, Dec 2023, Singapore (SG), Singapore. pp.11101-11112, &amp;#x27E8;10.18653/v1/2023.emnlp-main.684&amp;#x27E9;</idno>
          </publicationStmt>
          <seriesStmt>
            <idno type="stamp" n="UNICE">Université Nice Sophia Antipolis</idno>
            <idno type="stamp" n="CNRS">CNRS - Centre national de la recherche scientifique</idno>
            <idno type="stamp" n="INRIA">INRIA - Institut National de Recherche en Informatique et en Automatique</idno>
            <idno type="stamp" n="INRIA-SOPHIA">INRIA Sophia Antipolis - Méditerranée</idno>
            <idno type="stamp" n="I3S">Laboratoire d'Informatique, Signaux et Systèmes de Sophia-Antipolis</idno>
            <idno type="stamp" n="INRIASO">INRIA-SOPHIA</idno>
            <idno type="stamp" n="INRIA_TEST">INRIA - Institut National de Recherche en Informatique et en Automatique</idno>
            <idno type="stamp" n="TESTALAIN1">TESTALAIN1</idno>
            <idno type="stamp" n="WIMMICS">WIMMICS: Web-Instrumented Man-Machine Interactions, Communities, and Semantics</idno>
            <idno type="stamp" n="INRIA2">INRIA 2</idno>
            <idno type="stamp" n="UNIV-COTEDAZUR">Université Côte d'Azur</idno>
            <idno type="stamp" n="PNRIA">Programme National de Recherche en IA</idno>
            <idno type="stamp" n="3IA-COTEDAZUR">3IA Côte d’Azur – Interdisciplinary Institute for Artificial Intelligence</idno>
            <idno type="stamp" n="ANR">ANR</idno>
          </seriesStmt>
          <notesStmt>
            <note type="audience" n="2">International</note>
            <note type="invited" n="0">No</note>
            <note type="popular" n="0">No</note>
            <note type="peer" n="1">Yes</note>
            <note type="proceedings" n="1">Yes</note>
          </notesStmt>
          <sourceDesc>
            <biblStruct>
              <analytic>
                <title xml:lang="en">Argument-based Detection and Classification of Fallacies in Political Debates</title>
                <author role="aut">
                  <persName>
                    <forename type="first">Pierpaolo</forename>
                    <surname>Goffredo</surname>
                  </persName>
                  <email type="md5">542f193124a15940d78b84e5284e6967</email>
                  <email type="domain">i3s.unice.fr</email>
                  <idno type="idhal" notation="numeric">1251220</idno>
                  <idno type="halauthorid" notation="string">2659038-1251220</idno>
                  <affiliation ref="#struct-1039632" />
                  <affiliation ref="#struct-13009" />
                  <affiliation ref="#struct-1827" />
                  <affiliation ref="#struct-178918" />
                </author>
                <author role="aut">
                  <persName>
                    <forename type="first">Mariana</forename>
                    <surname>Chaves</surname>
                  </persName>
                  <email type="md5">fd7340137f7039f3dc393e8995611ee6</email>
                  <email type="domain">gmail.com</email>
                  <idno type="idhal" notation="string">mariana-chaves</idno>
                  <idno type="idhal" notation="numeric">1326934</idno>
                  <idno type="halauthorid" notation="string">1718730-1326934</idno>
                  <idno type="ORCID">https://orcid.org/0009-0001-8086-9034</idno>
                  <affiliation ref="#struct-1039632" />
                  <affiliation ref="#struct-13009" />
                  <affiliation ref="#struct-1827" />
                  <affiliation ref="#struct-178918" />
                </author>
                <author role="aut">
                  <persName>
                    <forename type="first">Serena</forename>
                    <surname>Villata</surname>
                  </persName>
                  <email type="md5">344072e78dc9a8131294cd6155a40995</email>
                  <email type="domain">i3s.unice.fr</email>
                  <idno type="idhal" notation="string">serena-villata</idno>
                  <idno type="idhal" notation="numeric">9409</idno>
                  <idno type="halauthorid" notation="string">28004-9409</idno>
                  <idno type="ORCID">https://orcid.org/0000-0003-3495-493X</idno>
                  <idno type="IDREF">https://www.idref.fr/200242911</idno>
                  <idno type="VIAF">https://viaf.org/viaf/28150202128003110141</idno>
                  <idno type="ISNI">http://isni.org/isni/0000000498435124</idno>
                  <idno type="GOOGLE SCHOLAR">https://scholar.google.fr/citations?user=Sy9G0pUAAAAJ</idno>
                  <affiliation ref="#struct-1039632" />
                  <affiliation ref="#struct-1827" />
                  <affiliation ref="#struct-13009" />
                  <affiliation ref="#struct-178918" />
                </author>
                <author role="aut">
                  <persName>
                    <forename type="first">Elena</forename>
                    <surname>Cabrio</surname>
                  </persName>
                  <email type="md5">0ce89e77e57cee1ae7625ba94ab93f06</email>
                  <email type="domain">inria.fr</email>
                  <idno type="idhal" notation="string">elena-cabrio</idno>
                  <idno type="idhal" notation="numeric">948573</idno>
                  <idno type="halauthorid" notation="string">779246-948573</idno>
                  <idno type="ORCID">https://orcid.org/0000-0001-9374-7872</idno>
                  <affiliation ref="#struct-1039632" />
                  <affiliation ref="#struct-441569" />
                  <affiliation ref="#struct-13009" />
                  <affiliation ref="#struct-178918" />
                </author>
              </analytic>
              <monogr>
                <title level="m">ACL Anthology</title>
                <meeting>
                  <title>EMNLP 2023 - Conference on Empirical Methods in Natural Language Processing</title>
                  <date type="start">2023-12-06</date>
                  <date type="end">2023-12-10</date>
                  <settlement>Singapore (SG)</settlement>
                  <country key="SG">Singapore</country>
                </meeting>
                <imprint>
                  <publisher>Association for Computational Linguistics</publisher>
                  <biblScope unit="serie">Findings of the Association for Computational Linguistics: EMNLP 2023</biblScope>
                  <biblScope unit="volume">2023.findings-emnlp.684</biblScope>
                  <biblScope unit="pp">11101–11112</biblScope>
                  <date type="datePub">2023-12</date>
                </imprint>
              </monogr>
              <idno type="doi">10.18653/v1/2023.emnlp-main.684</idno>
              <ref type="publisher">https://aclanthology.org/volumes/2023.findings-emnlp/</ref>
            </biblStruct>
          </sourceDesc>
          <profileDesc>
            <langUsage>
              <language ident="en">English</language>
            </langUsage>
            <textClass>
              <classCode scheme="halDomain" n="info.info-ai">Computer Science [cs]/Artificial Intelligence [cs.AI]</classCode>
              <classCode scheme="halTypology" n="COMM">Conference papers</classCode>
              <classCode scheme="halOldTypology" n="COMM">Conference papers</classCode>
              <classCode scheme="halTreeTypology" n="COMM">Conference papers</classCode>
            </textClass>
            <abstract xml:lang="en">
              <p>Fallacies are arguments that employ faulty reasoning. Given their persuasive and seemingly valid nature, fallacious arguments are often used in political debates. Employing these misleading arguments in politics can have detrimental consequences for society, since they can lead to inaccurate conclusions and invalid inferences from the public opinion and the policymakers. Automatically detecting and classifying fallacious arguments represents therefore a crucial challenge to limit the spread of misleading or manipulative claims and promote a more informed and healthier political discourse. Our contribution to address this challenging task is twofold. First, we extend the ElecDeb60To16 dataset of U.S. presidential debates annotated with fallacious arguments, by incorporating the most recent Trump-Biden presidential debate. We include updated tokenlevel annotations, incorporating argumentative components (i.e., claims and premises), the relations between these components (i.e., support and attack), and six categories of fallacious arguments (i.e., Ad Hominem, Appeal to Authority, Appeal to Emotion, False Cause, Slippery Slope, and Slogans). Second, we perform the twofold task of fallacious argument detection and classification by defining neural network architectures based on Transformers models, combining text, argumentative features, and engineered features. Our results show the advantages of complementing transformer-generated text representations with non-textual features.</p>
            </abstract>
          </profileDesc>
        </biblFull>
      </listBibl>
    </body>
    <back>
      <listOrg type="structures">
        <org type="regroupinstitution" xml:id="struct-1039632" status="VALID">
          <idno type="IdRef">241035694</idno>
          <idno type="ROR">https://ror.org/019tgvf94</idno>
          <orgName>Université Côte d'Azur</orgName>
          <orgName type="acronym">UniCA</orgName>
          <date type="start">2020-01-01</date>
          <desc>
            <address>
              <addrLine>Parc Valrose, 28, avenue Valrose 06108 Nice Cedex 2</addrLine>
              <country key="FR" />
            </address>
            <ref type="url">https://univ-cotedazur.fr</ref>
          </desc>
        </org>
        <org type="laboratory" xml:id="struct-13009" status="VALID">
          <orgName>Laboratoire d'Informatique, Signaux, et Systèmes de Sophia Antipolis</orgName>
          <orgName type="acronym">I3S</orgName>
          <desc>
            <address>
              <addrLine>2000, route des Lucioles - Les Algorithmes - bât. Euclide B 06900 Sophia Antipolis</addrLine>
              <country key="FR" />
            </address>
            <ref type="url">http://www.i3s.unice.fr/</ref>
          </desc>
          <listRelation>
            <relation active="#struct-117617" type="direct" />
            <relation name="UMR7271" active="#struct-441569" type="direct" />
            <relation active="#struct-1039632" type="direct" />
          </listRelation>
        </org>
        <org type="laboratory" xml:id="struct-1827" status="VALID">
          <orgName>CNRS-formation Entreprise</orgName>
          <orgName type="acronym">CFE</orgName>
          <desc>
            <address>
              <addrLine>bat. 31 Av de la terrasse 91198 GIF SUR YVETTE CEDEX</addrLine>
              <country key="FR" />
            </address>
            <ref type="url">http://www.cnrs-gif.fr/cnrsformation/</ref>
          </desc>
          <listRelation>
            <relation name="UPS1564" active="#struct-441569" type="direct" />
          </listRelation>
        </org>
        <org type="researchteam" xml:id="struct-178918" status="VALID">
          <idno type="RNSR">201221031M</idno>
          <orgName>Web-Instrumented Man-Machine Interactions, Communities and Semantics</orgName>
          <orgName type="acronym">WIMMICS</orgName>
          <desc>
            <address>
              <country key="FR" />
            </address>
            <ref type="url">http://wimmics.inria.fr/</ref>
          </desc>
          <listRelation>
            <relation active="#struct-34586" type="direct" />
            <relation active="#struct-300009" type="indirect" />
            <relation active="#struct-452156" type="direct" />
            <relation active="#struct-13009" type="indirect" />
            <relation active="#struct-117617" type="indirect" />
            <relation name="UMR7271" active="#struct-441569" type="indirect" />
            <relation active="#struct-1039632" type="indirect" />
          </listRelation>
        </org>
        <org type="regroupinstitution" xml:id="struct-441569" status="VALID">
          <idno type="IdRef">02636817X</idno>
          <idno type="ISNI">0000000122597504</idno>
          <idno type="ROR">https://ror.org/02feahw73</idno>
          <orgName>Centre National de la Recherche Scientifique</orgName>
          <orgName type="acronym">CNRS</orgName>
          <date type="start">1939-10-19</date>
          <desc>
            <address>
              <country key="FR" />
            </address>
            <ref type="url">https://www.cnrs.fr/</ref>
          </desc>
        </org>
        <org type="institution" xml:id="struct-117617" status="VALID">
          <idno type="IdRef">026403498</idno>
          <idno type="ISNI">0000000123372892</idno>
          <idno type="ROR">https://ror.org/02k9vew78</idno>
          <orgName>Université Nice Sophia Antipolis (1965 - 2019)</orgName>
          <orgName type="acronym">UNS</orgName>
          <date type="start">1965-10-23</date>
          <date type="end">2019-12-31</date>
          <desc>
            <address>
              <addrLine>Parc Valrose, 06100 Nice</addrLine>
              <country key="FR" />
            </address>
            <ref type="url">http://unice.fr/</ref>
          </desc>
        </org>
        <org type="laboratory" xml:id="struct-34586" status="VALID">
          <idno type="RNSR">198318250R</idno>
          <idno type="ROR">https://ror.org/01nzkaw91</idno>
          <orgName>Inria Sophia Antipolis - Méditerranée</orgName>
          <orgName type="acronym">CRISAM</orgName>
          <desc>
            <address>
              <addrLine>2004 route des Lucioles BP 93 06902 Sophia Antipolis</addrLine>
              <country key="FR" />
            </address>
            <ref type="url">http://www.inria.fr/centre/sophia/</ref>
          </desc>
          <listRelation>
            <relation active="#struct-300009" type="direct" />
          </listRelation>
        </org>
        <org type="institution" xml:id="struct-300009" status="VALID">
          <idno type="ROR">https://ror.org/02kvxyf05</idno>
          <orgName>Institut National de Recherche en Informatique et en Automatique</orgName>
          <orgName type="acronym">Inria</orgName>
          <desc>
            <address>
              <addrLine>Domaine de VoluceauRocquencourt - BP 10578153 Le Chesnay Cedex</addrLine>
              <country key="FR" />
            </address>
            <ref type="url">http://www.inria.fr/en/</ref>
          </desc>
        </org>
        <org type="department" xml:id="struct-452156" status="VALID">
          <orgName>Scalable and Pervasive softwARe and Knowledge Systems</orgName>
          <orgName type="acronym">Laboratoire I3S - SPARKS</orgName>
          <date type="start">2016-03-03</date>
          <desc>
            <address>
              <addrLine>Laboratoire I3SCS 4012106903 Sophia Antipolis Cedex</addrLine>
              <country key="FR" />
            </address>
            <ref type="url">http://www.i3s.unice.fr/sparks</ref>
          </desc>
          <listRelation>
            <relation active="#struct-13009" type="direct" />
            <relation active="#struct-117617" type="indirect" />
            <relation name="UMR7271" active="#struct-441569" type="indirect" />
            <relation active="#struct-1039632" type="indirect" />
          </listRelation>
        </org>
      </listOrg>
      <listOrg type="projects">
        <org type="anrProject" xml:id="projanr-50589" status="VALID">
          <idno type="anr">ANR-19-P3IA-0002</idno>
          <orgName>3IA@cote d'azur</orgName>
          <desc>3IA Côte d'Azur</desc>
          <date type="start">2019</date>
        </org>
        <org type="anrProject" xml:id="projanr-57260" status="VALID">
          <idno type="anr">ANR-21-CE23-0037</idno>
          <orgName>ATTENTION</orgName>
          <desc>Generation de contre-arguments pour lutter contre la désinformation en ligne</desc>
          <date type="start">2021</date>
        </org>
      </listOrg>
    </back>
  <teiCorpus>
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Argument-based Detection and Classification of Fallacies in Political Debates</title>
				<funder>
					<orgName type="full">3IA Côte d'Azur Investments</orgName>
				</funder>
				<funder ref="#_UhV2cKV">
					<orgName type="full">EU Horizon</orgName>
				</funder>
				<funder ref="#_6zvyP6d #_KkKXGtE">
					<orgName type="full">Agence Nationale de la Recherche</orgName>
					<orgName type="abbreviated">ANR</orgName>
				</funder>
				<funder>
					<orgName type="full">French government</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher />
				<availability status="unknown"><licence /></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Pierpaolo</forename><surname>Goffredo</surname></persName>
							<email>goffredo@i3s.unice.fr</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Université Côte d'Azur</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<orgName type="institution" key="instit3">Inria</orgName>
								<address>
									<region>I3S</region>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mariana</forename><surname>Chaves Espinoza</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Université Côte d'Azur</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<orgName type="institution" key="instit3">Inria</orgName>
								<address>
									<region>I3S</region>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Elena</forename><surname>Cabrio</surname></persName>
							<email>elena.cabrio@univ-cotedazur.fr</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Université Côte d'Azur</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<orgName type="institution" key="instit3">Inria</orgName>
								<address>
									<region>I3S</region>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Serena</forename><surname>Villata</surname></persName>
							<email>serena.villata@univ-cotedazur.fr</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Université Côte d'Azur</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<orgName type="institution" key="instit3">Inria</orgName>
								<address>
									<region>I3S</region>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Argument-based Detection and Classification of Fallacies in Political Debates</title>
					</analytic>
					<monogr>
						<imprint>
							<date />
						</imprint>
					</monogr>
					<idno type="MD5">1302D02CB3426EE7A4961848D2F34CAF</idno>
					<idno type="DOI">10.18653/v1/2023.emnlp-main.684</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-04-12T14:51+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid" />
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div><p>Fallacies are arguments that employ faulty reasoning. Given their persuasive and seemingly valid nature, fallacious arguments are often used in political debates. Employing these misleading arguments in politics can have detrimental consequences for society, since they can lead to inaccurate conclusions and invalid inferences from the public opinion and the policymakers. Automatically detecting and classifying fallacious arguments represents therefore a crucial challenge to limit the spread of misleading or manipulative claims and promote a more informed and healthier political discourse. Our contribution to address this challenging task is twofold. First, we extend the ElecDeb60To16 dataset of U.S. presidential debates annotated with fallacious arguments, by incorporating the most recent Trump-Biden presidential debate. We include updated tokenlevel annotations, incorporating argumentative components (i.e., claims and premises), the relations between these components (i.e., support and attack), and six categories of fallacious arguments (i.e., Ad Hominem, Appeal to Authority, Appeal to Emotion, False Cause, Slippery Slope, and Slogans). Second, we perform the twofold task of fallacious argument detection and classification by defining neural network architectures based on Transformers models, combining text, argumentative features, and engineered features. Our results show the advantages of complementing transformer-generated text representations with non-textual features.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div><head n="1">Introduction</head><p>Fallacious arguments have been firstly defined as defective inferences, i.e., logically invalid types of arguments <ref type="bibr" target="#b6">(Eemeren, 2001)</ref>. More recently, in a more pragmatic perspective, fallacious arguments have been defined as infringements of performance rules characteristic of a particular ideal type of argumentative engagement <ref type="bibr" target="#b7">(Eemeren and Grootendorst, 1987)</ref> and as illicit dialectical shifts across different dialogue types, highlighting that the attempted move is inappropriate with respect to its pragmatic application context <ref type="bibr" target="#b22">(Walton, 1995)</ref>. Despite their employment in many scenarios (e.g., online discussion platforms and blogs, TV roundtables), a natural testbed of this form of misleading argumentation is political debate. For instance, the ad hominem fallacy, where the plausibility of the argument depends on the credentials, personal background and past actions of the speaker, is probably one of the fallacy labels that is most often thrown around in political debate. This kind of arguments may sound convincing and has the goal to mislead the audience, persuading it about the validity of the argument. Given the potential nefarious impact of these misleading arguments on the society, identifying and classifying fallacious arguments is therefore a main open challenge in Argument Mining (AM) <ref type="bibr" target="#b1">(Cabrio and Villata, 2018;</ref><ref type="bibr" target="#b15">Lawrence and Reed, 2019;</ref><ref type="bibr" target="#b14">Lauscher et al., 2022)</ref>, and in NLP in general.</p><p>Existing approaches in the literature <ref type="bibr" target="#b9">(Habernal et al., 2017</ref><ref type="bibr" target="#b10">(Habernal et al., , 2018;;</ref><ref type="bibr" target="#b8">Goffredo et al., 2022;</ref><ref type="bibr" target="#b18">Vijayaraghavan and Vosoughi, 2022;</ref><ref type="bibr" target="#b0">Alhindi et al., 2022;</ref><ref type="bibr" target="#b20">Vorakitphan et al., 2022;</ref><ref type="bibr" target="#b17">Sahai et al., 2021)</ref> mainly concentrated on the classification of fallacious text snippets over a finite set of labels, leaving the challenging issue of identifying the fallacious text snippet and its boundaries under-investigated. In this paper, we tackle this open research question on a dataset of political debates from the U.S. presidential campaigns from 1960 to 2020.</p><p>More precisely, the contribution of this paper is twofold. First, we extend an existing resource of U.S. political debates from the presidential campaigns  annotated both with argument components and relations, and six fallacy categories (namely, Ad Hominem, Appeal to Authority, Appeal to Emotion, False Cause, Slippery Slope and Slogans). This new resource, named the ElecDeb60to20 dataset, includes now also the debates of the 2020 presidential campaign (Trump-Biden) with all the related annotations. Second, we propose a new approach, based on Transformers, to detect the fallacious text snippets in these debates, and then classify them along the six fallacy categories. This approach encodes the argument components (i.e., Premise, Claim), the argument relations (i.e., Support, Attack) and the PoS tags to successfully identify and classify fallacious arguments. Experimental results show that the proposed approach outperforms standard baselines and concurrent approaches with an average f1-score of 0.74, with our proposed model named MultiFusion BERT, on the task of fallacious argument detection and classification.</p><p>Whilst most of the computational approaches targeting fallacious argumentation focus on the pure classification of such nefarious content <ref type="bibr" target="#b9">(Habernal et al., 2017</ref><ref type="bibr" target="#b10">(Habernal et al., , 2018;;</ref><ref type="bibr" target="#b12">Jin et al., 2022;</ref><ref type="bibr" target="#b8">Goffredo et al., 2022;</ref><ref type="bibr" target="#b18">Vijayaraghavan and Vosoughi, 2022;</ref><ref type="bibr" target="#b0">Alhindi et al., 2022)</ref>, the originality of our contribution is that it proposes, to the best of our knowledge, the first neural architecture to both detect fallacious arguments and classify them in political debates, and one of the very few approaches to tackle this task in general, outperforming competing approaches <ref type="bibr" target="#b20">(Vorakitphan et al., 2022;</ref><ref type="bibr" target="#b17">Sahai et al., 2021)</ref>.</p><p>The urgency to study fallacies in political discourse is crucial both from the philosophical and the political perspective. It emphasizes the need to scrutinize political arguments for sound reasoning rather than deception. Understanding logical flaws is a main issue for informed decision-making, enabling the recognition and assessment of fallacious arguments in political discourse. Moreover, examining how fallacies are employed in political debates reveals their strategic role in influencing opinion and diverting attention. This strategic use of fallacies mirrors the subtleties of language, emphasizing the interplay between rhetoric, philosophy, and political communication. Furthermore, it offers valuable insights into the dynamics of public discourse and the need for critical analysis <ref type="bibr" target="#b21">(Walton, 1987</ref><ref type="bibr" target="#b22">(Walton, , 1995))</ref>.</p></div>
<div><head n="2">Related Work</head><p>Over the years, there has been a growing interest in the field of NLP to the detection of fallacies and related phenomena, including misinformation and propaganda <ref type="bibr">(Da San Martino et al., 2020b)</ref>. The pioneering work <ref type="bibr">of Da San Martino et al. (2019b)</ref> on fallacies in newspaper news has been a signif-icant source of inspiration in this area. Recently, researchers have made significant progress in identifying and classifying fallacies within discourse. In this section, we discuss the approaches proposed in the literature on the two tasks of fallacy classification (Section 2.1) and fallacy detection (Section 2.2).</p></div>
<div><head n="2.1">Fallacy Classification</head><p>For instance, <ref type="bibr" target="#b9">Habernal et al. (2017)</ref> aimed to improve fallacy detection by creating a publicly available software called "<software ContextAttributes="used">Argotario</software>". It serves as an educational gaming platform and a means to gather data from the crowd for annotating fallacy types in everyday arguments. In a subsequent study, <ref type="bibr" target="#b10">Habernal et al., 2018</ref> released annotated datasets of fallacious arguments in English and German. They conducted experiments using Support Vector Machine and BiLSTM models with German word vectors to classify six fallacious topic types (accuracy=50.9%, macro-F1 score=42.1%). <ref type="bibr" target="#b12">Jin et al. (2022)</ref> proposes an architecture based on a simple classifier that incorporates the structural information of fallacies. They augment a standard pre-trained language model for classification with a template that includes modified assumptions and premises based on a well-defined masking scheme. They achieve an F1 score of 58.77% on the classification task over 13 classes. They used a dataset collected from online teaching materials, specifically designed to teach and test students' understanding of logical fallacies.</p><p>In a more contextual approach, <ref type="bibr" target="#b8">Goffredo et al. (2022)</ref> use a dataset from U.S. Presidential Election Debates From 1960 to 2016. This dataset incorporates information such as the context and argumentative features (i.e., argument components and relations) for each fallacy. The proposed architecture uses one classifier for each feature in order to calculate the loss. On a sentence classification task encompassing six fallacy categories, their approach achieves an F1 score of 84%.</p><p>To address the identification and classification of fine-grained propaganda tweets, <ref type="bibr" target="#b18">Vijayaraghavan and Vosoughi (2022)</ref> proposed an end-to-end transformers-based approach that considers additional features like context, relational information, and external knowledge through data augmentation. Their dataset consists of approximately 211K tweets annotated with 19 classes (18 propaganda types and 1 non-propaganda). The approach yielded a 64% F1 score on a text classification task.</p><p>Alhindi et al. ( <ref type="formula">2022</ref>) introduced an instructionbased prompt in a multitask configuration using the T5 model to classify fallacies. This methodology involved leveraging multiple fallacy-based datasets, including Propaganda <ref type="bibr">(Da San Martino et al., 2019b)</ref>, Logic <ref type="bibr" target="#b12">(Jin et al., 2022)</ref>, <software ContextAttributes="used">Argotario</software> <ref type="bibr" target="#b9">(Habernal et al., 2017)</ref>, Covid-19 <ref type="bibr" target="#b16">(Musi et al., 2022)</ref>, and Climate <ref type="bibr" target="#b12">(Jin et al., 2022)</ref>. Their approach enabled the identification of 28 distinct fallacies across various domains and genres, facilitating the analysis of model size and prompt selection, and investigating the impact of annotation quality on model performance, potentially supplemented with external knowledge. The results obtained using their T5-large model yielded F1 scores of 41% for Propaganda, 62% for Logic, 59% for <software ContextAttributes="used">Argotario</software>, 26% for Covid-19, and 17% for Climate, respectively.</p></div>
<div><head n="2.2">Fallacy Detection and Classification Task</head><p>Vorakitphan et al. (2022) proposes a system capable of automatically identifying propaganda messages and classifying them based on the propaganda techniques employed. The system adopts a pipeline approach that firstly detects the text snippet containing potential propaganda, and then performs classification by leveraging semantic and argumentative features. Two standard benchmarks, NLP4IF'19 <ref type="bibr">(Da San Martino et al., 2019a)</ref> and Se-mEval'20 datasets <ref type="bibr">(Da San Martino et al., 2020a)</ref>, were used for this propaganda detection and classification task. The binary classification step, utilizing BERT, achieved a 72% F1 score, while the sentence-span multi-class classification task (14 classes) achieved a 64% micro F1 score using a RoBERTa-based architecture.</p><p>Fallacy detection and token-level classification tasks were also performed in <ref type="bibr" target="#b17">Sahai et al. (2021)</ref>. They narrowed their study to 8 fallacy types and created a corpus of fallacious arguments by annotating user comments on Reddit. They performed fallacy classification at comment-level and tokenlevel, relying on BERT and MGN <ref type="bibr">(Da San Martino et al., 2019a)</ref> models. The inclusion of the conversation context, represented by the parent comment or submission title, was used to enhance the predictions. Fine-tuned BERT with a classification head of a linear layer reported the best results on the token classification task (macro F1=53%).</p><p>Our present work expands the scope of these fallacy studies in several ways. Firstly, we employ a corpus specifically created for fallacy detection in political debates, which has been under investigated in this context even though fallacious argumentation is a main issue in political discourse. Secondly, while the majority of existing studies focus on fallacy classification, in this paper we focus on the fallacy detection task, which is of main importance in real-life scenarios where text or speech lacks pre-segmentation and a clear binary classification into fallacies or non-fallacies. Lastly, our study capitalizes on the annotations of argumentation features in our dataset, an element that is often absent in other corpora, allowing us to go beyond pure text-based approaches, and to rely on the argumentation structure.</p></div>
<div><head n="3">ElecDeb60to20 Dataset</head><p>To effectively address the task of detecting and classifying fallacious arguments within political debates, we decided to rely on the ElecDeb60To16 dataset <ref type="bibr" target="#b11">(Haddadan et al., 2019;</ref><ref type="bibr" target="#b8">Goffredo et al., 2022)</ref>. It comprises televised debates from U.S. presidential election campaigns spanning from 1960 to 2016. These debates were sourced from the website of the Commission on Presidential Debates 1 , which openly provides transcripts of debates broadcasted on television and featuring the prominent candidates for presidential and vicepresidential nominations in the United States. All information on this website is accessible to the public. Considering the most recent presidential election between Trump and Biden occurred in 2020, we expanded the dataset with the transcripts of the debates of this election campaign to include updated annotations, incorporating argumentative components such as Claims and Premises, as well as the relations between these components, i.e., Support and Attack. As a result of this annotation update, the dataset is renamed as ElecDeb60to20 2 , reflecting the coverage of debates spanning from 1960 to 2020. This updated dataset provides a more comprehensive and contemporary collection of fallacies in political debates, enhancing the relevance and applicability of the data for further analysis and research in the field. This resource is a valuable benchmark for investigating potential connections between specific argument components and relations that underlie the occurrence of fallacious arguments.</p></div>
<div><head n="3.1">Annotated Fallacies</head><p>During the annotation process of fallacies within the U.S. political debates of the 2020 presidential election, we rely on the six categories based on the annotation scheme proposed by <ref type="bibr">Da San Martino et al. (2019a)</ref>, the categorization outlined by <ref type="bibr" target="#b21">Walton (1987)</ref>, and the annotation of the previous debates in the first version of the dataset with these fallacy categories <ref type="bibr" target="#b8">(Goffredo et al., 2022)</ref>. Hence, we adopted the following categories: Ad Hominem, Appeal to Authority, Appeal to Emotion, False Cause, Slippery Slope, and Slogans. Below, we provide a concise description of each of these six categories.</p><p>Ad Hominem. When the argument becomes an excessive attack on an arguer's position <ref type="bibr" target="#b21">(Walton, 1987)</ref>.</p><p>Appeal to Emotion. The unessential loading of the argument with emotional language to exploit the audience emotional instinct.</p><p>Appeal to Authority. It occurs when the arguer relies on the endorsement of an authority figure or a group consensus without providing sufficient evidence. It may also involve the citation of nonexperts or the majority to support their claim.</p><p>Slippery Slope. This fallacy implies that an improbable or exaggerated consequence could result from a particular action.</p><p>False Cause. The misinterpretation of the correlation of two events for causation <ref type="bibr" target="#b21">(Walton, 1987)</ref>.</p><p>Slogan. It is a brief and striking phrase used to provoke excitement of the audience, and is often accompanied by another type of fallacy called argument by repetition.</p></div>
<div><head n="3.2">Annotation Phase</head><p>The updated annotations were conducted following the annotation scheme introduced in Haddadan et al. (2019); <ref type="bibr" target="#b8">Goffredo et al. (2022)</ref>. Following this approach, each debate was divided into sections, starting with either a moderator/panelist or an audience member asking a question on a new topic. To facilitate the annotation process, the semantic annotation platform INCEpTION <ref type="bibr" target="#b13">(Klie et al., 2018)</ref> was used.</p><p>Two annotators, with expertise in computational linguistics, independently annotated the new portion of the dataset (Trump vs. Biden debates) by identifying argumentative components, relations, and fallacies. To maintain objectivity and prevent bias, the annotation process for argumentative components was performed on raw data, without any pre-existing fallacy annotations. This approach was adopted to ensure that the annotation process remains unbiased and free from any preconceived notions related to fallacies. A set of 50 sentences randomly extracted from the debates was annotated to assess Inter-Annotator Agreement (IAA), and the results, visualized in Table <ref type="table" target="#tab_0">1</ref>, indicate a substantial level of agreement between the annotators.</p></div>
<div><head>Measure</head><p>Value Observed Agreement 0.857 Krippendorff's α 0.757 </p></div>
<div><head n="3.3">Statistics and Data Analysis</head><p>Table <ref type="table" target="#tab_1">2</ref> summarizes the Trump vs. Biden's debates annotations per category and argumentative features. We tokenized the annotated fallacious arguments to compute the average number of words in each category. In line with the guidelines of <ref type="bibr" target="#b8">Goffredo et al. (2022)</ref>, Slogans is the shortest with 5.0 tokens on average<ref type="foot" target="#foot_2">3</ref> , whereas SlipperySlope was the longest with 20.5 tokens on average. The train and test set split was performed considering the entire new dataset ElecDeb60to20. The training set accounts for 90% of the dataset, while the remaining 10% constitutes the test set. The distribution of fallacy labels is as follows: Appeal-toEmotion (59.94%), AppealtoAuthority (15.20%), AdHominem (13.58%), FalseCause (46.93%), Slipperyslope (3.97%), and Slogans (2.63%). In the last debate, the most used fallacies are AppealtoEmotion and AdHominem, confirming the trend of the previous debates. Behind this strategy, there are many references to the COVID-19 pandemic and some personal issues of the two candidates exploited during the debates. Despite being distinct and unrelated, these two topics held significant importance and consistently fueled intense debates.</p></div>
<div><head n="4">Fallacy Detection</head><p>We cast the fallacy detection task as an information extraction problem, where the goal is to identify and classify in the debates the textual snippets corresponding to the six categories of fallacies annotated in the context of a political debate (see Section 3.1 for the list of fallacies and their description).</p><p>We rely on the BIO/IOB data format, and specific tags are assigned to annotate the fallacies, i.e., B-AdHominem, I-AdHominem, B-AppealtoAuthority, I-AppealtoAuthority, B-AppealtoEmotion, I-AppealtoEmotion, B-FalseCause, I-FalseCause, B-Slipperyslope, I-Slipperyslope, B-Slogans, I-Slogans, O.</p><p>The fallacy detection and classification tasks consist therefore in assigning one of these thirteen predefined labels to each token.</p><p>To have a richer representation of fallacy annotations, we build a contextual framework that includes the sentence containing the fallacy, as well as the preceding and following sentences. When the fallacious sentence is the first or last in the dialogue, the preceding or following sentence is excluded.</p></div>
<div><head n="4.1">Method</head><p>To address the above-mentioned tasks, we employ transformer-based architectures in both their basic configuration and in a specialized configuration designed for token classification<ref type="foot" target="#foot_3">4</ref> , drawing inspiration from previous studies on fallacy detection and classification <ref type="bibr">(Da San Martino et al., 2020a;</ref><ref type="bibr" target="#b20">Vorakitphan et al., 2022;</ref><ref type="bibr" target="#b8">Goffredo et al., 2022)</ref> that have provided empirical evidence of the advantages of complementing transformer-generated text representations with non-textual features. Moreover, we enhance the specialized architecture by including additional argumentative features.</p></div>
<div><head n="4.1.1">Baselines</head><p>BERT + (Bi)LSTM(s) The simplest models consist of a pre-trained BERT model followed by either (i) an LSTM layer and a dense layer, or (ii) a BiLSTM layer with 0.2 dropout, an LSTM layer, and a dense layer. The weights of the transformer are kept frozen during training. The text serves as input for the transformer, and we extract the last hidden states (i.e., the embedded representation of each token). This output is then passed on to the subsequent layers. In the case where argumentative features are included in the model, we concatenate the last hidden states of the transformer with the one-hot-encoded representation of the argument components and relationships. This concatenated feature representation is then fed into the next RNN-based layers. All models used Adam optimizer with default <software>PyTorch</software> parameters.</p><p>BertForTokenClassification is a transformerbased model relying on a bidirectional approach to capture contextual information from surrounding words.</p><p>We tested two checkpoints:</p><p>bert base uncased and bert-large-cased-finetuned-conll03-engl.</p><p>DebertaForTokenClassification is based on a modified transformer architecture with improvements like "de-coupled attention" and "crosslayer parameter sharing" for enhanced language modeling capabilities. The checkpoint used is microsoft/deberta-base.</p><p><software>ElectraForTokenClassification</software> relies on a novel pre-training method called "discriminative pre-training," where a generator and a discriminator are trained to enhance the quality of the learned representations. The checkpoint used is: bhadresh-savani/ electra-base-discriminator-finetuned-conll03-english.</p><p>DistilbertForTokenClassification is a distilled version of BERT that retains much of its performance while significantly reducing the model size and computational resources required for training and inference. The checkpoint used are: distilbert-base-cased and distilbert-base-uncased. the ElecDeb60to20 dataset reported in Table <ref type="table">3</ref> 5 , we select the BertFTC model (bert-large-cased-finetuned-conll03-eng.) to be included in our proposed architecture. Despite the good performances of BertFTC, we propose to enhance its capabilities to detect fallacious text by integrating and "fusing" additional features, namely argumentative components (Claim, Premise), argumentative relations (Support, Attack), and Part-of-Speech (PoS) tags.</p></div>
<div><head n="4.1.2">MultiFusion BERT</head></div>
<div><head>Relying on the results obtained by the baselines on the fallacy detection task on</head></div>
<div><head>Model</head><p>Argumentative features were included to improve the model's understanding of an argument underlying structure, enabling it to detect when its logical structure is compromised. Since fallacies involve faulty reasoning, we hypothesized that providing this information to the model would be relevant. Results in Table <ref type="table">4</ref> provide empirical evidence to support this hypothesis. In particular, it has been shown that including argumentation features in- 5 The reported results represent the average performance based on the macro average F1 score.</p><p>creases the model performances in the context of fallacy classification <ref type="bibr" target="#b8">(Goffredo et al., 2022)</ref>. Such features can be extracted by specific annotations associated with each fallacy in the ElecDeb60to20 dataset: the type of argumentative component in which the fallacy may be present, and the argumentative relations among these different components.</p><p>The integration of PoS information is driven by the observation that certain fallacies exhibit distinctive language patterns that can be more easily discerned using PoS tagging. For example, in the LoadedLanguage fallacy (a subcategory of the AppealToEmotion category), the intensity of a sentence is often increased by using emotionally loaded phrases, expressed through the use of a sentiment lexicon, particularly concerning adjectives and adverbs. Similarly, in the AdHominem fallacy, where the focus shifts from attacking the argument to targeting the character, motives, or personal qualities of the political opponent, the reference to this opponent is expressed using a noun or pronoun and subsequently employs adjectives with negative connotations.</p><p>Figure <ref type="figure" target="#fig_0">1</ref> illustrates the proposed model, called MultiFusion BERT, for the detection and classification of fallacies in political debates. Multi-Fusion BERT computes logits (L) for each feature by employing a specialized TokenForClassification Transformer model adapted to the number of labels: 3 for components and relations, and 17 for part-of-speech tags. The architectures for argumentative features for components and relations share the same parameters, enabling us to obtain logits for both components and relations. An additional model, based on the number of PoS tags (i.e., 17), is used to obtain logits for PoS features. Consequently, distinct losses are computed for each model: fallacy loss (loss f al ), component loss (loss cmp ), relation loss (loss rel ), and part-of-speech loss (loss pos ). These individual losses are combined by multiplying them with an arbitrary α value of 0.1, yielding a unified average loss referred to as the joint loss <ref type="bibr" target="#b19">(Vorakitphan et al., 2021)</ref>. In our study, we opted for empirically investigating the optimal alpha value that yielded superior performance, as evidenced by our experiments (see Appendix D for the exhaustive evaluation). The back-propagation function incorporates all losses in the following way:</p><formula xml:id="formula_0">joint loss = α * (loss f al +losscmp+loss rel +loss P oS ) N loss</formula><p>, where N loss denotes the number of losses considered by the model. We conducted an exploration of various values for the α parameter.</p></div>
<div><head n="4.2">Experimental Setup</head><p>All models have been fine-tuned using the ElecDeb60to20 dataset. The implementation was based on <software ContextAttributes="used">HuggingFace 6</software> version 4.30. and on <software ContextAttributes="used">Py-Torch</software> 1.7.0. All models utilize the Adam optimizer, with a gradient clipping set to 10, a dropout of 0.1, a learning rate of 4e-05 and a training batch size of 8 and a test batch size of 4. The training process consists of 4 epochs, during which the models are fine-tuned and optimized. The dataset was split with 90% for training and 10% for testing, ensuring balanced distribution on the fallacy labels using stratification. The partitioning was performed using the train_and_test_split function from the 6 https://huggingface.co/docs/transformers/ index scikit-learn<ref type="foot" target="#foot_4">7</ref> library. The random seed was set to 42. The PoS tags were obtained using the spaCy<software>spaCy<ref type="foot" target="#foot_5">8</ref> library</software>. The maximum tensor size is set to 256, ensuring that all necessary text information is included without truncation. The representation of this encoding is showed in the Appendix A.</p><p>The proposed neural architecture contains 328 million parameters<ref type="foot" target="#foot_6">9</ref> . This large parameter count enables the model to capture intricate patterns and dependencies within the data, enhancing its capacity for complex information processing and generating more accurate predictions. We evaluated the "MultiFusion" approach on the baseline model that showed the best results. Despite having approximately half of the trainable parameters of BERT (e.g., 65M for DistilBERT vs. 109M for BERT), BERT outperformed DistilBERT. Consequently, we adopted BERT as the architecture for implementing the approach described above. We utilized the Nvidia Quadro RTX 8000 GPU (32 GB) for our experiments. The average runtime was of 21 minutes for training and testing all the configurations of our models.</p></div>
<div><head n="5">Evaluation</head><p>Table <ref type="table">3</ref> presents the results of the tested models for fallacies detection in the political debates. Results are calculated using the macro average F1 metric, considering the following fallacy labels: (i) Ad Hominem, (ii) Appeal to Authority, (iii) Appeal to Emotion, (iv) False Cause, (v) Slippery Slope, (vi) Slogans, and (vii) Other (B and I labels are merged). Despite the relatively smaller size of the dataset and the task complexity, the results obtained from the different models are promising. As introduced before, among the baselines, BERT "dbmdz/ bert-large-cased-finetuned-conll03-english" achieved the best performance. Thus, MultiFusion BERT, incorporating argumentative features (components and relations) as well as PoS tags, significantly outperformed the other models (the performance increase with respect to BertFTC is of 2.12%).</p></div>
<div><head n="5.1">Ablation Tests</head><p>To better analyze the impact of the different features incorporated in our architecture, we carried out ablation tests. Table <ref type="table">4</ref> presents the results obtained by MultiFusion BERT using all possible combinations of (i) argumentative components, (ii) argumentative relations, and (iii) context PoS tags. Incorporating argumentative components, relations, and PoS features individually or in pairs resulted in a decline in performance compared to the best baseline results (i.e., BertFTC "dbmdz/ bert-large-cased-finetuned-conll03-eng.), with an average degradation of 4.35% across the different configurations (excluding the one considering all three features). In contrast, when all three features are included (as described in Section 4.1.2) a significant improvement in model performance is observed, highlighting the importance of considering all of them together for fallacy detection.  Table <ref type="table" target="#tab_4">5</ref> provides an in-depth analysis of Multi-Fusion BERT's performances on the test set, considering the different target labels 10 . Notably, the identification of tokens labeled as Slogans exhibits the poorest results, despite being relatively easier to recognize for humans. This can be due to the limited presence of examples/tokens in both the training and the test set 11 . In addition, these results point out that recognizing slogans within political debates involves factors beyond syntactic and argumentative features (mostly semantics and pragmatics). On the contrary, tokens labeled as "Slippery Slope" and "False Cause" (with 332 and 321 examples, respectively) are much better classified 10 A detailed analysis of the performances with BIO tokens is provided in Appendix B 11 A detailed table with the count of each token can be found in Appendix C Table <ref type="table" target="#tab_5">6</ref> shows a few misclassified fallacy snippets. In the first example, the argument is misclassified as Appeal to Emotion instead of Appel to Authority, because the model is misled by the word "fear," which carries an important emotional connotation. In the next example, the argument is erroneously classified as being an Appeal to Authority argument whilst it is not a fallacious argument (O).</p></div>
<div><head n="5.2">Error Analysis</head><p>The third example shows another instance where the model confuses Appeal to Authority with Appeal to Emotion, while also failing to identify part of the fallacy in general. The third and the fourth examples show where the model partially identifies the correct fallacy or its absence.</p></div>
<div><head n="6">Conclusion</head><p>Existing argumentation schemes <ref type="bibr" target="#b22">(Walton, 1995)</ref> to identify flawed and invalid forms of reasoning often fall short when applied to fallacious arguments employed in real-world contexts like political debates. To tackle this challenge, the contribution of this paper is twofold. First, we extended the ElecDeb60to16 dataset by incorporating the Trump vs. Biden 2020 presidential debate along with argumentative annotations and fallacies. Second, we proposed and evaluated MultiFusion BERT, a transformer-based architecture that combines the debate text, the argumentative features (i.e., components and relations), and engineered features to perform the fallacy detection and classification task.</p><p>Our results highlight the main role of argumentative features in the correct identification and classification of fallacious arguments. This approach yields an average performance improvement of 2.12% compared to baseline methods and competing approaches.</p><p>As future research, we intend to delve deeper into fallacious argumentation by integrating knowledge in order to address more challenging fallacy categories like causal ones, where reasoning and knowledge-based features are required to identify the fallacy. Our further objective is to generate valid arguments from identified fallacious ones and their context. Additionally, a challenge we aim to tackle is to explore ways to counter the formal invalidity of fallacious arguments through the generation of new arguments.</p></div>
<div><head n="7">Limitations</head><p>Some limitations of this work require a discussion. Firstly, the used training corpus is focused on US political debates, which restricts the applicability of the model to English-language contexts only. Furthermore, the imbalanced distribution of labels had a noticeable impact on the model's performance and its ability to generalize during prediction. For instance, the label "Slogan" was significantly underrepresented compared to other labels, further affecting the model's performance. Finally, it is important to consider that the GPU requirements, specifically the need for Nvidia RTX 8000 with 32GB VRAM, may present limitations on the practical utilization of these models in resource-constrained environments. These limitations highlight the need for further research to address the dataset limitations with respect to the employed language and the label balance, to improve the model architecture, and explore additional strategies to enhance fallacy detection through knowledge injection.</p></div>
<div><head>A Encoding</head><p>A representation of the dataset's encoded example is illustrated in the Figure <ref type="figure" target="#fig_2">3</ref>, demonstrating how it is prepared for input into the architecture for token label prediction. Notably, the argument features align with the offset mapping approach employed by the tokenizer during tokenization.</p><p>Each argumentative feature is represented by a tensor of length 256 (the maximum token length of 216 is used) filled with label IDs (0: None, 1: Claim/Support, 2: Premise/Attack) up to the maximum length. The same process is applied to the Part-of-Speech tensor, where each tag is converted into its corresponding ID (0: ADJ, 1: ADP, 2: ADV, 3: AUX, 4: CCONJ, 5: DET, 6: INTJ, 7: NOUN, 8: NUM, 9: PART, 10: PRON, 11: PROPN, 12: PUNCT, 13: SCONJ, 14: SYM, 15: VERB, 16: X).</p></div>
<div><head>B Detailed Model Performance</head><p>In this appendix section, a comprehensive classification report of the best-performing model is presented, considering all BIO labels and all the three features (argumentative components, argumentative relations and PoS tags). This report provides a thorough assessment of the model performance, offering valuable insights into its accuracy and effectiveness in recognizing and classifying different categories to every token.    where N loss denotes the number of losses considered by the model. We conducted an exploration of various values for the α parameter. Table <ref type="table" target="#tab_9">9</ref> shows that the impact of the parameter varies depending on the features used in the model. That is, none of the values of α yields a generalized improvement in the macro F1 score across all feature combinations.</p></div>
<div><head>C Token Distribution</head></div><figure xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: MultiFusion BERT with joint loss approach.</figDesc><graphic coords="8,70.86,70.85,218.28,157.65" type="bitmap" /></figure>
<figure xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Normalized confusion matrix MultiFusion BERT. BIO labels are merged. Normalization is performed using the number of true elements in each class.</figDesc><graphic coords="9,306.14,70.84,218.28,174.62" type="bitmap" /></figure>
<figure xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Encoded example of a single item of the dataset ElecDeb60to20.</figDesc><graphic coords="12,306.14,70.85,218.27,321.97" type="bitmap" /></figure>
<figure type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>IAA agreement over 50 sentences randomly extracted from the debates Trump-Biden.</figDesc><table /></figure>
<figure type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Distribution of annotated fallacies per category and argumentative features of Trump vs. Biden's debates.</figDesc><table><row><cell>Category</cell><cell cols="4">Freq AvgTok Arg. Feature Freq</cell></row><row><cell>Ad Hominem</cell><cell>62</cell><cell>4,6</cell><cell>Claims</cell><cell>1513</cell></row><row><cell>AppealtoAuthority</cell><cell>17</cell><cell>18,6</cell><cell>Premise</cell><cell>332</cell></row><row><cell>AppealtoEmotion</cell><cell>147</cell><cell>6,81</cell><cell>Support Rel.</cell><cell>400</cell></row><row><cell>FalseCause</cell><cell>0</cell><cell>0</cell><cell>Attack Rel.</cell><cell>112</cell></row><row><cell>SlipperySlope</cell><cell>4</cell><cell>20,5</cell><cell /><cell /></row><row><cell>Slogans</cell><cell>2</cell><cell>5</cell><cell /><cell /></row><row><cell>Total</cell><cell>232</cell><cell>9,25</cell><cell>Total</cell><cell>2357</cell></row></table></figure>
<figure type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Classification report of Fallacy Detection and Classification with B and I labels merged.</figDesc><table /></figure>
<figure type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Examples of misclassification using the bestperforming model. Underlined text is to highlight the true label for each token, whereas Bold is for the predicted fallacy and Italic for predicted O tokens.</figDesc><table><row><cell>Fallacy snippet Franklin Roosevelt said in 1932 that the only thing we have to fear is fear itself. As the President said the other night, there will always be trou-bles in this ol' world, but the United States of America can be counted on to provide the vision that the world looks for from the United States of America. But as Admiral Yarnell has said, and he's been supported by most military authority, these islands that we're now talking about are not worth the bones of a single American soldier; and I know how difficult it is to sustain troops close to the shore under artillery bombard-ment. In a place like Chicago, where thousands of people have been killed, thousands over the last law and order. been killed. We have to bring back most 4,000 people in Chicago have Obama became president, overall al-4,000 have been killed since Barack number of years, in fact, almost</cell><cell>True fallacy Appeal to Authority O Appeal to Authority False Cause False Pred. fallacy Appeal to Emotion Appeal to Authority and O Appeal to Emotion and O Cause and O</cell></row></table></figure>
<figure type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>Classification report of fallacy entity classification with BIO labels.</figDesc><table /></figure>
<figure type="table" xml:id="tab_8"><head>Table 8 :</head><label>8</label><figDesc>Table 8 shows the distribution of the BIO tokens among the training and test set. Distribution of BIO fallacy tags among the train and test set.</figDesc><table><row><cell cols="2">Label</cell><cell>Train</cell><cell>Test</cell><cell>Total</cell></row><row><cell cols="2">B-AdHominem</cell><cell>243</cell><cell>27</cell><cell>270</cell></row><row><cell cols="2">B-AppealtoAuthority</cell><cell>272</cell><cell>30</cell><cell>302</cell></row><row><cell cols="2">B-AppealtoEmotion</cell><cell>1'073</cell><cell>120</cell><cell>1'193</cell></row><row><cell cols="2">B-FalseCause</cell><cell>84</cell><cell>9</cell><cell>93</cell></row><row><cell cols="2">B-Slipperyslope</cell><cell>71</cell><cell>8</cell><cell>79</cell></row><row><cell cols="2">B-Slogans</cell><cell>47</cell><cell>5</cell><cell>52</cell></row><row><cell cols="2">I-AdHominem</cell><cell>4'855</cell><cell>712</cell><cell>5'567</cell></row><row><cell cols="2">I-AppealtoAuthority</cell><cell>8'707</cell><cell>1'019</cell><cell>9'726</cell></row><row><cell cols="2">I-AppealtoEmotion</cell><cell>17'408</cell><cell>2'104</cell><cell>19'512</cell></row><row><cell cols="2">I-FalseCause</cell><cell>3'076</cell><cell>312</cell><cell>3'388</cell></row><row><cell cols="2">I-Slipperyslope</cell><cell>2'487</cell><cell>324</cell><cell>2'811</cell></row><row><cell cols="2">I-Slogans</cell><cell>254</cell><cell>44</cell><cell>298</cell></row><row><cell>O</cell><cell /><cell>74'493</cell><cell>7'914</cell><cell>82407</cell></row><row><cell cols="2">Total</cell><cell cols="3">113'070 12'628 125'698</cell></row><row><cell>α</cell><cell cols="3">Features Components Relationships PoS</cell><cell>Avg macro F1 Score</cell></row><row><cell /><cell>✓</cell><cell /><cell /><cell>0.6922</cell></row><row><cell /><cell /><cell>✓</cell><cell /><cell>0.6922</cell></row><row><cell /><cell /><cell /><cell>✓</cell><cell>0.7212</cell></row><row><cell>0.1</cell><cell>✓</cell><cell>✓</cell><cell /><cell>0.7278</cell></row><row><cell /><cell>✓</cell><cell /><cell>✓</cell><cell>0.7166</cell></row><row><cell /><cell /><cell>✓</cell><cell>✓</cell><cell>0.7166</cell></row><row><cell /><cell>✓</cell><cell>✓</cell><cell>✓</cell><cell>0.7394</cell></row><row><cell /><cell>✓</cell><cell /><cell /><cell>0.7054</cell></row><row><cell /><cell /><cell>✓</cell><cell /><cell>0.7054</cell></row><row><cell>0.3</cell><cell>✓</cell><cell>✓</cell><cell>✓</cell><cell>0.7214 0.6889</cell></row><row><cell /><cell>✓</cell><cell /><cell>✓</cell><cell>0.7160</cell></row><row><cell /><cell /><cell>✓</cell><cell>✓</cell><cell>0.7160</cell></row><row><cell /><cell>✓</cell><cell>✓</cell><cell>✓</cell><cell>0.7084</cell></row><row><cell /><cell>✓</cell><cell /><cell /><cell>0.7057</cell></row><row><cell /><cell /><cell>✓</cell><cell /><cell>0.7057</cell></row><row><cell /><cell /><cell /><cell>✓</cell><cell>0.6817</cell></row><row><cell>0.5</cell><cell>✓ ✓</cell><cell>✓</cell><cell>✓</cell><cell>0.7366 0.7054</cell></row><row><cell /><cell /><cell>✓</cell><cell>✓</cell><cell>0.7054</cell></row><row><cell /><cell>✓</cell><cell>✓</cell><cell>✓</cell><cell>0.7070</cell></row></table><note><p><p>D Analysis of Different α Values</p>MultiFusion BERT's loss function is defined as joint loss = α * (loss f al +losscmp+loss rel +loss P oS ) N loss ,</p></note></figure>
<figure type="table" xml:id="tab_9"><head>Table 9 :</head><label>9</label><figDesc>MultiFusion BERT's average macro F1 scores for fallacy detection using different features and values of the α parameter. The scores are based on an average of 3 runs. B and I labels were merged.</figDesc><table /></figure>
			<note place="foot" n="1" xml:id="foot_0"><p>https://www.debates.org/voter-education/ debate-transcripts/</p></note>
			<note place="foot" n="2" xml:id="foot_1"><p>https://github.com/pierpaologoffredo/ FallacyDetection</p></note>
			<note place="foot" n="3" xml:id="foot_2"><p>Slogan is by definition a short and striking phrase.</p></note>
			<note place="foot" n="4" xml:id="foot_3"><p>https://huggingface.co/docs/transformers/ tasks/token_classification</p></note>
			<note place="foot" n="7" xml:id="foot_4"><p>https://scikit-learn.org/stable/modules/ generated/sklearn.model_selection.train_test_ split.html</p></note>
			<note place="foot" n="8" xml:id="foot_5"><p>https://spacy.io</p></note>
			<note place="foot" n="9" xml:id="foot_6"><p>The standard BertFTC model is around three times smaller, approximately</p></note>
			<note place="foot" n="109" xml:id="foot_7"><p>million parameters.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="8">Acknowledgments</head><p>This work was partly supported by the <rs type="funder">French government</rs>, through the <rs type="funder">3IA Côte d'Azur Investments</rs> in the Future project managed by the <rs type="funder">National Research Agency (ANR)</rs> with the reference number <rs type="grantNumber">ANR-19-P3IA-0002</rs>. This work was partly supported also by <rs type="funder">EU Horizon</rs> <rs type="programName">2020</rs> project <rs type="projectName">AI4Media</rs>, under contract no. <rs type="grantNumber">951911</rs> (https://ai4media.eu/). This work has been partially supported by the <rs type="funder">ANR</rs> project <rs type="projectName">ATTENTION</rs> (<rs type="grantNumber">ANR-21-CE23-0037</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_6zvyP6d">
					<idno type="grant-number">ANR-19-P3IA-0002</idno>
				</org>
				<org type="funded-project" xml:id="_UhV2cKV">
					<idno type="grant-number">951911</idno>
					<orgName type="project" subtype="full">AI4Media</orgName>
					<orgName type="program" subtype="full">2020</orgName>
				</org>
				<org type="funded-project" xml:id="_KkKXGtE">
					<idno type="grant-number">ANR-21-CE23-0037</idno>
					<orgName type="project" subtype="full">ATTENTION</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Multitask instructionbased prompting for fallacy recognition</title>
		<author>
			<persName><forename type="first">Tuhin</forename><surname>Tariq Alhindi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elena</forename><surname>Chakrabarty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Smaranda</forename><surname>Musi</surname></persName>
		</author>
		<author>
			<persName><surname>Muresan</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.emnlp-main.560</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2022 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Abu Dhabi, United Arab Emirates</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="8172" to="8187" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Five years of argument mining: a data-driven analysis</title>
		<author>
			<persName><forename type="first">Elena</forename><surname>Cabrio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serena</forename><surname>Villata</surname></persName>
		</author>
		<idno type="DOI">10.24963/ijcai.2018/766</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence, IJCAI 2018</title>
		<meeting>the Twenty-Seventh International Joint Conference on Artificial Intelligence, IJCAI 2018<address><addrLine>Stockholm, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-07-13">2018. July 13-19, 2018</date>
			<biblScope unit="page" from="5427" to="5433" />
		</imprint>
	</monogr>
	<note>ijcai.org</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Findings of the NLP4IF-2019 shared task on fine-grained propaganda detection</title>
		<author>
			<persName><forename type="first">Giovanni</forename><surname>Da</surname></persName>
		</author>
		<author>
			<persName><forename type="first">San</forename><surname>Martino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alberto</forename><surname>Barrón-Cedeño</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-5024</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second Workshop on Natural Language Processing for Internet Freedom: Censorship, Disinformation, and Propaganda</title>
		<meeting>the Second Workshop on Natural Language Processing for Internet Freedom: Censorship, Disinformation, and Propaganda<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="162" to="170" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">SemEval-2020 task 11: Detection of propaganda techniques in news articles</title>
		<author>
			<persName><forename type="first">Giovanni</forename><surname>Da</surname></persName>
		</author>
		<author>
			<persName><forename type="first">San</forename><surname>Martino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alberto</forename><surname>Barrón-Cedeño</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henning</forename><surname>Wachsmuth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rostislav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.semeval-1.186</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourteenth Workshop on Semantic Evaluation</title>
		<meeting>the Fourteenth Workshop on Semantic Evaluation<address><addrLine>Barcelona (online</addrLine></address></meeting>
		<imprint>
			<publisher>International Committee for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1377" to="1414" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A survey on computational propaganda detection</title>
		<author>
			<persName><forename type="first">Giovanni</forename><surname>Da</surname></persName>
		</author>
		<author>
			<persName><forename type="first">San</forename><surname>Martino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Cresci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alberto</forename><surname>Barrón-Cedeño</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seunghak</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roberto</forename><surname>Di</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
		</author>
		<idno type="DOI">10.24963/ijcai.2020/672</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence, IJCAI-20</title>
		<meeting>the Twenty-Ninth International Joint Conference on Artificial Intelligence, IJCAI-20</meeting>
		<imprint>
			<publisher>International Joint Conferences on Artificial Intelligence Organization</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4826" to="4832" />
		</imprint>
	</monogr>
	<note>Survey track</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Fine-grained analysis of propaganda in news article</title>
		<author>
			<persName><forename type="first">Giovanni</forename><surname>Da</surname></persName>
		</author>
		<author>
			<persName><forename type="first">San</forename><surname>Martino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seunghak</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alberto</forename><surname>Barrón-Cedeno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rostislav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 conference on empirical methods in natural language processing and the 9th international joint conference on natural language processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 conference on empirical methods in natural language processing and the 9th international joint conference on natural language processing (EMNLP-IJCNLP)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5636" to="5646" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Fallacies. Critical concepts in argumentation theory</title>
		<author>
			<persName><forename type="first">H</forename><surname>Frans</surname></persName>
		</author>
		<author>
			<persName><surname>Van Eemeren</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="135" to="164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Fallacies in pragma-dialectical perspective</title>
		<author>
			<persName><forename type="first">H</forename><surname>Frans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rob</forename><surname>Van Eemeren</surname></persName>
		</author>
		<author>
			<persName><surname>Grootendorst</surname></persName>
		</author>
		<idno type="DOI">10.1007/bf00136779</idno>
	</analytic>
	<monogr>
		<title level="j">Argumentation</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="283" to="301" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Fallacious argument classification in political debates</title>
		<author>
			<persName><forename type="first">Pierpaolo</forename><surname>Goffredo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shohreh</forename><surname>Haddadan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vorakit</forename><surname>Vorakitphan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elena</forename><surname>Cabrio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serena</forename><surname>Villata</surname></persName>
		</author>
		<idno type="DOI">10.24963/ijcai.2022/575</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence, IJCAI-22</title>
		<meeting>the Thirty-First International Joint Conference on Artificial Intelligence, IJCAI-22</meeting>
		<imprint>
			<publisher>Main Track</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="4143" to="4149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Argotario: Computational argumentation meets serious games</title>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Habernal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raffael</forename><surname>Hannemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Pollak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Klamm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Pauli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D17-2002</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing: System Demonstrations<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="7" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Adapting Serious Game for Fallacious Argumentation to German: Pitfalls, Insights, and Best Practices</title>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Habernal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Pauli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018)</title>
		<meeting>the Eleventh International Conference on Language Resources and Evaluation (LREC 2018)<address><addrLine>Miyazaki, Japan</addrLine></address></meeting>
		<imprint>
			<publisher>European Language Resources Association (ELRA</publisher>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Yes, we can! mining arguments in 50 years of US presidential campaign debates</title>
		<author>
			<persName><forename type="first">Shohreh</forename><surname>Haddadan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elena</forename><surname>Cabrio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serena</forename><surname>Villata</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1463</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4684" to="4690" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Logical fallacy detection</title>
		<author>
			<persName><forename type="first">Zhijing</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhinav</forename><surname>Lalwani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tejas</forename><surname>Vaidhya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyu</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiwen</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiheng</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mrinmaya</forename><surname>Sachan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.13758</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The INCEpTION platform: Machine-assisted and knowledge-oriented interactive annotation</title>
		<author>
			<persName><forename type="first">Jan-Christoph</forename><surname>Klie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Bugert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Beto</forename><surname>Boullosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Eckart De Castilho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Computational Linguistics: System Demonstrations</title>
		<meeting>the 27th International Conference on Computational Linguistics: System Demonstrations<address><addrLine>Santa Fe, New Mexico</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Scientia potentia est -on the role of knowledge in computational argumentation</title>
		<author>
			<persName><forename type="first">Anne</forename><surname>Lauscher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henning</forename><surname>Wachsmuth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Goran</forename><surname>Glavas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. Assoc. Comput. Linguistics</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1392" to="1422" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Argument mining: A survey</title>
		<author>
			<persName><forename type="first">John</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Reed</surname></persName>
		</author>
		<idno type="DOI">10.1162/coli_a_00364</idno>
	</analytic>
	<monogr>
		<title level="j">Comput. Linguistics</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="765" to="818" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Developing fake news immunity: fallacies as misinformation triggers during the pandemic</title>
		<author>
			<persName><forename type="first">Elena</forename><surname>Musi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myrto</forename><surname>Aloumpi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elinor</forename><surname>Carmi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simeon</forename><surname>Yates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kay O'</forename><surname>Halloran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Online Journal of Communication and Media Technologies</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Breaking down the invisible wall of informal fallacies in online discussions</title>
		<author>
			<persName><forename type="first">Saumya</forename><surname>Sahai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oana</forename><surname>Balalau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roxana</forename><surname>Horincar</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.53</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="644" to="657" />
		</imprint>
	</monogr>
	<note>Long Papers</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">TWEETSPIN: Fine-grained propaganda detection in social media using multi-view representations</title>
		<author>
			<persName><forename type="first">Prashanth</forename><surname>Vijayaraghavan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soroush</forename><surname>Vosoughi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.naacl-main.251</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Seattle, United States</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="3433" to="3448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Don't discuss": Investigating Semantic and Argumentative Features for Supervised Propagandist Message Detection and Classification</title>
		<author>
			<persName><forename type="first">Vorakit</forename><surname>Vorakitphan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elena</forename><surname>Cabrio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serena</forename><surname>Villata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">RANLP 2021 -Recent Advances in Natural Language Processing</title>
		<meeting><address><addrLine>Varna / Virtual, Bulgaria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Protect: A pipeline for propaganda detection and classification</title>
		<author>
			<persName><forename type="first">Vorakit</forename><surname>Vorakitphan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elena</forename><surname>Cabrio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serena</forename><surname>Villata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CLiC-it 2021-Italian Conference on Computational Linguistics</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Informal Fallacies: Towards a Theory of Argument Criticisms</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">N</forename><surname>Walton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pragmatics &amp; beyond companion series</title>
		<imprint>
			<publisher>J. Benjamins Publishing Company</publisher>
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A Pragmatic Theory of Fallacy</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">N</forename><surname>Walton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Studies in rhetoric and communication</title>
		<imprint>
			<publisher>University of Alabama Press</publisher>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</teiCorpus></text>
</TEI>