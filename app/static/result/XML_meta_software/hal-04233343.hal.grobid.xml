<?xml version='1.0' encoding='utf-8'?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" version="1.1" xsi:schemaLocation="http://www.tei-c.org/ns/1.0 http://api.archives-ouvertes.fr/documents/aofr-sword.xsd">
  <teiHeader>
    <fileDesc>
      <titleStmt>
        <title>HAL TEI export of hal-04233343</title>
      </titleStmt>
      <publicationStmt>
        <distributor>CCSD</distributor>
        <availability status="restricted">
          <licence target="http://creativecommons.org/licenses/by/">Attribution</licence>
        </availability>
        <date when="2024-04-29T11:47:02+02:00" />
      </publicationStmt>
      <sourceDesc>
        <p part="N">HAL API platform</p>
      </sourceDesc>
    </fileDesc>
  </teiHeader>
  <text>
    <body>
      <listBibl>
        <biblFull>
          <titleStmt>
            <title xml:lang="en">WAM-Studio: A Web-Based Digital Audio Workstation to Empower Cochlear Implant Users</title>
            <author role="aut">
              <persName>
                <forename type="first">Michel</forename>
                <surname>Buffa</surname>
              </persName>
              <email type="md5">b2aa6d56b7e1f56528400513799687fa</email>
              <email type="domain">unice.fr</email>
              <idno type="idhal" notation="string">michel-buffa</idno>
              <idno type="idhal" notation="numeric">739332</idno>
              <idno type="halauthorid" notation="string">35134-739332</idno>
              <idno type="ORCID">https://orcid.org/0000-0003-1900-0871</idno>
              <affiliation ref="#struct-178918" />
              <affiliation ref="#struct-13009" />
              <affiliation ref="#struct-1039632" />
            </author>
            <author role="aut">
              <persName>
                <forename type="first">Antoine</forename>
                <surname>Vidal-Mazuy</surname>
              </persName>
              <idno type="halauthorid">2916313-0</idno>
              <affiliation ref="#struct-1039632" />
            </author>
            <author role="aut">
              <persName>
                <forename type="first">Lloyd</forename>
                <surname>May</surname>
              </persName>
              <idno type="idhal" notation="numeric">1291738</idno>
              <idno type="halauthorid" notation="string">2916314-1291738</idno>
              <idno type="ORCID">https://orcid.org/0000-0003-4692-8261</idno>
              <affiliation ref="#struct-73500" />
            </author>
            <author role="aut">
              <persName>
                <forename type="first">Marco</forename>
                <surname>Winckler</surname>
              </persName>
              <email type="md5">001862a56370d3dfc171ddd66c5cf79e</email>
              <email type="domain">unice.fr</email>
              <idno type="idhal" notation="string">marco-winckler</idno>
              <idno type="idhal" notation="numeric">21769</idno>
              <idno type="halauthorid" notation="string">40679-21769</idno>
              <idno type="ORCID">https://orcid.org/0000-0002-0756-6934</idno>
              <idno type="IDREF">https://www.idref.fr/081569319</idno>
              <affiliation ref="#struct-452156" />
              <affiliation ref="#struct-13009" />
              <affiliation ref="#struct-178918" />
              <affiliation ref="#struct-1039632" />
            </author>
            <editor role="depositor">
              <persName>
                <forename>Marco</forename>
                <surname>Winckler</surname>
              </persName>
              <email type="md5">001862a56370d3dfc171ddd66c5cf79e</email>
              <email type="domain">unice.fr</email>
            </editor>
          </titleStmt>
          <editionStmt>
            <edition n="v1" type="current">
              <date type="whenSubmitted">2023-10-09 14:18:18</date>
              <date type="whenModified">2024-02-26 11:22:07</date>
              <date type="whenReleased">2023-10-09 16:23:42</date>
              <date type="whenProduced">2023-08-28</date>
              <date type="whenEndEmbargoed">2023-10-09</date>
              <ref type="file" target="https://inria.hal.science/hal-04233343/document">
                <date notBefore="2023-10-09" />
              </ref>
              <ref type="file" subtype="author" n="1" target="https://inria.hal.science/hal-04233343/file/camera%20ready.pdf">
                <date notBefore="2023-10-09" />
              </ref>
            </edition>
            <respStmt>
              <resp>contributor</resp>
              <name key="661600">
                <persName>
                  <forename>Marco</forename>
                  <surname>Winckler</surname>
                </persName>
                <email type="md5">001862a56370d3dfc171ddd66c5cf79e</email>
                <email type="domain">unice.fr</email>
              </name>
            </respStmt>
          </editionStmt>
          <publicationStmt>
            <distributor>CCSD</distributor>
            <idno type="halId">hal-04233343</idno>
            <idno type="halUri">https://inria.hal.science/hal-04233343</idno>
            <idno type="halBibtex">buffa:hal-04233343</idno>
            <idno type="halRefHtml">&lt;i&gt;INTERACT 2023 - 19th IFIP TC13 International Conference&lt;/i&gt;, IFIP TC13, University of York, UK, Aug 2023, York, United Kingdom. pp.101-110, &lt;a target="_blank" href="https://dx.doi.org/10.1007/978-3-031-42280-5_6"&gt;&amp;#x27E8;10.1007/978-3-031-42280-5_6&amp;#x27E9;&lt;/a&gt;</idno>
            <idno type="halRef">INTERACT 2023 - 19th IFIP TC13 International Conference, IFIP TC13, University of York, UK, Aug 2023, York, United Kingdom. pp.101-110, &amp;#x27E8;10.1007/978-3-031-42280-5_6&amp;#x27E9;</idno>
            <availability status="restricted">
              <licence target="http://creativecommons.org/licenses/by/">Attribution</licence>
            </availability>
          </publicationStmt>
          <seriesStmt>
            <idno type="stamp" n="UNICE">Université Nice Sophia Antipolis</idno>
            <idno type="stamp" n="CNRS">CNRS - Centre national de la recherche scientifique</idno>
            <idno type="stamp" n="INRIA">INRIA - Institut National de Recherche en Informatique et en Automatique</idno>
            <idno type="stamp" n="INRIA-SOPHIA">INRIA Sophia Antipolis - Méditerranée</idno>
            <idno type="stamp" n="I3S">Laboratoire d'Informatique, Signaux et Systèmes de Sophia-Antipolis</idno>
            <idno type="stamp" n="INRIASO">INRIA-SOPHIA</idno>
            <idno type="stamp" n="INRIA_TEST">INRIA - Institut National de Recherche en Informatique et en Automatique</idno>
            <idno type="stamp" n="TESTALAIN1">TESTALAIN1</idno>
            <idno type="stamp" n="WIMMICS">WIMMICS: Web-Instrumented Man-Machine Interactions, Communities, and Semantics</idno>
            <idno type="stamp" n="INRIA2">INRIA 2</idno>
            <idno type="stamp" n="UNIV-COTEDAZUR">Université Côte d'Azur</idno>
            <idno type="stamp" n="INRIA_WEB">Inria &amp; web</idno>
            <idno type="stamp" n="INRIA-ETATSUNIS">Copublications Inria-Etats-Unis</idno>
          </seriesStmt>
          <notesStmt>
            <note type="audience" n="2">International</note>
            <note type="invited" n="0">No</note>
            <note type="popular" n="0">No</note>
            <note type="peer" n="1">Yes</note>
            <note type="proceedings" n="1">Yes</note>
          </notesStmt>
          <sourceDesc>
            <biblStruct>
              <analytic>
                <title xml:lang="en">WAM-Studio: A Web-Based Digital Audio Workstation to Empower Cochlear Implant Users</title>
                <author role="aut">
                  <persName>
                    <forename type="first">Michel</forename>
                    <surname>Buffa</surname>
                  </persName>
                  <email type="md5">b2aa6d56b7e1f56528400513799687fa</email>
                  <email type="domain">unice.fr</email>
                  <idno type="idhal" notation="string">michel-buffa</idno>
                  <idno type="idhal" notation="numeric">739332</idno>
                  <idno type="halauthorid" notation="string">35134-739332</idno>
                  <idno type="ORCID">https://orcid.org/0000-0003-1900-0871</idno>
                  <affiliation ref="#struct-178918" />
                  <affiliation ref="#struct-13009" />
                  <affiliation ref="#struct-1039632" />
                </author>
                <author role="aut">
                  <persName>
                    <forename type="first">Antoine</forename>
                    <surname>Vidal-Mazuy</surname>
                  </persName>
                  <idno type="halauthorid">2916313-0</idno>
                  <affiliation ref="#struct-1039632" />
                </author>
                <author role="aut">
                  <persName>
                    <forename type="first">Lloyd</forename>
                    <surname>May</surname>
                  </persName>
                  <idno type="idhal" notation="numeric">1291738</idno>
                  <idno type="halauthorid" notation="string">2916314-1291738</idno>
                  <idno type="ORCID">https://orcid.org/0000-0003-4692-8261</idno>
                  <affiliation ref="#struct-73500" />
                </author>
                <author role="aut">
                  <persName>
                    <forename type="first">Marco</forename>
                    <surname>Winckler</surname>
                  </persName>
                  <email type="md5">001862a56370d3dfc171ddd66c5cf79e</email>
                  <email type="domain">unice.fr</email>
                  <idno type="idhal" notation="string">marco-winckler</idno>
                  <idno type="idhal" notation="numeric">21769</idno>
                  <idno type="halauthorid" notation="string">40679-21769</idno>
                  <idno type="ORCID">https://orcid.org/0000-0002-0756-6934</idno>
                  <idno type="IDREF">https://www.idref.fr/081569319</idno>
                  <affiliation ref="#struct-452156" />
                  <affiliation ref="#struct-13009" />
                  <affiliation ref="#struct-178918" />
                  <affiliation ref="#struct-1039632" />
                </author>
              </analytic>
              <monogr>
                <idno type="isbn">978-3-031-42279-9</idno>
                <title level="m">Springer Lecture Notes in Computer Sciences</title>
                <meeting>
                  <title>INTERACT 2023 - 19th IFIP TC13 International Conference</title>
                  <date type="start">2023-08-28</date>
                  <date type="end">2023-09-01</date>
                  <settlement>York</settlement>
                  <country key="GB">United Kingdom</country>
                </meeting>
                <respStmt>
                  <resp>conferenceOrganizer</resp>
                  <name>IFIP TC13, University of York, UK</name>
                </respStmt>
                <imprint>
                  <publisher>Springer Nature Switzerland</publisher>
                  <pubPlace>Cham</pubPlace>
                  <biblScope unit="serie">Human-Computer Interaction – INTERACT 2023: 19th IFIP TC13 International Conference,  Part I</biblScope>
                  <biblScope unit="volume">LNCS. 14142</biblScope>
                  <biblScope unit="pp">101-110</biblScope>
                  <date type="datePub">2023-08-25</date>
                </imprint>
              </monogr>
              <idno type="doi">10.1007/978-3-031-42280-5_6</idno>
              <ref type="publisher">https://link.springer.com/book/10.1007/978-3-031-42280-5</ref>
            </biblStruct>
          </sourceDesc>
          <profileDesc>
            <langUsage>
              <language ident="en">English</language>
            </langUsage>
            <textClass>
              <keywords scheme="author">
                <term xml:lang="en">Web Audio</term>
                <term xml:lang="en">DAWs</term>
                <term xml:lang="en">plugin architecture</term>
                <term xml:lang="en">Web standards</term>
              </keywords>
              <classCode scheme="acm" n="I">I.: Computing Methodologies</classCode>
              <classCode scheme="halDomain" n="info">Computer Science [cs]</classCode>
              <classCode scheme="halTypology" n="COMM">Conference papers</classCode>
              <classCode scheme="halOldTypology" n="COMM">Conference papers</classCode>
              <classCode scheme="halTreeTypology" n="COMM">Conference papers</classCode>
            </textClass>
            <abstract xml:lang="en">
              <p>This paper introduces WAM-Studio, an online Digital Audio Workstation (DAW) for recording, mixing, producing, and playing multitrack music. WAM-Studio advances music development by proposing a web-based environment based on a visual programming paradigm of end-user programming (EUP). In this paper, we describe how users can associate individual tracks with real-time audio processing plugins that can then be customized to produce a desired audio effect. Moreover, we describe how users can visually create macros to control multiple plugin parameters at once. While programming macro controls and customizing track parameters might have many applications in the music industry, they also present an opportunity to afford Hard-of-Hearing users greater control over their music listening. To illustrate the potential of WAM-Studio, we present a case study illustrating how this tool could be used by Hard-of-Hearing users to modify individual musical elements in a multitrack listening context to create a more enjoyable listening experience.</p>
            </abstract>
          </profileDesc>
        </biblFull>
      </listBibl>
    </body>
    <back>
      <listOrg type="structures">
        <org type="researchteam" xml:id="struct-178918" status="VALID">
          <idno type="RNSR">201221031M</idno>
          <orgName>Web-Instrumented Man-Machine Interactions, Communities and Semantics</orgName>
          <orgName type="acronym">WIMMICS</orgName>
          <desc>
            <address>
              <country key="FR" />
            </address>
            <ref type="url">http://wimmics.inria.fr/</ref>
          </desc>
          <listRelation>
            <relation active="#struct-34586" type="direct" />
            <relation active="#struct-300009" type="indirect" />
            <relation active="#struct-452156" type="direct" />
            <relation active="#struct-13009" type="indirect" />
            <relation active="#struct-117617" type="indirect" />
            <relation name="UMR7271" active="#struct-441569" type="indirect" />
            <relation active="#struct-1039632" type="indirect" />
          </listRelation>
        </org>
        <org type="laboratory" xml:id="struct-13009" status="VALID">
          <orgName>Laboratoire d'Informatique, Signaux, et Systèmes de Sophia Antipolis</orgName>
          <orgName type="acronym">I3S</orgName>
          <desc>
            <address>
              <addrLine>2000, route des Lucioles - Les Algorithmes - bât. Euclide B 06900 Sophia Antipolis</addrLine>
              <country key="FR" />
            </address>
            <ref type="url">http://www.i3s.unice.fr/</ref>
          </desc>
          <listRelation>
            <relation active="#struct-117617" type="direct" />
            <relation name="UMR7271" active="#struct-441569" type="direct" />
            <relation active="#struct-1039632" type="direct" />
          </listRelation>
        </org>
        <org type="regroupinstitution" xml:id="struct-1039632" status="VALID">
          <idno type="IdRef">241035694</idno>
          <idno type="ROR">https://ror.org/019tgvf94</idno>
          <orgName>Université Côte d'Azur</orgName>
          <orgName type="acronym">UniCA</orgName>
          <date type="start">2020-01-01</date>
          <desc>
            <address>
              <addrLine>Parc Valrose, 28, avenue Valrose 06108 Nice Cedex 2</addrLine>
              <country key="FR" />
            </address>
            <ref type="url">https://univ-cotedazur.fr</ref>
          </desc>
        </org>
        <org type="regroupinstitution" xml:id="struct-73500" status="VALID">
          <idno type="ROR">https://ror.org/00f54p054</idno>
          <orgName>Stanford University</orgName>
          <desc>
            <address>
              <addrLine>450 Serra Mall, Stanford, CA 94305-2004</addrLine>
              <country key="US" />
            </address>
            <ref type="url">https://www.stanford.edu/</ref>
          </desc>
        </org>
        <org type="department" xml:id="struct-452156" status="VALID">
          <orgName>Scalable and Pervasive softwARe and Knowledge Systems</orgName>
          <orgName type="acronym">Laboratoire I3S - SPARKS</orgName>
          <date type="start">2016-03-03</date>
          <desc>
            <address>
              <addrLine>Laboratoire I3SCS 4012106903 Sophia Antipolis Cedex</addrLine>
              <country key="FR" />
            </address>
            <ref type="url">http://www.i3s.unice.fr/sparks</ref>
          </desc>
          <listRelation>
            <relation active="#struct-13009" type="direct" />
            <relation active="#struct-117617" type="indirect" />
            <relation name="UMR7271" active="#struct-441569" type="indirect" />
            <relation active="#struct-1039632" type="indirect" />
          </listRelation>
        </org>
        <org type="laboratory" xml:id="struct-34586" status="VALID">
          <idno type="RNSR">198318250R</idno>
          <idno type="ROR">https://ror.org/01nzkaw91</idno>
          <orgName>Inria Sophia Antipolis - Méditerranée</orgName>
          <orgName type="acronym">CRISAM</orgName>
          <desc>
            <address>
              <addrLine>2004 route des Lucioles BP 93 06902 Sophia Antipolis</addrLine>
              <country key="FR" />
            </address>
            <ref type="url">http://www.inria.fr/centre/sophia/</ref>
          </desc>
          <listRelation>
            <relation active="#struct-300009" type="direct" />
          </listRelation>
        </org>
        <org type="institution" xml:id="struct-300009" status="VALID">
          <idno type="ROR">https://ror.org/02kvxyf05</idno>
          <orgName>Institut National de Recherche en Informatique et en Automatique</orgName>
          <orgName type="acronym">Inria</orgName>
          <desc>
            <address>
              <addrLine>Domaine de VoluceauRocquencourt - BP 10578153 Le Chesnay Cedex</addrLine>
              <country key="FR" />
            </address>
            <ref type="url">http://www.inria.fr/en/</ref>
          </desc>
        </org>
        <org type="institution" xml:id="struct-117617" status="VALID">
          <idno type="IdRef">026403498</idno>
          <idno type="ISNI">0000000123372892</idno>
          <idno type="ROR">https://ror.org/02k9vew78</idno>
          <orgName>Université Nice Sophia Antipolis (1965 - 2019)</orgName>
          <orgName type="acronym">UNS</orgName>
          <date type="start">1965-10-23</date>
          <date type="end">2019-12-31</date>
          <desc>
            <address>
              <addrLine>Parc Valrose, 06100 Nice</addrLine>
              <country key="FR" />
            </address>
            <ref type="url">http://unice.fr/</ref>
          </desc>
        </org>
        <org type="regroupinstitution" xml:id="struct-441569" status="VALID">
          <idno type="IdRef">02636817X</idno>
          <idno type="ISNI">0000000122597504</idno>
          <idno type="ROR">https://ror.org/02feahw73</idno>
          <orgName>Centre National de la Recherche Scientifique</orgName>
          <orgName type="acronym">CNRS</orgName>
          <date type="start">1939-10-19</date>
          <desc>
            <address>
              <country key="FR" />
            </address>
            <ref type="url">https://www.cnrs.fr/</ref>
          </desc>
        </org>
      </listOrg>
    </back>
  <teiCorpus>
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">WAM-Studio: A Web-based Digital Audio Workstation To Empower Cochlear Implant Users</title>
			</titleStmt>
			<publicationStmt>
				<publisher />
				<availability status="unknown"><licence /></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Michel</forename><surname>Buffa</surname></persName>
							<email>michel.buffa@univ-cotedazur.fr</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University Côte d'Azur</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<orgName type="institution" key="instit3">INRIA</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Antoine</forename><surname>Vidal-Mazuy</surname></persName>
							<email>antoine.vidal-mazuy@univ-cotedazur.fr</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University Côte d'Azur</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<orgName type="institution" key="instit3">INRIA</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lloyd</forename><surname>May</surname></persName>
							<email>lloydmay@stanford.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Marco</forename><surname>Winckler</surname></persName>
							<email>marco.winckler@univ-cotedazur.fr</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University Côte d'Azur</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<orgName type="institution" key="instit3">INRIA</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">WAM-Studio: A Web-based Digital Audio Workstation To Empower Cochlear Implant Users</title>
					</analytic>
					<monogr>
						<imprint>
							<date />
						</imprint>
					</monogr>
					<idno type="MD5">08832EFA2532888520F4F8DF1B06D2A8</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-04-12T14:51+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid" />
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Web Audio</term>
					<term>DAWs</term>
					<term>plugin architecture</term>
					<term>Web standards</term>
				</keywords>
			</textClass>
			<abstract>
<div><p>This paper introduces <software>WAM-Studio</software>, an online Digital Audio Workstation (<software ContextAttributes="created">DAW</software>) for recording, mixing, producing, and playing multitrack music. <software ContextAttributes="created">WAM-Studio</software> advances music development by proposing a web-based environment based on a visual programming paradigm of end-user programming (EUP). In this paper, we describe how users can associate individual tracks with real-time audio processing plugins that can then be customized to produce a desired audio effect. Moreover, we describe how users can visually create macros to control multiple plugin parameters at once. While programming macro controls and customizing track parameters might have many applications in the music industry, they also present an opportunity to afford Hard-of-Hearing users greater control over their music listening. To illustrate the potential of <software ContextAttributes="created">WAM-Studio</software>, we present a case study illustrating how this tool could be used by Hard-of-Hearing users to modify individual musical elements in a multitrack listening context to create a more enjoyable listening experience.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div><head n="1">Introduction</head><p>The advent of synthesizers, samplers, and sequencers completely changed the paradigm in the music creation process <ref type="bibr" target="#b3">[4]</ref>. Computer-assisted music production is a rapidly evolving field that utilizes computers to record, edit, and produce music. Many musicians embrace the use of Digital Audio Workstations (DAWs) for creating and manipulating digital audio and MIDI content to create music.</p><p>A <software>DAW</software> is a feature-rich software, resulting in a notably high complexity of use. It allows musicians to create multi-track songs by: (1) using audio samples directly (e.g., by incorporating an audio file into a track or recording from a microphone or sound card input), (2) synthesizing audio using virtual instruments (e.g., a software recreation of a piano), (3) mixing various audio tracks together, and (4) applying sound effects to each track (e.g., reverb, frequency equalization, or auto-tune on vocals).</p><p>The four DAWs with the lion-share of the market (<software ContextAttributes="created">Logic Audio</software>, <software ContextAttributes="created">Ableton</software>, Pro Tools, <software ContextAttributes="created">Cubase</software>) <ref type="foot" target="#foot_0">3</ref> , are all large standalone software applications that must be installed. The first online DAWs appeared in 2008, using Flash technology, while the first DAWs using HTML5 and the Web Audio API for audio processing only appeared between 2015 and 2016 <ref type="bibr" target="#b2">[3]</ref>. Online DAWs present distinct advantages over conventional DAWs such as lower barriers to entry and the ability to easily share and access projects from any device with an internet connection. A large variety of audio plugins (a kind of software module) extends the functionality of DAWs, offering users greater flexibility and control over their music production <ref type="bibr" target="#b1">[2]</ref>. Since 1997, a significant market has developed for thirdparty plugin developers offering thousands of plugins that are compatible with all major DAWs.</p><p>A <software ContextAttributes="created">DAW</software> is a complex application. A variety of individually adjustable parameters are available on each track, including volume, stereo panning, plugins, and their associated parameters, as well as the plugin order. Finally the "master track" sums all individual tracks together and presents an additional opportunity for plugin use and parameter tweaking. Given the amount of customization and complexity present in DAWs, they can be daunting applications for many users to engage with <ref type="bibr" target="#b3">[4]</ref>. However, previous work has shown that even basic <software ContextAttributes="created">DAW</software> controls can effectively be used to empower cochlear implant (CI) users to customize their music listening experience <ref type="bibr" target="#b6">[7]</ref>. This is largely due to the reduced frequency resolution and speech-focused nature of CI processing.</p><p>Previous work has illustrated the similarities between end-user programming (EUP) tools and music composition tools such as DAWs <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b14">15]</ref>. EUP aims to solve the mismatch between end users' high expectations and specific domain knowledge but limited programming expertise <ref type="bibr" target="#b13">[14]</ref>. Current practice shows that computer musicians become a kind of end-user programmer who face challenges that are similar to their professional counterparts in software engineering <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b11">12]</ref>. The difference is that DAWs aim to enable end users (musicians) to design, tailor, and customize audio. We suggest that some of the difficulties might be a result of the choice of programming paradigm. The user's ability to negotiate the constraints of the tool and assimilate its particular language is crucial in either case, whether engaging with visual metaphors or learning system-specific languages to build highly determined musical processes.</p><p>In this paper, we present a new Web-based digital audio workstation, <software>WAM-Studio</software>, that employs the visual programming paradigm for creating and editing multitrack audio. Section 2 introduces the main features of <software ContextAttributes="created">WAM-Studio</software> and highlights how <software ContextAttributes="created">WAM-Studio</software> enables users to create macro controls for adjusting many plugin parameters simultaneously (section 2.4). Section 3 describes the overall tasks orchestration, demonstrating how the macro manager of <software ContextAttributes="created">WAM-Studio</software> helps to simplify the process of adjusting various parameters at the same time. Apart from its conventional application in the music industry, we have discovered a potential utilization of the macro control feature to empower Hardof-Hearing users to customize their multi-track listening experience, as detailed in section 4. The remaining sections compare <software ContextAttributes="created">WAM-Studio</software> (section 5) with similar tools before presenting conclusions and future work.</p></div>
<div><head n="2">WAM-Studio's Main Features</head><p>Figure <ref type="figure" target="#fig_0">1</ref> shows <software ContextAttributes="created">WAM-Studio</software>'s user interface. This includes multiple tracks, their associated audio waveform, and a selection of available effect plugins (located inside the window titled WAM2 Pedalboard on the right-hand side) that can also be uniquely assigned and configured to each track. </p></div>
<div><head n="2.1">Overview of tracks</head><p>A track is a container for audio-related data that comes with an interactive display of these data, editing, and processing facilities as well as basic control parameters such as volume and left/right panning. <software ContextAttributes="created">WAM-Studio</software> supports two types of tracks: audio tracks and MIDI tracks. Audio tracks contain recorded audio, such as a vocal take, a guitar recording, or any other type of audio signal, that is generally rendered graphically as waveforms. These audio tracks can be edited, processed, and mixed by copying, cutting, and pasting audio samples in the audio buffer associated with the track. The audio track's output can be further processed by a chain of audio effects, or plugins. As the name suggests, MIDI tracks do not contain audio information but rather MIDI data (the pitch of notes, velocity, and duration), which is used to control virtual instruments such as synthesizers. Figure <ref type="figure" target="#fig_1">2</ref> shows an isolated audio track with the waveform display of the associated audio buffer, and the default track controls/parameters on the left side (mute/solo, volume, stereo panning). Tracks can be added or removed from <software ContextAttributes="created">WAM-Studio</software>, played in isolation, or with other tracks. They can also be armed for recording, allowing the armed track to record selected incoming audio while all other tracks can play along. Each track output is connected to a single "master track" where the global volume and panning of the final mix can be adjusted. It is also possible to apply audio effect plugins to the master track, allowing for a final adjustment of dynamics, frequency balancing, etc. A plugin, or multiple, can be applied to individual tracks, as shown in Figure <ref type="figure" target="#fig_2">3</ref>, to apply audio effects or synthesize audio (in the case of virtual instruments).</p><p>All audio effects and virtual instruments are plugins in the WAM-Studio. This design gives an extensive degree of control and adaptability and enables users to blend and manipulate the sound of each track with high precision and sophistication, thus making it easier to create intricate audio productions. So that, when one presses the play button of the <software>DAW</software>, all the tracks are rendered simultaneously, resulting in the final output signal, i.e. "the mix". </p></div>
<div><head n="2.2">Managing plugin chains</head><p>To apply a chain of effect plugins, <software ContextAttributes="created">WAM-Studio</software> provides a special plugin, called Pedalboard <ref type="bibr" target="#b1">[2]</ref>, that acts as a central host for all other plugins, as illustrated by Figure <ref type="figure" target="#fig_2">3</ref>. The Pedalboard connects to a plugin server that sends back the list of plugins available as a JSON array of URIs. From this list of URIs, the Pedalboard plugin retrieves the descriptors and initializes the plugins to be displayed (upper part of Fig. <ref type="figure" target="#fig_2">3</ref>). To create a chain of effects, plugins can be moved to the bottom part of the window in Figure <ref type="figure" target="#fig_2">3</ref>, re-ordered, and have their parameters set to create special effects.</p><p>Any configuration can be saved as a named preset (e.g. "guitar crunch 1"). Presets can be organized into banks ("rock", "funk", etc). Naming and management of banks of presets is a task of the Pedalboard plugin. The parameters exposed correspond to the entire set of parameters of the active preset, with this whole process able to be automatad <ref type="bibr" target="#b2">[3]</ref>. When a project is saved, the state of each track is saved, along with the state of the plugin configurations.</p></div>
<div><head n="2.3">Recording, dealing with latency, other features</head><p>Recording in a <software ContextAttributes="created">DAW</software> is one of the most delicate features to implement correctly. For instance, when recording a guitar track with real-time effects plugins, and playing over drum and bass tracks, it is crucial to ensure that the latency during recording is not noticeable as the musician must be able to play comfortably. Additionally, input latency is introduced as the time between a signal being emitted from a physical instrument and the time it is actually digitally recorded is non-trivial. This time depends on the sound card, operating system, drivers, audio buffer size, etc. The recorded audio must then be shifted back in time so that during playback, it is perfectly synchronized with the other tracks. There are many strategies to deal with latency issues <ref type="bibr" target="#b2">[3]</ref> and WAM-Studio is able to automatically select the best strategy according to the context of use.</p></div>
<div><head n="2.4">Macro control for synchronous multi-track management</head><p>A macro is a customized control that allows users to adjust a variety of settings at once, making it easier to manipulate sounds and effects in real-time, as illustrated in Figure <ref type="figure" target="#fig_3">4</ref>. A macro control refers to a way of controlling multiple parameters belonging to a plugin (effect or instrument) using a single knob, slider, or button. Therefore, when a parameter is assigned to a macro, a mapping is created that associates it to a specific range of values. When the macro is adjusted, the associated parameters change accordingly. For example, one might assign the filter cutoff, resonance, and envelope amount of a synth to a macro, so that it is possible to adjust all of these settings at once using a single knob called "timbre". Overall, macros are a powerful tool in <software ContextAttributes="created">WAM-Studio</software> that can help streamline the workflow of sound production. <software ContextAttributes="created">WAM-Studio</software>'s macros are inspired by the system available in the <software ContextAttributes="created">Ableton</software> Live <software ContextAttributes="created">DAW</software><ref type="foot" target="#foot_1">4</ref> . Macros can be created, loaded, and saved dynamically within the WAM<software ContextAttributes="created">-Studio</software> macro manager. They can be saved as presets, allowing custom settings to be quickly recalled and reused in future projects. When a project is saved, the current state and configuration of macros are saved as well.</p></div>
<div><head n="3">Tasks for creating a multitrack song in a nutshell</head><p>The process of recording and mixing a new song is often iterative. Here are the different main tasks involved:</p><p>-Hardware and instrument set-up: Connect the computer, sound card, instruments, MIDI controllers, etc. and calibrate latency<ref type="foot" target="#foot_2">5</ref> . -Create a new track: For each audio element being recorded or played back; -Add plugins: As needed on a track-by-track basis, such as audio effects for voice or virtual instrument to synthesise MIDI data, and adjust parameters; -Arm the track and record: Record the desired section of audio and, if other tracks are present, recording is made while playing back other tracks in time. -Mix: Adjust the volume and stereo panning of each track, and refine the sound by adjusting the plugin parameters of each track, including the master track.</p><p>All these steps are time-consuming, and require a certain amount of expertise and knowledge about each plugin. Macros play an important role in simplifying the often complicated process of plugin parameter adjustment, especially for users unfamiliar with audio processing. Unlike most existing DAWs, the WAM-Studio is a web application so that projects can be shared via simple http links, allowing remote collaborators to record and tweak settings. Therefore, macros play an additional role as they allow collaborators to create and share macros easily among collaborators, or even to the entire community of users.</p></div>
<div><head n="4">Using Macro Controls To Customize Multi-track Listening for Cochlear Implant Users</head><p>More than 1 in 5 people worldwide are D/deaf or Heard-of-Hearing (DHH) <ref type="foot" target="#foot_3">6</ref> , with many of them experiencing a large quality of life improvements through the use of hearing assistive technologies, such as hearing aids or cochlear implants (CIs) <ref type="bibr" target="#b17">[18]</ref>. CIs are electronic devices that convert acoustic sound signals into electrical ones that are used to stimulate the cochlea. Currently, the internal audio processing on CIs is optimized for speech, resulting in the perceptual experiences of music and other complex auditory stimuli varying greatly among CI users <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b9">10]</ref>. CI users experience a maximum dynamic range that is reduced from 40-80 dB and a frequency resolution approximately 10-20 times lower than people with traditional hearing <ref type="bibr">[5,</ref><ref type="bibr" target="#b18">19]</ref>. Therefore, CI users' perceptions of certain musical features, such as rhythm and tempo, are comparable to those with traditional hearing; however, the perception of timbral, harmonic, and melodic information differs greatly <ref type="bibr" target="#b8">[9]</ref>. Whilst the target users of <software ContextAttributes="created">WAM-Studio</software> are musicians, we have found that macro controls could be useful for Hard-of-Hearing (HH) users. 10 CI users pro-vided feedback and rated various mixes using a mix of qualitative and quantitative metrics, allowing us to determine mixing strategies and plug-in combinations that were often used in highly-rated mixes. Using <software ContextAttributes="created">WAM-Studio</software>'s macro controls to adjust certain sound parameters may increase CI user's enjoyment of listening to recorded music. In order to make the use of controls more easier to users, we have created a simplified view of macro controllers for HH users shown in as shown in Figure <ref type="figure" target="#fig_4">5</ref>. The first simplification is the replacement of the audio buffer waveform associated with the track by relevant macro sliders.</p><p>As we shall see in Figure <ref type="figure" target="#fig_4">5</ref>, the macro controllers are generated through previous research and given non-technical labels, such as "clarity", "punch", or "weight", allowing users to granularly adjust multiple settings of a plug-in chain using a single slider. A series of macros have been developed to be used on multiple instruments of varying genres. This includes macros tailored to both genre and instrumentation, for example:</p><p>-"Clarity (Pop Vocal)": Increasing this macro increases the amount of 2kHz and 5kHz present in the signal, increases the wet/dry mix of a de-esser, and adds a medium-attack, slow-release compressor. -''Punch (Rock Drums)": Increases the compression ratio and wet/dry mix of a compressor with medium-fast attack and fast release, increasing the wet/dry mix on a sub-octave pitch-shifter, as well as the wet/dry mix on a saturation plug-in. -"Shine (Country Guitar)": This reduces the amount of sub-250 Hz information, increases the wet/dry mix on a one-octave-above pitch-shifter, and increases the ratio and wet/dry on a slow-attack slow-release compressor.</p><p>Several plugins have been developed specifically for HH users, such as an octaver and a tracking-EQ which boosts the fundamental frequency of the signal. The macros controls for HH users were created by the research team and advanced users using the WAM-plugin macro creator. The macro created can be exported for use in the simplified view that is aimed to reach a large audience of CI's users.</p></div>
<div><head n="5">Related work</head><p>In <ref type="bibr" target="#b16">[17]</ref> a comparison of an online <software ContextAttributes="created">DAW</software> (Soundtrap) with two native-based solutions (Avid Cloud Collaboration and VST Transit) showed that web-based DAWs "have the potential for widespread adoption and may even surpass the usage of the existing paradigms in professional audio mixing practice in future". Additionally, the use of a custom <software ContextAttributes="created">DAW</software> for empowering HH users to customize multitrack listening is an original contribution to the field. Current approaches to enhance music enjoyment for CI users include adjustments to the internal signal processing on the CI itself, the creation of music composed specifically for CI users, and algorithmic pre-processing <ref type="bibr" target="#b12">[13]</ref>. These approaches certainly have merits, but they fail to recognize the enormous diversity and variance in auditory perception and aesthetic preferences among CI users. Additionally, these processes assume a passive CI listener with a limited desire to play an active role in their listening experience.</p><p><software ContextAttributes="created">WAM-Studio</software> is an ongoing work, nevertheless, it is the only web-based <software ContextAttributes="created">DAW</software> that features a macro system, that is open source, and that supports third-partyplugins. A survey of online DAWs can be found in <ref type="bibr" target="#b2">[3]</ref> and a comparison with collaborative solutions based on native DAWs in <ref type="bibr" target="#b16">[17]</ref>.</p></div>
<div><head n="6">Discussion and future work</head><p>In this paper, we have presented the design details and various features of WAM-Studio. Whilst the target users of WAM-Studio are musicians, we have found that some of its features (in particular the macro controls for multi-track management) could be useful for HH users. We illustrated how some macros can be created to help to adjust multitrack music using a simplified view of the WAM-Studio. This unexpected use of <software ContextAttributes="created">WAM-Studio</software> presents an exciting opportunity to research questions of practical and theoretical importance about the uses of tools for creating sounds and customizing listening experiences. Of particular interest are CI users, as this user population is largely under-designed for, despite there being over 736K registered CI users (as of December 2019) <ref type="bibr" target="#b0">[1]</ref>. CI users are additionally extremely heterogenous, potentially being well served by the level of customization and personalization offered by <software ContextAttributes="created">WAM-Studio</software>.</p><p>It is worthy of notice that all tools described in this paper use recent Web technologies: W3C APIs <software>WebAudio</software>, <software ContextAttributes="created">WebMidi</software>, Web Components, <software ContextAttributes="created">WebAssembly</software> and have been developed as an open source demonstrator of what can be done on the web today in real time audio processing. <software ContextAttributes="created">WAM-Studio</software> is readily available at http://annonymousURL.</p><p>Future work should include user testing with both versions of the tools including musicians and Hard-of-Hearing users, as well as additional focused usertesting and co-design sessions with CI users to adjust the mapping of various macros and further tailor the UI to increase the ease of use of the customized multitrack music player.</p></div><figure xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Overview of WAM-Studio, featuring a set of tracks (on the left-side) and the associated effect plugins (the WAM2 Pedalboard window to the right-side).</figDesc><graphic coords="3,150.52,254.94,311.23,165.34" type="bitmap" /></figure>
<figure xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. A track in WAW-Studio.</figDesc><graphic coords="4,152.06,115.84,311.24,53.09" type="bitmap" /></figure>
<figure xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Selecting a track and associating it to a chain of plugins.</figDesc><graphic coords="4,150.52,424.33,311.23,207.18" type="bitmap" /></figure>
<figure xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Editing Macros in WAM-Studio: macro editor above the plugin chain.</figDesc><graphic coords="6,152.06,115.84,311.23,196.93" type="bitmap" /></figure>
<figure xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Simplified view of WAM-Studio allowing DHH users to adjust sound properties.</figDesc><graphic coords="7,152.06,404.14,311.23,167.40" type="bitmap" /></figure>
			<note place="foot" n="3" xml:id="foot_0"><p>https://tinyurl.com/s4tbjzew</p></note>
			<note place="foot" n="4" xml:id="foot_1"><p>https://www.youtube.com/watch?v=NOufylMAEA&amp;t = 177s</p></note>
			<note place="foot" n="5" xml:id="foot_2"><p>See at<ref type="bibr" target="#b2">[3]</ref> details about why gear setup is required for every new hardware connected and security constraints prevent hardware discovery.</p></note>
			<note place="foot" n="6" xml:id="foot_3"><p>DHH is an expansive term for people with hearing loss or are otherwise aurally diverse, including those who identify as culturally Deaf and may use a signed language as their primary language.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m">Nidcd fact sheet: Cochlear implants</title>
		<imprint>
			<publisher>National Institute of Deafness and Other Communication Disorders</publisher>
			<date type="published" when="2021-03">Mar 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Making a guitar rack plugin -WebAudio Modules 2.0</title>
		<author>
			<persName><forename type="first">M</forename><surname>Buffa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kouyoumdjian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Beauchet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Forner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Marynowic</surname></persName>
		</author>
		<ptr target="https://hal.inria.fr/hal-03812948" />
	</analytic>
	<monogr>
		<title level="m">Web Audio Conference 2022</title>
		<meeting><address><addrLine>Cannes, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022-07">Jul 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Wam-studio, a digital audio workstation (daw) for the web</title>
		<author>
			<persName><forename type="first">M</forename><surname>Buffa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vidal-Mazuy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Web Conference 2023 -DevTrack</title>
		<meeting><address><addrLine>Austin, Texas, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">An empirical study of end-user programmers in the computer music community</title>
		<author>
			<persName><forename type="first">G</forename><surname>Burlet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hindle</surname></persName>
		</author>
		<idno type="DOI">10.1109/MSR.2015</idno>
		<ptr target="https://doi.org/10.1109/MSR.2015" />
	</analytic>
	<monogr>
		<title level="m">IEEE/ACM 12th Working Conference on Mining Software Repositories</title>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
			<biblScope unit="page" from="292" to="302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Central responses to electrical stimulation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Hartmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kral</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Cochlear implants: Auditory prostheses and electric hearing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="213" to="285" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Hillerson</surname></persName>
		</author>
		<title level="m">Programming Sound with Pure Data</title>
		<imprint>
			<publisher>The Pragmatic Bookshelf</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page">196</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Novel web-based music re-engineering software for enhancement of music enjoyment among cochlear implantees</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">P</forename><surname>Hwa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">L</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Caruana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mancuso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">P</forename><surname>Cellum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Lalwani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Otology &amp; Neurotology</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1347" to="1354" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The state of the art in end-user software engineering</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Abraham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Beckwith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Blackwell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Burnett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Erwig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Scaffidi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lawrance</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lieberman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Myers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">B</forename><surname>Rosson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Rothermel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wiedenbeck</surname></persName>
		</author>
		<idno type="DOI">10.1145/1922649.1922658</idno>
		<ptr target="https://doi.org/10.1145/1922649.1922658" />
	</analytic>
	<monogr>
		<title level="j">ACM Comput. Surv</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2011-04">apr 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Technological, biological, and acoustical constraints to music perception in cochlear implant users</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Limb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">T</forename><surname>Roy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Hearing research</title>
		<imprint>
			<biblScope unit="volume">308</biblScope>
			<biblScope unit="page" from="13" to="26" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A cochlear implant user with exceptional musical hearing ability</title>
		<author>
			<persName><forename type="first">M</forename><surname>Maarefvand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Marozeau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Blamey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of audiology</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="424" to="432" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Manhattan: End-user programming for music</title>
		<author>
			<persName><forename type="first">C</forename><surname>Nash</surname></persName>
		</author>
		<idno type="DOI">10.5281/zenodo.1178891</idno>
		<ptr target="https://doi.org/10.5281/zenodo.1178891" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on New Interfaces for Musical Expression</title>
		<meeting>the International Conference on New Interfaces for Musical Expression<address><addrLine>Zenodo</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-06">Jun 2014</date>
			<biblScope unit="page" from="221" to="226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Cognitive issues in computer music programming</title>
		<author>
			<persName><forename type="first">H</forename><surname>Nishino</surname></persName>
		</author>
		<idno type="DOI">10.5281/zenodo.1178123</idno>
		<ptr target="https://doi.org/10.5281/zenodo.1178123" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on New Interfaces for Musical Expression</title>
		<meeting>the International Conference on New Interfaces for Musical Expression<address><addrLine>Zenodo</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-06">Jun 2011</date>
			<biblScope unit="page" from="499" to="502" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Making music more accessible for cochlear implant listeners: recent developments</title>
		<author>
			<persName><forename type="first">W</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nagathil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Martin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="115" to="127" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<idno type="DOI">10.1007/978-3-319-60291-2</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-60291-2" />
		<title level="m">New Perspectives in End-User Development</title>
		<editor>
			<persName><forename type="first">F</forename><surname>Paternò</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">V</forename><surname>Wulf</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Sarkar</surname></persName>
		</author>
		<title level="m">Proceedings of the 27th Annual Conference of the Psychology of Programming Interest Group (PPIG</title>
		<meeting>the 27th Annual Conference of the Psychology of Programming Interest Group (PPIG</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Perception of musical tension in cochlear implant listeners</title>
		<author>
			<persName><forename type="first">S</forename><surname>Spangmose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hjortkjaer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Marozeau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in neuroscience</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">987</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A new audio mixing paradigm: evaluation from professional practitioners' perspectives</title>
		<author>
			<persName><forename type="first">S</forename><surname>Stickland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Athauda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Scott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Creative Industries Journal</title>
		<imprint>
			<biblScope unit="page" from="1" to="49" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<ptr target="https://www.who.int/news-room/fact-sheets/detail/deafness-and-hearing-loss" />
		<title level="m">WorldHealthOrganization: Deafness and hearing loss fact sheet</title>
		<imprint>
			<date type="published" when="2023-02">Feb 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Abnormal pitch perception produced by cochlear implant stimulation</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">G</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PloS one</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">88662</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</teiCorpus></text>
</TEI>