<?xml version='1.0' encoding='utf-8'?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" version="1.1" xsi:schemaLocation="http://www.tei-c.org/ns/1.0 http://api.archives-ouvertes.fr/documents/aofr-sword.xsd">
  <teiHeader>
    <fileDesc>
      <titleStmt>
        <title>HAL TEI export of hal-03905917</title>
      </titleStmt>
      <publicationStmt>
        <distributor>CCSD</distributor>
        <availability status="restricted">
          <licence target="http://creativecommons.org/licenses/by/4.0/">Distributed under a Creative Commons Attribution 4.0 International License</licence>
        </availability>
        <date when="2024-04-23T09:32:08+02:00" />
      </publicationStmt>
      <sourceDesc>
        <p part="N">HAL API platform</p>
      </sourceDesc>
    </fileDesc>
  </teiHeader>
  <text>
    <body>
      <listBibl>
        <biblFull>
          <titleStmt>
            <title xml:lang="en">Collaborative Algorithms for Online Personalized Mean Estimation</title>
            <author role="aut">
              <persName>
                <forename type="first">Mahsa</forename>
                <surname>Asadi</surname>
              </persName>
              <idno type="halauthorid">1473098-0</idno>
              <affiliation ref="#struct-432650" />
            </author>
            <author role="aut">
              <persName>
                <forename type="first">Aurélien</forename>
                <surname>Bellet</surname>
              </persName>
              <email type="md5">7c92d2fc696e1875415477238a601d34</email>
              <email type="domain">inria.fr</email>
              <ptr type="url" target="http://researchers.lille.inria.fr/abellet/" />
              <idno type="idhal" notation="string">aurelien-bellet</idno>
              <idno type="idhal" notation="numeric">9877</idno>
              <idno type="halauthorid" notation="string">30290-9877</idno>
              <idno type="ORCID">https://orcid.org/0000-0003-3440-1251</idno>
              <idno type="GOOGLE SCHOLAR">https://scholar.google.fr/citations?user=j8svx3IAAAAJ</idno>
              <idno type="IDREF">https://www.idref.fr/17653136X</idno>
              <idno type="ARXIV">https://arxiv.org/a/bellet_a_1</idno>
              <affiliation ref="#struct-432650" />
            </author>
            <author role="aut">
              <persName>
                <forename type="first">Odalric-Ambrym</forename>
                <surname>Maillard</surname>
              </persName>
              <email type="md5">2e4c2e554a9ead49425e540bd93d475b</email>
              <email type="domain">inria.fr</email>
              <idno type="idhal" notation="string">odalric-ambrym-maillard</idno>
              <idno type="idhal" notation="numeric">5563</idno>
              <idno type="halauthorid" notation="string">16038-5563</idno>
              <idno type="IDREF">https://www.idref.fr/158055594</idno>
              <idno type="ORCID">https://orcid.org/0000-0001-7935-7026</idno>
              <affiliation ref="#struct-1042631" />
            </author>
            <author role="aut">
              <persName>
                <forename type="first">Marc</forename>
                <surname>Tommasi</surname>
              </persName>
              <email type="md5">1200567ad0ac568c3e246f8dac3f3e48</email>
              <email type="domain">univ-lille3.fr</email>
              <idno type="idhal" notation="string">marc-tommasi</idno>
              <idno type="idhal" notation="numeric">399</idno>
              <idno type="halauthorid" notation="string">4412-399</idno>
              <idno type="IDREF">https://www.idref.fr/121846385</idno>
              <idno type="ORCID">https://orcid.org/0000-0003-2838-4408</idno>
              <affiliation ref="#struct-432650" />
            </author>
            <editor role="depositor">
              <persName>
                <forename>Aurélien</forename>
                <surname>Bellet</surname>
              </persName>
              <email type="md5">7c92d2fc696e1875415477238a601d34</email>
              <email type="domain">inria.fr</email>
            </editor>
          </titleStmt>
          <editionStmt>
            <edition n="v1" type="current">
              <date type="whenSubmitted">2022-12-19 10:29:01</date>
              <date type="whenModified">2024-01-24 09:54:24</date>
              <date type="whenReleased">2022-12-19 11:02:13</date>
              <date type="whenProduced">2022</date>
              <date type="whenEndEmbargoed">2022-12-19</date>
              <ref type="file" target="https://inria.hal.science/hal-03905917/document">
                <date notBefore="2022-12-19" />
              </ref>
              <ref type="file" subtype="author" n="1" target="https://inria.hal.science/hal-03905917/file/tmlr.pdf">
                <date notBefore="2022-12-19" />
              </ref>
              <ref type="externalLink" target="http://arxiv.org/pdf/2208.11530" />
            </edition>
            <respStmt>
              <resp>contributor</resp>
              <name key="179317">
                <persName>
                  <forename>Aurélien</forename>
                  <surname>Bellet</surname>
                </persName>
                <email type="md5">7c92d2fc696e1875415477238a601d34</email>
                <email type="domain">inria.fr</email>
              </name>
            </respStmt>
          </editionStmt>
          <publicationStmt>
            <distributor>CCSD</distributor>
            <idno type="halId">hal-03905917</idno>
            <idno type="halUri">https://inria.hal.science/hal-03905917</idno>
            <idno type="halBibtex">asadi:hal-03905917</idno>
            <idno type="halRefHtml">&lt;i&gt;Transactions on Machine Learning Research Journal&lt;/i&gt;, 2022</idno>
            <idno type="halRef">Transactions on Machine Learning Research Journal, 2022</idno>
          </publicationStmt>
          <seriesStmt>
            <idno type="stamp" n="CNRS">CNRS - Centre national de la recherche scientifique</idno>
            <idno type="stamp" n="INRIA">INRIA - Institut National de Recherche en Informatique et en Automatique</idno>
            <idno type="stamp" n="INRIA-LILLE">INRIA Lille - Nord Europe</idno>
            <idno type="stamp" n="INRIA_TEST">INRIA - Institut National de Recherche en Informatique et en Automatique</idno>
            <idno type="stamp" n="TESTALAIN1">TESTALAIN1</idno>
            <idno type="stamp" n="CRISTAL">Centre de Recherche en Informatique, Signal et Automatique de Lille (CRISTAL)</idno>
            <idno type="stamp" n="INRIA2">INRIA 2</idno>
            <idno type="stamp" n="CRISTAL-MAGNET" corresp="CRISTAL">CRISTAL-MAGNET</idno>
            <idno type="stamp" n="UNIV-LILLE">Université de Lille</idno>
            <idno type="stamp" n="CRISTAL-SCOOL" corresp="CRISTAL">SCOOL</idno>
          </seriesStmt>
          <notesStmt>
            <note type="audience" n="2">International</note>
            <note type="popular" n="0">No</note>
            <note type="peer" n="1">Yes</note>
          </notesStmt>
          <sourceDesc>
            <biblStruct>
              <analytic>
                <title xml:lang="en">Collaborative Algorithms for Online Personalized Mean Estimation</title>
                <author role="aut">
                  <persName>
                    <forename type="first">Mahsa</forename>
                    <surname>Asadi</surname>
                  </persName>
                  <idno type="halauthorid">1473098-0</idno>
                  <affiliation ref="#struct-432650" />
                </author>
                <author role="aut">
                  <persName>
                    <forename type="first">Aurélien</forename>
                    <surname>Bellet</surname>
                  </persName>
                  <email type="md5">7c92d2fc696e1875415477238a601d34</email>
                  <email type="domain">inria.fr</email>
                  <ptr type="url" target="http://researchers.lille.inria.fr/abellet/" />
                  <idno type="idhal" notation="string">aurelien-bellet</idno>
                  <idno type="idhal" notation="numeric">9877</idno>
                  <idno type="halauthorid" notation="string">30290-9877</idno>
                  <idno type="ORCID">https://orcid.org/0000-0003-3440-1251</idno>
                  <idno type="GOOGLE SCHOLAR">https://scholar.google.fr/citations?user=j8svx3IAAAAJ</idno>
                  <idno type="IDREF">https://www.idref.fr/17653136X</idno>
                  <idno type="ARXIV">https://arxiv.org/a/bellet_a_1</idno>
                  <affiliation ref="#struct-432650" />
                </author>
                <author role="aut">
                  <persName>
                    <forename type="first">Odalric-Ambrym</forename>
                    <surname>Maillard</surname>
                  </persName>
                  <email type="md5">2e4c2e554a9ead49425e540bd93d475b</email>
                  <email type="domain">inria.fr</email>
                  <idno type="idhal" notation="string">odalric-ambrym-maillard</idno>
                  <idno type="idhal" notation="numeric">5563</idno>
                  <idno type="halauthorid" notation="string">16038-5563</idno>
                  <idno type="IDREF">https://www.idref.fr/158055594</idno>
                  <idno type="ORCID">https://orcid.org/0000-0001-7935-7026</idno>
                  <affiliation ref="#struct-1042631" />
                </author>
                <author role="aut">
                  <persName>
                    <forename type="first">Marc</forename>
                    <surname>Tommasi</surname>
                  </persName>
                  <email type="md5">1200567ad0ac568c3e246f8dac3f3e48</email>
                  <email type="domain">univ-lille3.fr</email>
                  <idno type="idhal" notation="string">marc-tommasi</idno>
                  <idno type="idhal" notation="numeric">399</idno>
                  <idno type="halauthorid" notation="string">4412-399</idno>
                  <idno type="IDREF">https://www.idref.fr/121846385</idno>
                  <idno type="ORCID">https://orcid.org/0000-0003-2838-4408</idno>
                  <affiliation ref="#struct-432650" />
                </author>
              </analytic>
              <monogr>
                <idno type="halJournalId" status="VALID">363201</idno>
                <idno type="issn">2835-8856</idno>
                <title level="j">Transactions on Machine Learning Research Journal</title>
                <imprint>
                  <publisher>[Amherst Massachusetts]: OpenReview.net, 2022</publisher>
                  <date type="datePub">2022</date>
                </imprint>
              </monogr>
              <idno type="arxiv">2208.11530</idno>
            </biblStruct>
          </sourceDesc>
          <profileDesc>
            <langUsage>
              <language ident="en">English</language>
            </langUsage>
            <textClass>
              <classCode scheme="halDomain" n="info.info-lg">Computer Science [cs]/Machine Learning [cs.LG]</classCode>
              <classCode scheme="halDomain" n="stat.ml">Statistics [stat]/Machine Learning [stat.ML]</classCode>
              <classCode scheme="halTypology" n="ART">Journal articles</classCode>
              <classCode scheme="halOldTypology" n="ART">Journal articles</classCode>
              <classCode scheme="halTreeTypology" n="ART">Journal articles</classCode>
            </textClass>
            <abstract xml:lang="en">
              <p>We consider an online estimation problem involving a set of agents. Each agent has access to a (personal) process that generates samples from a real-valued distribution and seeks to estimate its mean. We study the case where some of the distributions have the same mean, and the agents are allowed to actively query information from other agents. The goal is to design an algorithm that enables each agent to improve its mean estimate thanks to communication with other agents. The means as well as the number of distributions with same mean are unknown, which makes the task nontrivial. We introduce a novel collaborative strategy to solve this online personalized mean estimation problem. We analyze its time complexity and introduce variants that enjoy good performance in numerical experiments. We also extend our approach to the setting where clusters of agents with similar means seek to estimate the mean of their cluster.</p>
            </abstract>
          </profileDesc>
        </biblFull>
      </listBibl>
    </body>
    <back>
      <listOrg type="structures">
        <org type="researchteam" xml:id="struct-432650" status="VALID">
          <idno type="RNSR">201321079K</idno>
          <orgName>Machine Learning in Information Networks</orgName>
          <orgName type="acronym">MAGNET</orgName>
          <date type="start">2015-01-01</date>
          <desc>
            <address>
              <country key="FR" />
            </address>
            <ref type="url">http://www.inria.fr/equipes/magnet</ref>
          </desc>
          <listRelation>
            <relation active="#struct-104752" type="direct" />
            <relation active="#struct-300009" type="indirect" />
            <relation active="#struct-410272" type="direct" />
            <relation name="UMR9189" active="#struct-120930" type="indirect" />
            <relation name="UMR9189" active="#struct-374570" type="indirect" />
            <relation name="UMR9189" active="#struct-441569" type="indirect" />
          </listRelation>
        </org>
        <org type="researchteam" xml:id="struct-1042631" status="VALID">
          <idno type="RNSR">202023603Y</idno>
          <orgName>Scool</orgName>
          <orgName type="acronym">Scool</orgName>
          <desc>
            <address>
              <addrLine>Parc Scientifique de la Haute Borne 40, avenue Halley Bât.A, Park Plaza 59650 Villeneuve d'Ascq </addrLine>
              <country key="FR" />
            </address>
          </desc>
          <listRelation>
            <relation active="#struct-104752" type="direct" />
            <relation active="#struct-300009" type="indirect" />
            <relation active="#struct-410272" type="direct" />
            <relation name="UMR9189" active="#struct-120930" type="indirect" />
            <relation name="UMR9189" active="#struct-374570" type="indirect" />
            <relation name="UMR9189" active="#struct-441569" type="indirect" />
          </listRelation>
        </org>
        <org type="laboratory" xml:id="struct-104752" status="VALID">
          <idno type="RNSR">200818245B</idno>
          <idno type="ROR">https://ror.org/04eej9726</idno>
          <orgName>Inria Lille - Nord Europe</orgName>
          <desc>
            <address>
              <addrLine>Parc Scientifique de la Haute Borne 40, avenue Halley Bât.A, Park Plaza 59650 Villeneuve d'Ascq</addrLine>
              <country key="FR" />
            </address>
            <ref type="url">http://www.inria.fr/lille/</ref>
          </desc>
          <listRelation>
            <relation active="#struct-300009" type="direct" />
          </listRelation>
        </org>
        <org type="institution" xml:id="struct-300009" status="VALID">
          <idno type="ROR">https://ror.org/02kvxyf05</idno>
          <orgName>Institut National de Recherche en Informatique et en Automatique</orgName>
          <orgName type="acronym">Inria</orgName>
          <desc>
            <address>
              <addrLine>Domaine de VoluceauRocquencourt - BP 10578153 Le Chesnay Cedex</addrLine>
              <country key="FR" />
            </address>
            <ref type="url">http://www.inria.fr/en/</ref>
          </desc>
        </org>
        <org type="laboratory" xml:id="struct-410272" status="VALID">
          <idno type="IdRef">18388695X</idno>
          <idno type="RNSR">201521249L</idno>
          <idno type="ROR">https://ror.org/05vrs3189</idno>
          <orgName>Centre de Recherche en Informatique, Signal et Automatique de Lille - UMR 9189</orgName>
          <orgName type="acronym">CRIStAL</orgName>
          <date type="start">2015-01-01</date>
          <desc>
            <address>
              <addrLine>Université de Lille - Campus scientifique - Bâtiment ESPRIT - Avenue Henri Poincaré - 59655 Villeneuve d’Ascq</addrLine>
              <country key="FR" />
            </address>
            <ref type="url">https://www.cristal.univ-lille.fr/</ref>
          </desc>
          <listRelation>
            <relation name="UMR9189" active="#struct-120930" type="direct" />
            <relation name="UMR9189" active="#struct-374570" type="direct" />
            <relation name="UMR9189" active="#struct-441569" type="direct" />
          </listRelation>
        </org>
        <org type="institution" xml:id="struct-120930" status="VALID">
          <idno type="IdRef">256304629</idno>
          <idno type="ISNI">0000000122034461</idno>
          <idno type="ROR">https://ror.org/01x441g73</idno>
          <orgName>Centrale Lille</orgName>
          <desc>
            <address>
              <addrLine>École Centrale de Lille - Cité Scientifique - CS 20048 59651 Villeneuve d'Ascq Cedex</addrLine>
              <country key="FR" />
            </address>
            <ref type="url">https://centralelille.fr/</ref>
          </desc>
        </org>
        <org type="regroupinstitution" xml:id="struct-374570" status="VALID">
          <idno type="IdRef">223446556</idno>
          <idno type="ISNI">0000 0001 2242 6780</idno>
          <idno type="ROR">https://ror.org/02kzqn938</idno>
          <idno type="Wikidata">Q3551621</idno>
          <orgName>Université de Lille</orgName>
          <desc>
            <address>
              <addrLine>EPE Université de Lille. -- 42 rue Paul Duez, 59000 Lille</addrLine>
              <country key="FR" />
            </address>
            <ref type="url">https://www.univ-lille.fr/</ref>
          </desc>
        </org>
        <org type="regroupinstitution" xml:id="struct-441569" status="VALID">
          <idno type="IdRef">02636817X</idno>
          <idno type="ISNI">0000000122597504</idno>
          <idno type="ROR">https://ror.org/02feahw73</idno>
          <orgName>Centre National de la Recherche Scientifique</orgName>
          <orgName type="acronym">CNRS</orgName>
          <date type="start">1939-10-19</date>
          <desc>
            <address>
              <country key="FR" />
            </address>
            <ref type="url">https://www.cnrs.fr/</ref>
          </desc>
        </org>
      </listOrg>
    </back>
  <teiCorpus>
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Collaborative Algorithms for Online Personalized Mean Estimation</title>
				<funder>
					<orgName type="full">Inria</orgName>
				</funder>
				<funder ref="#_4v3q8Sg">
					<orgName type="full">I-SITE ULNE</orgName>
				</funder>
				<funder>
					<orgName type="full">Métropole Européenne de Lille</orgName>
					<orgName type="abbreviated">MEL</orgName>
				</funder>
				<funder>
					<orgName type="full">Université de Lille</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher />
				<availability status="unknown"><licence /></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Mahsa</forename><surname>Asadi</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">UMR</orgName>
								<orgName type="institution" key="instit1">Univ. Lille</orgName>
								<orgName type="institution" key="instit2">Inria</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<orgName type="institution" key="instit4">Centrale Lille</orgName>
								<address>
									<addrLine>9189 -CRIStAL</addrLine>
									<postCode>F-59000</postCode>
									<settlement>Lille</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Aurélien</forename><surname>Bellet</surname></persName>
							<email>aurelien.bellet@inria.fr</email>
							<affiliation key="aff1">
								<orgName type="laboratory">UMR</orgName>
								<orgName type="institution" key="instit1">Univ. Lille</orgName>
								<orgName type="institution" key="instit2">Inria</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<orgName type="institution" key="instit4">Centrale Lille</orgName>
								<address>
									<addrLine>9189 -CRIStAL</addrLine>
									<postCode>F-59000</postCode>
									<settlement>Lille</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><surname>Maillard</surname></persName>
							<affiliation key="aff2">
								<orgName type="laboratory">UMR</orgName>
								<orgName type="institution" key="instit1">Univ. Lille</orgName>
								<orgName type="institution" key="instit2">Inria</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<orgName type="institution" key="instit4">Centrale Lille</orgName>
								<address>
									<addrLine>9189 -CRIStAL</addrLine>
									<postCode>F-59000</postCode>
									<settlement>Lille</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Marc</forename><surname>Tommasi</surname></persName>
							<email>marc.tommasi@inria.fr</email>
							<affiliation key="aff3">
								<orgName type="laboratory">UMR</orgName>
								<orgName type="institution" key="instit1">Univ. Lille</orgName>
								<orgName type="institution" key="instit2">Inria</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<orgName type="institution" key="instit4">Centrale Lille</orgName>
								<address>
									<addrLine>9189 -CRIStAL</addrLine>
									<postCode>F-59000</postCode>
									<settlement>Lille</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Collaborative Algorithms for Online Personalized Mean Estimation</title>
					</analytic>
					<monogr>
						<imprint>
							<date />
						</imprint>
					</monogr>
					<idno type="MD5">C94D06CF165FE6D63296F0C90B0D9DE0</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-04-12T14:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid" />
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div><p>We consider an online estimation problem involving a set of agents. Each agent has access to a (personal) process that generates samples from a real-valued distribution and seeks to estimate its mean. We study the case where some of the distributions have the same mean, and the agents are allowed to actively query information from other agents. The goal is to design an algorithm that enables each agent to improve its mean estimate thanks to communication with other agents. The means as well as the number of distributions with same mean are unknown, which makes the task nontrivial. We introduce a novel collaborative strategy to solve this online personalized mean estimation problem. We analyze its time complexity and introduce variants that enjoy good performance in numerical experiments. We also extend our approach to the setting where clusters of agents with similar means seek to estimate the mean of their cluster.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div><head n="1">Introduction</head><p>With the widespread of personal digital devices, ubiquitous computing and IoT (Internet of Things), the need for decentralized and collaborative computing has become more pressing. Indeed, devices are first of all designed to collect data and this data may be sensitive and/or too large to be transmitted. Therefore, it is often preferable to keep the data on-device, where it has been collected. Local processing on a single device is a always possible option but learning in isolation suffers from slow convergence time when data arrives slowly. In that case, collaborative strategies can be investigated in order to increase statistical power and accelerate learning. In recent years, such collaborative approaches have been broadly referred to as federated learning <ref type="bibr" target="#b16">(Kairouz et al., 2021)</ref>.</p><p>The data collected at each device reflects the specific usage, production patterns and objective of the associated agent. Therefore, we must solve a set of personalized tasks over heterogeneous data distributions. Even though the tasks are personalized, collaboration can play a significant role in reducing the time complexity and accelerating learning in presence of agents who share similar objectives. An important building block to design collaborative algorithms is then to identify agents acquiring data from the same (or similar) distribution. This is particularly difficult to do in an online setting, in which data becomes available sequentially over time.</p><p>In this work, we explore this challenging objective in the context of a new problem: online personalized mean estimation. Formally, each agent continuously receives data from a personal σ-sub-Gaussian distribution and aims to construct an accurate estimation of its mean as fast as possible. At each step, each agent receives a new sample from its distribution but is also allowed to query the current local average of another agent. To enable collaboration, we assume the existence of an underlying class structure where agents in the same class have the same mean value. We also consider a relaxed assumption where the means of agents in a class are close (but not necessarily equal). Such assumptions are natural in many real-world applications <ref type="bibr" target="#b0">(Adi et al., 2020)</ref>. A simple example is that of in different environments, monitoring parameters such as temperature in order to accurately estimate their mean (see for instance <ref type="bibr" target="#b25">Mateo et al., 2013)</ref>. Another example is collaborative filtering, where the goal is to estimate user preferences by leveraging the existence of clusters of users with similar preferences <ref type="bibr">(Su &amp; Khoshgoftaar, 2009a)</ref>. Crucially, the number of classes and their cardinality are unknown to the agents and must be discovered in an online fashion.</p><p>We propose collaborative algorithms to solve this problem, where agents identify the class they belong to in an online fashion so as to better and faster estimate their own mean by assigning weights to other agents' estimates. Our approach is grounded in Probably Approximately Correct (PAC) theory, allowing agents to iteratively discard agents in different classes with high confidence. We provide a theoretical analysis of our approach by bounding the time required by an agent to correctly estimate its class with high probability, as well as the time required by an agent to estimate its mean to the desired accuracy. Our results highlight the dependence on the gaps between the true means of agents in different classes, and show that in some settings our approach achieves nearly the same time complexity as an oracle who would know the classes beforehand. Our numerical experiments on synthetic data are in line with our theoretical findings and show that some empirical variants of our approach can further improve the performance in practice.</p><p>The paper is organized as follows. Section 2 discusses the related work on federated learning and collaborative online learning. In Section 3, we formally describe the problem setting and introduce relevant notations. In Section 4, we introduce our algorithm and its variants. Section 5 presents our theoretical analysis of the proposed algorithm in terms of class and mean estimation time complexity. Section 6 is devoted to illustrative numerical experiments. Section 7 extends our approach to the case where classes consist of agents with similar (but not necessarily equal) means and agents seek to estimate the mean of their class. We conclude and discuss perspectives for future work in Section 8.</p></div>
<div><head n="2">Related Work</head><p>Over the last few years, collaborative estimation and learning problems involving several agents with local datasets have been extensively investigated under the broad term of Federated Learning (FL) <ref type="bibr" target="#b16">(Kairouz et al., 2021)</ref>. While traditional FL algorithms learn a global estimate for all agents, more personalized approaches have recently attracted a lot of interest (see for instance <ref type="bibr" target="#b36">Vanhaesebrouck et al., 2017;</ref><ref type="bibr" target="#b31">Smith et al., 2017;</ref><ref type="bibr" target="#b11">Fallah et al., 2020;</ref><ref type="bibr" target="#b28">Sattler et al., 2020;</ref><ref type="bibr" target="#b13">Hanzely et al., 2020;</ref><ref type="bibr">Marfoq et al., 2021, and references therein)</ref>. With the exception of recent work on simple linear regression settings <ref type="bibr" target="#b8">(Cheng et al., 2021)</ref>, these approaches typically lack clear statistical assumptions on the relation between local data distributions and do not provide error guarantees with respect to these underlying distributions. More importantly, the above methods focus on the batch learning setting and are not suitable for online learning.</p><p>In the online setting, the work on collaborative learning has largely focused on multi-armed bandits (MAB). Most approaches however consider a single MAB instance which is solved collaboratively by multiple agents. Collaboration between agents can be implemented through broadcast messages to all agents <ref type="bibr" target="#b14">(Hillel et al., 2013;</ref><ref type="bibr" target="#b35">Tao et al., 2019)</ref>, via a server <ref type="bibr">(Wang et al., 2020b)</ref>, or relying only on local message exchanges over a network graph <ref type="bibr" target="#b27">(Sankararaman et al., 2019;</ref><ref type="bibr" target="#b24">Martínez-Rubio et al., 2019;</ref><ref type="bibr">Wang et al., 2020a;</ref><ref type="bibr" target="#b20">Landgren et al., 2021;</ref><ref type="bibr" target="#b21">Madhushani et al., 2021)</ref>. Other approaches do not allow explicit communication but instead consider a collision model where agents receive no reward if several agents pull the same arm <ref type="bibr" target="#b4">(Boursier &amp; Perchet, 2019;</ref><ref type="bibr">Wang et al., 2020a)</ref>. In any case, all agents aim at solving the same task. Some recent work considered collaborative MAB settings where the arm means vary across agents. Extending their previous work <ref type="bibr" target="#b4">(Boursier &amp; Perchet, 2019)</ref>, <ref type="bibr" target="#b5">Boursier et al. (2020)</ref> consider the case where arm means can vary among players. Under their collision model, the problem reduces to finding a one-to-one assignment of agents to arms. In <ref type="bibr">Shi &amp; Shen (2021)</ref>, the local arm means of each agent are IID random realizations of fixed global means and the goal is to solve the global MAB using only observations from the local arms with an algorithm inspired from traditional FL. Similarly, <ref type="bibr" target="#b17">Karpov &amp; Zhang (2022)</ref> extend the work of <ref type="bibr" target="#b35">Tao et al. (2019)</ref> by considering different local arm means for each agent with the goal to identify the arm with largest aggregated mean. <ref type="bibr">Shi et al. (2021)</ref> introduce a limited amount of personalization by extending the model of <ref type="bibr">Shi &amp; Shen (2021)</ref> to optimize a mixture between the global and local MAB objectives. <ref type="bibr" target="#b26">Réda et al. (2022)</ref> further consider a known weighted combination of the local MAB objectives, and focus on the pure exploration setting (best arm identification) rather than regret minimization. A crucial difference with our work is that there is no need to discover relations between local distributions to solve the above problems.</p><p>Another related problem is to identify a graph structure on top of the arms in MAB. <ref type="bibr" target="#b18">Kocák &amp; Garivier (2020;</ref><ref type="bibr">2021)</ref> construct a similarity graph while solving the best arm identification problem, but consider only a single agent. In contrast, our work considers a multi-agent setting with personalized estimation tasks, and our approach discovers similarities across agents' tasks in an online manner.</p></div>
<div><head n="3">Problem Setting</head><p>We consider a mean estimation problem involving A agents. The goal of each agent a ∈ [A] = {1, 2, . . . , A} is to estimate the mean µ a of a personal distribution ν a over R. In this work, we assume that there exists σ ≥ 0 such that each ν a is σ-sub-Gaussian, i.e.:</p><formula xml:id="formula_0">∀λ ∈ R, log E x∼νa exp(λ(x -µ a )) ≤ 1 2 λ 2 σ 2 .</formula><p>This classical assumption captures a property of strong tail decay, and includes in particular Gaussian distributions (in that case, the smallest possible σ 2 corresponds to the variance) as well as any distribution supported on a bounded interval (e.g., Bernoulli distributions).</p><p>We consider an online and collaborative setting where data points are received sequentially and agents can query each other to learn about their respective distributions. Agents should be thought of as different user devices which operate in parallel. Therefore, they all receive a new sample and query another agent at each time step.</p><p>Formally, we assume that time is synchronized between agents and at each time step t, each agent a receives a new sample x t a from its personal distribution ν a with mean µ a , which is used to update its local mean estimate xt a,a = 1 t ∑ t t ′ =1 x t ′ a . It also chooses another agent l to query. As a response from querying agent l, agent a receives the local average xt l,l of agent l (i.e., the average of t independent samples from the personal distribution ν l ) and stores it in its memory xt a,l along with the corresponding number of samples n t a,l = t. Each agent a thus keeps a memory [(x t a,1 , n t a,1 ), . . . , (x t a,A , n t a,A )] of the last local averages (and associated number of samples) that it received from other agents. The information contained in this memory is used to compute an estimate µ t a of µ a at each time t. Our goal is to design a query and estimation procedure for each agent.</p><p>As described above, note that when an agent queries another agent at time t, it does not receive one sample from this agent (as e.g. in multi-armed bandits), but receives the full statistics of observations of this agent up to time t. This is considerably much more information than in typical MAB settings, and naturally requires specific strategies.</p><p>The goal of each agent to find a good estimate of its personal mean as fast as possible, without consideration for the quality of the estimates in earlier steps (i.e., we do not seek to minimize a notion of regret). In the online learning terminology, this is referred to as a pure exploration setting (like best arm identification in multi-armed bandits, see <ref type="bibr" target="#b1">Audibert et al., 2010)</ref>. Formally, we will measure the performance of an algorithm using the notion of (ϵ, δ)-convergence in probability <ref type="bibr" target="#b3">(Bertsekas &amp; Tsitsiklis, 2002;</ref><ref type="bibr" target="#b40">Wasserman, 2013)</ref>, which we recall below.</p><p>Definition 1 (PAC-convergence). An estimation procedure for agent a is called (ϵ, δ)-convergent if there exists τ a ∈ N such that the probability that the mean estimator µ t a of agent a is ϵ-distant from the true mean for any time t &gt; τ a is at least 1 -δ:</p><formula xml:id="formula_1">P(∀t &gt; τ a ∶ |µ t a -µ a | ≤ ϵ) &gt; 1 -δ. (<label>1</label></formula><formula xml:id="formula_2">)</formula><p>While it is easy to design (ϵ, δ)-convergent estimation procedure for a single agent taken in isolation, the goal of this paper is to propose collaborative algorithms where agents benefit from information from other agents by taking advantage of the relation between the agents' distributions. This will allow them to build up more accurate estimations in less time, i.e., with smaller time complexity τ a .</p><p>Specifically, to foster collaboration between agents, we consider that the set of agents [A] is partitioned into equivalence classes that correspond to agents with the same mean.<ref type="foot" target="#foot_0">1</ref> In real scenarios, these classes may represent sensors in the same environment, objects with the same technical characteristics, users with the same behavior, etc. This assumption makes it possible for an agent to design strategies to identify other agents in the same class and to use their estimates in order to speed up the estimation of his/her own mean. Formally, we define the class of a as the set of agents who have the same mean as a.</p><p>Definition 2 (Similarity class). The similarity class of agent a is given by:</p><formula xml:id="formula_3">C a = {l ∈ [A] ∶ ∆ a,l = 0},</formula><p>where ∆ a,l = |µ a -µ l | is the gap between the means of agent a and agent l.</p><p>The gaps {∆ a,l } a,l∈ <ref type="bibr">[A]</ref> define the problem structure. We consider that the agents do not know the means, the gaps, or even the number of underlying classes. Hence the classes {C a } a∈ <ref type="bibr">[A]</ref> are completely unknown. This makes the problem quite challenging.</p><p>Remark 1 (Scalability). In large-scale systems, it may be impractical for agents to query all other agents and/or to maintain a memory size that is linear in the number of agents A. From a practical point of view, each agent can instead consider a restricted subset of agents of reasonable size. This subset could be picked uniformly at random, be composed of neighboring nodes in the network or in the physical world, or be based on prior knowledge on who is more likely to be in the same class (when available).</p></div>
<div><head n="4">Proposed Approach</head><p>In this section, we first introduce some of the key technical components used in our approach, and then present our proposed algorithm.</p></div>
<div><head n="4.1">Main Concepts</head><p>In our approach, each agent a computes confidence intervals I a,l = [x t a,l -β δ (n t a,l ), xt a,l + β δ (n t a,l )] for the mean estimators [x t a,1 , . . . , xt a,A ] that it holds in memory at time t. The generic confidence bound β δ (n t a,l ) takes as input the number of samples n t a,l seen for agent l at time t, and δ corresponds to the risk parameter so that with probability at least 1 -δ the true mean µ l falls within the confidence interval I a,l .</p><p>Agent a will use these confidence intervals to assess whether another agent l belongs to the class C a through the evaluation of an optimistic distance defined below.</p><p>Definition 3 (Optimistic distance). The optimistic distance with agent l from the perspective of agent a is defined as:</p><formula xml:id="formula_4">d t a,δ (l) = |x t a,a -xt a,l | -β δ (n t a,a ) -β δ (n t a,l ).<label>(2)</label></formula><p>The "optimistic" terminology is justified by the fact that d t a,δ (l) is, with high probability, a lower bound on the distance between the true means µ a and µ l of distributions ν a and ν l . Recall that two agents belong to the same class if the distance of their true mean is zero. Since these values are unknown, the idea of the Algorithm 1 ColME Parameters: agent a, time horizon H, risk δ, weighting scheme α, and query strategy choose_agent ∀l ∈</p><formula xml:id="formula_5">[A]: x0 a,l ← 0, n 0 a,l ← 0 C 0 a ← {l ∈ [A] ∶ d t a,δ (l) ≤ 0} = [A] for t = 1, . . . , H do ∀l ∈ [A]: xt a,l ← xt-1 a,l , n t a,l ← n t-1 a,l Perceive: Receive sample x t a ∼ ν a xt a,a ← xt-1 a,a × t-1 t + x t a × 1 t , n t a,a ← t Query: C t a ← {l ∈ [A] ∶ d t a,δ (l) ≤ 0} Query agent l = choose_agent(C t a ) to get xt l,l xt a,l ← xt l,l , n t a,l ← t Estimate: C t a ← {l ∈ [A] ∶ d t a,δ (l) ≤ 0} µ t a ← ∑ l∈C t a α t a,l × xt a,l</formula><p>end for Output: µ H a above heuristic is to provide a proxy based on observed data and high probability confidence bounds. In particular, we will adopt the Optimism in Face of Uncertainty Principle (OFU) (see <ref type="bibr" target="#b2">Auer et al., 2002)</ref> and consider that two agents may be in the same class if the optimistic distance is zero or less. Hence, we define an optimistic notion of class accordingly.</p><p>Definition 4. The optimistic similarity class from the perspective of agent a at time t is defined as:</p><formula xml:id="formula_6">C t a = {l ∈ [A] ∶ d t a,δ (l) ≤ 0}.</formula><p>Having introduced the above concepts, we can now present our algorithm.</p></div>
<div><head n="4.2">Algorithm</head><p>The collaborative mean estimation algorithm we propose, called ColME, is given in Algorithm 1 (taking the perspective of agent a). For conciseness, we consider that β δ (0) = +∞. At each step t, agent a performs three main steps.</p><p>In the Perceive step, the agent receives a sample from its distribution and updates its local average together with the number of samples.</p><p>In the Query step, agent a selects another agent following a query strategy given as a parameter to the ColME algorithm. Agent a runs the choose_agent function to select another agent l and asks for its current local estimate to update its memory. We propose two variants for the choose_agent function:</p><p>• Round-Robin: cycle over the set [A] of agents one by one in a fixed order.</p><p>• Restricted-Round-Robin: like round-robin but ignores agents that are not in the set of optimistically similar agents C t a .</p><p>The focus on round-robin-style strategies is justified by the information structure of our problem setting, which is very different from classic bandits. Indeed, querying an agent at time t produces an estimate computed on the t observations collected by this agent so far. The choice of variant (Round-Robin or Restricted-Round-Robin) will affect the class identification time complexity, as we shall discuss later.</p><p>Finally, in the Estimate step, agent a computes the optimistic similarity class C t a based on available information, and constructs its mean estimate as a weighted aggregation of the local averages of agents that belong to C t a . We propose different weighting mechanisms:</p><p>Simple weighting. This is a natural weighting mechanism for aggregating samples:</p><formula xml:id="formula_7">α t a,l = n t a,l ∑ l∈C t a n t a,l .</formula><p>Soft weighting. This is a heuristic weighting mechanism which leverages the intuition that the more the confidence intervals of two agents overlap, the more likely that they are in the same class. Moreover, the smaller the union of the agent means, the more confident we are that the agents are in the same class. In other words, we are not equally confident about all the agents that are selected for estimation, and this weighting mechanism incorporates this information:</p><formula xml:id="formula_8">α t a,l = n t a,l |I a,a ∩ I a,l | |I a,a ∪ I a,l | × 1 Z soft ,</formula><p>where</p><formula xml:id="formula_9">Z soft = ∑ i∈C t a n t a,i |Ia,a∪Ia,i| |Ia,a∩Ia,i|</formula><p>is a normalization factor.</p><p>Aggressive weighting. This is an extension of the previous soft weighting mechanism that is more selective. Not only does it consider the overlap and intersection of the agents' confidence intervals, but it also requires the size of the intersection to be larger than half the size of both confidence intervals from the two agents. Let us denote the binary value associated with this condition by E a,l = 1 {|Ia,a∩I a,l |&gt;min{β δ (n t a,l ),β δ (n t a,a )}} . Then</p><formula xml:id="formula_10">α t a,l = n t a,l |I a,a ∩ I a,l | |I a,a ∪ I a,l | × E a,l Z agg ,</formula><p>where</p><formula xml:id="formula_11">Z agg = ∑ i∈C t a n t a,i |Ia,a∪Ia,i|×Ea,i |Ia,a∩Ia,i|</formula><p>is a normalization factor.</p></div>
<div><head n="4.3">Baselines</head><p>We introduce two baselines that will be used to put the performance of our approach into perspective, both theoretically and empirically.</p><p>Local estimation. Estimates are computed without any collaboration, using only samples received from the agent's own distribution, i.e. µ t a = xt a,a .</p><p>Oracle weighting. The agent knows the true class C a via an oracle and uses the simple weighting α t a,l =</p><formula xml:id="formula_12">n t a,l ∑ l∈Ca n t a,l</formula><p>.</p></div>
<div><head n="5">Theoretical Analysis</head><p>In this section, we provide a theoretical analysis of our algorithm ColME for the query strategy Restricted-Round-Robin and the simple weighting scheme. Specifically, we bound the time complexity in probability for both class and mean estimation. Below, we explain the key steps involved in our analysis and state our main results. All proofs can be found in the appendix.</p><p>A key aspect of our analysis is to characterize when the optimistic similarity class (Definition 4) coincides with the true classes. We show that this is the case when two conditions hold. First, for a given agent a, we need the confidence interval computed by a about agent l to contain the true mean µ l for all l ∈ A.</p><p>Definition 5. We define the following events:</p><formula xml:id="formula_13">E t a = ⋂ l∈[A] |x t a,l -µ l | ≤ β δ (n t a,l ),<label>(3)</label></formula><formula xml:id="formula_14">E a = ⋂ t∈N E t a . (<label>4</label></formula><formula xml:id="formula_15">)</formula><p>We can guarantee that E a holds with high probability via an appropriate parameterization of confidence intervals. We use the so-called Laplace method <ref type="bibr" target="#b22">(Maillard, 2019)</ref>.</p><formula xml:id="formula_16">Lemma 1. Let δ ∈ (0, 1), a ∈ [A]. Setting β δ (n) = σ √ 2 1 n × (1 + 1 n ) ln( √ n + 1/γ(δ)) with γ(δ) = δ 8×A , we have: P(E a ) ≥ 1 - δ 8 . (<label>5</label></formula><formula xml:id="formula_17">)</formula><p>The second condition is that agent's a memory about the local estimates of other agents should contain enough samples. Let us denote by ⌈β -1 δ (x)⌉ the smallest integer n such that x &gt; β δ (n). Definition 6. From the perspective of agent a and at time t, event G t a is defined as:</p><formula xml:id="formula_18">G t a = ⋂ l∈[A] n t a,l &gt; n ⋆ a,l , (<label>6</label></formula><formula xml:id="formula_19">)</formula><formula xml:id="formula_20">where n ⋆ a,l = ⎧ ⎪ ⎪ ⎨ ⎪ ⎪ ⎩ ⌈β -1 δ ( ∆ a,l 4 )⌉ if l ∉ C a , ⌈β -1 δ ( ∆a 4 )⌉ otherwise, with ∆ a = min l∈[A]/Ca ∆ a,l .</formula><p>Note that the required number of samples is inversely proportional to the gaps between the means of agents in different classes. Having enough samples and knowing that the true means fall within the confidence bounds, we can show that the class-estimation rule</p><formula xml:id="formula_21">d t a,δ (l) ≤ 0 indicates the membership of l in C a . Lemma 2 (Class membership rule). Under E t a ∧ G t a and ∀l ∈ [A] and at time t: d t a,δ (l) &gt; 0 ⇐⇒ l ∈ [A]/C a .</formula><p>Using the above lemma, we obtain the following result for the time complexity of class estimation.</p><p>Theorem 1 (ColME class estimation time complexity). For any δ ∈ (0, 1), employing Restricted-Round-Robin query strategy, we have:</p><formula xml:id="formula_22">P(∃t &gt; ζ a ∶ C t a ≠ C a ) ≤ δ 8 , with ζ a = n ⋆ a,a + A -1 -∑ l∈[A]∖Ca 1 {n ⋆ a,a &gt;n ⋆ a,l +A-1} . (<label>7</label></formula><formula xml:id="formula_23">)</formula><p>The dominating term in the class estimation time complexity ζ a for agent a is equal to the number n ⋆ a,a of samples required to distinguish agent a from the one who has smallest nonzero gap ∆ a to a, which is of order Õ(1/∆<ref type="foot" target="#foot_1">2</ref> a ). 2 There is then an additional term of A -1 since all others agents that are not in C a could require the same number of samples. Finally, the last term in (7) accounts for agents that require less samples and had thus been eliminated before, which reflects the gain of using Restricted-Round-Robin query strategy over Round-Robin. When we have enough samples (at least ζ a ), Theorem 1 guarantees that we correctly learn the class (C a = C t a ) with high probability. We build upon this result to quantify the mean estimation time complexity of our approach.</p><p>Theorem 2 (ColME mean estimation time complexity). Given the risk parameter δ, using the Restricted-Round-Robin query strategy and simple weighting, the mean estimator µ t a of agent a is (ϵ, δ 4 )-convergent, that is:</p><formula xml:id="formula_24">P(∀t &gt; τ a ∶ |µ t a -µ a | ≤ ϵ) &gt; 1 - δ 4 , with τ a = max(ζ a , ⌈β -1 δ (ϵ)⌉ |C a | + |C a | -1 2 ).<label>(8)</label></formula><p>Several comments are in order. First, recall that collaboration induces a bias in mean estimation before class estimation time. Because the problem structure is unknown, any collaborative algorithm that aggregate observations from different agents will suffer from this bias, but the bias vanishes as soon as the class is estimated and we outperform local estimation.</p><p>Then, to interpret the guarantees provided by Theorem 2, it is useful to compare them with the local estimation baseline, which has time complexity ⌈β -1 δ (ϵ)⌉ = Õ(1/ϵ 2 ). Inspecting (8), we see that our approach has a time complexity of Õ(max{1/∆ 2 a , 1/ϵ 2 |C a |}). In other words, it is faster than local estimation as long as the time ζ a needed to correctly identify the class C a is smaller than ⌈β -1 δ (ϵ)⌉, that is precisely when:</p><formula xml:id="formula_25">ϵ &lt; β δ (n ⋆ a,a + A -1 - ∑ l∈[A]/{Ca} 1 {n ⋆ a,a &gt;n ⋆ a,l +A-1} ).<label>(9)</label></formula><p>This condition, which roughly amounts to ϵ &lt; ∆ a , relates the desired precision of the solution ϵ to the problem structure captured by the gaps {∆ a,l } l∈ <ref type="bibr">[A]</ref> between the true means through {n ⋆ a,l } l∈ <ref type="bibr">[A]</ref> (see Definition 6). We will see in our experiments that our theory predicts quite well whether an agent empirically benefits from collaboration.</p><p>Remarkably, our approach can be up to |C a | times faster than local estimation: this happens roughly when ϵ &lt; ∆ a / √ |C a |, i.e., for large enough gaps or small enough ϵ. In this regime, the speed-up achieved by our approach is nearly optimal. Indeed, the time complexity of the oracle weighting baseline introduced in Section 4.3 is precisely</p><formula xml:id="formula_26">⌈β -1 δ (ϵ)⌉ |Ca| + |Ca|-1 2 = Õ(1/ϵ 2 |C a |)</formula><p>, just like our approach. Note that in a full information setting where agent a would know C a and would also have access to up-to-date samples from all agents at each step, the time complexity would be only slightly smaller, namely</p><formula xml:id="formula_27">⌈β -1 δ (ϵ)⌉</formula><p>|Ca| . Remark 2 (Frequency of communication). For simplicity, we consider that agents communicate each time they collect a new sample, which is standard in the literature of collaborative learning (see for instance the collaborative MAB approaches discussed in Section 2). However, different trade-offs between communication and data collection can be considered. In particular, it is straightforward to adapt the setting and our results to the case where each agent collects m samples between each communication: it amounts to multiplying by m the number of observations in our confidence intervals and empirical estimates. This provides a way to reduce communication, as well as to mitigate privacy concerns by ensuring that only sufficiently aggregated quantities are exchanged (even in early rounds). Extensions to cases where the number of samples between each communication is random and/or varies across agents are left for future work.</p></div>
<div><head n="6">Numerical Results</head><p>In this section, we provide numerical experiments on synthetic data to illustrate our theoretical results and assess the practical performance of our proposed algorithms.<ref type="foot" target="#foot_2">3</ref> </p></div>
<div><head n="6.1">Experimental Setting</head><p>We consider A = 200 agents, a time horizon of 2500 steps and a risk parameter δ = 0.001. The personal distributions of agents are all Gaussian with variance σ 2 = 0.25 and belong to one of 3 classes with means 0.2, 0.4 and 0.8. The class membership of each agent (and thus the value of its true mean) is chosen uniformly at random among the three classes. We thus obtain roughly balanced class sizes. While the evaluation presented in this section focuses on this 3-class problem, in the appendix we provide additional results on a simpler 2-class problem (with means 0.2 and 0.8) where the benefits of our algorithm is even more significant.</p><p>We consider several variants of our algorithm ColME: we compare query strategies Round-Robin and Restricted-Round-Robin with simple weighting, and also evaluate the use of soft and aggressive weighting schemes in the Restricted-Round-Robin case. This gives 4 variants of our algorithm: Round-Robin, Restricted-Round-Robin, Soft-Restricted-Round-Robin and Aggressive-Restricted-Round-Robin.</p><p>Regarding competing approaches, we recall that our setting is novel and we are not aware of existing algorithms addressing the same problem. We can however compare against two baseline strategies. The Local baseline corresponds to the case of no collaboration. On the other hand, the Oracle baseline represents an upper bound on the achievable performance by any collaborative algorithm as it is given as input the true class membership of each agent and thus does not need to perform class estimation.   Table <ref type="table">1</ref>: Empirical class estimation times of Round-Robin and Restricted-RR on the 3-class problem (Gaussian distributions with true means 0.2, 0.4, 0.8). We report the average, standard deviation and maximum across agents and runs. We also report the high-probability class estimation time ζ a given by our theory.</p><p>All algorithms are compared across 20 random runs corresponding to 20 different samples. In a given run, at each time step, each agent receives the same sample for all algorithms.</p></div>
<div><head n="6.2">Class Estimation</head><p>We start by investigating the performance in class estimation. In this experiment, only Round-Robin and Restricted-Round-Robin are shown since the different weighting schemes have no effect on class estimation.</p><p>We first look at how well an agent a estimates its true class C a with its heuristic class C t a across time. To measure this, we consider the precision at time t computed as follows:</p><formula xml:id="formula_28">precision C t a = |C t a ∩ C a | |C t a | . (<label>10</label></formula><formula xml:id="formula_29">)</formula><p>We compute the average and standard deviation of (10) across runs, and then average these over all agents. Figure <ref type="figure" target="#fig_1">1</ref>(a) shows how the precision of class estimation varies across time as agents progressively remove others from their heuristic class and eventually identify their true class. As can be seen clearly in Figure <ref type="figure">3</ref> in the appendix, the classes 0.2 and 0.8 are separated very early, quickly followed by 0.4 and 0.8 and finally, after sufficiently many samples have been collected, the pair with the smallest gap (0.4 and 0.2).</p><p>We also observe that Round-Robin and Restricted-Round-Robin only differ slightly in the last time steps before classes are identified. This is in line with Theorem 1, which shows that class estimation time mainly depends on n ⋆ a,a , the time needed to eliminate the agent with smallest nonzero gap. This dominant term and the fact that querying an agent at time t yields the full statistics of observations of this agent up to time t explain that the gain of Restricted-Round-Robin is small compared to vanilla Round-Robin.</p><p>Table <ref type="table">1</ref> shows statistics about the average, standard deviation and maximum time taken by an agent to correctly identify its class. As expected, agents from class 0.8 identify their class much faster as the gaps Table <ref type="table">2</ref>: Empirical convergence times (see Eq. 12) of different algorithms on the 3-class problem (Gaussian distributions with true means 0.2, 0.4, 0.8) for a target estimation error of ϵ = 0.1 (unfavorable regime) and ϵ = 0.01 (favorable regime). We report the average, standard deviation and maximum across agents and runs. We also report the high-probability mean estimation times τ a given by our theory for Restricted-Round-Robin and Local. In line with our theory, we see that our approach largely outperforms the local estimation baseline in the favorable regime and remains competitive in the unfavorable regime.</p><p>with respect to other classes are larger. Table <ref type="table">1</ref> also reports the high-probability class estimation time ζ a of Restricted-Round-Robin given by our theoretical analysis (Theorem 1). This theoretical value is rather close to the maximum value we observe: although these values are not directly comparable, it suggests that our analysis captures the correct order of magnitude.</p></div>
<div><head n="6.3">Mean Estimation</head><p>We now turn to our main objective: mean estimation. The error of an agent a at time t is evaluated as the absolute difference of its mean estimate with its true mean:</p><formula xml:id="formula_30">error t a = |µ t a -µ a |. (<label>11</label></formula><formula xml:id="formula_31">)</formula><p>Similar to above, we compute the average and standard deviation of this quantity across runs, and then for each time step we report in Figure <ref type="figure" target="#fig_1">1</ref>(b) the average of these quantities across all agents for the different algorithms (Soft-Restricted-Round-Robin, Aggressive-Restricted-Round-Robin, Restricted-Round-Robin, Round-Robin, Oracle, and Local).</p><p>As expected, all variants of ColME suffer from mean estimation bias in the early steps (due to optimistic class estimation). However, as the estimated class of each agent gets more precise (see Figure <ref type="figure" target="#fig_1">1(a)</ref>), agents progressively eliminate this bias and eventually learn estimates with similar error and variance as the Oracle baseline. On the other hand, Local does not have estimation bias (hence achieves smaller error on average in early rounds) but exhibits much higher variance, and its average error converges very slowly towards zero. These results show the ability of our collaborative algorithms to construct highly accurate mean estimates much faster than without collaboration. We can also see that Soft-Restricted-Round-Robin and Aggressive-Restricted-Round-Robin converge much quicker to low error estimates than Restricted-Round-Robin. This shows that our proposed heuristic weighting schemes successfully reduce the relative weight given to agents that actually belong to different classes well before they are identified as such with sufficient confidence. The aggressive weighting scheme is observed to perform best in practice.</p><p>In the appendix, we plot the error in mean estimation for each class separately and observe that agents with mean 0.8 (i.e., in the class that is easiest to discriminate from others) are the fastest to reach highly accurate estimates, followed by those with mean 0.4, and finally those with mean 0.2.</p><p>Finally, we quantitatively compare the convergence time of different algorithms with an empirical measure inspired by our theoretical PAC criterion (Definition 1). We define the empirical convergence time of an agent as the earliest time step where the estimation error of the agent always stays lower than some ϵ:</p><formula xml:id="formula_32">conv a (ϵ) = min{τ ∈ N ∶ ∀t ≥ τ, error t a ≤ ϵ}. (<label>12</label></formula><formula xml:id="formula_33">)</formula><p>We denote by conv(ϵ) the average of the above quantity across all runs and all agents.</p><p>Table <ref type="table">2</ref> reports the average, standard deviation and maximum empirical convergence time across agents and runs for two values of ϵ (we also provide per-class tables in the appendix). These values were chosen to reflect the two different regimes suggested by our theoretical analysis. Indeed, recall that our theory gives a criterion to predict whether our collaborative algorithms will outperform the Local baseline: this is the case when the desired accuracy of the solution ϵ is small enough for the given problem instance (see Eq. 9).</p><p>For the problem considered here, Eq. ( <ref type="formula" target="#formula_25">9</ref>) gives that Restricted-Round-Robin will outperform Local for all agents as soon as ϵ is smaller than 0.049. We thus choose ϵ = 0.01 as the favorable regime (where we should beat Local) and ϵ = 0.1 as the unfavorable regime. We provide the corresponding high-probability mean estimation times τ a of Restricted-Round-Robin and Local given by our theoretical analysis (Theorem 2).</p><p>The results in Table <ref type="table">2</ref> are consistent with our theory. All variants of our algorithms largely outperform Local for ϵ = 0.01,<ref type="foot" target="#foot_3">4</ref> while Local is better for ϵ = 0.1 as agents can reach this precision using only their own samples faster than they can reliably estimate their class. Overall, Restricted-Round-Robin performs marginally better than Round-Robin, while Soft-Restricted-Round-Robin and Aggressive-Restricted-Round-Robin significantly outperform Round-Robin and Restricted-Round-Robin in both cases. Note that Soft-Restricted-Round-Robin and Aggressive-Restricted-Round-Robin perform roughly the same as Local in the unfavorable regime, and get very close to the performance of the Oracle baseline in the favorable regime. These results again show the relevance of our collaborative algorithms and heuristic weighting schemes. We observe that our theoretical results get looser as ϵ → 0, which is somewhat expected.</p></div>
<div><head n="7">Extension to Imperfect Classes</head><p>So far we have assumed that two agents are in the same class if their personal distributions have exactly the same mean, which can be restrictive for some use-cases. In this section, we show that we can extend the problem setup and our approach to the case where two agents are considered to be in the same class if their means are close enough and agents seek to estimate the mean of their class.</p><p>Formally, we define a new notion of similarity class parameterized by a radius η, which generalizes our previous notion introduced in Definition 2.</p><p>Definition 7. Given η &gt; 0, the η-similarity class of agent a is given by:</p><formula xml:id="formula_34">C η,a = {l ∈ [A] ∶ ∆ a,l ≤ η},</formula><p>where ∆ a,l = |µ a -µ l | is the gap between the means of agent a and agent l.</p><p>This notion of "imperfect" similarity class allows to capture situations where clusters of agents have similar (but not necessarily equal) means. Such discrepancies between the means of agents in the same class may for instance stem from the presence of local measurement bias (e.g., due to local variations in the environment, see <ref type="bibr" target="#b34">Taghavi et al., 2016)</ref>. They can also be used to model groups of agents with similar preferences, behavior, or goals, in applications like collaborative filtering <ref type="bibr">(Su &amp; Khoshgoftaar, 2009b)</ref>,</p><p>In this context, it is natural to slightly redefine the estimation objective. Instead of estimating its personal mean µ a as considered so far, each agent a aims to estimate the mean of its class:</p><formula xml:id="formula_35">µ η,a = 1 |C η,a | ∑ l∈Cη,a µ l . (<label>13</label></formula><formula xml:id="formula_36">)</formula><p>For instance, in the presence of (centered) local measurement bias, estimating the class mean (instead of the local mean) allows to debias the estimate.</p><p>Remark 3 (Non-separated clusters). We do not formally require that the η-similarity classes form separated clusters, in the sense that for three distinct agents a, l, i ∈ [A] we may have simultaneously i ∈ C η,a , i ∈ C η,l and C η,a ≠ C η,l . This happens when ∆ a,i ≤ η, ∆ l,i ≤ η and η &lt; ∆ a,l ≤ 2η. In this case, the "class" of an agent simply corresponds to a ball of radius η around its mean, which potentially overlaps with others and thus violates the transitivity property of equivalence classes. For consistency with the rest of the paper and with an slight abuse of terminology, we continue to use the term "class". Although the case of separated clusters appears more natural, we note that our proposed approach will still work in the non-separated setting, in the sense that agents will correctly estimate the mean of their class as defined in Eq. 13.</p><p>Based on the above, we can adapt the notion of optimistic similarity class (Definition 2) and the condition on the number of samples required for this optimistic class to coincide with the true class (Definition 6) by incorporating η. Definition 8. The η-optimistic similarity class from the perspective of agent a at time t is defined as:</p><formula xml:id="formula_37">C t η,a = {l ∈ [A] ∶ d t a,δ (l) ≤ η}. Definition 9.</formula><p>From the perspective of agent a and at time t, event G t η,a is defined as:</p><formula xml:id="formula_38">G t η,a = ⋂ l∈[A] n t a,l &gt; n η a,l ,<label>(14)</label></formula><p>where</p><formula xml:id="formula_39">n η a,l = ⎧ ⎪ ⎪ ⎨ ⎪ ⎪ ⎩ ⌈β -1 δ ( ∆ a,l -η 4 )⌉ if l ∉ C a , ⌈β -1 δ ( ∆η,a-η 4 )⌉ otherwise, with ∆ η,a = min l∈[A]/Cη,a ∆ a,l . Lemma 3 (Class membership rule). Under E t a ∧ G t η,a and ∀l ∈ [A] and at time t: d t a,δ (l) &gt; η ⇐⇒ l ∈ [A]/C η,a .</formula><p>We can see from the above that ruling out an agent l from the optimistic class C η,a requires more samples for larger η, which is expected as the size of the confidence interval needs to be smaller to make this decision reliably.</p><p>With these tools in place, we can use our collaborative mean estimation algorithm ColME (Algorithm 1) presented before, with only minor modifications: we simply need to replace the notion of optimistic similarity class by the η-version of Definition 8, and compute the estimate µ t η,a at time t using a simple class-uniform weighting scheme α t a,l = 1 |C t η,a | to match the objective in Eq. 13. We refer to this algorithm as η-ColME. Note that η becomes a parameter of the algorithm, allowing to choose the desired radius for the class structure.</p><p>We can now state the class and mean estimation complexity of η-ColME. The proofs can be found in the appendix. Theorem 3 (η-ColME class estimation time complexity). For any δ ∈ (0, 1), employing Restricted-Round-Robin query strategy, we have:</p><formula xml:id="formula_40">P(∃t &gt; ζ η a ∶ C t η,a ≠ C η,a ) ≤ δ 8 , with ζ η a = n η a,a + A -1 - ∑ l∈[A]∖Cη,a 1 {n η a,a &gt;n η a,l +A-1} . (<label>15</label></formula><formula xml:id="formula_41">)</formula><p>Theorem 4 (η-ColME mean estimation time complexity). Given the risk parameter δ, using the Restricted-Round-Robin query strategy and class-uniform weighting (while employing C η,a ), the mean estimator µ t a of agent a is (ϵ, δ 4 )-convergent, that is:</p><formula xml:id="formula_42">P(∀t &gt; τ η a ∶ |µ t η,a -µ η,a | ≤ ϵ) &gt; 1 - δ 4 , with τ η a = max(ζ η a , β -1 δ (ϵ) + |C η,a | -1). (<label>16</label></formula><formula xml:id="formula_43">)</formula><p>The class estimation result (Theorem 3) is similar to its "perfect" class counterpart (Theorem 1) except that it involves η-dependent quantities. The mean estimation result (Theorem 4) differs slightly more from the perfect class case (Theorem 2) because the estimation objective (see Eq. 13) and weighting scheme are different. But most importantly, we see that for large enough gaps or small enough precision ϵ (similar to Eq. 9), we again achieve an optimal speed since the time complexity of an oracle weighting baseline that would know the true classes beforehand is β -1 δ (ϵ) + |C η,a | -1.</p></div>
<div><head n="8">Conclusion</head><p>We have presented collaborative online algorithms where each agent learns the set (class) of agents who shares the same (or similar) objective and uses this knowledge to speed up the estimation of its personalized mean. We have provided PAC-style guarantees for the class and mean estimation time complexity of our algorithms. In addition, we have introduced a number of sample weighting mechanisms to decrease the bias of the estimates in the early rounds of learning, whose benefits are illustrated empirically.</p><p>Our work initiates the study of online, collaborative and personalized estimation and learning problems, which we believe to be a promising area for future work. We outline a few interesting directions below.</p><p>Optimistic vs conservative. Instead of the optimistic approach taken in this work, one could try to design a more conservative algorithm where an agent incorporates the estimate of another agent only when it knows (with sufficient probability) that it belongs to the same class. This would avoid introducing bias in the estimation in early steps. However, a downside of such an approach is that agents would typically need some knowledge of the gaps between their true means in order to determine when another agent can be considered to be in the same class, which would be a big practical limitation.</p><p>Large-scale variants. While a simple way to scale up our approach to a large number of agents is to have each agent focus on a restricted subset of other agents (see Remark 1), an interesting direction to allow more exploration in large-scale systems could rely on the idea of peer sampling <ref type="bibr" target="#b15">(Jelasity et al., 2007)</ref>, i.e., randomly sampling a few agents from time to time to discover potential new members of the class beyond the initial subset.</p><p>Handling data drift. We would like to extend our approach to handle data drift, where the means of agents can change over time. Here, one could try to adapt ideas from non-stationary bandits, such as slidingwindow UCB <ref type="bibr" target="#b12">(Garivier &amp; Moulines, 2011)</ref> or UCB strategies mixed with change-point detection algorithms <ref type="bibr" target="#b6">(Cao et al., 2019)</ref>.</p><p>Privacy guarantees. In use cases where data is considered sensitive (e.g., personal data), it is important to provide strong privacy guarantees to the agents. While our approach only requires agents to share aggregate quantities (see also Remark 2), these may still leak sensitive information. In future work, we would like to use tools from differential privacy <ref type="bibr" target="#b9">(Dwork &amp; Roth, 2013)</ref>, such as the tree aggregation technique for sharing cumulative sums in an online way <ref type="bibr" target="#b10">(Dwork et al., 2010;</ref><ref type="bibr" target="#b7">Chan et al., 2011)</ref>, to provide formal privacy guarantees and analyze the resulting trade-offs between privacy and utility.</p><p>Extensions to personalized learning tasks. Finally, the problem could be extended to cases where each agent aims to solve a personalized machine learning task <ref type="bibr" target="#b36">(Vanhaesebrouck et al., 2017)</ref> based on the data it receives online. In that case, a structure in the distribution conditioned by the outputs of the learned models can be inferred, introducing an interesting exploration-exploitation dilemma in the learning task.</p><formula xml:id="formula_44">P(∃t ∈ N, µ t a -µ a ≥ σ √ 2 t (1 + 1 t ) ln( √ t + 1/δ)) ≤ δ , (<label>17</label></formula><formula xml:id="formula_45">) P(∃t ∈ N, µ a -µ t a ≥ σ √ 2 t (1 + 1 t ) ln( √ t + 1/δ)) ≤ δ . (<label>18</label></formula><formula xml:id="formula_46">)</formula><p>Proof. The two inequalities are proved in the same way as a direct consequence of <ref type="bibr" target="#b22">Maillard (2019)</ref>, Lemma 2.7 therein. Let Y 1 , . . . , Y t be a sequence of independent real-valued random variables where for each s ≤ t, Y s has mean µ s and is σ s -sub-Gaussian, then for all δ ∈ (0, 1), it holds that</p><formula xml:id="formula_47">P(∃t ∈ N, t ∑ s=1 (Y s -µ s ) ≥ 2 t ∑ s=1 σ 2 s (1 + 1 t ) ln( √ t + 1/δ)) ≤ δ .</formula><p>When all random variables Y s have the same mean µ a and variance σ, we have</p><formula xml:id="formula_48">P(∃t ∈ N, t ∑ s=1 (Y s -µ a ) ≥ √ 2tσ 2 (1 + 1 t ) ln( √ t + 1/δ)) ≤ δ,</formula><p>Taking the average rather than the sum, i.e. dividing both sides by t we obtain that:</p><formula xml:id="formula_49">P(∃t ∈ N, t ∑ s=1 ( Y s t - µ a t ) ≥ √ 2 t σ 2 (1 + 1 t ) ln( √ t + 1/δ)) ≤ δ, P(∃t ∈ N, t ∑ s=1 Y s t -µ a ≥ √ 2 t σ 2 (1 + 1 t ) ln( √ t + 1/δ)) ≤ δ .</formula><p>And denoting µ t a = ∑ t s=1 Ys t , we conclude</p><formula xml:id="formula_50">P(∃t ∈ N, µ t a -µ a ≥ σ √ 2 t (1 + 1 t ) ln( √ t + 1/δ)) ≤ δ . Lemma 1. Let δ ∈ (0, 1), a ∈ [A]. Setting β δ (n) = σ √ 2 1 n × (1 + 1 n ) ln( √ n + 1/γ(δ)) with γ(δ) = δ 8×A , we have: P(E a ) ≥ 1 - δ 8 . (5) Proof. Let us recall that E a = ⋂ t∈N ⋂ l∈[A] |x t a,l -µ l | ≤ β δ (n t a,l ). Then P(E a ) = 1 -P( Ēa ), = 1 -P(∃t ∈ N, ∃l ∈ [A] ∶ |x t a,l -µ l | &gt; β δ (n t a,l )), ≥ 1 -∑ l∈[A] P(∃t ∈ N ∶ |x t a,l -µ l | &gt; β δ (n t a,l )).</formula><p>defining γ(δ) = δ 8×A and using Lemma 4</p><formula xml:id="formula_51">P(E a ) ≥ 1 -∑ l∈[A] P(∃t ∈ N ∶ |x t a,l -µ l | &gt; σ 2 n t a,l × (1 + 1 n t a,l ) ln( √ n t a,l + 1/γ(δ))), ≥ 1 -∑ l∈[A] γ(δ) = 1 -∑ l∈[A] δ 8A = 1 - δ 8 .</formula></div>
<div><head>Appendix B Proof of Theorem 1</head><p>In this section, we prove Theorem 1. We first show Lemma 2 using two technical lemmas. We then prove Lemma 7, which combined with Lemma 2, yields the main result (Theorem 1). Let us first remark that</p><formula xml:id="formula_52">d t a,δ (l) = |x t a,a -xt a,l | -β δ (n t a,a ) -β δ (n t a,l ), = |(x t a,a -µ a ) -(x t a,l -µ l ) + (µ a -µ l )| -β δ (n t a,a ) -β δ (n t a,l ).</formula><p>As a consequence we have</p><formula xml:id="formula_53">d t a,δ (l) ≤ ∆ a,l + |x t a,a -µ a | + |x t a,l -µ l | -β δ (n t a,a ) -β δ (n t a,l ). (<label>19</label></formula><formula xml:id="formula_54">) d t a,δ (l) ≥ ∆ a,l -|x t a,a -µ a | -|x t a,l -µ l | -β δ (n t a,a ) -β δ (n t a,l ). (<label>20</label></formula><formula xml:id="formula_55">) Lemma 5. Under E a , ∀l ∈ [A], if l / ∈ C a then ∀n t a,l ≥ n ⋆ a,l = ⌈β -1 δ ( ∆ a,l 4 )⌉ we have d t a,δ (l) &gt; 0.</formula><p>Proof. Under E a , we have |x t a,l -µ l | ≤ β δ (n t a,l ) and |x t a,a -µ a | ≤ β δ (n t a,a ). Since n t a,a ≥ n t a,l , we also have β δ (n t a,a ) ≤ β δ (n t a,l ). Hence, using (20),</p><formula xml:id="formula_56">d t a,δ (l) ≥ ∆ a,l -2β δ (n t a,a ) -2β δ (n t a,l ) ≥ ∆ a,l -4β δ (n t a,l ). If l ∈ C a then ∆ a,l = 0 and since β δ (n t a,l ) &gt; 0 we cannot ensure that ∆ a,l -4β δ (n t a,l ) &gt; 0. If l / ∈ C a then to ensure that d t a,δ (l) ≥ ∆ a,l -4β δ (n t a,l ) &gt; 0, we need that ∆ a,l 4 &gt; β δ (n t a,l ) and hence n ⋆ a,l = ⌈β -1 δ ( ∆ a,l 4 )⌉. 5 Lemma 6. Under E a , ∀l ∈ [A], ∀t ∈ N, if l ∈ C a then d t a,δ (l) ≤ 0.</formula><p>Proof. Again, recall that under E t a , we have |x t a,l -µ l | ≤ β δ (n t a,l ) and |x t a,a -µ a | ≤ β δ (n t a,a ). Hence, using (19), d t a,δ (l) ≤ ∆ a,l + β δ (n t a,a ) + β δ (n t a,l ) -β δ (n t a,a ) -β δ (n t a,l ) = ∆ a,l . If l ∈ C a then ∆ a,l = 0 and thus d t a,δ (l) ≤ 0.</p><p>We can now prove Lemma 2, which we restate here for convenience.  </p><formula xml:id="formula_57">ζ a = n ⋆ a,a -1 + A -∑ l∈[A]∖Ca 1 {n ⋆ a,a &gt;n ⋆ a,l -1+A} . Proof. According to Algorithm 1, C 0 a = [A]</formula><formula xml:id="formula_58">ζ a = n ⋆ a,a -1 + A -∑ l∈[A]∖Ca 1 {n ⋆ a,a &gt;n ⋆ a,l -1+A} .</formula><p>Finally, we use the above lemmas to prove Theorem 1, which we restate below for convenience. </p></div>
<div><head>Appendix C Proof of Theorem 2</head><p>In this section we detail the proof of Theorem 2, about the PAC mean estimation properties of the ColME strategy. We restate the theorem below for convenience.</p><p>Theorem 2 (ColME mean estimation time complexity). Given the risk parameter δ, using the Restricted-Round-Robin query strategy and simple weighting, the mean estimator µ t a of agent a is (ϵ, δ 4 )-convergent, that is: </p><formula xml:id="formula_59">P(∀t &gt; τ a ∶ |µ t a -µ a | ≤ ϵ) &gt; 1 - δ 4 , with τ a = max(ζ a , ⌈β -1 δ (ϵ)⌉ |C a | + |C a | -1 2 ).<label>(8</label></formula></div>
<div><head>Appendix E Proof of Theorem 4</head><p>In this section we detail the proof of Theorem 4, about the PAC mean estimation properties of the η-ColME strategy. We restate the theorem below for convenience.</p><p>Theorem 4 (η-ColME mean estimation time complexity). Given the risk parameter δ, using the Restricted-Round-Robin query strategy and class-uniform weighting (while employing C η,a ), the mean estimator µ t a of agent a is (ϵ, δ 4 )-convergent, that is: Remark that µ t a is not equivalent to the average of all the samples of agents in C η,a : it is the average of the mean values for each agent in C η,a . Therefore, although some agents may have more samples than the others, all are assigned uniform weights. We would like to have |µ t η,a -µ η,a | ≤ ϵ. When E a holds, we can rewrite this as A sufficient condition for the above inequality to hold is to ensure that each term is bounded by ϵ:</p><formula xml:id="formula_60">P(∀t &gt; τ η a ∶ |µ t η,a -µ η,a | ≤ ϵ) &gt; 1 - δ 4 ,</formula><formula xml:id="formula_61">∀l ∈ C η,a ∶ |x t a,l -µ l | ≤ ϵ (21)</formula><p>This is achieved when β δ (n t a,l ) &lt; ϵ for all l ∈ C η,a . Since we are using Restricted-Round-Robin and also that C t η,a = C η,a , the number of samples required for each agent in C η,a are n t a,1 , n t a,1 -1, n t a,1 -2, . . . , n t a,1 -|C η,a |+1 where we consider the one with the maximum number of observations to have index 1 for notation simplicity (which corresponds to index a). For Eq. 21 to hold, it is thus sufficient to have: </p><formula xml:id="formula_62">β -1 δ (ϵ) &lt; n t a,a -|C η,a | + 1 β -1 δ (ϵ) + |C η,a | -1 &lt; n t a,a</formula></div><figure xml:id="fig_0"><head /><label /><figDesc>Error in mean estimation over all agents</figDesc></figure>
<figure xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Results across time on the 3-class problem (Gaussian distributions with true means 0.2, 0.4, 0.8). Thanks to our collaborative algorithms (Soft-Restricted-Round-Robin, Aggressive-Restricted-Round-Robin, Restricted-Round-Robin, Round-Robin), agents are able to estimate their true class (Fig. 1(a)) and thereby obtain accurate mean estimates much more quickly than using purely local estimation (Fig. 1(b)).</figDesc></figure>
<figure xml:id="fig_2"><head /><label /><figDesc>µ l | ≤ |C η,a | × ϵ,</figDesc></figure>
<figure type="table" xml:id="tab_4"><head /><label /><figDesc>Proof. From Lemma 7 and Lemma 2, we deduce that if E a holds and knowing thatC t a = {l ∈ [A] ∶ d t a,δ (l) ≤ 0} then ∀t &gt; ζ a , C a = C t a . Hence, P(∀t &gt; ζ a , C a = C t a ) ≥ P(E a ) ≥ 1 -δ/8 using Lemma 1.</figDesc><table><row><cell cols="5">Theorem 1 (ColME class estimation time complexity). For any δ ∈ (0, 1), employing Restricted-Round-</cell></row><row><cell>Robin query strategy, we have:</cell><cell /><cell /><cell /><cell /></row><row><cell>P(∃t &gt; ζ a ∶ C t a ≠ C a ) ≤</cell><cell>δ 8</cell><cell>l∈[A]∖Ca a,a + A -1 -∑ , with ζ a = n ⋆</cell><cell>1 {n ⋆ a,a &gt;n ⋆ a,l +A-1} .</cell><cell>(7)</cell></row></table></figure>
<figure type="table" xml:id="tab_5"><head>)</head><label /><figDesc>Proof. Let us assume that at time t we have C t a = C a . Therefore is the sum of all n t a,l samples received by all agents l in C a . In other words, µ t a is the estimation of µ a with ∑ l∈Ca n t a,l examples. Hence in order to have |µ t a -µ a | ≤ ϵ when E a holds, we should have β(∑ l∈Ca n t a,l ) ≤ ϵ. Let us see at what time denoted by n ϵ,a we have ⌈β -1 (ϵ)⌉ = ∑ l∈Ca n t a,l . With Algorithm 13 using Restricted-Round-Robin, we know that when C t a = C a , then only members of C a are queried. Therefore,⌈β -1 (ϵ)⌉ = n ϵ,a + (n ϵ,a -1) + ⋅ ⋅ ⋅ + (n ϵ,a -|C a | + 1) = |C a |n ϵ,a -|C a | -1 2 |C a |, As a summary, if E a holds, then we have ∀t ≥ n ϵ,a , C t a = C a implies that |µ t a -µ a | ≤ ϵ. Now, following Theorem 1, we have P(∃t &gt; ζ a ∶ C t a ≠ C a ) ≤ δ 8 . Since τ a = max(ζ a , n ϵ,a ) ≥ ζ a , then P(∃t &gt; τ a ∶ |µ t a -µ a | &gt; ϵ) ≤ Under E a , ∀l ∈ [A], ∀t ∈ N, if l ∈ C η,a then d t a,δ (l) ≤ η. Lemma 10. Under E a , and using Restricted-Round-Robin algorithm, G t η,a holds when t &gt; ζ η a where</figDesc><table><row><cell cols="4">Lemma 9. ζ η a = n η a,a -1 + A -</cell><cell cols="3">∑ l∈[A]∖Cη,a</cell><cell>1 {n η a,a &gt;n η a,l -1+A} .</cell></row><row><cell /><cell>µ t a = ∑ l∈Ca</cell><cell cols="3">xt a,l α t a,l =</cell><cell cols="2">∑ l∈Ca ∑ l∈Ca n t xt a,l n t a,l a,l</cell><cell>.</cell></row><row><cell>Remark that ∑ l∈Ca</cell><cell cols="2">xt a,l n t a,l n ϵ,a =</cell><cell cols="2">⌈β -1 (ϵ)⌉ |C a |</cell><cell>+</cell><cell>|C a | -1 2</cell><cell>.</cell></row><row><cell>δ 8 + P( Ēa ) = δ 4 .</cell><cell /><cell /><cell /><cell /><cell /></row></table></figure>
<figure type="table" xml:id="tab_6"><head /><label /><figDesc>with τ η a = max(ζ η a , β -1 δ (ϵ) + |C η,a | -1). (16)Proof. Since t &gt; τ η a &gt; ζ η a , at time t we have C t η,a = C η,a . Therefore</figDesc><table><row><cell>µ t a = ∑</cell><cell>xt a,l α t a,l =</cell><cell>∑ l∈Cη,a</cell><cell>xt</cell></row><row><cell>l∈Cη,a</cell><cell /><cell /><cell /></row></table><note><p>a,l |C η,a | .</p></note></figure>
<figure type="table" xml:id="tab_7"><head /><label /><figDesc>Therefore τ η a = max(ζ η a , β -1 δ (ϵ) + |C η,a | -1). As a summary, if E a holds, then we have ∀t ≥ τ η a , C t η,a = C η,a implies that |µ t η,a -µ η,a | ≤ ϵ. Now, following Theorem 3, we have P(∃t &gt; ζ η</figDesc><table><row><cell>δ 8 + P( Ēa ) = δ 4 .</cell><cell>a ∶ C t η,a ≠ C η,a ) ≤ δ 8 . Since τ η a = max(ζ η a , n η ϵ,a ) ≥ ζ η a , then P(∃t &gt; τ η a ∶ |µ t a -µ η,a | &gt; ϵ) ≤</cell></row></table></figure>
			<note place="foot" n="1" xml:id="foot_0"><p>In Section 7, we will consider a relaxed version of this assumption where classes consist of agents with similar (not necessarily equal) means.</p></note>
			<note place="foot" n="2" xml:id="foot_1"><p>We use Õ(⋅) to hide constant and logarithmic terms.</p></note>
			<note place="foot" n="3" xml:id="foot_2"><p>The code can be found at https://github.com/llvllahsa/CollaborativePersonalizedMeanEstimation</p></note>
			<note place="foot" n="4" xml:id="foot_3"><p>We had to run Local for a much larger time horizon of 30000 steps for all runs to converge to accuracy ϵ = 0.01.</p></note>
			<note place="foot" n="5" xml:id="foot_4"><p>In extremely rare cases, the expression β -1 δ ( ∆ a,l 4 ) could be an integer and we should add 1 to get a strict inequality. But for conciseness of the expression, we omit the +1 in the definition of n ⋆ a,l .</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>The authors thank the reviewers for their insightful comments that allowed to improve the paper. This work was funded in part by <rs type="funder">Métropole Européenne de Lille (MEL)</rs>, <rs type="funder">Inria</rs>, <rs type="funder">Université de Lille</rs>, and the <rs type="funder">I-SITE ULNE</rs> through the <rs type="projectName">AI chair Apprenf</rs> number <rs type="grantNumber">R-PILOTE-19-004-APPRENF</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_4v3q8Sg">
					<idno type="grant-number">R-PILOTE-19-004-APPRENF</idno>
					<orgName type="project" subtype="full">AI chair Apprenf</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div><head>Appendix A Proof of Lemma 1</head><p>Lemma 4. Let µ t a be the mean value of t independent real-valued random variables with the true mean µ a and is σ-sub-gaussian. For all δ ∈ (0, 1), it holds:</p></div>
<div><head>Appendix D Proof of Theorem 3</head><p>The proof of Theorem 3 follows the same step as that of Theorem 1, up to replacing the 0 threshold by η. We only state the intermediate lemmas (which are adaptations of Lemmas 5-6-7) and omit the detailed proof.</p><p>Lemma 8. Under E a , ∀l ∈ [A], if l / ∈ C η,a then ∀n t a,l ≥ n η a,l = ⌈β -1 δ ( ∆ a,l -η 4</p><p>)⌉ we have d t a,δ (l) &gt; η.</p></div>
<div><head>Appendix F Additional Experimental Results</head><p>In this section we provide additional illustrative results to better understand different aspects of our ColME algorithm. </p></div>
<div><head>F.1 Per-Class behavior on the 3-class problem</head><p>For the 3-class problem described in the main text, we provide complementary figures to show the error in mean estimation in each class separately. These plots are shown in Figure <ref type="figure">2</ref>. We can see that the average class identification time (represented by yellow dots) is different for different classes. For instance, as the gap between the class with mean 0.8 and the other two is larger, this class requires less samples to be identified. Indeed, the average class identification time is less than t = 400 for that class (Figure <ref type="figure">2(c</ref>)), while it is about t = 1400 for the other two (Figures 2(a)-2(b)). Therefore, agents from the class with mean 0.8 reach a highly accurate estimate much faster than agents from other classes.</p><p>We also show the class estimation precision across each pair of classes in Figure <ref type="figure">3</ref>. We see that classes 0.2 and 0.8, who have the biggest gap, are separated first (Figure <ref type="figure">3(b)</ref>), then 0.4 and 0.8 (Figure <ref type="figure">3(c)</ref>) and finally 0.2 and 0.4 (Figure <ref type="figure">3(a)</ref>).</p><p>Finally, for mean estimation, we provide the per-class counterparts of Table <ref type="table">2</ref><ref type="table">3</ref><ref type="table">4</ref><ref type="table">5</ref>. In line with previous results on class estimation, agents of class 0.8 are the ones that converge faster to the desired mean estimation accuracy. Interestingly, observe that for ϵ = 0.1, agents of class 0.2 converge almost as fast as those from class 0.8: this is because they do not need to eliminate all agents from class 0.4 to reach this accuracy. This illustrates how our approach can naturally adapt to the gaps and desired estimation accuracy.</p></div>
<div><head>F.2 Results on a 2-class problem</head><p>We experiment with a 2-class problem generated in the same way as the 3-class problem considered in the main text, except that the means are chosen among {0.2, 0.8}. This makes the problem easier since the gap between the two classes corresponds to the largest gap in the 3-class problem. The results shown in Figure <ref type="figure">4</ref> reflect this: agents correctly identify their class and reach highly accurate mean estimates much faster than  Table <ref type="table">3</ref>: Empirical convergence times (see Eq. 12) for class 0.2 of different algorithms on the 3-class problem (Gaussian distributions with true means 0.2, 0.4, 0.8) for a target estimation error of ϵ = 0.1 (unfavorable regime) and ϵ = 0.01 (favorable regime). We report the average, standard deviation and maximum across agents and runs. We also report the high-probability mean estimation times τ a given by our theory for Restricted-Round-Robin and Local.</p><p>in the 3-class problem. Consequently, the improvement compared to Local is even more significant and our approach almost matches the performance of Oracle. We omit the per-class figures as they are essentially the same as Figure <ref type="figure">4</ref>(b).  Table <ref type="table">5</ref>: Empirical convergence times (see Eq. 12) for class 0.8 of different algorithms on the 3-class problem (Gaussian distributions with true means 0.2, 0.4, 0.8) for a target estimation error of ϵ = 0.1 (unfavorable regime) and ϵ = 0.01 (favorable regime). We report the average, standard deviation and maximum across agents and runs. We also report the high-probability mean estimation times τ a given by our theory for Restricted-Round-Robin and Local.  </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Machine learning and data analytics for the iot</title>
		<author>
			<persName><forename type="first">Erwin</forename><surname>Adi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adnan</forename><surname>Anwar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zubair</forename><surname>Baig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sherali</forename><surname>Zeadally</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computing and Applications</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">20</biblScope>
			<biblScope unit="page" from="16205" to="16233" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Best arm identification in multi-armed bandits</title>
		<author>
			<persName><forename type="first">Jean-Yves</forename><surname>Audibert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sébastien</forename><surname>Bubeck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rémi</forename><surname>Munos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLT</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Finite-time analysis of the multiarmed bandit problem</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Auer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolo</forename><surname>Cesa-Bianchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Fischer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="235" to="256" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Introduction to probability</title>
		<author>
			<persName><forename type="first">P</forename><surname>Dimitri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">N</forename><surname>Bertsekas</surname></persName>
		</author>
		<author>
			<persName><surname>Tsitsiklis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
			<publisher>Athena Scientific</publisher>
			<biblScope unit="volume">1</biblScope>
			<pubPlace>Belmont, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Sic -mmab: Synchronisation involves communication in multiplayer multi-armed bandits</title>
		<author>
			<persName><forename type="first">Etienne</forename><surname>Boursier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vianney</forename><surname>Perchet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A practical algorithm for multiplayer bandits when arm means vary among players</title>
		<author>
			<persName><forename type="first">Etienne</forename><surname>Boursier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emilie</forename><surname>Kaufmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abbas</forename><surname>Mehrabian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vianney</forename><surname>Perchet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Nearly optimal adaptive procedure with change detection for piecewise-stationary bandit</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Branislav</forename><surname>Kveton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yao</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Private and continual release of statistics</title>
		<author>
			<persName><forename type="first">T.-H</forename><surname>Hubert Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elaine</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Information Systems Security</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="26" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Federated asymptotics: a model to compare federated learning algorithms</title>
		<author>
			<persName><forename type="first">Gary</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karan</forename><surname>Chadha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Duchi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.07313</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The Algorithmic Foundations of Differential Privacy</title>
		<author>
			<persName><forename type="first">Cynthia</forename><surname>Dwork</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends® in Theoretical Computer Science</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="211" to="407" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Differential privacy under continual observation</title>
		<author>
			<persName><forename type="first">Cynthia</forename><surname>Dwork</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moni</forename><surname>Naor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Toniann</forename><surname>Pitassi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guy</forename><forename type="middle">N</forename><surname>Rothblum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">STOC</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Personalized federated learning: A meta-learning approach</title>
		<author>
			<persName><forename type="first">Alireza</forename><surname>Fallah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aryan</forename><surname>Mokhtari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Asuman</forename><surname>Ozdaglar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">On upper-confidence bound policies for switching bandit problems</title>
		<author>
			<persName><forename type="first">Aurélien</forename><surname>Garivier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Moulines</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ALT</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Lower bounds and optimal algorithms for personalized federated learning</title>
		<author>
			<persName><forename type="first">Filip</forename><surname>Hanzely</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Slavomír</forename><surname>Hanzely</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Horváth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Richtárik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Distributed exploration in multi-armed bandits</title>
		<author>
			<persName><forename type="first">Eshcar</forename><surname>Hillel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zohar</forename><surname>Shay Karnin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomer</forename><surname>Koren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronny</forename><surname>Lempel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oren</forename><surname>Somekh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Gossipbased peer sampling</title>
		<author>
			<persName><forename type="first">Márk</forename><surname>Jelasity</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Spyros</forename><surname>Voulgaris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rachid</forename><surname>Guerraoui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anne-Marie</forename><surname>Kermarrec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Van Steen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Computer Systems (TOCS)</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">8</biblScope>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Advances and open problems in federated learning</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">Brendan</forename><surname>Peter Kairouz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brendan</forename><surname>Mcmahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aurélien</forename><surname>Avent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehdi</forename><surname>Bellet</surname></persName>
		</author>
		<author>
			<persName><surname>Bennis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nitin</forename><surname>Arjun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kallista</forename><surname>Bhagoji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Bonawitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rachel</forename><surname>Cormode</surname></persName>
		</author>
		<author>
			<persName><surname>Cummings</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">L</forename><surname>Rafael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hubert</forename><surname>D'oliveira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Salim</forename><forename type="middle">El</forename><surname>Eichner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Rouayheb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josh</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrià</forename><surname>Garrett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Badih</forename><surname>Gascón</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><forename type="middle">B</forename><surname>Ghazi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Gibbons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zaid</forename><surname>Gruteser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chaoyang</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lie</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhouyuan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Huo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Justin</forename><surname>Hutchinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tara</forename><surname>Jaggi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gauri</forename><surname>Javidi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikhail</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakub</forename><surname>Khodak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandra</forename><surname>Konecný</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Farinaz</forename><surname>Korolova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanmi</forename><surname>Koushanfar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tancrède</forename><surname>Koyejo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Lepoint</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prateek</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehryar</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Mohri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ayfer</forename><surname>Nock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rasmus</forename><surname>Özgür</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Pagh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ramesh</forename><surname>Ramage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mariana</forename><surname>Raskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawn</forename><surname>Raykova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weikang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><forename type="middle">U</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziteng</forename><surname>Stich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ananda</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florian</forename><surname>Theertha Suresh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Praneeth</forename><surname>Tramèr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianyu</forename><surname>Vepakomma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><forename type="middle">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sen</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends® in Machine Learning</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="1" to="210" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">Nikolai</forename><surname>Karpov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qin</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2207.08015</idno>
		<title level="m">Collaborative best arm identification with limited communication on non-iid data</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Best arm identification in spectral bandits</title>
		<author>
			<persName><forename type="first">Tomáš</forename><surname>Kocák</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aurélien</forename><surname>Garivier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Epsilon best arm identification in spectral bandits</title>
		<author>
			<persName><forename type="first">Tomáš</forename><surname>Kocák</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aurélien</forename><surname>Garivier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Distributed cooperative decision making in multi-agent multi-armed bandits</title>
		<author>
			<persName><forename type="first">Vaibhav</forename><surname>Peter Landgren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naomi</forename><forename type="middle">Ehrich</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><surname>Leonard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Automatica</title>
		<imprint>
			<biblScope unit="volume">125</biblScope>
			<biblScope unit="page">109445</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">One more step towards reality: Cooperative bandits with imperfect communication</title>
		<author>
			<persName><forename type="first">Udari</forename><surname>Madhushani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhimanyu</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naomi</forename><forename type="middle">Ehrich</forename><surname>Leonard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Pentland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Mathematics of statistical sequential decision making</title>
		<author>
			<persName><forename type="first">Odalric-Ambrym</forename><surname>Maillard</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
		<respStmt>
			<orgName>Université de Lille, Sciences et Technologies</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Federated Multi-Task Learning under a Mixture of Distributions</title>
		<author>
			<persName><forename type="first">Othmane</forename><surname>Marfoq</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giovanni</forename><surname>Neglia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aurélien</forename><surname>Bellet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laetitia</forename><surname>Kameni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Vidal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Decentralized cooperative stochastic bandits</title>
		<author>
			<persName><forename type="first">David</forename><surname>Martínez-Rubio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Varun</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Rebeschini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NIPS</title>
		<meeting>of NIPS</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Machine learning methods to forecast temperature in buildings</title>
		<author>
			<persName><forename type="first">Fernando</forename><surname>Mateo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juan</forename><forename type="middle">José</forename><surname>Carrasco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abderrahim</forename><surname>Sellami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mónica</forename><surname>Millán-Giraldo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manuel</forename><surname>Domínguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emilio</forename><surname>Soria-Olivas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Systems with Applications</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1061" to="1068" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Near-optimal collaborative learning in bandits</title>
		<author>
			<persName><forename type="first">Clémence</forename><surname>Réda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sattar</forename><surname>Vakili</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emilie</forename><surname>Kaufmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Social learning in multi agent multi armed bandits</title>
		<author>
			<persName><forename type="first">Abishek</forename><surname>Sankararaman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ayalvadi</forename><surname>Ganesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjay</forename><surname>Shakkottai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM on Measurement and Analysis of Computing Systems</title>
		<meeting>the ACM on Measurement and Analysis of Computing Systems</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Clustered federated learning: Model-agnostic distributed multitask optimization under privacy constraints</title>
		<author>
			<persName><forename type="first">Felix</forename><surname>Sattler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Klaus-Robert</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Samek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Federated multi-armed bandits</title>
		<author>
			<persName><forename type="first">Chengshuai</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cong</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Federated multi-armed bandits with personalization</title>
		<author>
			<persName><forename type="first">Chengshuai</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AIS-TATS</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Federated multi-task learning</title>
		<author>
			<persName><forename type="first">Virginia</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao-Kai</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maziar</forename><surname>Sanjabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ameet</forename><surname>Talwalkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A survey of collaborative filtering techniques</title>
		<author>
			<persName><forename type="first">Xiaoyuan</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taghi</forename><forename type="middle">M</forename><surname>Khoshgoftaar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A survey of collaborative filtering techniques</title>
		<author>
			<persName><forename type="first">Xiaoyuan</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taghi</forename><forename type="middle">M</forename><surname>Khoshgoftaar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A practical bias estimation algorithm for multisensor-multitarget tracking</title>
		<author>
			<persName><forename type="first">Ehsan</forename><surname>Taghavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ratnasingham</forename><surname>Tharmarasa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thia</forename><surname>Kirubarajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaakov</forename><surname>Bar-Shalom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Mcdonald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Aerospace and Electronic Systems</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2" to="19" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Collaborative learning with limited interaction: Tight bounds for distributed exploration in multi-armed bandits</title>
		<author>
			<persName><forename type="first">Chao</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FOCS</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Decentralized collaborative learning of personalized models over networks</title>
		<author>
			<persName><forename type="first">Paul</forename><surname>Vanhaesebrouck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aurélien</forename><surname>Bellet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Tommasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of AISTATS</title>
		<meeting>of AISTATS</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Optimal algorithms for multiplayer multi-armed bandits</title>
		<author>
			<persName><forename type="first">Po-An</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Proutiere</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaito</forename><surname>Ariu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yassir</forename><surname>Jedra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessio</forename><surname>Russo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<biblScope unit="page" from="4120" to="4129" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title />
		<author>
			<persName><surname>Pmlr</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Distributed bandit learning: Near-optimal regret with efficient communication</title>
		<author>
			<persName><forename type="first">Yuanhao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiachen</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">All of statistics: a concise course in statistical inference</title>
		<author>
			<persName><forename type="first">Larry</forename><surname>Wasserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</teiCorpus></text>
</TEI>